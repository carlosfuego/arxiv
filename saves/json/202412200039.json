[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.16179v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v4",
                "updated": "2024-12-18T17:36:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    36,
                    36,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13779v1",
                "updated": "2024-12-18T12:16:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:16:41Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "title": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization"
                },
                "summary": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Yuying Wang"
                    },
                    {
                        "name": "Tianzhe Xiao"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Yining Qi"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13771v1",
                "updated": "2024-12-18T12:07:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:07:58Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "title": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization"
                },
                "summary": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems."
                },
                "authors": [
                    {
                        "name": "Guanghan Li"
                    },
                    {
                        "name": "Xun Zhang"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Yifan Yin"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "arxiv_comment": "7 pages, 3 figures, AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v2",
                "updated": "2024-12-18T09:47:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    47,
                    25,
                    2,
                    353,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13649v1",
                "updated": "2024-12-18T09:27:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T09:27:33Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation"
                },
                "summary": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods."
                },
                "authors": [
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Zhenglin Wang"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Yilong Lai"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08584v2",
                "updated": "2024-12-18T07:45:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    7,
                    45,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-11T07:24:21Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification"
                },
                "summary": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs."
                },
                "authors": [
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Hong Zhou"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13509v1",
                "updated": "2024-12-18T05:16:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T05:16:11Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "title": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation"
                },
                "summary": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization."
                },
                "authors": [
                    {
                        "name": "Yunqi Guo"
                    },
                    {
                        "name": "Kaiyuan Hou"
                    },
                    {
                        "name": "Heming Fu"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Jiang"
                },
                "author": "Xiaofan Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12486v2",
                "updated": "2024-12-18T05:08:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    8,
                    39,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-17T02:43:54Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    43,
                    54,
                    1,
                    352,
                    0
                ],
                "title": "Boosting Long-Context Management via Query-Guided Activation Refilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Long-Context Management via Query-Guided Activation Refilling"
                },
                "summary": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Defu Lian"
                    }
                ],
                "author_detail": {
                    "name": "Defu Lian"
                },
                "author": "Defu Lian",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v2",
                "updated": "2024-12-17T20:41:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    20,
                    41,
                    59,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v3",
                "updated": "2024-12-17T14:45:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    45,
                    12,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Code is available at https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12953v1",
                "updated": "2024-12-17T14:34:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T14:34:51Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "title": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning"
                },
                "summary": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/."
                },
                "authors": [
                    {
                        "name": "Moritz Reuss"
                    },
                    {
                        "name": "Jyothish Pari"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Rudolf Lioutikov"
                    }
                ],
                "author_detail": {
                    "name": "Rudolf Lioutikov"
                },
                "author": "Rudolf Lioutikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12798v1",
                "updated": "2024-12-17T11:00:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T11:00:56Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "title": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation"
                },
                "summary": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI."
                },
                "authors": [
                    {
                        "name": "Shiqi Huang"
                    },
                    {
                        "name": "Shuting He"
                    },
                    {
                        "name": "Bihan Wen"
                    }
                ],
                "author_detail": {
                    "name": "Bihan Wen"
                },
                "author": "Bihan Wen",
                "arxiv_comment": "AAAI 2025, code see https://github.com/HuangShiqi128/ZoRI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12706v1",
                "updated": "2024-12-17T09:20:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T09:20:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression"
                },
                "summary": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture."
                },
                "authors": [
                    {
                        "name": "Jiebin Zhang"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Chuqiao Kuang"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "13pages,7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v2",
                "updated": "2024-12-17T09:11:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    11,
                    47,
                    1,
                    352,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08585v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08585v3",
                "updated": "2024-12-17T05:40:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    40,
                    9,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-11T18:03:05Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    3,
                    5,
                    2,
                    346,
                    0
                ],
                "title": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs"
                },
                "summary": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "James Hensman"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Victor Ruhle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08585v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08585v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12543v1",
                "updated": "2024-12-17T05:09:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    9,
                    45,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T05:09:45Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    9,
                    45,
                    1,
                    352,
                    0
                ],
                "title": "Personalized Federated Deep Reinforcement Learning for Heterogeneous\n  Edge Content Caching Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Federated Deep Reinforcement Learning for Heterogeneous\n  Edge Content Caching Networks"
                },
                "summary": "Proactive caching is essential for minimizing latency and improving Quality\nof Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement\nLearning (FDRL) is a promising approach for developing cache policies tailored\nto dynamic content requests. However, FDRL faces challenges such as an\nexpanding caching action space due to increased content numbers and difficulty\nin adapting global information to heterogeneous edge environments. In this\npaper, we propose a Personalized Federated Deep Reinforcement Learning\nframework for Caching, called PF-DRL-Ca, with the aim to maximize system\nutility while satisfying caching capability constraints. To manage the\nexpanding action space, we employ a new DRL algorithm, Multi-head Deep\nQ-Network (MH-DQN), which reshapes the action output layers of DQN into a\nmulti-head structure where each head generates a sub-dimensional action. We\nnext integrate the proposed MH-DQN into a personalized federated training\nframework, employing a layer-wise approach for training to derive a\npersonalized model that can adapt to heterogeneous environments while\nexploiting the global information to accelerate learning convergence. Our\nextensive experimental results demonstrate the superiority of MH-DQN over\ntraditional DRL algorithms on a single server, as well as the advantages of the\npersonal federated training architecture compared to other frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proactive caching is essential for minimizing latency and improving Quality\nof Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement\nLearning (FDRL) is a promising approach for developing cache policies tailored\nto dynamic content requests. However, FDRL faces challenges such as an\nexpanding caching action space due to increased content numbers and difficulty\nin adapting global information to heterogeneous edge environments. In this\npaper, we propose a Personalized Federated Deep Reinforcement Learning\nframework for Caching, called PF-DRL-Ca, with the aim to maximize system\nutility while satisfying caching capability constraints. To manage the\nexpanding action space, we employ a new DRL algorithm, Multi-head Deep\nQ-Network (MH-DQN), which reshapes the action output layers of DQN into a\nmulti-head structure where each head generates a sub-dimensional action. We\nnext integrate the proposed MH-DQN into a personalized federated training\nframework, employing a layer-wise approach for training to derive a\npersonalized model that can adapt to heterogeneous environments while\nexploiting the global information to accelerate learning convergence. Our\nextensive experimental results demonstrate the superiority of MH-DQN over\ntraditional DRL algorithms on a single server, as well as the advantages of the\npersonal federated training architecture compared to other frameworks."
                },
                "authors": [
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Tan Li"
                    },
                    {
                        "name": "Hai Liu"
                    },
                    {
                        "name": "Tse-Tin Chan"
                    }
                ],
                "author_detail": {
                    "name": "Tse-Tin Chan"
                },
                "author": "Tse-Tin Chan",
                "arxiv_comment": "8 pages, 8 figures, WiOpt 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12488v1",
                "updated": "2024-12-17T02:44:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    44,
                    43,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T02:44:43Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    44,
                    43,
                    1,
                    352,
                    0
                ],
                "title": "A System for Microserving of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A System for Microserving of LLMs"
                },
                "summary": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies."
                },
                "authors": [
                    {
                        "name": "Hongyi Jin"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Charlie F. Ruan"
                    },
                    {
                        "name": "Yingcheng Wang"
                    },
                    {
                        "name": "Todd C. Mowry"
                    },
                    {
                        "name": "Xupeng Miao"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Tianqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianqi Chen"
                },
                "author": "Tianqi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12444v1",
                "updated": "2024-12-17T01:12:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T01:12:35Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "title": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers"
                },
                "summary": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency."
                },
                "authors": [
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yanyu Li"
                    },
                    {
                        "name": "Yifan Gong"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Henghui Ding"
                    },
                    {
                        "name": "Zhihao Shu"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiuxiang Gu"
                },
                "author": "Jiuxiang Gu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11828v1",
                "updated": "2024-12-16T14:49:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T14:49:32Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "title": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey"
                },
                "summary": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field."
                },
                "authors": [
                    {
                        "name": "Sergey Zinchenko"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11741v1",
                "updated": "2024-12-16T13:01:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T13:01:53Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "title": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation"
                },
                "summary": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments."
                },
                "authors": [
                    {
                        "name": "Hongxuan Zhang"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v1",
                "updated": "2024-12-16T12:28:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11685v1",
                "updated": "2024-12-16T11:55:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T11:55:26Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "title": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning"
                },
                "summary": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU."
                },
                "authors": [
                    {
                        "name": "Xingchi Chen"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    },
                    {
                        "name": "Xuerui Li"
                    },
                    {
                        "name": "Yuying Chen"
                    },
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Wenqi Ren"
                    }
                ],
                "author_detail": {
                    "name": "Wenqi Ren"
                },
                "author": "Wenqi Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.02388v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.02388v3",
                "updated": "2024-12-15T03:29:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    3,
                    29,
                    54,
                    6,
                    350,
                    0
                ],
                "published": "2023-05-03T19:07:06Z",
                "published_parsed": [
                    2023,
                    5,
                    3,
                    19,
                    7,
                    6,
                    2,
                    123,
                    0
                ],
                "title": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)"
                },
                "summary": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone."
                },
                "authors": [
                    {
                        "name": "Yupeng Tang"
                    },
                    {
                        "name": "Seung-seob Lee"
                    },
                    {
                        "name": "Abhishek Bhattacharjee"
                    },
                    {
                        "name": "Anurag Khandelwal"
                    }
                ],
                "author_detail": {
                    "name": "Anurag Khandelwal"
                },
                "author": "Anurag Khandelwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.02388v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.02388v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11021v1",
                "updated": "2024-12-15T02:30:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "published": "2024-12-15T02:30:09Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "title": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array"
                },
                "summary": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works."
                },
                "authors": [
                    {
                        "name": "Xiaobing Ni"
                    },
                    {
                        "name": "Mengke Ge"
                    },
                    {
                        "name": "Jiaheng Ruan"
                    },
                    {
                        "name": "Song Chen"
                    },
                    {
                        "name": "Yi Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Kang"
                },
                "author": "Yi Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10685v1",
                "updated": "2024-12-14T05:20:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "published": "2024-12-14T05:20:50Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "title": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs"
                },
                "summary": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm."
                },
                "authors": [
                    {
                        "name": "Baljinder Singh Heera"
                    },
                    {
                        "name": "Shrinivas Petale"
                    },
                    {
                        "name": "Yatindra Nath Singh"
                    },
                    {
                        "name": "Suresh Subramaniam"
                    }
                ],
                "author_detail": {
                    "name": "Suresh Subramaniam"
                },
                "author": "Suresh Subramaniam",
                "arxiv_comment": "The preliminary work was presented at ONDM 2023 conference.\n  https://doi.org/10.23919/ONDM57372.2023.10144866",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10319v1",
                "updated": "2024-12-13T17:59:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:59:52Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods"
                },
                "summary": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v2",
                "updated": "2024-12-13T17:53:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    53,
                    25,
                    4,
                    348,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Efficient Multi-LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Efficient Multi-LLM Serving"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10302v1",
                "updated": "2024-12-13T17:37:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:37:48Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "title": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding"
                },
                "summary": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2."
                },
                "authors": [
                    {
                        "name": "Zhiyu Wu"
                    },
                    {
                        "name": "Xiaokang Chen"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Xingchao Liu"
                    },
                    {
                        "name": "Wen Liu"
                    },
                    {
                        "name": "Damai Dai"
                    },
                    {
                        "name": "Huazuo Gao"
                    },
                    {
                        "name": "Yiyang Ma"
                    },
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Bingxuan Wang"
                    },
                    {
                        "name": "Zhenda Xie"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Kai Hu"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Yaofeng Sun"
                    },
                    {
                        "name": "Yukun Li"
                    },
                    {
                        "name": "Yishi Piao"
                    },
                    {
                        "name": "Kang Guan"
                    },
                    {
                        "name": "Aixin Liu"
                    },
                    {
                        "name": "Xin Xie"
                    },
                    {
                        "name": "Yuxiang You"
                    },
                    {
                        "name": "Kai Dong"
                    },
                    {
                        "name": "Xingkai Yu"
                    },
                    {
                        "name": "Haowei Zhang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Yisong Wang"
                    },
                    {
                        "name": "Chong Ruan"
                    }
                ],
                "author_detail": {
                    "name": "Chong Ruan"
                },
                "author": "Chong Ruan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18566v2",
                "updated": "2024-12-13T16:13:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    13,
                    39,
                    4,
                    348,
                    0
                ],
                "published": "2024-11-27T18:09:29Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "title": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software"
                },
                "summary": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software."
                },
                "authors": [
                    {
                        "name": "Oliver Maximilian Zobel"
                    },
                    {
                        "name": "Johannes Maierhofer"
                    },
                    {
                        "name": "Andreas Kstler"
                    },
                    {
                        "name": "Daniel J. Rixen"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J. Rixen"
                },
                "author": "Daniel J. Rixen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10153v1",
                "updated": "2024-12-13T14:11:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T14:11:42Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector"
                },
                "summary": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhang"
                    },
                    {
                        "name": "Shuzhao Xie"
                    },
                    {
                        "name": "Chengwei Ren"
                    },
                    {
                        "name": "Siyi Xie"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shijia Ge"
                    },
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12021v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12021v2",
                "updated": "2024-12-13T14:08:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    8,
                    55,
                    4,
                    348,
                    0
                ],
                "published": "2024-09-18T14:31:33Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "title": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues"
                },
                "summary": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized)."
                },
                "authors": [
                    {
                        "name": "Thore Thieen"
                    },
                    {
                        "name": "Jan Vahrenhold"
                    }
                ],
                "author_detail": {
                    "name": "Jan Vahrenhold"
                },
                "author": "Jan Vahrenhold",
                "arxiv_doi": "10.4230/LIPIcs.ISAAC.2024.55",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4230/LIPIcs.ISAAC.2024.55",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.12021v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12021v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, full version of the paper in ISAAC 2024; minor changes",
                "arxiv_journal_ref": "Thore Thie{\\ss}en and Jan Vahrenhold. Optimal offline ORAM with\n  perfect security via simple oblivious priority queues. In 35th International\n  Symposium on Algorithms and Computation (ISAAC 2024), 18 pages. 2024",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12178v1",
                "updated": "2024-12-13T02:26:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T02:26:54Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "title": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models"
                },
                "summary": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize compression\nrate while maintaining great accuracy. LLMs' Feed-Forward Network (FFN)\ncomponents, which typically comprise a large proportion of parameters (around\n3/2), ensure that our FFN optimizations would have a better chance of achieving\neffective compression. Moreover, our findings are beneficial to general LLMs\nand are not restricted to ReLU-based models. This work systematically\ninvestigates the tradeoff between enforcing activation sparsity and perplexity\n(accuracy) on state-of-the-art LLMs. Our empirical analysis demonstrates that\nwe can obtain around 50% of main memory and computing reductions for critical\nFFN components with negligible accuracy degradation. This extra 50% sparsity\ndoes not naturally exist in the current LLMs, which require tuning LLMs'\nactivation outputs by injecting zero-enforcing thresholds. To obtain the\nbenefits of activation sparsity, we provide a guideline for the system\narchitect for LLM prediction and prefetching. The success prediction allows the\nsystem to prefetch the necessary weights while omitting the inactive ones and\ntheir successors, therefore lowering cache and memory pollution and reducing\nLLM execution time on resource-constrained edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize compression\nrate while maintaining great accuracy. LLMs' Feed-Forward Network (FFN)\ncomponents, which typically comprise a large proportion of parameters (around\n3/2), ensure that our FFN optimizations would have a better chance of achieving\neffective compression. Moreover, our findings are beneficial to general LLMs\nand are not restricted to ReLU-based models. This work systematically\ninvestigates the tradeoff between enforcing activation sparsity and perplexity\n(accuracy) on state-of-the-art LLMs. Our empirical analysis demonstrates that\nwe can obtain around 50% of main memory and computing reductions for critical\nFFN components with negligible accuracy degradation. This extra 50% sparsity\ndoes not naturally exist in the current LLMs, which require tuning LLMs'\nactivation outputs by injecting zero-enforcing thresholds. To obtain the\nbenefits of activation sparsity, we provide a guideline for the system\narchitect for LLM prediction and prefetching. The success prediction allows the\nsystem to prefetch the necessary weights while omitting the inactive ones and\ntheir successors, therefore lowering cache and memory pollution and reducing\nLLM execution time on resource-constrained edge devices."
                },
                "authors": [
                    {
                        "name": "Nobel Dhar"
                    },
                    {
                        "name": "Bobin Deng"
                    },
                    {
                        "name": "Md Romyull Islam"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Kun Suo"
                    }
                ],
                "author_detail": {
                    "name": "Kun Suo"
                },
                "author": "Kun Suo",
                "arxiv_comment": "Conference submission for IPCCC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09474v1",
                "updated": "2024-12-12T17:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    20,
                    26,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T17:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    20,
                    26,
                    3,
                    347,
                    0
                ],
                "title": "Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for\n  Edge and Distributed Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for\n  Edge and Distributed Performance"
                },
                "summary": "A Content Delivery Network (CDN) is a powerful system of distributed caching\nservers that aims to accelerate content delivery, like high-definition video,\nIoT applications, and ultra-low-latency services, efficiently and with fast\nvelocity. This has become of paramount importance in the post-pandemic era.\nChallenges arise when exponential content volume growth and scalability across\ndifferent geographic locations are required. This paper investigates\ndata-driven evaluations of CDN algorithms in dynamic server selection for\nlatency reduction, bandwidth throttling for efficient resource management,\nreal-time Round Trip Time analysis for adaptive routing, and programmatic\nnetwork delay simulation to emulate various conditions. Key performance\nmetrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to\nevaluate scalability and algorithmic efficiency through two experimental\nsetups: a constrained edge-like local system and a scalable FABRIC testbed. The\nstatistical validation of RTT trends, alongside CPU utilization, is presented\nin the results. The optimization process reveals significant trade-offs between\nscalability and resource consumption, providing actionable insights for\neffectively deploying and enhancing CDN algorithms in edge and distributed\ncomputing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Content Delivery Network (CDN) is a powerful system of distributed caching\nservers that aims to accelerate content delivery, like high-definition video,\nIoT applications, and ultra-low-latency services, efficiently and with fast\nvelocity. This has become of paramount importance in the post-pandemic era.\nChallenges arise when exponential content volume growth and scalability across\ndifferent geographic locations are required. This paper investigates\ndata-driven evaluations of CDN algorithms in dynamic server selection for\nlatency reduction, bandwidth throttling for efficient resource management,\nreal-time Round Trip Time analysis for adaptive routing, and programmatic\nnetwork delay simulation to emulate various conditions. Key performance\nmetrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to\nevaluate scalability and algorithmic efficiency through two experimental\nsetups: a constrained edge-like local system and a scalable FABRIC testbed. The\nstatistical validation of RTT trends, alongside CPU utilization, is presented\nin the results. The optimization process reveals significant trade-offs between\nscalability and resource consumption, providing actionable insights for\neffectively deploying and enhancing CDN algorithms in edge and distributed\ncomputing environments."
                },
                "authors": [
                    {
                        "name": "Md Nurul Absur"
                    },
                    {
                        "name": "Sourya Saha"
                    },
                    {
                        "name": "Sifat Nawrin Nova"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Md Rahat Ul Nasib"
                    }
                ],
                "author_detail": {
                    "name": "Md Rahat Ul Nasib"
                },
                "author": "Md Rahat Ul Nasib",
                "arxiv_comment": "6 Pages, 10 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09416v1",
                "updated": "2024-12-12T16:24:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T16:24:35Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "title": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors"
                },
                "summary": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v3",
                "updated": "2024-12-12T15:39:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    39,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08760v2",
                "updated": "2024-12-12T14:43:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    14,
                    43,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-10-11T12:19:18Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "title": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation"
                },
                "summary": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL."
                },
                "authors": [
                    {
                        "name": "Konstantin Burlachenko"
                    },
                    {
                        "name": "Peter Richtrik"
                    }
                ],
                "author_detail": {
                    "name": "Peter Richtrik"
                },
                "author": "Peter Richtrik",
                "arxiv_comment": "55 pages, 12 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4; C.3; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06282v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06282v3",
                "updated": "2024-12-12T12:24:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    24,
                    18,
                    3,
                    347,
                    0
                ],
                "published": "2024-06-10T14:01:21Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    14,
                    1,
                    21,
                    0,
                    162,
                    0
                ],
                "title": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone"
                },
                "summary": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation."
                },
                "authors": [
                    {
                        "name": "Zhenliang Xue"
                    },
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Xinrui Zheng"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06282v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06282v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v3",
                "updated": "2024-12-12T12:03:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    3,
                    19,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01415v2",
                "updated": "2024-12-12T10:07:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    7,
                    17,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-02T11:57:03Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    57,
                    3,
                    0,
                    337,
                    0
                ],
                "title": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure"
                },
                "summary": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW."
                },
                "authors": [
                    {
                        "name": "A. B. Batrakov"
                    },
                    {
                        "name": "S. Yu. Karelin"
                    },
                    {
                        "name": "O. M. Lebedenko"
                    },
                    {
                        "name": "V. S. Mukhin"
                    },
                    {
                        "name": "I. N. Onishchenko"
                    },
                    {
                        "name": "O. L. Rak"
                    },
                    {
                        "name": "V. G. Sinitsin"
                    },
                    {
                        "name": "M. V. Volovenko"
                    }
                ],
                "author_detail": {
                    "name": "M. V. Volovenko"
                },
                "author": "M. V. Volovenko",
                "arxiv_comment": "4 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09057v1",
                "updated": "2024-12-12T08:33:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T08:33:39Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "title": "PhishIntel: Toward Practical Deployment of Reference-based Phishing\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhishIntel: Toward Practical Deployment of Reference-based Phishing\n  Detection"
                },
                "summary": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) achieve high precision by analyzing\nbrand-domain consistency, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) achieve high precision by analyzing\nbrand-domain consistency, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility."
                },
                "authors": [
                    {
                        "name": "Yuexin Li"
                    },
                    {
                        "name": "Hiok Kuek Tan"
                    },
                    {
                        "name": "Qiaoran Meng"
                    },
                    {
                        "name": "Mei Lin Lock"
                    },
                    {
                        "name": "Tri Cao"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Nay Oo"
                    },
                    {
                        "name": "Hoon Wei Lim"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09036v1",
                "updated": "2024-12-12T07:52:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    52,
                    56,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T07:52:56Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    52,
                    56,
                    3,
                    347,
                    0
                ],
                "title": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based\n  on Layer Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based\n  on Layer Uncertainty"
                },
                "summary": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance."
                },
                "authors": [
                    {
                        "name": "Meizhi Zhong"
                    },
                    {
                        "name": "Xikai Liu"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yikun Lei"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v3",
                "updated": "2024-12-12T03:21:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    21,
                    13,
                    3,
                    347,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Forecasting GPU Performance for Deep Learning Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting GPU Performance for Deep Learning Training and Inference"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 121.4% and 30.8% to 2.3% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior work, where both GPT3 and H100 were not used\nto train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 121.4% and 30.8% to 2.3% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior work, where both GPT3 and H100 were not used\nto train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "arxiv_doi": "10.1145/3669940.3707265",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3669940.3707265",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.13853v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS), 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08890v1",
                "updated": "2024-12-12T03:00:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    0,
                    29,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T03:00:29Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    0,
                    29,
                    3,
                    347,
                    0
                ],
                "title": "Lexico: Extreme KV Cache Compression via Sparse Coding over Universal\n  Dictionaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lexico: Extreme KV Cache Compression via Sparse Coding over Universal\n  Dictionaries"
                },
                "summary": "We introduce Lexico, a novel KV cache compression method that leverages\nsparse coding with a universal dictionary. Our key finding is that key-value\ncache in modern LLMs can be accurately approximated using sparse linear\ncombination from a small, input-agnostic dictionary of ~4k atoms, enabling\nefficient compression across different input prompts, tasks and models. Using\northogonal matching pursuit for sparse approximation, Lexico achieves flexible\ncompression ratios through direct sparsity control. On GSM8K, across multiple\nmodel families (Mistral, Llama 3, Qwen2.5), Lexico maintains 90-95% of the\noriginal performance while using only 15-25% of the full KV-cache memory,\noutperforming both quantization and token eviction methods. Notably, Lexico\nremains effective in low memory regimes where 2-bit quantization fails,\nachieving up to 1.7x better compression on LongBench and GSM8K while\nmaintaining high accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Lexico, a novel KV cache compression method that leverages\nsparse coding with a universal dictionary. Our key finding is that key-value\ncache in modern LLMs can be accurately approximated using sparse linear\ncombination from a small, input-agnostic dictionary of ~4k atoms, enabling\nefficient compression across different input prompts, tasks and models. Using\northogonal matching pursuit for sparse approximation, Lexico achieves flexible\ncompression ratios through direct sparsity control. On GSM8K, across multiple\nmodel families (Mistral, Llama 3, Qwen2.5), Lexico maintains 90-95% of the\noriginal performance while using only 15-25% of the full KV-cache memory,\noutperforming both quantization and token eviction methods. Notably, Lexico\nremains effective in low memory regimes where 2-bit quantization fails,\nachieving up to 1.7x better compression on LongBench and GSM8K while\nmaintaining high accuracy."
                },
                "authors": [
                    {
                        "name": "Junhyuck Kim"
                    },
                    {
                        "name": "Jongho Park"
                    },
                    {
                        "name": "Jaewoong Cho"
                    },
                    {
                        "name": "Dimitris Papailiopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Papailiopoulos"
                },
                "author": "Dimitris Papailiopoulos",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08521v1",
                "updated": "2024-12-11T16:35:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T16:35:13Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "title": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance"
                },
                "summary": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task."
                },
                "authors": [
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Xinzhu Ma"
                    },
                    {
                        "name": "Zihan Geng"
                    },
                    {
                        "name": "Shutao Xia"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v3",
                "updated": "2024-12-11T12:03:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    12,
                    3,
                    40,
                    2,
                    346,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Pushing the Limits of In-Network Caching for Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing the Limits of In-Network Caching for Key-Value Stores"
                },
                "summary": "We present OrbitCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, OrbitCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement an OrbitCache prototype on an Intel Tofino\nswitch. Our experimental results show that OrbitCache can balance highly skewed\nworkloads and is robust to various system conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present OrbitCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, OrbitCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement an OrbitCache prototype on an Intel Tofino\nswitch. Our experimental results show that OrbitCache can balance highly skewed\nworkloads and is robust to various system conditions."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "arxiv_comment": "To be appeared in USENIX NSDI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08176v1",
                "updated": "2024-12-11T08:07:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    7,
                    12,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T08:07:12Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    7,
                    12,
                    2,
                    346,
                    0
                ],
                "title": "TextRefiner: Internal Visual Feature as Efficient Refiner for\n  Vision-Language Models Prompt Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextRefiner: Internal Visual Feature as Efficient Refiner for\n  Vision-Language Models Prompt Tuning"
                },
                "summary": "Despite the efficiency of prompt learning in transferring vision-language\nmodels (VLMs) to downstream tasks, existing methods mainly learn the prompts in\na coarse-grained manner where the learned prompt vectors are shared across all\ncategories. Consequently, the tailored prompts often fail to discern\nclass-specific visual concepts, thereby hindering the transferred performance\nfor classes that share similar or complex visual attributes. Recent advances\nmitigate this challenge by leveraging external knowledge from Large Language\nModels (LLMs) to furnish class descriptions, yet incurring notable inference\ncosts. In this paper, we introduce TextRefiner, a plug-and-play method to\nrefine the text prompts of existing methods by leveraging the internal\nknowledge of VLMs. Particularly, TextRefiner builds a novel local cache module\nto encapsulate fine-grained visual concepts derivedfrom local tokens within the\nimage branch. By aggregating and aligning the cached visual descriptions with\nthe original output of the text branch, TextRefiner can efficiently refine and\nenrich the learned prompts from existing methods without relying on any\nexternal expertise. For example, it improves the performance of CoOp from 71.66\n% to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise\nfeatures for text prompts. Equipped with TextRefiner, PromptKD achieves\nstate-of-the-art performance and is efficient in inference. Our code is relesed\nat https://github.com/xjjxmu/TextRefiner",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the efficiency of prompt learning in transferring vision-language\nmodels (VLMs) to downstream tasks, existing methods mainly learn the prompts in\na coarse-grained manner where the learned prompt vectors are shared across all\ncategories. Consequently, the tailored prompts often fail to discern\nclass-specific visual concepts, thereby hindering the transferred performance\nfor classes that share similar or complex visual attributes. Recent advances\nmitigate this challenge by leveraging external knowledge from Large Language\nModels (LLMs) to furnish class descriptions, yet incurring notable inference\ncosts. In this paper, we introduce TextRefiner, a plug-and-play method to\nrefine the text prompts of existing methods by leveraging the internal\nknowledge of VLMs. Particularly, TextRefiner builds a novel local cache module\nto encapsulate fine-grained visual concepts derivedfrom local tokens within the\nimage branch. By aggregating and aligning the cached visual descriptions with\nthe original output of the text branch, TextRefiner can efficiently refine and\nenrich the learned prompts from existing methods without relying on any\nexternal expertise. For example, it improves the performance of CoOp from 71.66\n% to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise\nfeatures for text prompts. Equipped with TextRefiner, PromptKD achieves\nstate-of-the-art performance and is efficient in inference. Our code is relesed\nat https://github.com/xjjxmu/TextRefiner"
                },
                "authors": [
                    {
                        "name": "Jingjing Xie"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Jun Peng"
                    },
                    {
                        "name": "Zhaohong Huang"
                    },
                    {
                        "name": "Liujuan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Liujuan Cao"
                },
                "author": "Liujuan Cao",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08063v1",
                "updated": "2024-12-11T03:15:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    15,
                    49,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T03:15:49Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    15,
                    49,
                    2,
                    346,
                    0
                ],
                "title": "ContextModule: Improving Code Completion via Repository-level Contextual\n  Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextModule: Improving Code Completion via Repository-level Contextual\n  Information"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode completion tasks, where they assist developers by predicting and\ngenerating new code in real-time. However, existing LLM-based code completion\nsystems primarily rely on the immediate context of the file being edited, often\nmissing valuable repository-level information, user behaviour and edit history\nthat could improve suggestion accuracy. Additionally, challenges such as\nefficiently retrieving relevant code snippets from large repositories,\nincorporating user behavior, and balancing accuracy with low-latency\nrequirements in production environments remain unresolved. In this paper, we\npropose ContextModule, a framework designed to enhance LLM-based code\ncompletion by retrieving and integrating three types of contextual information\nfrom the repository: user behavior-based code, similar code snippets, and\ncritical symbol definitions. By capturing user interactions across files and\nleveraging repository-wide static analysis, ContextModule improves the\nrelevance and precision of generated code. We implement performance\noptimizations, such as index caching, to ensure the system meets the latency\nconstraints of real-world coding environments. Experimental results and\nindustrial practise demonstrate that ContextModule significantly improves code\ncompletion accuracy and user acceptance rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode completion tasks, where they assist developers by predicting and\ngenerating new code in real-time. However, existing LLM-based code completion\nsystems primarily rely on the immediate context of the file being edited, often\nmissing valuable repository-level information, user behaviour and edit history\nthat could improve suggestion accuracy. Additionally, challenges such as\nefficiently retrieving relevant code snippets from large repositories,\nincorporating user behavior, and balancing accuracy with low-latency\nrequirements in production environments remain unresolved. In this paper, we\npropose ContextModule, a framework designed to enhance LLM-based code\ncompletion by retrieving and integrating three types of contextual information\nfrom the repository: user behavior-based code, similar code snippets, and\ncritical symbol definitions. By capturing user interactions across files and\nleveraging repository-wide static analysis, ContextModule improves the\nrelevance and precision of generated code. We implement performance\noptimizations, such as index caching, to ensure the system meets the latency\nconstraints of real-world coding environments. Experimental results and\nindustrial practise demonstrate that ContextModule significantly improves code\ncompletion accuracy and user acceptance rates."
                },
                "authors": [
                    {
                        "name": "Zhanming Guan"
                    },
                    {
                        "name": "Junlin Liu"
                    },
                    {
                        "name": "Jierui Liu"
                    },
                    {
                        "name": "Chao Peng"
                    },
                    {
                        "name": "Dexin Liu"
                    },
                    {
                        "name": "Ningyuan Sun"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Wenchao Li"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Hang Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Zhu"
                },
                "author": "Hang Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12952v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12952v2",
                "updated": "2024-12-10T22:53:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    22,
                    53,
                    16,
                    1,
                    345,
                    0
                ],
                "published": "2024-03-19T17:54:34Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    17,
                    54,
                    34,
                    1,
                    79,
                    0
                ],
                "title": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization\n  with Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization\n  with Vision-Language Models"
                },
                "summary": "Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 image classification datasets involving natural distribution shifts\nand cross-dataset generalization, as well as in context-dependent visual\nreasoning, demonstrate TPS's superior performance, achieving state-of-the-art\nresults while reducing resource requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 image classification datasets involving natural distribution shifts\nand cross-dataset generalization, as well as in context-dependent visual\nreasoning, demonstrate TPS's superior performance, achieving state-of-the-art\nresults while reducing resource requirements."
                },
                "authors": [
                    {
                        "name": "Elaine Sui"
                    },
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Serena Yeung-Levy"
                    }
                ],
                "author_detail": {
                    "name": "Serena Yeung-Levy"
                },
                "author": "Serena Yeung-Levy",
                "arxiv_comment": "Accepted at WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12952v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12952v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v1",
                "updated": "2024-12-10T18:59:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Causal Video Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Causal Video Generators"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to a causal\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nsupports fast streaming generation of high quality videos at 9.4 FPS on a\nsingle GPU thanks to KV caching. Our approach also enables streaming\nvideo-to-video translation, image-to-video, and dynamic prompting in a\nzero-shot manner. We will release the code based on an open-source model in the\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to a causal\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nsupports fast streaming generation of high quality videos at 9.4 FPS on a\nsingle GPU thanks to KV caching. Our approach also enables streaming\nvideo-to-video translation, image-to-video, and dynamic prompting in a\nzero-shot manner. We will release the code based on an open-source model in the\nfuture."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v1",
                "updated": "2024-12-10T18:50:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}"
                },
                "authors": [
                    {
                        "name": "Korbinian Pppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07720v1",
                "updated": "2024-12-10T18:13:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:13:20Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "title": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer"
                },
                "summary": "The recent surge of interest in comprehensive multimodal models has\nnecessitated the unification of diverse modalities. However, the unification\nsuffers from disparate methodologies. Continuous visual generation necessitates\nthe full-sequence diffusion-based approach, despite its divergence from the\nautoregressive modeling in the text domain. We posit that autoregressive\nmodeling, i.e., predicting the future based on past deterministic experience,\nremains crucial in developing both a visual generation model and a potential\nunified multimodal model. In this paper, we explore an interpolation between\nthe autoregressive modeling and full-parameters diffusion to model visual\ninformation. At its core, we present ACDiT, an Autoregressive blockwise\nConditional Diffusion Transformer, where the block size of diffusion, i.e., the\nsize of autoregressive units, can be flexibly adjusted to interpolate between\ntoken-wise autoregression and full-sequence diffusion. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) during\ntraining. During inference, the process iterates between diffusion denoising\nand autoregressive decoding that can make full use of KV-Cache. We verify the\neffectiveness of ACDiT on image and video generation tasks. We also demonstrate\nthat benefitted from autoregressive modeling, ACDiT can be seamlessly used in\nvisual understanding tasks despite being trained on the diffusion objective.\nThe analysis of the trade-off between autoregressive modeling and diffusion\ndemonstrates the potential of ACDiT to be used in long-horizon visual\ngeneration tasks. These strengths make it promising as the backbone of future\nunified models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent surge of interest in comprehensive multimodal models has\nnecessitated the unification of diverse modalities. However, the unification\nsuffers from disparate methodologies. Continuous visual generation necessitates\nthe full-sequence diffusion-based approach, despite its divergence from the\nautoregressive modeling in the text domain. We posit that autoregressive\nmodeling, i.e., predicting the future based on past deterministic experience,\nremains crucial in developing both a visual generation model and a potential\nunified multimodal model. In this paper, we explore an interpolation between\nthe autoregressive modeling and full-parameters diffusion to model visual\ninformation. At its core, we present ACDiT, an Autoregressive blockwise\nConditional Diffusion Transformer, where the block size of diffusion, i.e., the\nsize of autoregressive units, can be flexibly adjusted to interpolate between\ntoken-wise autoregression and full-sequence diffusion. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) during\ntraining. During inference, the process iterates between diffusion denoising\nand autoregressive decoding that can make full use of KV-Cache. We verify the\neffectiveness of ACDiT on image and video generation tasks. We also demonstrate\nthat benefitted from autoregressive modeling, ACDiT can be seamlessly used in\nvisual understanding tasks despite being trained on the diffusion objective.\nThe analysis of the trade-off between autoregressive modeling and diffusion\ndemonstrates the potential of ACDiT to be used in long-horizon visual\ngeneration tasks. These strengths make it promising as the backbone of future\nunified models."
                },
                "authors": [
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Yuxuan Song"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Mingxuan Wang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14485v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14485v4",
                "updated": "2024-12-10T12:45:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    12,
                    45,
                    31,
                    1,
                    345,
                    0
                ],
                "published": "2024-09-22T15:13:31Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    15,
                    13,
                    31,
                    6,
                    266,
                    0
                ],
                "title": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding"
                },
                "summary": "Long video understanding poses a significant challenge for current\nMulti-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained\nby their limited context lengths and the substantial costs while processing\nlong videos. Although several existing methods attempt to reduce visual tokens,\ntheir strategies encounter severe bottleneck, restricting MLLMs' ability to\nperceive fine-grained visual details. In this work, we propose Video-XL, a\nnovel approach that leverages MLLMs' inherent key-value (KV) sparsification\ncapacity to condense the visual input. Specifically, we introduce a new special\ntoken, the Visual Summarization Token (VST), for each interval of the video,\nwhich summarizes the visual information within the interval as its associated\nKV. The VST module is trained by instruction fine-tuning, where two optimizing\nstrategies are offered. 1.Curriculum learning, where VST learns to make small\n(easy) and large compression (hard) progressively. 2. Composite data curation,\nwhich integrates single-image, multi-image, and synthetic data to overcome the\nscarcity of long-video instruction data. The compression quality is further\nimproved by dynamic compression, which customizes compression granularity based\non the information density of different video intervals. Video-XL's\neffectiveness is verified from three aspects. First, it achieves a superior\nlong-video understanding capability, outperforming state-of-the-art models of\ncomparable sizes across multiple popular benchmarks. Second, it effectively\npreserves video information, with minimal compression loss even at 16x\ncompression ratio. Third, it realizes outstanding cost-effectiveness, enabling\nhigh-quality processing of thousands of frames on a single A100 GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long video understanding poses a significant challenge for current\nMulti-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained\nby their limited context lengths and the substantial costs while processing\nlong videos. Although several existing methods attempt to reduce visual tokens,\ntheir strategies encounter severe bottleneck, restricting MLLMs' ability to\nperceive fine-grained visual details. In this work, we propose Video-XL, a\nnovel approach that leverages MLLMs' inherent key-value (KV) sparsification\ncapacity to condense the visual input. Specifically, we introduce a new special\ntoken, the Visual Summarization Token (VST), for each interval of the video,\nwhich summarizes the visual information within the interval as its associated\nKV. The VST module is trained by instruction fine-tuning, where two optimizing\nstrategies are offered. 1.Curriculum learning, where VST learns to make small\n(easy) and large compression (hard) progressively. 2. Composite data curation,\nwhich integrates single-image, multi-image, and synthetic data to overcome the\nscarcity of long-video instruction data. The compression quality is further\nimproved by dynamic compression, which customizes compression granularity based\non the information density of different video intervals. Video-XL's\neffectiveness is verified from three aspects. First, it achieves a superior\nlong-video understanding capability, outperforming state-of-the-art models of\ncomparable sizes across multiple popular benchmarks. Second, it effectively\npreserves video information, with minimal compression loss even at 16x\ncompression ratio. Third, it realizes outstanding cost-effectiveness, enabling\nhigh-quality processing of thousands of frames on a single A100 GPU."
                },
                "authors": [
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Junjie Zhou"
                    },
                    {
                        "name": "Zhengyang Liang"
                    },
                    {
                        "name": "Tiejun Huang"
                    },
                    {
                        "name": "Bo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhao"
                },
                "author": "Bo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14485v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14485v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v3",
                "updated": "2024-12-09T01:44:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    44,
                    10,
                    0,
                    344,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01844v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01844v3",
                "updated": "2024-12-09T01:39:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    39,
                    15,
                    0,
                    344,
                    0
                ],
                "published": "2024-05-03T04:27:32Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    4,
                    27,
                    32,
                    4,
                    124,
                    0
                ],
                "title": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges"
                },
                "summary": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Shazia Riaz"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "arxiv_doi": "10.1145/3706630",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706630",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.01844v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01844v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05896v1",
                "updated": "2024-12-08T11:32:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    8,
                    11,
                    32,
                    8,
                    6,
                    343,
                    0
                ],
                "published": "2024-12-08T11:32:08Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    11,
                    32,
                    8,
                    6,
                    343,
                    0
                ],
                "title": "XKV: Personalized KV Cache Memory Reduction for Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XKV: Personalized KV Cache Memory Reduction for Long-Context LLM\n  Inference"
                },
                "summary": "Recently the generative Large Language Model (LLM) has achieved remarkable\nsuccess in numerous applications. Notably its inference generates output tokens\none-by-one, leading to many redundant computations. The widely-used KV-Cache\nframework makes a compromise between time and space complexities. However,\ncaching data generates the increasingly growing memory demand, that can quickly\nexhaust the limited memory capacity of the modern accelerator like GPUs,\nparticularly in long-context inference tasks. Existing studies reduce memory\nconsumption by evicting some of cached data that have less important impact on\ninference accuracy. But the benefit in practice is far from ideal due to the\nstatic cache allocation across different LLM network layers. This paper\nobserves that the layer-specific cached data have very different impacts on\naccuracy. We quantify this difference, and give experimental and theoretical\nvalidation. We accordingly make a formal analysis and shows that customizing\nthe cache size for each layer in a personalized manner can yield a significant\nmemory reduction, while still providing comparable accuracy. We simulate the\ncache allocation as a combinatorial optimization problem and give a global\noptimal solution. In particular, we devise a mini- and sampling-based inference\nover a lightweight variant of the LLM model, so as to quickly capture the\ndifference and then feed it into the personalized algorithms. Extensive\nexperiments on real-world datasets demonstrate that our proposals can reduce KV\ncache memory consumption by 61.6% on average, improve computational efficiency\nby 2.1x and then increase the throughput by up to 5.5x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently the generative Large Language Model (LLM) has achieved remarkable\nsuccess in numerous applications. Notably its inference generates output tokens\none-by-one, leading to many redundant computations. The widely-used KV-Cache\nframework makes a compromise between time and space complexities. However,\ncaching data generates the increasingly growing memory demand, that can quickly\nexhaust the limited memory capacity of the modern accelerator like GPUs,\nparticularly in long-context inference tasks. Existing studies reduce memory\nconsumption by evicting some of cached data that have less important impact on\ninference accuracy. But the benefit in practice is far from ideal due to the\nstatic cache allocation across different LLM network layers. This paper\nobserves that the layer-specific cached data have very different impacts on\naccuracy. We quantify this difference, and give experimental and theoretical\nvalidation. We accordingly make a formal analysis and shows that customizing\nthe cache size for each layer in a personalized manner can yield a significant\nmemory reduction, while still providing comparable accuracy. We simulate the\ncache allocation as a combinatorial optimization problem and give a global\noptimal solution. In particular, we devise a mini- and sampling-based inference\nover a lightweight variant of the LLM model, so as to quickly capture the\ndifference and then feed it into the personalized algorithms. Extensive\nexperiments on real-world datasets demonstrate that our proposals can reduce KV\ncache memory consumption by 61.6% on average, improve computational efficiency\nby 2.1x and then increase the throughput by up to 5.5x."
                },
                "authors": [
                    {
                        "name": "Weizhuo Li"
                    },
                    {
                        "name": "Zhigang Wang"
                    },
                    {
                        "name": "Yu Gu"
                    },
                    {
                        "name": "Ge Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ge Yu"
                },
                "author": "Ge Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05831v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05831v1",
                "updated": "2024-12-08T06:37:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "published": "2024-12-08T06:37:27Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "title": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval"
                },
                "summary": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval."
                },
                "authors": [
                    {
                        "name": "Shanti Stewart"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Lie Lu"
                    },
                    {
                        "name": "Andrea Fanelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Fanelli"
                },
                "author": "Andrea Fanelli",
                "arxiv_comment": "4 pages + 1 reference page, 2 figures, 2 tables. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05831v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05831v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05704v1",
                "updated": "2024-12-07T17:22:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    17,
                    22,
                    14,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-07T17:22:14Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    17,
                    22,
                    14,
                    5,
                    342,
                    0
                ],
                "title": "Ultrafast lattice and electron dynamics induced in a PbSe crystal by an\n  intense terahertz pulse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultrafast lattice and electron dynamics induced in a PbSe crystal by an\n  intense terahertz pulse"
                },
                "summary": "We have studied the ultrafast optical response of a PbSe crystal to an\nintense picosecond terahertz pulse with a peak electric field strength of up to\n$\\sim$ 500 kV/cm. The reflectivity anisotropy signal contains oscillations at\nthe fundamental frequency of the resonant infrared-active phonon mode as well\nas its second, third, and fourth harmonics. The effect is ascribed to coherent\nanharmonic phonons resonantly excited by the strong terahertz field. Pump\nterahertz pulses also induce an almost instantaneous Kerr effect and a\nlong-lived optical anisotropy of the crystal with a characteristic decay time\nof $\\gtrsim$ 100 ps. We consider lattice distortion and phonon-assisted side\nvalley population as possible origins of this metastable state.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We have studied the ultrafast optical response of a PbSe crystal to an\nintense picosecond terahertz pulse with a peak electric field strength of up to\n$\\sim$ 500 kV/cm. The reflectivity anisotropy signal contains oscillations at\nthe fundamental frequency of the resonant infrared-active phonon mode as well\nas its second, third, and fourth harmonics. The effect is ascribed to coherent\nanharmonic phonons resonantly excited by the strong terahertz field. Pump\nterahertz pulses also induce an almost instantaneous Kerr effect and a\nlong-lived optical anisotropy of the crystal with a characteristic decay time\nof $\\gtrsim$ 100 ps. We consider lattice distortion and phonon-assisted side\nvalley population as possible origins of this metastable state."
                },
                "authors": [
                    {
                        "name": "A. A. Melnikov"
                    },
                    {
                        "name": "Yu. G. Selivanov"
                    },
                    {
                        "name": "D. G. Poydashev"
                    },
                    {
                        "name": "S. V. Chekalin"
                    }
                ],
                "author_detail": {
                    "name": "S. V. Chekalin"
                },
                "author": "S. V. Chekalin",
                "arxiv_comment": "7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v1",
                "updated": "2024-12-07T16:41:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06567v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06567v2",
                "updated": "2024-12-07T13:23:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    13,
                    23,
                    39,
                    5,
                    342,
                    0
                ],
                "published": "2024-06-03T13:28:43Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    13,
                    28,
                    43,
                    0,
                    155,
                    0
                ],
                "title": "DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via\n  Adaptive Heads Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via\n  Adaptive Heads Fusion"
                },
                "summary": "Large language models (LLMs) with billions of parameters demonstrate\nimpressive performance. However, the widely used Multi-Head Attention (MHA) in\nLLMs incurs substantial computational and memory costs during inference. While\nsome efforts have optimized attention mechanisms by pruning heads or sharing\nparameters among heads, these methods often lead to performance degradation or\nnecessitate substantial continued pre-training costs to restore performance.\nBased on the analysis of attention redundancy, we design a Decoupled-Head\nAttention (DHA) mechanism. DHA adaptively configures group sharing for key\nheads and value heads across various layers, achieving a better balance between\nperformance and efficiency. Inspired by the observation of clustering similar\nheads, we propose to progressively transform the MHA checkpoint into the DHA\nmodel through linear fusion of similar head parameters step by step, retaining\nthe parametric knowledge of the MHA checkpoint. We construct DHA models by\ntransforming various scales of MHA checkpoints given target head budgets. Our\nexperiments show that DHA remarkably requires a mere 0.25\\% of the original\nmodel's pre-training budgets to achieve 97.6\\% of performance while saving 75\\%\nof KV cache. Compared to Group-Query Attention (GQA), DHA achieves a 5$\\times$\ntraining acceleration, a maximum of 13.93\\% performance improvement under\n0.01\\% pre-training budget, and 4\\% relative improvement under 0.05\\%\npre-training budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with billions of parameters demonstrate\nimpressive performance. However, the widely used Multi-Head Attention (MHA) in\nLLMs incurs substantial computational and memory costs during inference. While\nsome efforts have optimized attention mechanisms by pruning heads or sharing\nparameters among heads, these methods often lead to performance degradation or\nnecessitate substantial continued pre-training costs to restore performance.\nBased on the analysis of attention redundancy, we design a Decoupled-Head\nAttention (DHA) mechanism. DHA adaptively configures group sharing for key\nheads and value heads across various layers, achieving a better balance between\nperformance and efficiency. Inspired by the observation of clustering similar\nheads, we propose to progressively transform the MHA checkpoint into the DHA\nmodel through linear fusion of similar head parameters step by step, retaining\nthe parametric knowledge of the MHA checkpoint. We construct DHA models by\ntransforming various scales of MHA checkpoints given target head budgets. Our\nexperiments show that DHA remarkably requires a mere 0.25\\% of the original\nmodel's pre-training budgets to achieve 97.6\\% of performance while saving 75\\%\nof KV cache. Compared to Group-Query Attention (GQA), DHA achieves a 5$\\times$\ntraining acceleration, a maximum of 13.93\\% performance improvement under\n0.01\\% pre-training budget, and 4\\% relative improvement under 0.05\\%\npre-training budget."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Linhao Zhang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yu Sun"
                },
                "author": "Yu Sun",
                "arxiv_comment": "Accepted at NeurIPS 2024 10 pages, 9 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06567v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06567v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v2",
                "updated": "2024-12-07T13:23:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    13,
                    23,
                    39,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "12 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v2",
                "updated": "2024-12-07T04:08:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    4,
                    8,
                    56,
                    5,
                    342,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05392v1",
                "updated": "2024-12-06T19:35:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    19,
                    35,
                    52,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T19:35:52Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    19,
                    35,
                    52,
                    4,
                    341,
                    0
                ],
                "title": "Effect of electric field on excitons in wide quantum wells",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effect of electric field on excitons in wide quantum wells"
                },
                "summary": "A microscopic model of a heterostructure with a quantum well (QW) is proposed\nto study the exciton behavior in an external electric field. The effect of an\nelectric field ranging from 0 to 6 kV/cm applied to the GaAs/AlGaAs QW\nstructure in the growth direction is studied for several QWs of various widths\nup to 100 nm. The three-dimensional Schr\\\"odinger equation (SE) of exciton is\nnumerically solved using the finite difference method. Wave functions and\nenergies for several states of the heavy-hole and light-hole excitons are\ncalculated. Dependencies of the exciton state energy, the binding energy, the\nradiative broadening, and the static dipole moment on the applied electric\nfields are determined. The threshold of exciton dissociation for the 100-nm QW\nis also determined. In addition, we found the electric-field-induced shift of\nthe center of mass of the heavy-hole and light-hole exciton in the QWs.\nFinally, we have modeled reflection spectra of heterostructures with the\nGaAs/AlGaAs QWs in the electric field using the calculated energies and\nradiative broadenings of excitons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A microscopic model of a heterostructure with a quantum well (QW) is proposed\nto study the exciton behavior in an external electric field. The effect of an\nelectric field ranging from 0 to 6 kV/cm applied to the GaAs/AlGaAs QW\nstructure in the growth direction is studied for several QWs of various widths\nup to 100 nm. The three-dimensional Schr\\\"odinger equation (SE) of exciton is\nnumerically solved using the finite difference method. Wave functions and\nenergies for several states of the heavy-hole and light-hole excitons are\ncalculated. Dependencies of the exciton state energy, the binding energy, the\nradiative broadening, and the static dipole moment on the applied electric\nfields are determined. The threshold of exciton dissociation for the 100-nm QW\nis also determined. In addition, we found the electric-field-induced shift of\nthe center of mass of the heavy-hole and light-hole exciton in the QWs.\nFinally, we have modeled reflection spectra of heterostructures with the\nGaAs/AlGaAs QWs in the electric field using the calculated energies and\nradiative broadenings of excitons."
                },
                "authors": [
                    {
                        "name": "Shiming Zheng"
                    },
                    {
                        "name": "E. S. Khramtsov"
                    },
                    {
                        "name": "I. V. Ignatiev"
                    }
                ],
                "author_detail": {
                    "name": "I. V. Ignatiev"
                },
                "author": "I. V. Ignatiev",
                "arxiv_comment": "12 pages, 8 figures, to be published in Physical Review B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05228v1",
                "updated": "2024-12-06T17:58:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "title": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips"
                },
                "summary": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate."
                },
                "authors": [
                    {
                        "name": "Ismet Dagli"
                    },
                    {
                        "name": "James Crea"
                    },
                    {
                        "name": "Soner Seckiner"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Seluk Kse"
                    },
                    {
                        "name": "Mehmet E. Belviranli"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet E. Belviranli"
                },
                "author": "Mehmet E. Belviranli",
                "arxiv_comment": "This paper is accepted to 2025 Design, Automation Test in Europe\n  Conference Exhibition (DATE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02031v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02031v2",
                "updated": "2024-12-06T11:47:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    47,
                    6,
                    4,
                    341,
                    0
                ],
                "published": "2024-07-02T07:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    7,
                    59,
                    8,
                    1,
                    184,
                    0
                ],
                "title": "SwiftDiffusion: Efficient Diffusion Model Serving with Add-on Modules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftDiffusion: Efficient Diffusion Model Serving with Add-on Modules"
                },
                "summary": "Text-to-image (T2I) generation using diffusion models has become a\nblockbuster service in today's AI cloud. A production T2I service typically\ninvolves a serving workflow where a base diffusion model is augmented with\nvarious \"add-on\" modules, notably ControlNet and LoRA, to enhance image\ngeneration control. Compared to serving the base model alone, these add-on\nmodules introduce significant loading and computational overhead, resulting in\nincreased latency. In this paper, we present SwiftDiffusion, a system that\nefficiently serves a T2I workflow through a holistic approach. SwiftDiffusion\ndecouples ControNet from the base model and deploys it as a separate,\nindependently scaled service on dedicated GPUs, enabling ControlNet caching,\nparallelization, and sharing. To mitigate the high loading overhead of LoRA\nserving, SwiftDiffusion employs a bounded asynchronous LoRA loading (BAL)\ntechnique, allowing LoRA loading to overlap with the initial base model\nexecution by up to k steps without compromising image quality. Furthermore,\nSwiftDiffusion optimizes base model execution with a novel latent parallelism\ntechnique. Collectively, these designs enable SwiftDiffusion to outperform the\nstate-of-the-art T2I serving systems, achieving up to 7.8x latency reduction\nand 1.6x throughput improvement in serving SDXL models on H800 GPUs, without\nsacrificing image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) generation using diffusion models has become a\nblockbuster service in today's AI cloud. A production T2I service typically\ninvolves a serving workflow where a base diffusion model is augmented with\nvarious \"add-on\" modules, notably ControlNet and LoRA, to enhance image\ngeneration control. Compared to serving the base model alone, these add-on\nmodules introduce significant loading and computational overhead, resulting in\nincreased latency. In this paper, we present SwiftDiffusion, a system that\nefficiently serves a T2I workflow through a holistic approach. SwiftDiffusion\ndecouples ControNet from the base model and deploys it as a separate,\nindependently scaled service on dedicated GPUs, enabling ControlNet caching,\nparallelization, and sharing. To mitigate the high loading overhead of LoRA\nserving, SwiftDiffusion employs a bounded asynchronous LoRA loading (BAL)\ntechnique, allowing LoRA loading to overlap with the initial base model\nexecution by up to k steps without compromising image quality. Furthermore,\nSwiftDiffusion optimizes base model execution with a novel latent parallelism\ntechnique. Collectively, these designs enable SwiftDiffusion to outperform the\nstate-of-the-art T2I serving systems, achieving up to 7.8x latency reduction\nand 1.6x throughput improvement in serving SDXL models on H800 GPUs, without\nsacrificing image quality."
                },
                "authors": [
                    {
                        "name": "Suyi Li"
                    },
                    {
                        "name": "Lingyun Yang"
                    },
                    {
                        "name": "Xiaoxiao Jiang"
                    },
                    {
                        "name": "Hanfeng Lu"
                    },
                    {
                        "name": "Dakai An"
                    },
                    {
                        "name": "Zhipeng Di"
                    },
                    {
                        "name": "Weiyi Lu"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Kan Liu"
                    },
                    {
                        "name": "Yinghao Yu"
                    },
                    {
                        "name": "Tao Lan"
                    },
                    {
                        "name": "Guodong Yang"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Liping Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02031v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02031v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04757v1",
                "updated": "2024-12-06T03:46:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    46,
                    6,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T03:46:06Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    46,
                    6,
                    4,
                    341,
                    0
                ],
                "title": "Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free\n  Dynamic Triangular Attention Pattern",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free\n  Dynamic Triangular Attention Pattern"
                },
                "summary": "The quadratic computational complexity of the attention mechanism in current\nLarge Language Models (LLMs) renders inference with long contexts prohibitively\nexpensive. To address this challenge, various approaches aim to retain critical\nportions of the context to optimally approximate Full Attention (FA) through\nKey-Value (KV) compression or Sparse Attention (SA), enabling the processing of\nvirtually unlimited text lengths in a streaming manner. However, these methods\nstruggle to achieve performance levels comparable to FA, particularly in\nretrieval tasks. In this paper, our analysis of attention head patterns reveals\nthat LLMs' attention distributions show strong local correlations, naturally\nreflecting a chunking mechanism for input context. We propose Ltri-LLM\nframework, which divides KVs into spans, stores them in an offline index, and\nretrieves the relevant KVs into memory for various queries. Experimental\nresults on popular long text benchmarks show that Ltri-LLM can achieve\nperformance close to FA while maintaining efficient, streaming-based inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic computational complexity of the attention mechanism in current\nLarge Language Models (LLMs) renders inference with long contexts prohibitively\nexpensive. To address this challenge, various approaches aim to retain critical\nportions of the context to optimally approximate Full Attention (FA) through\nKey-Value (KV) compression or Sparse Attention (SA), enabling the processing of\nvirtually unlimited text lengths in a streaming manner. However, these methods\nstruggle to achieve performance levels comparable to FA, particularly in\nretrieval tasks. In this paper, our analysis of attention head patterns reveals\nthat LLMs' attention distributions show strong local correlations, naturally\nreflecting a chunking mechanism for input context. We propose Ltri-LLM\nframework, which divides KVs into spans, stores them in an offline index, and\nretrieves the relevant KVs into memory for various queries. Experimental\nresults on popular long text benchmarks show that Ltri-LLM can achieve\nperformance close to FA while maintaining efficient, streaming-based inference."
                },
                "authors": [
                    {
                        "name": "Hongyin Tang"
                    },
                    {
                        "name": "Di Xiu"
                    },
                    {
                        "name": "Lanrui Wang"
                    },
                    {
                        "name": "Xiurui Geng"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    }
                ],
                "author_detail": {
                    "name": "Xunliang Cai"
                },
                "author": "Xunliang Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04698v1",
                "updated": "2024-12-06T01:20:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    20,
                    47,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T01:20:47Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    20,
                    47,
                    4,
                    341,
                    0
                ],
                "title": "One-Hop Sub-Query Result Caches for Graph Database Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-Hop Sub-Query Result Caches for Graph Database Systems"
                },
                "summary": "This paper introduces a novel one-hop sub-query result cache for processing\ngraph read transactions, gR-Txs, in a graph database system. The one-hop\nnavigation is from a vertex using either its in-coming or out-going edges with\nselection predicates that filter edges and vertices. Its cache entry identifies\na unique one-hop sub-query (key) and its result set consisting of immutable\nvertex ids (value). When processing a gR-Tx, the query processor identifies its\nsequence of individual one-hop sub-queries and looks up their results in the\ncache. A cache hit fetches less data from the storage manager and eliminates\nthe requirement to process the one-hop sub-query. A cache miss populates the\ncache asynchronously and in a transactional manner, maintaining the separation\nof read and write paths of our transactional storage manager. A graph read and\nwrite transaction, gRW-Tx, identifies the impacted cache entries and either\ndeletes or updates them. Our implementation of the cache is inside the graph\nquery processing engine and transparent to a user application. We evaluate the\ncache using our eCommerce production workload and with rules that re-write\ngraph queries to maximize the performance enhancements observed with the cache.\nObtained results show the cache enhances 95th and 99th percentile of query\nresponse times by at least 2x and 1.63x, respectively. When combined with query\nre-writing, the enhancements are at least 2.33x and 4.48x, respectively. An\ninteresting result is the significant performance enhancement observed by the\nindirect beneficiaries of the cache, gRW-Txs and gR-Txs that do not reference\none-hop sub-queries. The cache frees system resources to expedite their\nprocessing significantly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel one-hop sub-query result cache for processing\ngraph read transactions, gR-Txs, in a graph database system. The one-hop\nnavigation is from a vertex using either its in-coming or out-going edges with\nselection predicates that filter edges and vertices. Its cache entry identifies\na unique one-hop sub-query (key) and its result set consisting of immutable\nvertex ids (value). When processing a gR-Tx, the query processor identifies its\nsequence of individual one-hop sub-queries and looks up their results in the\ncache. A cache hit fetches less data from the storage manager and eliminates\nthe requirement to process the one-hop sub-query. A cache miss populates the\ncache asynchronously and in a transactional manner, maintaining the separation\nof read and write paths of our transactional storage manager. A graph read and\nwrite transaction, gRW-Tx, identifies the impacted cache entries and either\ndeletes or updates them. Our implementation of the cache is inside the graph\nquery processing engine and transparent to a user application. We evaluate the\ncache using our eCommerce production workload and with rules that re-write\ngraph queries to maximize the performance enhancements observed with the cache.\nObtained results show the cache enhances 95th and 99th percentile of query\nresponse times by at least 2x and 1.63x, respectively. When combined with query\nre-writing, the enhancements are at least 2.33x and 4.48x, respectively. An\ninteresting result is the significant performance enhancement observed by the\nindirect beneficiaries of the cache, gRW-Txs and gR-Txs that do not reference\none-hop sub-queries. The cache frees system resources to expedite their\nprocessing significantly."
                },
                "authors": [
                    {
                        "name": "Hieu Nguyen"
                    },
                    {
                        "name": "Jun Li"
                    },
                    {
                        "name": "Shahram Ghandeharizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Shahram Ghandeharizadeh"
                },
                "author": "Shahram Ghandeharizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04652v1",
                "updated": "2024-12-05T22:47:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    47,
                    17,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T22:47:17Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    47,
                    17,
                    3,
                    340,
                    0
                ],
                "title": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference"
                },
                "summary": "KV cache pruning has emerged as a promising technique for reducing memory and\ncomputation costs in long-context auto-regressive generation. Existing methods\nfor vision-language models (VLMs) typically rely on self-attention scores from\nlarge language models (LLMs) to identify and prune irrelevant tokens. However,\nthese approaches overlook the inherent distributional discrepancies between\nmodalities, often leading to inaccurate token importance estimation and the\nover-pruning of critical visual tokens. To address this, we propose decomposing\nattention scores into intra-modality attention (within the same modality) and\ninter-modality attention (across modalities), enabling more precise KV cache\npruning by independently managing these distinct attention types. Additionally,\nwe introduce an n-softmax function to counteract distribution shifts caused by\npruning, preserving the original smoothness of attention scores and ensuring\nstable performance. Our final training-free method,\n\\textbf{C}ross-\\textbf{S}elf \\textbf{P}runing (CSP), achieves competitive\nperformance compared to models with full KV caches while significantly\noutperforming previous pruning methods. Extensive evaluations on MileBench, a\nbenchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness,\nachieving up to a 41\\% performance improvement on challenging tasks like\nconversational embodied dialogue while reducing the KV cache budget by 13.6\\%.\nThe code is available at https://github.com/TerryPei/CSP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache pruning has emerged as a promising technique for reducing memory and\ncomputation costs in long-context auto-regressive generation. Existing methods\nfor vision-language models (VLMs) typically rely on self-attention scores from\nlarge language models (LLMs) to identify and prune irrelevant tokens. However,\nthese approaches overlook the inherent distributional discrepancies between\nmodalities, often leading to inaccurate token importance estimation and the\nover-pruning of critical visual tokens. To address this, we propose decomposing\nattention scores into intra-modality attention (within the same modality) and\ninter-modality attention (across modalities), enabling more precise KV cache\npruning by independently managing these distinct attention types. Additionally,\nwe introduce an n-softmax function to counteract distribution shifts caused by\npruning, preserving the original smoothness of attention scores and ensuring\nstable performance. Our final training-free method,\n\\textbf{C}ross-\\textbf{S}elf \\textbf{P}runing (CSP), achieves competitive\nperformance compared to models with full KV caches while significantly\noutperforming previous pruning methods. Extensive evaluations on MileBench, a\nbenchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness,\nachieving up to a 41\\% performance improvement on challenging tasks like\nconversational embodied dialogue while reducing the KV cache budget by 13.6\\%.\nThe code is available at https://github.com/TerryPei/CSP"
                },
                "authors": [
                    {
                        "name": "Xiaohuan Pei"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04634v1",
                "updated": "2024-12-05T22:06:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    6,
                    23,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T22:06:23Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    6,
                    23,
                    3,
                    340,
                    0
                ],
                "title": "Neural Two-Level Monte Carlo Real-Time Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Two-Level Monte Carlo Real-Time Rendering"
                },
                "summary": "We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte\nCarlo, MLMC) estimator for real-time rendering of scenes with global\nillumination. Using MLMC we split the shading integral into two parts: the\nradiance cache integral and the residual error integral that compensates for\nthe bias of the first one. For the first part, we developed the Neural Incident\nRadiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks\nas a building block, which is trained on the fly. The cache is designed to\nprovide a fast and reasonable approximation of the incident radiance: an\nevaluation takes 2-25x less compute time than a path tracing sample. This\nenables us to estimate the radiance cache integral with a high number of\nsamples and by this achieve faster convergence. For the residual error\nintegral, we compute the difference between the NIRC predictions and the\nunbiased path tracing simulation. Our method makes no assumptions about the\ngeometry, materials, or lighting of a scene and has only few intuitive\nhyper-parameters. We provide a comprehensive comparative analysis in different\nexperimental scenarios. Since the algorithm is trained in an on-line fashion,\nit demonstrates significant noise level reduction even for dynamic scenes and\ncan easily be combined with other importance sampling schemes and noise\nreduction techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte\nCarlo, MLMC) estimator for real-time rendering of scenes with global\nillumination. Using MLMC we split the shading integral into two parts: the\nradiance cache integral and the residual error integral that compensates for\nthe bias of the first one. For the first part, we developed the Neural Incident\nRadiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks\nas a building block, which is trained on the fly. The cache is designed to\nprovide a fast and reasonable approximation of the incident radiance: an\nevaluation takes 2-25x less compute time than a path tracing sample. This\nenables us to estimate the radiance cache integral with a high number of\nsamples and by this achieve faster convergence. For the residual error\nintegral, we compute the difference between the NIRC predictions and the\nunbiased path tracing simulation. Our method makes no assumptions about the\ngeometry, materials, or lighting of a scene and has only few intuitive\nhyper-parameters. We provide a comprehensive comparative analysis in different\nexperimental scenarios. Since the algorithm is trained in an on-line fashion,\nit demonstrates significant noise level reduction even for dynamic scenes and\ncan easily be combined with other importance sampling schemes and noise\nreduction techniques."
                },
                "authors": [
                    {
                        "name": "Mikhail Dereviannykh"
                    },
                    {
                        "name": "Dmitrii Klepikov"
                    },
                    {
                        "name": "Johannes Hanika"
                    },
                    {
                        "name": "Carsten Dachsbacher"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Dachsbacher"
                },
                "author": "Carsten Dachsbacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04449v1",
                "updated": "2024-12-05T18:58:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:58:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay"
                },
                "summary": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. The majority of computation stems from the\noverwhelming volume of vision tokens processed by the transformer decoder. In\nthis paper, we propose to build efficient MLLMs by leveraging the\nMixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects\nessential vision tokens to process while skipping redundant ones. However,\nintegrating MoD into MLLMs is non-trivial. To address the challenges of\ntraining and inference stability as well as limited training data, we adapt the\nMoD module with two novel designs: tanh-gated weight normalization (TanhNorm)\nand symmetric token reweighting (STRing). Moreover, we observe that vision\ntokens exhibit higher redundancy in deeper layer and thus design a progressive\nratio decay (PRD) strategy, which gradually reduces the token retention ratio\nlayer by layer, employing a shifted cosine schedule. This crucial design fully\nunleashes the potential of MoD, significantly boosting the efficiency and\nperformance of our models. To validate the effectiveness of our approach, we\nconduct extensive experiments with two baseline models across 14 benchmarks.\nOur model, p-MoD, matches or even surpasses the performance of the baseline\nmodels, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and\n77.7% GPU hours during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. The majority of computation stems from the\noverwhelming volume of vision tokens processed by the transformer decoder. In\nthis paper, we propose to build efficient MLLMs by leveraging the\nMixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects\nessential vision tokens to process while skipping redundant ones. However,\nintegrating MoD into MLLMs is non-trivial. To address the challenges of\ntraining and inference stability as well as limited training data, we adapt the\nMoD module with two novel designs: tanh-gated weight normalization (TanhNorm)\nand symmetric token reweighting (STRing). Moreover, we observe that vision\ntokens exhibit higher redundancy in deeper layer and thus design a progressive\nratio decay (PRD) strategy, which gradually reduces the token retention ratio\nlayer by layer, employing a shifted cosine schedule. This crucial design fully\nunleashes the potential of MoD, significantly boosting the efficiency and\nperformance of our models. To validate the effectiveness of our approach, we\nconduct extensive experiments with two baseline models across 14 benchmarks.\nOur model, p-MoD, matches or even surpasses the performance of the baseline\nmodels, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and\n77.7% GPU hours during training."
                },
                "authors": [
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Desen Meng"
                    },
                    {
                        "name": "Ji Qi"
                    },
                    {
                        "name": "Zhenpeng Huang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "arxiv_comment": "Technical Report; Code released at https://github.com/MCG-NJU/p-MoD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03960v2",
                "updated": "2024-12-05T14:56:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    56,
                    56,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-04T22:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation"
                },
                "summary": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb."
                },
                "authors": [
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19574v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19574v2",
                "updated": "2024-12-05T12:19:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    19,
                    38,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-29T09:42:38Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    42,
                    38,
                    4,
                    334,
                    0
                ],
                "title": "KV Shifting Attention Enhances Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Shifting Attention Enhances Language Modeling"
                },
                "summary": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters."
                },
                "authors": [
                    {
                        "name": "Mingyu Xu"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19574v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19574v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01485v2",
                "updated": "2024-12-05T06:52:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    6,
                    52,
                    42,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-02T12:35:53Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "title": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts"
                },
                "summary": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v3",
                "updated": "2024-12-05T04:29:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    4,
                    29,
                    49,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "01. AI"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Albert Wang"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Ethan Dai"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zirui Zhang"
                },
                "author": "Zirui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.01516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.01516v2",
                "updated": "2024-12-05T01:50:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    1,
                    50,
                    27,
                    3,
                    340,
                    0
                ],
                "published": "2023-05-02T15:27:16Z",
                "published_parsed": [
                    2023,
                    5,
                    2,
                    15,
                    27,
                    16,
                    1,
                    122,
                    0
                ],
                "title": "F2: Designing a Key-Value Store for Large Skewed Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F2: Designing a Key-Value Store for Large Skewed Workloads"
                },
                "summary": "Many real-world workloads present a challenging set of requirements: point\noperations requiring high throughput, working sets much larger than main\nmemory, and natural skew in key access patterns for both reads and writes. We\nfind that modern key-value designs are either optimized for memory-efficiency,\nsacrificing high-performance (LSM-tree designs), or achieve high-performance,\nsaturating modern NVMe SSD bandwidth, at the cost of substantial memory\nresources or high disk wear (CPU-optimized designs). Unfortunately these\ndesigns are not able to handle meet the challenging demands of such\nlarger-than-memory, skewed workloads.\n  To this end, we present F2, a new key-value store that bridges this gap by\ncombining the strengths of both approaches. F2 adopts a tiered, record-oriented\narchitecture inspired by LSM-trees to effectively separate hot from cold\nrecords, while incorporating concurrent latch-free mechanisms from\nCPU-optimized engines to maximize performance on modern NVMe SSDs. To realize\nthis design, we tackle key challenges and introduce several innovations,\nincluding new latch-free algorithms for multi-threaded log compaction and user\noperations (e.g., RMWs), as well as new components: a two-level hash index to\nreduce indexing overhead for cold records and a read-cache for serving read-hot\ndata.\n  Detailed experimental results show that F2 matches or outperforms existing\nsolutions, achieving on average better throughput on memory-constrained\nenvironments compared to state-of-the-art systems like RocksDB (11.75x),\nSplinterDB (4.52x), KVell (10.56x), LeanStore (2.04x), and FASTER (2.38x). F2\nalso maintains its high performance across varying workload skewness levels and\nmemory budgets, while achieving low disk write amplification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many real-world workloads present a challenging set of requirements: point\noperations requiring high throughput, working sets much larger than main\nmemory, and natural skew in key access patterns for both reads and writes. We\nfind that modern key-value designs are either optimized for memory-efficiency,\nsacrificing high-performance (LSM-tree designs), or achieve high-performance,\nsaturating modern NVMe SSD bandwidth, at the cost of substantial memory\nresources or high disk wear (CPU-optimized designs). Unfortunately these\ndesigns are not able to handle meet the challenging demands of such\nlarger-than-memory, skewed workloads.\n  To this end, we present F2, a new key-value store that bridges this gap by\ncombining the strengths of both approaches. F2 adopts a tiered, record-oriented\narchitecture inspired by LSM-trees to effectively separate hot from cold\nrecords, while incorporating concurrent latch-free mechanisms from\nCPU-optimized engines to maximize performance on modern NVMe SSDs. To realize\nthis design, we tackle key challenges and introduce several innovations,\nincluding new latch-free algorithms for multi-threaded log compaction and user\noperations (e.g., RMWs), as well as new components: a two-level hash index to\nreduce indexing overhead for cold records and a read-cache for serving read-hot\ndata.\n  Detailed experimental results show that F2 matches or outperforms existing\nsolutions, achieving on average better throughput on memory-constrained\nenvironments compared to state-of-the-art systems like RocksDB (11.75x),\nSplinterDB (4.52x), KVell (10.56x), LeanStore (2.04x), and FASTER (2.38x). F2\nalso maintains its high performance across varying workload skewness levels and\nmemory budgets, while achieving low disk write amplification."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Badrish Chandramouli"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.01516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.01516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v2",
                "updated": "2024-12-04T18:40:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    40,
                    24,
                    2,
                    339,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03361v1",
                "updated": "2024-12-04T14:47:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    47,
                    42,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T14:47:42Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    47,
                    42,
                    2,
                    339,
                    0
                ],
                "title": "Measurement of electron beam induced sample heating in SEM experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurement of electron beam induced sample heating in SEM experiments"
                },
                "summary": "Scanning Electron Microscopy (SEM) experiments provide detailed insights into\nmaterial microstructures, enabling high-resolution imaging as well as\ncrystallographic analysis through advanced techniques like Electron Backscatter\nDiffraction (EBSD). However, the interaction of the high-energy electron beam\nwith the material can lead to localized heating, which may significantly impact\nspecimen integrity, especially in applications requiring prolonged beam\nexposure, for instance when mapping the crystal structure using EBSD. This\nstudy examines electron-beam-induced heating effects on a model metal sample\n(iron), directly measuring the locally deposited electron beam energy with a\nMEMS-based heating device and validating these measurements through\nsimulations, including Monte Carlo and Finite Element methods. The analysis\nfocuses on the effects of various experimental parameters such as acceleration\nvoltage (from 5 to 30 kV), beam current (from 0.17 nA to 22 nA), dwell time\n(from 1$\\mu$s to 1ms) and sample tilt (0{\\deg} to 70{\\deg}). The findings\nreveal that local sample temperatures can increase by up to 70 {\\deg}C during\nEBSD experiments, primarily affected by the choice in beam current and\nacceleration voltage, with beam current having the most significant impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scanning Electron Microscopy (SEM) experiments provide detailed insights into\nmaterial microstructures, enabling high-resolution imaging as well as\ncrystallographic analysis through advanced techniques like Electron Backscatter\nDiffraction (EBSD). However, the interaction of the high-energy electron beam\nwith the material can lead to localized heating, which may significantly impact\nspecimen integrity, especially in applications requiring prolonged beam\nexposure, for instance when mapping the crystal structure using EBSD. This\nstudy examines electron-beam-induced heating effects on a model metal sample\n(iron), directly measuring the locally deposited electron beam energy with a\nMEMS-based heating device and validating these measurements through\nsimulations, including Monte Carlo and Finite Element methods. The analysis\nfocuses on the effects of various experimental parameters such as acceleration\nvoltage (from 5 to 30 kV), beam current (from 0.17 nA to 22 nA), dwell time\n(from 1$\\mu$s to 1ms) and sample tilt (0{\\deg} to 70{\\deg}). The findings\nreveal that local sample temperatures can increase by up to 70 {\\deg}C during\nEBSD experiments, primarily affected by the choice in beam current and\nacceleration voltage, with beam current having the most significant impact."
                },
                "authors": [
                    {
                        "name": "Christina Koenig"
                    },
                    {
                        "name": "Alice Bastos da Silva Fanta"
                    },
                    {
                        "name": "Joerg R. Jinschek"
                    }
                ],
                "author_detail": {
                    "name": "Joerg R. Jinschek"
                },
                "author": "Joerg R. Jinschek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03213v1",
                "updated": "2024-12-04T10:58:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T10:58:27Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "title": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Chenqi Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v1",
                "updated": "2024-12-04T08:51:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying KV Cache Compression for Large Language Models with LeanKV"
                },
                "summary": "Large language models (LLMs) demonstrate exceptional performance but incur\nhigh serving costs due to substantial memory demands, with the key-value (KV)\ncache being a primary bottleneck. Existing KV cache compression methods,\nincluding quantization and pruning, struggle with limitations such as uniform\ntreatment of keys and values and static memory allocation across attention\nheads. To address these challenges, we introduce LeanKV, a unified KV cache\ncompression framework that enhances LLM serving efficiency without compromising\naccuracy through three innovations: (1) Hetero-KV quantization, which stores\nkeys at a higher precision than values to reflect their greater impact on\nattention computations; (2) per-head dynamic sparsity, which allocates memory\nbased on token importance per head and per request; and (3) unified KV\ncompression, integrating mixed-precision quantization and selective pruning to\nenable a smooth tradeoff between model accuracy and memory efficiency. To\nefficiently support these techniques, LeanKV introduces systems optimizations\nincluding unified paging and on-GPU parallel memory management. Implemented on\nvLLM, LeanKV compresses the KV cache by $3.0\\times$ to $5.0\\times$ without\naccuracy loss and up to $11.0\\times$ with under 5% accuracy loss, enhancing\nthroughput by $1.9\\times$ to $2.5\\times$, and up to $6.9\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate exceptional performance but incur\nhigh serving costs due to substantial memory demands, with the key-value (KV)\ncache being a primary bottleneck. Existing KV cache compression methods,\nincluding quantization and pruning, struggle with limitations such as uniform\ntreatment of keys and values and static memory allocation across attention\nheads. To address these challenges, we introduce LeanKV, a unified KV cache\ncompression framework that enhances LLM serving efficiency without compromising\naccuracy through three innovations: (1) Hetero-KV quantization, which stores\nkeys at a higher precision than values to reflect their greater impact on\nattention computations; (2) per-head dynamic sparsity, which allocates memory\nbased on token importance per head and per request; and (3) unified KV\ncompression, integrating mixed-precision quantization and selective pruning to\nenable a smooth tradeoff between model accuracy and memory efficiency. To\nefficiently support these techniques, LeanKV introduces systems optimizations\nincluding unified paging and on-GPU parallel memory management. Implemented on\nvLLM, LeanKV compresses the KV cache by $3.0\\times$ to $5.0\\times$ without\naccuracy loss and up to $11.0\\times$ with under 5% accuracy loss, enhancing\nthroughput by $1.9\\times$ to $2.5\\times$, and up to $6.9\\times$."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.08066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.08066v2",
                "updated": "2024-12-04T05:32:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    5,
                    32,
                    12,
                    2,
                    339,
                    0
                ],
                "published": "2023-02-06T13:46:08Z",
                "published_parsed": [
                    2023,
                    2,
                    6,
                    13,
                    46,
                    8,
                    0,
                    37,
                    0
                ],
                "title": "PASCAL: A Learning-aided Cooperative Bandwidth Control Policy for\n  Hierarchical Storage Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PASCAL: A Learning-aided Cooperative Bandwidth Control Policy for\n  Hierarchical Storage Systems"
                },
                "summary": "Nowadays, the Hierarchical Storage System (HSS) is considered as an ideal\nmodel to meet the cost-performance demand. The data migration between storing\ntiers of HSS is the way to achieve the cost-performance goal. The bandwidth\ncontrol is to limit the maximum amount of data migration. Most of previous\nresearch about HSS focus on studying the data migration policy instead of\nbandwidth control. However, the recent research about cache and networking\noptimization suggest that the bandwidth control has significant impact on the\nsystem performance. Few previous work achieves a satisfactory bandwidth control\nin HSS since it is hard to control bandwidth for so many data migration tasks\nsimultaneously. In this paper, we first give a stochastic programming model to\nformalize the bandwidth control problem in HSS. Then we propose a\nlearning-aided bandwidth control policy for HSS, named \\Pascal{}, which learns\nto control the bandwidth of different data migration task in an cooperative\nway. We implement \\Pascal{} on a commercial HSS and compare it with three\nstrong baselines over a group of workloads. Our evaluation on the physical\nsystem shows that \\Pascal{} can effectively decrease 1.95X the tail latency and\ngreatly improve throughput stability (2X $\\downarrow$ throughput jitter), and\nmeanwhile keep the throughput at a relatively high level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, the Hierarchical Storage System (HSS) is considered as an ideal\nmodel to meet the cost-performance demand. The data migration between storing\ntiers of HSS is the way to achieve the cost-performance goal. The bandwidth\ncontrol is to limit the maximum amount of data migration. Most of previous\nresearch about HSS focus on studying the data migration policy instead of\nbandwidth control. However, the recent research about cache and networking\noptimization suggest that the bandwidth control has significant impact on the\nsystem performance. Few previous work achieves a satisfactory bandwidth control\nin HSS since it is hard to control bandwidth for so many data migration tasks\nsimultaneously. In this paper, we first give a stochastic programming model to\nformalize the bandwidth control problem in HSS. Then we propose a\nlearning-aided bandwidth control policy for HSS, named \\Pascal{}, which learns\nto control the bandwidth of different data migration task in an cooperative\nway. We implement \\Pascal{} on a commercial HSS and compare it with three\nstrong baselines over a group of workloads. Our evaluation on the physical\nsystem shows that \\Pascal{} can effectively decrease 1.95X the tail latency and\ngreatly improve throughput stability (2X $\\downarrow$ throughput jitter), and\nmeanwhile keep the throughput at a relatively high level."
                },
                "authors": [
                    {
                        "name": "Xijun Li"
                    },
                    {
                        "name": "Yunfan Zhou"
                    },
                    {
                        "name": "Ji Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ji Zhang"
                },
                "author": "Ji Zhang",
                "arxiv_comment": "for modifying part of contents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.08066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.08066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03023v1",
                "updated": "2024-12-04T04:29:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    29,
                    12,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T04:29:12Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    29,
                    12,
                    2,
                    339,
                    0
                ],
                "title": "A Multi-Functional Web Tool for Comprehensive Threat Detection Through\n  IP Address Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Functional Web Tool for Comprehensive Threat Detection Through\n  IP Address Analysis"
                },
                "summary": "In recent years, the advances in digitalisation have also adversely\ncontributed to the significant rise in cybercrimes. Hence, building the threat\nintelligence to shield against rising cybercrimes has become a fundamental\nrequisite. Internet Protocol (IP) addresses play a crucial role in the threat\nintelligence and prevention of cyber crimes. However, we have noticed the lack\nof one-stop, free, and open-source tools that can analyse IP addresses. Hence,\nthis work introduces a comprehensive web tool for advanced IP address\ncharacterisation. Our tool offers a wide range of features, including\ngeolocation, blocklist check, VPN detection, proxy detection, bot detection,\nTor detection, port scan, and accurate domain statistics that include the\ndetails about the name servers and registrar information. In addition, our tool\ncalculates a confidence score based on a weighted sum of publicly accessible\nonline results from different reliable sources to give users a dependable\nmeasure of accuracy. Further, to improve performance, our tool also\nincorporates a local database for caching the results, to enable fast content\nretrieval with minimal external Web API calls. Our tool supports domain names\nand IPv4 addresses, making it a multi-functional and powerful IP analyser tool\nfor threat intelligence. Our tool is available at www.ipanalyzer.in",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the advances in digitalisation have also adversely\ncontributed to the significant rise in cybercrimes. Hence, building the threat\nintelligence to shield against rising cybercrimes has become a fundamental\nrequisite. Internet Protocol (IP) addresses play a crucial role in the threat\nintelligence and prevention of cyber crimes. However, we have noticed the lack\nof one-stop, free, and open-source tools that can analyse IP addresses. Hence,\nthis work introduces a comprehensive web tool for advanced IP address\ncharacterisation. Our tool offers a wide range of features, including\ngeolocation, blocklist check, VPN detection, proxy detection, bot detection,\nTor detection, port scan, and accurate domain statistics that include the\ndetails about the name servers and registrar information. In addition, our tool\ncalculates a confidence score based on a weighted sum of publicly accessible\nonline results from different reliable sources to give users a dependable\nmeasure of accuracy. Further, to improve performance, our tool also\nincorporates a local database for caching the results, to enable fast content\nretrieval with minimal external Web API calls. Our tool supports domain names\nand IPv4 addresses, making it a multi-functional and powerful IP analyser tool\nfor threat intelligence. Our tool is available at www.ipanalyzer.in"
                },
                "authors": [
                    {
                        "name": "Cebajel Tanan"
                    },
                    {
                        "name": "Sameer G. Kulkarni"
                    },
                    {
                        "name": "Tamal Das"
                    },
                    {
                        "name": "Manjesh K. Hanawal"
                    }
                ],
                "author_detail": {
                    "name": "Manjesh K. Hanawal"
                },
                "author": "Manjesh K. Hanawal",
                "arxiv_comment": "Presented at ICIE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.12622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.12622v2",
                "updated": "2024-12-03T22:48:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    48,
                    9,
                    1,
                    338,
                    0
                ],
                "published": "2023-10-19T10:02:52Z",
                "published_parsed": [
                    2023,
                    10,
                    19,
                    10,
                    2,
                    52,
                    3,
                    292,
                    0
                ],
                "title": "cRVR: A Stackelberg Game Approach for Joint Privacy-Aware Video\n  Requesting and Edge Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cRVR: A Stackelberg Game Approach for Joint Privacy-Aware Video\n  Requesting and Edge Caching"
                },
                "summary": "As users conveniently stream their favorite online videos, video request\nrecords are automatically stored by video content providers, which have a high\nchance of privacy leakage. Unfortunately, most existing privacy-enhancing\napproaches are not applicable for protecting user privacy in video requests,\nbecause they cannot be easily altered or distorted by users and must be visible\nfor content providers to stream correct videos. To preserve request privacy in\nonline video services, it is possible to request additional videos that are\nirrelevant to users' interests so that content providers cannot precisely infer\nusers' interest information. However, a naive redundant requesting approach\nwould significantly degrade the performance of edge caches and increase\nbandwidth overhead. In this paper, we are among the first to propose a\nCache-Friendly Redundant Video Requesting (cRVR) algorithm for User Devices\n(UDs) and its corresponding caching algorithm for the Edge Cache (EC), which\ncan effectively mitigate the problem of request privacy leakage with minimal\nimpact on the EC's performance. To tackle the problem, we first develop a\nStackelberg game to analyze the dedicated interaction between UDs and EC, and\nobtain their optimal strategies to maximize their respective utility. For UDs,\nthe utility function is a combination of both video playback utility and\nprivacy protection utility. We prove the existence and uniqueness of the\nequilibrium of the Stackelberg game. Extensive experiments are conducted with\nreal traces to demonstrate that cRVR can effectively protect video request\nprivacy by reducing up to 59.03\\% of privacy disclosure compared to baseline\nalgorithms. Meanwhile, the caching performance of EC is only slightly affected.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As users conveniently stream their favorite online videos, video request\nrecords are automatically stored by video content providers, which have a high\nchance of privacy leakage. Unfortunately, most existing privacy-enhancing\napproaches are not applicable for protecting user privacy in video requests,\nbecause they cannot be easily altered or distorted by users and must be visible\nfor content providers to stream correct videos. To preserve request privacy in\nonline video services, it is possible to request additional videos that are\nirrelevant to users' interests so that content providers cannot precisely infer\nusers' interest information. However, a naive redundant requesting approach\nwould significantly degrade the performance of edge caches and increase\nbandwidth overhead. In this paper, we are among the first to propose a\nCache-Friendly Redundant Video Requesting (cRVR) algorithm for User Devices\n(UDs) and its corresponding caching algorithm for the Edge Cache (EC), which\ncan effectively mitigate the problem of request privacy leakage with minimal\nimpact on the EC's performance. To tackle the problem, we first develop a\nStackelberg game to analyze the dedicated interaction between UDs and EC, and\nobtain their optimal strategies to maximize their respective utility. For UDs,\nthe utility function is a combination of both video playback utility and\nprivacy protection utility. We prove the existence and uniqueness of the\nequilibrium of the Stackelberg game. Extensive experiments are conducted with\nreal traces to demonstrate that cRVR can effectively protect video request\nprivacy by reducing up to 59.03\\% of privacy disclosure compared to baseline\nalgorithms. Meanwhile, the caching performance of EC is only slightly affected."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Linchang Xiao"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    }
                ],
                "author_detail": {
                    "name": "Quan Z. Sheng"
                },
                "author": "Quan Z. Sheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.12622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.12622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02867v1",
                "updated": "2024-12-03T22:02:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    2,
                    42,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T22:02:42Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    2,
                    42,
                    1,
                    338,
                    0
                ],
                "title": "GoldFish: Serverless Actors with Short-Term Memory State for the\n  Edge-Cloud Continuum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoldFish: Serverless Actors with Short-Term Memory State for the\n  Edge-Cloud Continuum"
                },
                "summary": "Serverless Computing is a computing paradigm that provides efficient\ninfrastructure management and elastic scalability. Serverless functions scale\nup or down based on demand, which means that functions are not directly\naddressable and rely on platform-managed invocation. Serverless stateless\nnature requires functions to leverage external services, such as object storage\nand KVS, to exchange data. Serverless actors have emerged as a solution to\nthese issues. However, the state-of-the-art serverless lifecycle and\nevent-trigger invocation force actors to leverage remote services to manage\ntheir state and exchange data, which impacts the performance and incurs\nadditional costs and dependency on third-party services.\n  To address these issues, in this paper, we introduce a novel serverless\nlifecycle model that allows short-term stateful actors, enabling actors to\nmaintain their state between executions. Additionally, we propose a novel\nserverless Invocation Model that enables serverless actors to influence the\nprocessing of future messages. We present GoldFish, a lightweight WebAssembly\nshort-term stateful serverless actor platform that provides a novel serverless\nactor lifecycle and invocation model. GoldFish leverages WebAssembly to provide\nthe actors with lightweight sandbox isolation, making them suitable for the\nEdge-Cloud Continuum, where computational resources are limited. Experimental\nresults show that GoldFish optimizes the data exchange latency by up to 92% and\nincreases the throughput by up to 10x compared to OpenFaaS and Spin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless Computing is a computing paradigm that provides efficient\ninfrastructure management and elastic scalability. Serverless functions scale\nup or down based on demand, which means that functions are not directly\naddressable and rely on platform-managed invocation. Serverless stateless\nnature requires functions to leverage external services, such as object storage\nand KVS, to exchange data. Serverless actors have emerged as a solution to\nthese issues. However, the state-of-the-art serverless lifecycle and\nevent-trigger invocation force actors to leverage remote services to manage\ntheir state and exchange data, which impacts the performance and incurs\nadditional costs and dependency on third-party services.\n  To address these issues, in this paper, we introduce a novel serverless\nlifecycle model that allows short-term stateful actors, enabling actors to\nmaintain their state between executions. Additionally, we propose a novel\nserverless Invocation Model that enables serverless actors to influence the\nprocessing of future messages. We present GoldFish, a lightweight WebAssembly\nshort-term stateful serverless actor platform that provides a novel serverless\nactor lifecycle and invocation model. GoldFish leverages WebAssembly to provide\nthe actors with lightweight sandbox isolation, making them suitable for the\nEdge-Cloud Continuum, where computational resources are limited. Experimental\nresults show that GoldFish optimizes the data exchange latency by up to 92% and\nincreases the throughput by up to 10x compared to OpenFaaS and Spin."
                },
                "authors": [
                    {
                        "name": "Cynthia Marcelino"
                    },
                    {
                        "name": "Jack Shahhoud"
                    },
                    {
                        "name": "Stefan Nastic"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Nastic"
                },
                "author": "Stefan Nastic",
                "arxiv_doi": "10.1145/3703790.3703797",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3703790.3703797",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.02867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14th International Conference on the Internet of Things (IoT 2024),\n  November 19--22, 2024, Oulu, Finland",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v3",
                "updated": "2024-12-03T12:36:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    12,
                    36,
                    19,
                    1,
                    338,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer. Comprehensive empirical evidence demonstrates ResFormer\nachieves equivalent validation loss with 10.4% fewer model parameters and 13.6%\nless training data compared to Transformer, while maintaining similar memory\nusage and computational cost. Besides, SVFormer reduces KV cache size by nearly\nhalf with only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate. Further\nvisualization results suggest that Resformer and SVFormer alleviate attention\nconcentration in deeper layers through avoiding value-state drains and enhance\nrepresentation across most layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer. Comprehensive empirical evidence demonstrates ResFormer\nachieves equivalent validation loss with 10.4% fewer model parameters and 13.6%\nless training data compared to Transformer, while maintaining similar memory\nusage and computational cost. Besides, SVFormer reduces KV cache size by nearly\nhalf with only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate. Further\nvisualization results suggest that Resformer and SVFormer alleviate attention\nconcentration in deeper layers through avoiding value-state drains and enhance\nrepresentation across most layers."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02252v1",
                "updated": "2024-12-03T08:29:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T08:29:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity"
                },
                "summary": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance."
                },
                "authors": [
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Situo Zhang"
                    },
                    {
                        "name": "Yuxun Miao"
                    },
                    {
                        "name": "Su Zhu"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Hanqi Li"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Lei Pan"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02122v1",
                "updated": "2024-12-03T03:20:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    20,
                    40,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T03:20:40Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    20,
                    40,
                    1,
                    338,
                    0
                ],
                "title": "Improving Sequential Recommender Systems with Online and In-store User\n  Behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Sequential Recommender Systems with Online and In-store User\n  Behavior"
                },
                "summary": "Online e-commerce platforms have been extending in-store shopping, which\nallows users to keep the canonical online browsing and checkout experience\nwhile exploring in-store shopping. However, the growing transition between\nonline and in-store becomes a challenge to sequential recommender systems for\nfuture online interaction prediction due to the lack of holistic modeling of\nhybrid user behaviors (online and in-store). The challenges are twofold. First,\ncombining online and in-store user behavior data into a single data schema and\nsupporting multiple stages in the model life cycle (pre-training, training,\ninference, etc.) organically needs a new data pipeline design. Second, online\nrecommender systems, which solely rely on online user behavior sequences, must\nbe redesigned to support online and in-store user data as input under the\nsequential modeling setting. To overcome the first challenge, we propose a\nhybrid, omnichannel data pipeline to compile online and in-store user behavior\ndata by caching information from diverse data sources. Later, we introduce a\nmodel-agnostic encoder module to the sequential recommender system to interpret\nthe user in-store transaction and augment the modeling capacity for better\nonline interaction prediction given the hybrid user behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online e-commerce platforms have been extending in-store shopping, which\nallows users to keep the canonical online browsing and checkout experience\nwhile exploring in-store shopping. However, the growing transition between\nonline and in-store becomes a challenge to sequential recommender systems for\nfuture online interaction prediction due to the lack of holistic modeling of\nhybrid user behaviors (online and in-store). The challenges are twofold. First,\ncombining online and in-store user behavior data into a single data schema and\nsupporting multiple stages in the model life cycle (pre-training, training,\ninference, etc.) organically needs a new data pipeline design. Second, online\nrecommender systems, which solely rely on online user behavior sequences, must\nbe redesigned to support online and in-store user data as input under the\nsequential modeling setting. To overcome the first challenge, we propose a\nhybrid, omnichannel data pipeline to compile online and in-store user behavior\ndata by caching information from diverse data sources. Later, we introduce a\nmodel-agnostic encoder module to the sequential recommender system to interpret\nthe user in-store transaction and augment the modeling capacity for better\nonline interaction prediction given the hybrid user behavior."
                },
                "authors": [
                    {
                        "name": "Luyi Ma"
                    },
                    {
                        "name": "Aashika Padmanabhan"
                    },
                    {
                        "name": "Anjana Ganesh"
                    },
                    {
                        "name": "Shengwei Tang"
                    },
                    {
                        "name": "Jiao Chen"
                    },
                    {
                        "name": "Xiaohan Li"
                    },
                    {
                        "name": "Lalitesh Morishetti"
                    },
                    {
                        "name": "Kaushiki Nag"
                    },
                    {
                        "name": "Malay Patel"
                    },
                    {
                        "name": "Jason Cho"
                    },
                    {
                        "name": "Sushant Kumar"
                    },
                    {
                        "name": "Kannan Achan"
                    }
                ],
                "author_detail": {
                    "name": "Kannan Achan"
                },
                "author": "Kannan Achan",
                "arxiv_comment": "6 pages, IEEE BigData 2024 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01959v1",
                "updated": "2024-12-02T20:39:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T20:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Development and Application of a Decentralized Domain Name Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Application of a Decentralized Domain Name Service"
                },
                "summary": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01827v1",
                "updated": "2024-12-02T18:59:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T18:59:53Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "title": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders"
                },
                "summary": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/."
                },
                "authors": [
                    {
                        "name": "Ziqi Pang"
                    },
                    {
                        "name": "Tianyuan Zhang"
                    },
                    {
                        "name": "Fujun Luan"
                    },
                    {
                        "name": "Yunze Man"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Xiong Wang"
                },
                "author": "Yu-Xiong Wang",
                "arxiv_comment": "Project page: https://rand-ar.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01659v1",
                "updated": "2024-12-02T16:10:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    10,
                    26,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T16:10:26Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    10,
                    26,
                    0,
                    337,
                    0
                ],
                "title": "Local and Regional Contributions to Tropospheric Ozone Concentrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local and Regional Contributions to Tropospheric Ozone Concentrations"
                },
                "summary": "The Wasatch Front in Utah, USA, is currently a non-attainment area for ozone\naccording to the Environmental Protection Agency's (EPA) National Ambient Air\nQuality Standards (NAAQS). Nitrogen oxides ($\\mathrm{NO_x = NO_2 + NO}$) and\nvolatile organic compounds (VOCs), in the presence of sunlight, lead to ozone\nformation in the troposphere. When the rate of oxidant production, defined as\nthe sum of $\\mathrm{O_3}$ and $\\mathrm{NO_2}$, is faster than the rate of\n$\\mathrm{NO_x}$ production, a region is said to be $\\mathrm{NO_x}$limited, and\nozone formation will be limited by the concentration of $\\mathrm{NO_x}$ species\nin the region. The inverse of this situation makes the region VOC-limited.\nKnowing whether a region is $\\mathrm{NO_x}$-limited or VOC-limited can aid in\ngenerating effective mitigation strategies. Understanding the background or\nregional contributions to ozone in a region, whether from the transport of\nprecursors or of ozone, provides information about the lower limit for ozone\nconcentrations that a region can achieve through regulation of local\nprecursors. In this paper, measured oxidant and $\\mathrm{NO_x}$ concentrations\nare analyzed from 14 counties in the state of Utah to calculate the regional\nand local contributions to ozone for each region. This analysis is used to\ndetermine the nature of the atmosphere in each county by identifying whether\nthe region is VOC or $\\mathrm{NO_x}$-limited. Furthermore, this analysis is\nperformed for each county for the years 2012 and 2022 to assess changes in the\noxidative nature and quantify regional and local contributions to ozone over a\n10-year period. All studied counties--except for Washington County--in Utah\nwere found to be VOC-limited in 2012. This shifted in 2022, with most counties\nbeing either in a transitional state or $\\mathrm{NO_x}$-limited. Local\ncontributions to ozone increased in two major counties, Cache and Salt Lake\nCounties, but decreased in Carbon, Davis, Duchesne, Uinta, Utah, Washington,\nand Weber Counties. Generally, the regional contributions to oxidant\nconcentrations decreased across the state. A summertime spike in both regional\nand local contributions to oxidants was observed. Smoke from wildfires was\nfound to increase regional contributions to oxidants and shift the local regime\nto be more $\\mathrm{NO_x}$-limited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Wasatch Front in Utah, USA, is currently a non-attainment area for ozone\naccording to the Environmental Protection Agency's (EPA) National Ambient Air\nQuality Standards (NAAQS). Nitrogen oxides ($\\mathrm{NO_x = NO_2 + NO}$) and\nvolatile organic compounds (VOCs), in the presence of sunlight, lead to ozone\nformation in the troposphere. When the rate of oxidant production, defined as\nthe sum of $\\mathrm{O_3}$ and $\\mathrm{NO_2}$, is faster than the rate of\n$\\mathrm{NO_x}$ production, a region is said to be $\\mathrm{NO_x}$limited, and\nozone formation will be limited by the concentration of $\\mathrm{NO_x}$ species\nin the region. The inverse of this situation makes the region VOC-limited.\nKnowing whether a region is $\\mathrm{NO_x}$-limited or VOC-limited can aid in\ngenerating effective mitigation strategies. Understanding the background or\nregional contributions to ozone in a region, whether from the transport of\nprecursors or of ozone, provides information about the lower limit for ozone\nconcentrations that a region can achieve through regulation of local\nprecursors. In this paper, measured oxidant and $\\mathrm{NO_x}$ concentrations\nare analyzed from 14 counties in the state of Utah to calculate the regional\nand local contributions to ozone for each region. This analysis is used to\ndetermine the nature of the atmosphere in each county by identifying whether\nthe region is VOC or $\\mathrm{NO_x}$-limited. Furthermore, this analysis is\nperformed for each county for the years 2012 and 2022 to assess changes in the\noxidative nature and quantify regional and local contributions to ozone over a\n10-year period. All studied counties--except for Washington County--in Utah\nwere found to be VOC-limited in 2012. This shifted in 2022, with most counties\nbeing either in a transitional state or $\\mathrm{NO_x}$-limited. Local\ncontributions to ozone increased in two major counties, Cache and Salt Lake\nCounties, but decreased in Carbon, Davis, Duchesne, Uinta, Utah, Washington,\nand Weber Counties. Generally, the regional contributions to oxidant\nconcentrations decreased across the state. A summertime spike in both regional\nand local contributions to oxidants was observed. Smoke from wildfires was\nfound to increase regional contributions to oxidants and shift the local regime\nto be more $\\mathrm{NO_x}$-limited."
                },
                "authors": [
                    {
                        "name": "Callum E. Flowerday"
                    },
                    {
                        "name": "Ryan Thalman"
                    },
                    {
                        "name": "Jaron C. Hansen"
                    }
                ],
                "author_detail": {
                    "name": "Jaron C. Hansen"
                },
                "author": "Jaron C. Hansen",
                "arxiv_doi": "10.3390/atmos14081262",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/atmos14081262",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.01659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Atmosphere 2023, 14, 1262",
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06892v2",
                "updated": "2024-12-02T11:24:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    24,
                    20,
                    0,
                    337,
                    0
                ],
                "published": "2024-03-11T16:48:25Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    16,
                    48,
                    25,
                    0,
                    71,
                    0
                ],
                "title": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head"
                },
                "summary": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}"
                },
                "authors": [
                    {
                        "name": "Tiancheng Zhao"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Xuan He"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Kyusong Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kyusong Lee"
                },
                "author": "Kyusong Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01380v1",
                "updated": "2024-12-02T11:07:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T11:07:51Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "title": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking"
                },
                "summary": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which result in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46% reduction in memory and 40% increase in throughput with $<$ 0.1\nloss in perplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which result in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46% reduction in memory and 40% increase in throughput with $<$ 0.1\nloss in perplexity."
                },
                "authors": [
                    {
                        "name": "Marco Federici"
                    },
                    {
                        "name": "Davide Belli"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Amir Jalalirad"
                    },
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Bence Major"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    }
                ],
                "author_detail": {
                    "name": "Paul Whatmough"
                },
                "author": "Paul Whatmough",
                "arxiv_comment": "Main Text: 10 pages, 11 figures. Appendix: 3 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01195v1",
                "updated": "2024-12-02T06:57:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    57,
                    46,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T06:57:46Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    57,
                    46,
                    0,
                    337,
                    0
                ],
                "title": "Memory-Efficient Training for Deep Speaker Embedding Learning in Speaker\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Training for Deep Speaker Embedding Learning in Speaker\n  Verification"
                },
                "summary": "Recent speaker verification (SV) systems have shown a trend toward adopting\ndeeper speaker embedding extractors. Although deeper and larger neural networks\ncan significantly improve performance, their substantial memory requirements\nhinder training on consumer GPUs. In this paper, we explore a memory-efficient\ntraining strategy for deep speaker embedding learning in resource-constrained\nscenarios. Firstly, we conduct a systematic analysis of GPU memory allocation\nduring SV system training. Empirical observations show that activations and\noptimizer states are the main sources of memory consumption. For activations,\nwe design two types of reversible neural networks which eliminate the need to\nstore intermediate activations during back-propagation, thereby significantly\nreducing memory usage without performance loss. For optimizer states, we\nintroduce a dynamic quantization approach that replaces the original 32-bit\nfloating-point values with a dynamic tree-based 8-bit data type. Experimental\nresults on VoxCeleb demonstrate that the reversible variants of ResNets and\nDF-ResNets can perform training without the need to cache activations in GPU\nmemory. In addition, the 8-bit versions of SGD and Adam save 75% of memory\ncosts while maintaining performance compared to their 32-bit counterparts.\nFinally, a detailed comparison of memory usage and performance indicates that\nour proposed models achieve up to 16.2x memory savings, with nearly identical\nparameters and performance compared to the vanilla systems. In contrast to the\nprevious need for multiple high-end GPUs such as the A100, we can effectively\ntrain deep speaker embedding extractors with just one or two consumer-level\n2080Ti GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent speaker verification (SV) systems have shown a trend toward adopting\ndeeper speaker embedding extractors. Although deeper and larger neural networks\ncan significantly improve performance, their substantial memory requirements\nhinder training on consumer GPUs. In this paper, we explore a memory-efficient\ntraining strategy for deep speaker embedding learning in resource-constrained\nscenarios. Firstly, we conduct a systematic analysis of GPU memory allocation\nduring SV system training. Empirical observations show that activations and\noptimizer states are the main sources of memory consumption. For activations,\nwe design two types of reversible neural networks which eliminate the need to\nstore intermediate activations during back-propagation, thereby significantly\nreducing memory usage without performance loss. For optimizer states, we\nintroduce a dynamic quantization approach that replaces the original 32-bit\nfloating-point values with a dynamic tree-based 8-bit data type. Experimental\nresults on VoxCeleb demonstrate that the reversible variants of ResNets and\nDF-ResNets can perform training without the need to cache activations in GPU\nmemory. In addition, the 8-bit versions of SGD and Adam save 75% of memory\ncosts while maintaining performance compared to their 32-bit counterparts.\nFinally, a detailed comparison of memory usage and performance indicates that\nour proposed models achieve up to 16.2x memory savings, with nearly identical\nparameters and performance compared to the vanilla systems. In contrast to the\nprevious need for multiple high-end GPUs such as the A100, we can effectively\ntrain deep speaker embedding extractors with just one or two consumer-level\n2080Ti GPUs."
                },
                "authors": [
                    {
                        "name": "Bei Liu"
                    },
                    {
                        "name": "Yanmin Qian"
                    }
                ],
                "author_detail": {
                    "name": "Yanmin Qian"
                },
                "author": "Yanmin Qian",
                "arxiv_comment": "Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04762v2",
                "updated": "2024-12-02T06:30:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    30,
                    4,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-07T14:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "title": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems"
                },
                "summary": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Jiaxu Wu"
                    },
                    {
                        "name": "Zemin Sun"
                    },
                    {
                        "name": "Long He"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Shiwen Mao"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Mao"
                },
                "author": "Shiwen Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00977v1",
                "updated": "2024-12-01T21:47:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    21,
                    47,
                    35,
                    6,
                    336,
                    0
                ],
                "published": "2024-12-01T21:47:35Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    21,
                    47,
                    35,
                    6,
                    336,
                    0
                ],
                "title": "Optimal Power Allocation in Uplink NOMA with Simultaneous Cache-Enabled\n  D2D Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Power Allocation in Uplink NOMA with Simultaneous Cache-Enabled\n  D2D Communications"
                },
                "summary": "Non-orthogonal multiple access (NOMA) is widely viewed as a potential\ncandidate for providing enhanced multiple access in future mobile networks by\neliminating the orthogonal distribution of radio resources amongst the users.\nNevertheless, the performance of NOMA can be significantly improved by\ncombining it with other sophisticated technologies such as wireless data\ncaching and device-to-device (D2D) communications. In this letter, we propose a\nnovel cellular system model which integrates uplink NOMA with cache based\ndevice-to-device (D2D) communications. The proposed system would enable a\ncellular user to upload data file to base station while simultaneously\nexchanging useful cache content with another nearby user. We maximize the\nsystem sum rate by deriving closed form solutions for optimal power allocation.\nSimulation results demonstrate the superior performance of our proposed model\nover other potential combinations of uplink NOMA and D2D communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-orthogonal multiple access (NOMA) is widely viewed as a potential\ncandidate for providing enhanced multiple access in future mobile networks by\neliminating the orthogonal distribution of radio resources amongst the users.\nNevertheless, the performance of NOMA can be significantly improved by\ncombining it with other sophisticated technologies such as wireless data\ncaching and device-to-device (D2D) communications. In this letter, we propose a\nnovel cellular system model which integrates uplink NOMA with cache based\ndevice-to-device (D2D) communications. The proposed system would enable a\ncellular user to upload data file to base station while simultaneously\nexchanging useful cache content with another nearby user. We maximize the\nsystem sum rate by deriving closed form solutions for optimal power allocation.\nSimulation results demonstrate the superior performance of our proposed model\nover other potential combinations of uplink NOMA and D2D communications."
                },
                "authors": [
                    {
                        "name": "Aditya Powari"
                    },
                    {
                        "name": "Daniel K. C. So"
                    }
                ],
                "author_detail": {
                    "name": "Daniel K. C. So"
                },
                "author": "Daniel K. C. So",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v1",
                "updated": "2024-12-01T15:45:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    }
                ],
                "author_detail": {
                    "name": "Peiran Dong"
                },
                "author": "Peiran Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02532v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02532v3",
                "updated": "2024-11-30T21:33:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    21,
                    33,
                    59,
                    5,
                    335,
                    0
                ],
                "published": "2024-06-04T17:53:36Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    53,
                    36,
                    1,
                    156,
                    0
                ],
                "title": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM\n  Inference on Consumer Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM\n  Inference on Consumer Devices"
                },
                "summary": "As large language models gain widespread adoption, running them efficiently\nbecomes crucial. Recent works on LLM inference use speculative decoding to\nachieve extreme speedups. However, most of these works implicitly design their\nalgorithms for high-end datacenter hardware. In this work, we ask the opposite\nquestion: how fast can we run LLMs on consumer machines? Consumer GPUs can no\nlonger fit the largest available models (50B+ parameters) and must offload them\nto RAM or SSD. When running with offloaded parameters, the inference engine can\nprocess batches of hundreds or thousands of tokens at the same time as just one\ntoken, making it a natural fit for speculative decoding. We propose SpecExec\n(Speculative Execution), a simple parallel decoding method that can generate up\nto 20 tokens per target model iteration for popular LLM families. It utilizes\nthe high spikiness of the token probabilities distribution in modern LLMs and a\nhigh degree of alignment between model output probabilities. SpecExec takes the\nmost probable tokens continuation from the draft model to build a \"cache\" tree\nfor the target model, which then gets validated in a single pass. Using\nSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with\nRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens\nper second with 16-bit weights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models gain widespread adoption, running them efficiently\nbecomes crucial. Recent works on LLM inference use speculative decoding to\nachieve extreme speedups. However, most of these works implicitly design their\nalgorithms for high-end datacenter hardware. In this work, we ask the opposite\nquestion: how fast can we run LLMs on consumer machines? Consumer GPUs can no\nlonger fit the largest available models (50B+ parameters) and must offload them\nto RAM or SSD. When running with offloaded parameters, the inference engine can\nprocess batches of hundreds or thousands of tokens at the same time as just one\ntoken, making it a natural fit for speculative decoding. We propose SpecExec\n(Speculative Execution), a simple parallel decoding method that can generate up\nto 20 tokens per target model iteration for popular LLM families. It utilizes\nthe high spikiness of the token probabilities distribution in modern LLMs and a\nhigh degree of alignment between model output probabilities. SpecExec takes the\nmost probable tokens continuation from the draft model to build a \"cache\" tree\nfor the target model, which then gets validated in a single pass. Using\nSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with\nRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens\nper second with 16-bit weights."
                },
                "authors": [
                    {
                        "name": "Ruslan Svirschevski"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Max Ryabinin"
                    }
                ],
                "author_detail": {
                    "name": "Max Ryabinin"
                },
                "author": "Max Ryabinin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02532v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02532v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00209v1",
                "updated": "2024-11-29T19:14:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    19,
                    14,
                    45,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T19:14:45Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    19,
                    14,
                    45,
                    4,
                    334,
                    0
                ],
                "title": "Digital Twin in Industries: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Twin in Industries: A Comprehensive Survey"
                },
                "summary": "Industrial networks are undergoing rapid transformation driven by the\nconvergence of emerging technologies that are revolutionizing conventional\nworkflows, enhancing operational efficiency, and fundamentally redefining the\nindustrial landscape across diverse sectors. Amidst this revolution, Digital\nTwin (DT) emerges as a transformative innovation that seamlessly integrates\nreal-world systems with their virtual counterparts, bridging the physical and\ndigital realms. In this article, we present a comprehensive survey of the\nemerging DT-enabled services and applications across industries, beginning with\nan overview of DT fundamentals and its components to a discussion of key\nenabling technologies for DT. Different from literature works, we investigate\nand analyze the capabilities of DT across a wide range of industrial services,\nincluding data sharing, data offloading, integrated sensing and communication,\ncontent caching, resource allocation, wireless networking, and metaverse. In\nparticular, we present an in-depth technical discussion of the roles of DT in\nindustrial applications across various domains, including manufacturing,\nhealthcare, transportation, energy, agriculture, space, oil and gas, as well as\nrobotics. Throughout the technical analysis, we delve into real-time data\ncommunications between physical and virtual platforms to enable industrial DT\nnetworking. Subsequently, we extensively explore and analyze a wide range of\nmajor privacy and security issues in DT-based industry. Taxonomy tables and the\nkey research findings from the survey are also given, emphasizing important\ninsights into the significance of DT in industries. Finally, we point out\nfuture research directions to spur further research in this promising area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industrial networks are undergoing rapid transformation driven by the\nconvergence of emerging technologies that are revolutionizing conventional\nworkflows, enhancing operational efficiency, and fundamentally redefining the\nindustrial landscape across diverse sectors. Amidst this revolution, Digital\nTwin (DT) emerges as a transformative innovation that seamlessly integrates\nreal-world systems with their virtual counterparts, bridging the physical and\ndigital realms. In this article, we present a comprehensive survey of the\nemerging DT-enabled services and applications across industries, beginning with\nan overview of DT fundamentals and its components to a discussion of key\nenabling technologies for DT. Different from literature works, we investigate\nand analyze the capabilities of DT across a wide range of industrial services,\nincluding data sharing, data offloading, integrated sensing and communication,\ncontent caching, resource allocation, wireless networking, and metaverse. In\nparticular, we present an in-depth technical discussion of the roles of DT in\nindustrial applications across various domains, including manufacturing,\nhealthcare, transportation, energy, agriculture, space, oil and gas, as well as\nrobotics. Throughout the technical analysis, we delve into real-time data\ncommunications between physical and virtual platforms to enable industrial DT\nnetworking. Subsequently, we extensively explore and analyze a wide range of\nmajor privacy and security issues in DT-based industry. Taxonomy tables and the\nkey research findings from the survey are also given, emphasizing important\ninsights into the significance of DT in industries. Finally, we point out\nfuture research directions to spur further research in this promising area."
                },
                "authors": [
                    {
                        "name": "Md Bokhtiar Al Zami"
                    },
                    {
                        "name": "Shaba Shaon"
                    },
                    {
                        "name": "Vu Khanh Quy"
                    },
                    {
                        "name": "Dinh C. Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Dinh C. Nguyen"
                },
                "author": "Dinh C. Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19730v1",
                "updated": "2024-11-29T14:23:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    23,
                    25,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T14:23:25Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    23,
                    25,
                    4,
                    334,
                    0
                ],
                "title": "Ten Ways in which Virtual Reality Differs from Video Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ten Ways in which Virtual Reality Differs from Video Streaming"
                },
                "summary": "Virtual Reality (VR) applications have a number of unique characteristics\nthat set them apart from traditional video streaming. These characteristics\nhave major implications on the design of VR rendering, adaptation, prefetching,\ncaching, and transport mechanisms. This paper contrasts VR to video streaming,\nstored 2D video streaming in particular, and discusses how to rethink system\nand network support for VR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual Reality (VR) applications have a number of unique characteristics\nthat set them apart from traditional video streaming. These characteristics\nhave major implications on the design of VR rendering, adaptation, prefetching,\ncaching, and transport mechanisms. This paper contrasts VR to video streaming,\nstored 2D video streaming in particular, and discusses how to rethink system\nand network support for VR."
                },
                "authors": [
                    {
                        "name": "Gustavo de Veciana"
                    },
                    {
                        "name": "Sonia Fahmy"
                    },
                    {
                        "name": "George Kesidis"
                    },
                    {
                        "name": "Voicu Popescu"
                    }
                ],
                "author_detail": {
                    "name": "Voicu Popescu"
                },
                "author": "Voicu Popescu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01852v1",
                "updated": "2024-11-29T10:21:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    21,
                    12,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T10:21:12Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    21,
                    12,
                    4,
                    334,
                    0
                ],
                "title": "Communication efficient application of sequences of planar rotations to\n  a matrix",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication efficient application of sequences of planar rotations to\n  a matrix"
                },
                "summary": "We present an efficient algorithm for the application of sequences of planar\nrotations to a matrix. Applying such sequences efficiently is important in many\nnumerical linear algebra algorithms for eigenvalues. Our algorithm is novel in\nthree main ways. First, we introduce a new kernel that is optimized for\nregister reuse in a novel way. Second, we introduce a blocking and packing\nscheme that improves the cache efficiency of the algorithm. Finally, we\nthoroughly analyze the memory operations of the algorithm which leads to\nimportant theoretical insights and makes it easier to select good parameters.\nNumerical experiments show that our algorithm outperforms the state-of-the-art\nand achieves a flop rate close to the theoretical peak on modern hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an efficient algorithm for the application of sequences of planar\nrotations to a matrix. Applying such sequences efficiently is important in many\nnumerical linear algebra algorithms for eigenvalues. Our algorithm is novel in\nthree main ways. First, we introduce a new kernel that is optimized for\nregister reuse in a novel way. Second, we introduce a blocking and packing\nscheme that improves the cache efficiency of the algorithm. Finally, we\nthoroughly analyze the memory operations of the algorithm which leads to\nimportant theoretical insights and makes it easier to select good parameters.\nNumerical experiments show that our algorithm outperforms the state-of-the-art\nand achieves a flop rate close to the theoretical peak on modern hardware."
                },
                "authors": [
                    {
                        "name": "Thijs Steel"
                    },
                    {
                        "name": "Julien Langou"
                    }
                ],
                "author_detail": {
                    "name": "Julien Langou"
                },
                "author": "Julien Langou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F15, 65Y05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07533v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07533v3",
                "updated": "2024-11-29T08:48:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    48,
                    1,
                    4,
                    334,
                    0
                ],
                "published": "2024-05-13T08:03:32Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    8,
                    3,
                    32,
                    0,
                    134,
                    0
                ],
                "title": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials"
                },
                "summary": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities."
                },
                "authors": [
                    {
                        "name": "Sandro Rodriguez Garzon"
                    },
                    {
                        "name": "Dennis Natusch"
                    },
                    {
                        "name": "Artur Philipp"
                    },
                    {
                        "name": "Axel Kpper"
                    },
                    {
                        "name": "Hans Joachim Einsiedler"
                    },
                    {
                        "name": "Daniela Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Schneider"
                },
                "author": "Daniela Schneider",
                "arxiv_comment": "Accepted by and presented at 21st Annual International Conference on\n  Privacy, Security, and Trust (PST2024). Publication by IEEE still pending",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07533v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18191v2",
                "updated": "2024-11-29T08:33:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    33,
                    49,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-27T10:14:38Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    14,
                    38,
                    2,
                    332,
                    0
                ],
                "title": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks"
                },
                "summary": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference."
                },
                "authors": [
                    {
                        "name": "Xinyao Zheng"
                    },
                    {
                        "name": "Husheng Han"
                    },
                    {
                        "name": "Shangyi Shi"
                    },
                    {
                        "name": "Qiyan Fang"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Qi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Qi Guo"
                },
                "author": "Qi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.14169v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14169v1",
                "updated": "2024-12-18T18:59:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    59,
                    53,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T18:59:53Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    59,
                    53,
                    2,
                    353,
                    0
                ],
                "title": "Autoregressive Video Generation without Vector Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Video Generation without Vector Quantization"
                },
                "summary": "This paper presents a novel approach that enables autoregressive video\ngeneration with high efficiency. We propose to reformulate the video generation\nproblem as a non-quantized autoregressive modeling of temporal frame-by-frame\nprediction and spatial set-by-set prediction. Unlike raster-scan prediction in\nprior autoregressive models or joint distribution modeling of fixed-length\ntokens in diffusion models, our approach maintains the causal property of\nGPT-style models for flexible in-context capabilities, while leveraging\nbidirectional modeling within individual frames for efficiency. With the\nproposed approach, we train a novel video autoregressive model without vector\nquantization, termed NOVA. Our results demonstrate that NOVA surpasses prior\nautoregressive video models in data efficiency, inference speed, visual\nfidelity, and video fluency, even with a much smaller model capacity, i.e.,\n0.6B parameters. NOVA also outperforms state-of-the-art image diffusion models\nin text-to-image generation tasks, with a significantly lower training cost.\nAdditionally, NOVA generalizes well across extended video durations and enables\ndiverse zero-shot applications in one unified model. Code and models are\npublicly available at https://github.com/baaivision/NOVA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach that enables autoregressive video\ngeneration with high efficiency. We propose to reformulate the video generation\nproblem as a non-quantized autoregressive modeling of temporal frame-by-frame\nprediction and spatial set-by-set prediction. Unlike raster-scan prediction in\nprior autoregressive models or joint distribution modeling of fixed-length\ntokens in diffusion models, our approach maintains the causal property of\nGPT-style models for flexible in-context capabilities, while leveraging\nbidirectional modeling within individual frames for efficiency. With the\nproposed approach, we train a novel video autoregressive model without vector\nquantization, termed NOVA. Our results demonstrate that NOVA surpasses prior\nautoregressive video models in data efficiency, inference speed, visual\nfidelity, and video fluency, even with a much smaller model capacity, i.e.,\n0.6B parameters. NOVA also outperforms state-of-the-art image diffusion models\nin text-to-image generation tasks, with a significantly lower training cost.\nAdditionally, NOVA generalizes well across extended video durations and enables\ndiverse zero-shot applications in one unified model. Code and models are\npublicly available at https://github.com/baaivision/NOVA."
                },
                "authors": [
                    {
                        "name": "Haoge Deng"
                    },
                    {
                        "name": "Ting Pan"
                    },
                    {
                        "name": "Haiwen Diao"
                    },
                    {
                        "name": "Zhengxiong Luo"
                    },
                    {
                        "name": "Yufeng Cui"
                    },
                    {
                        "name": "Huchuan Lu"
                    },
                    {
                        "name": "Shiguang Shan"
                    },
                    {
                        "name": "Yonggang Qi"
                    },
                    {
                        "name": "Xinlong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinlong Wang"
                },
                "author": "Xinlong Wang",
                "arxiv_comment": "22 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14169v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14169v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14164v1",
                "updated": "2024-12-18T18:58:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    58,
                    50,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T18:58:50Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    58,
                    50,
                    2,
                    353,
                    0
                ],
                "title": "MetaMorph: Multimodal Understanding and Generation via Instruction\n  Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaMorph: Multimodal Understanding and Generation via Instruction\n  Tuning"
                },
                "summary": "In this work, we propose Visual-Predictive Instruction Tuning (VPiT) - a\nsimple and effective extension to visual instruction tuning that enables a\npretrained LLM to quickly morph into an unified autoregressive model capable of\ngenerating both text and visual tokens. VPiT teaches an LLM to predict discrete\ntext tokens and continuous visual tokens from any input sequence of image and\ntext data curated in an instruction-following format. Our empirical\ninvestigation reveals several intriguing properties of VPiT: (1) visual\ngeneration ability emerges as a natural byproduct of improved visual\nunderstanding, and can be unlocked efficiently with a small amount of\ngeneration data; (2) while we find understanding and generation to be mutually\nbeneficial, understanding data contributes to both capabilities more\neffectively than generation data. Building upon these findings, we train our\nMetaMorph model and achieve competitive performance on both visual\nunderstanding and generation. In visual generation, MetaMorph can leverage the\nworld knowledge and reasoning abilities gained from LLM pretraining, and\novercome common failure modes exhibited by other generation models. Our results\nsuggest that LLMs may have strong \"prior\" vision capabilities that can be\nefficiently adapted to both visual understanding and generation with a\nrelatively simple instruction tuning process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose Visual-Predictive Instruction Tuning (VPiT) - a\nsimple and effective extension to visual instruction tuning that enables a\npretrained LLM to quickly morph into an unified autoregressive model capable of\ngenerating both text and visual tokens. VPiT teaches an LLM to predict discrete\ntext tokens and continuous visual tokens from any input sequence of image and\ntext data curated in an instruction-following format. Our empirical\ninvestigation reveals several intriguing properties of VPiT: (1) visual\ngeneration ability emerges as a natural byproduct of improved visual\nunderstanding, and can be unlocked efficiently with a small amount of\ngeneration data; (2) while we find understanding and generation to be mutually\nbeneficial, understanding data contributes to both capabilities more\neffectively than generation data. Building upon these findings, we train our\nMetaMorph model and achieve competitive performance on both visual\nunderstanding and generation. In visual generation, MetaMorph can leverage the\nworld knowledge and reasoning abilities gained from LLM pretraining, and\novercome common failure modes exhibited by other generation models. Our results\nsuggest that LLMs may have strong \"prior\" vision capabilities that can be\nefficiently adapted to both visual understanding and generation with a\nrelatively simple instruction tuning process."
                },
                "authors": [
                    {
                        "name": "Shengbang Tong"
                    },
                    {
                        "name": "David Fan"
                    },
                    {
                        "name": "Jiachen Zhu"
                    },
                    {
                        "name": "Yunyang Xiong"
                    },
                    {
                        "name": "Xinlei Chen"
                    },
                    {
                        "name": "Koustuv Sinha"
                    },
                    {
                        "name": "Michael Rabbat"
                    },
                    {
                        "name": "Yann LeCun"
                    },
                    {
                        "name": "Saining Xie"
                    },
                    {
                        "name": "Zhuang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhuang Liu"
                },
                "author": "Zhuang Liu",
                "arxiv_comment": "Project page at tsb0601.github.io/metamorph",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14161v1",
                "updated": "2024-12-18T18:55:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    55,
                    40,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T18:55:40Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    55,
                    40,
                    2,
                    353,
                    0
                ],
                "title": "TheAgentCompany: Benchmarking LLM Agents on Consequential Real World\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TheAgentCompany: Benchmarking LLM Agents on Consequential Real World\n  Tasks"
                },
                "summary": "We interact with computers on an everyday basis, be it in everyday life or\nwork, and many aspects of work can be done entirely with access to a computer\nand the Internet. At the same time, thanks to improvements in large language\nmodels (LLMs), there has also been a rapid development in AI agents that\ninteract with and affect change in their surrounding environments. But how\nperformant are AI agents at helping to accelerate or even autonomously perform\nwork-related tasks? The answer to this question has important implications for\nboth industry looking to adopt AI into their workflows, and for economic policy\nto understand the effects that adoption of AI may have on the labor market. To\nmeasure the progress of these LLM agents' performance on performing real-world\nprofessional tasks, in this paper, we introduce TheAgentCompany, an extensible\nbenchmark for evaluating AI agents that interact with the world in similar ways\nto those of a digital worker: by browsing the Web, writing code, running\nprograms, and communicating with other coworkers. We build a self-contained\nenvironment with internal web sites and data that mimics a small software\ncompany environment, and create a variety of tasks that may be performed by\nworkers in such a company. We test baseline agents powered by both closed\nAPI-based and open-weights language models (LMs), and find that with the most\ncompetitive agent, 24% of the tasks can be completed autonomously. This paints\na nuanced picture on task automation with LM agents -- in a setting simulating\na real workplace, a good portion of simpler tasks could be solved autonomously,\nbut more difficult long-horizon tasks are still beyond the reach of current\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We interact with computers on an everyday basis, be it in everyday life or\nwork, and many aspects of work can be done entirely with access to a computer\nand the Internet. At the same time, thanks to improvements in large language\nmodels (LLMs), there has also been a rapid development in AI agents that\ninteract with and affect change in their surrounding environments. But how\nperformant are AI agents at helping to accelerate or even autonomously perform\nwork-related tasks? The answer to this question has important implications for\nboth industry looking to adopt AI into their workflows, and for economic policy\nto understand the effects that adoption of AI may have on the labor market. To\nmeasure the progress of these LLM agents' performance on performing real-world\nprofessional tasks, in this paper, we introduce TheAgentCompany, an extensible\nbenchmark for evaluating AI agents that interact with the world in similar ways\nto those of a digital worker: by browsing the Web, writing code, running\nprograms, and communicating with other coworkers. We build a self-contained\nenvironment with internal web sites and data that mimics a small software\ncompany environment, and create a variety of tasks that may be performed by\nworkers in such a company. We test baseline agents powered by both closed\nAPI-based and open-weights language models (LMs), and find that with the most\ncompetitive agent, 24% of the tasks can be completed autonomously. This paints\na nuanced picture on task automation with LM agents -- in a setting simulating\na real workplace, a good portion of simpler tasks could be solved autonomously,\nbut more difficult long-horizon tasks are still beyond the reach of current\nsystems."
                },
                "authors": [
                    {
                        "name": "Frank F. Xu"
                    },
                    {
                        "name": "Yufan Song"
                    },
                    {
                        "name": "Boxuan Li"
                    },
                    {
                        "name": "Yuxuan Tang"
                    },
                    {
                        "name": "Kritanjali Jain"
                    },
                    {
                        "name": "Mengxue Bao"
                    },
                    {
                        "name": "Zora Z. Wang"
                    },
                    {
                        "name": "Xuhui Zhou"
                    },
                    {
                        "name": "Zhitong Guo"
                    },
                    {
                        "name": "Murong Cao"
                    },
                    {
                        "name": "Mingyang Yang"
                    },
                    {
                        "name": "Hao Yang Lu"
                    },
                    {
                        "name": "Amaad Martin"
                    },
                    {
                        "name": "Zhe Su"
                    },
                    {
                        "name": "Leander Maben"
                    },
                    {
                        "name": "Raj Mehta"
                    },
                    {
                        "name": "Wayne Chi"
                    },
                    {
                        "name": "Lawrence Jang"
                    },
                    {
                        "name": "Yiqing Xie"
                    },
                    {
                        "name": "Shuyan Zhou"
                    },
                    {
                        "name": "Graham Neubig"
                    }
                ],
                "author_detail": {
                    "name": "Graham Neubig"
                },
                "author": "Graham Neubig",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14159v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14159v1",
                "updated": "2024-12-18T18:53:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    53,
                    27,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T18:53:27Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    53,
                    27,
                    2,
                    353,
                    0
                ],
                "title": "Measuring the Transverse Velocity of Strongly Lensed Gravitational Wave\n  Sources with Ground Based Detectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring the Transverse Velocity of Strongly Lensed Gravitational Wave\n  Sources with Ground Based Detectors"
                },
                "summary": "Observations of strongly gravitationally lensed gravitational wave (GW)\nsources provide a unique opportunity for constraining their transverse motion,\nwhich otherwise is exceedingly hard for GW mergers in general. Strong lensing\nmakes this possible when two or more images of the lensed GW source are\nobserved, as each image essentially allows the observer to see the GW source\nfrom different directional lines-of-sight. If the GW source is moving relative\nto the lens and observer, the observed GW signal from one image will therefore\ngenerally appear blue- or redshifted compared to GW signal from the other\nimage. This velocity induced differential Doppler shift gives rise to an\nobservable GW phase shift between the GW signals from the different images,\nwhich provides a rare glimpse into the relative motion of GW sources and their\nhost environment across redshift. We illustrate that detecting such GW phase\nshifts is within reach of next-generation ground-based detectors such as\nEinstein Telescope, that is expected to detect $\\sim$hundreds of lensed GW\nmergers per year. This opens up completely new ways of inferring the\nenvironment of GW sources, as well as studying cosmological velocity flows\nacross redshift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observations of strongly gravitationally lensed gravitational wave (GW)\nsources provide a unique opportunity for constraining their transverse motion,\nwhich otherwise is exceedingly hard for GW mergers in general. Strong lensing\nmakes this possible when two or more images of the lensed GW source are\nobserved, as each image essentially allows the observer to see the GW source\nfrom different directional lines-of-sight. If the GW source is moving relative\nto the lens and observer, the observed GW signal from one image will therefore\ngenerally appear blue- or redshifted compared to GW signal from the other\nimage. This velocity induced differential Doppler shift gives rise to an\nobservable GW phase shift between the GW signals from the different images,\nwhich provides a rare glimpse into the relative motion of GW sources and their\nhost environment across redshift. We illustrate that detecting such GW phase\nshifts is within reach of next-generation ground-based detectors such as\nEinstein Telescope, that is expected to detect $\\sim$hundreds of lensed GW\nmergers per year. This opens up completely new ways of inferring the\nenvironment of GW sources, as well as studying cosmological velocity flows\nacross redshift."
                },
                "authors": [
                    {
                        "name": "Johan Samsing"
                    },
                    {
                        "name": "Lorenz Zwick"
                    },
                    {
                        "name": "Pankaj Saini"
                    },
                    {
                        "name": "Daniel J. D'Orazio"
                    },
                    {
                        "name": "Kai Hendriks"
                    },
                    {
                        "name": "Jose Mara Ezquiaga"
                    },
                    {
                        "name": "Rico K. L. Lo"
                    },
                    {
                        "name": "Luka Vujeva"
                    },
                    {
                        "name": "Georgi D. Radev"
                    },
                    {
                        "name": "Yan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yan Yu"
                },
                "author": "Yan Yu",
                "arxiv_comment": "comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14159v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14146v1",
                "updated": "2024-12-18T18:44:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    44,
                    8,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T18:44:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    44,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Advanced Reasoning and Transformation Engine for Multi-Step Insight\n  Synthesis in Data Analytics with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Reasoning and Transformation Engine for Multi-Step Insight\n  Synthesis in Data Analytics with Large Language Models"
                },
                "summary": "This paper presents the Advanced Reasoning and Transformation Engine for\nMulti-Step Insight Synthesis in Data Analytics (ARTEMIS-DA), a novel framework\ndesigned to augment Large Language Models (LLMs) for solving complex,\nmulti-step data analytics tasks. ARTEMIS-DA integrates three core components:\nthe Planner, which dissects complex user queries into structured, sequential\ninstructions encompassing data preprocessing, transformation, predictive\nmodeling, and visualization; the Coder, which dynamically generates and\nexecutes Python code to implement these instructions; and the Grapher, which\ninterprets generated visualizations to derive actionable insights. By\norchestrating the collaboration between these components, ARTEMIS-DA\neffectively manages sophisticated analytical workflows involving advanced\nreasoning, multi-step transformations, and synthesis across diverse data\nmodalities. The framework achieves state-of-the-art (SOTA) performance on\nbenchmarks such as WikiTableQuestions and TabFact, demonstrating its ability to\ntackle intricate analytical tasks with precision and adaptability. By combining\nthe reasoning capabilities of LLMs with automated code generation and execution\nand visual analysis, ARTEMIS-DA offers a robust, scalable solution for\nmulti-step insight synthesis, addressing a wide range of challenges in data\nanalytics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the Advanced Reasoning and Transformation Engine for\nMulti-Step Insight Synthesis in Data Analytics (ARTEMIS-DA), a novel framework\ndesigned to augment Large Language Models (LLMs) for solving complex,\nmulti-step data analytics tasks. ARTEMIS-DA integrates three core components:\nthe Planner, which dissects complex user queries into structured, sequential\ninstructions encompassing data preprocessing, transformation, predictive\nmodeling, and visualization; the Coder, which dynamically generates and\nexecutes Python code to implement these instructions; and the Grapher, which\ninterprets generated visualizations to derive actionable insights. By\norchestrating the collaboration between these components, ARTEMIS-DA\neffectively manages sophisticated analytical workflows involving advanced\nreasoning, multi-step transformations, and synthesis across diverse data\nmodalities. The framework achieves state-of-the-art (SOTA) performance on\nbenchmarks such as WikiTableQuestions and TabFact, demonstrating its ability to\ntackle intricate analytical tasks with precision and adaptability. By combining\nthe reasoning capabilities of LLMs with automated code generation and execution\nand visual analysis, ARTEMIS-DA offers a robust, scalable solution for\nmulti-step insight synthesis, addressing a wide range of challenges in data\nanalytics."
                },
                "authors": [
                    {
                        "name": "Atin Sakkeer Hussain"
                    }
                ],
                "author_detail": {
                    "name": "Atin Sakkeer Hussain"
                },
                "author": "Atin Sakkeer Hussain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.07019v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.07019v3",
                "updated": "2024-12-18T18:41:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    41,
                    41,
                    2,
                    353,
                    0
                ],
                "published": "2023-10-10T21:12:15Z",
                "published_parsed": [
                    2023,
                    10,
                    10,
                    21,
                    12,
                    15,
                    1,
                    283,
                    0
                ],
                "title": "Case Law Grounding: Using Precedents to Align Decision-Making for Humans\n  and AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Case Law Grounding: Using Precedents to Align Decision-Making for Humans\n  and AI"
                },
                "summary": "Communities and groups often need to make decisions grounded by social norms\nand preferences, such as when moderating content or providing judgments for\naligning AI systems. Prevailing approaches to provide this grounding have\nprimarily centered around constructing high-level guidelines and criteria,\nsimilar to legal ``constitutions''. However, it can be challenging to specify\nsocial norms and preferences consistently and accurately through constitutions\nalone. In this work, we take inspiration from legal systems and introduce\n``case law grounding'' (CLG) -- a novel approach for grounding decision-making\nthat uses past cases and decisions (precedents) to ground future decisions in a\nway that can be utilized by human-led processes or implemented through\nprompting large language models (LLMs). We evaluate how accurately CLG grounds\ndecisions with five groups and communities spread across two decision task\ndomains, comparing against a traditional constitutional grounding approach, and\nfind that in 4 out of 5 groups, decisions produced with CLG were significantly\nmore accurately aligned to ground truth: 16.0--23.3 %-points higher accuracy\nusing the human-led process, and 20.8--32.9 %-points higher when prompting\nLLMs. We also evaluate the impact of different configurations of CLG, such as\nthe case retrieval window size and whether to enforce binding decisions based\non selected precedents, showing support for using binding decisions and\npreferring larger retrieval windows. Finally, we discuss the limitations of our\ncase-based approach as well as how it may be best used to augment existing\nconstitutional approaches when it comes to aligning human and AI decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communities and groups often need to make decisions grounded by social norms\nand preferences, such as when moderating content or providing judgments for\naligning AI systems. Prevailing approaches to provide this grounding have\nprimarily centered around constructing high-level guidelines and criteria,\nsimilar to legal ``constitutions''. However, it can be challenging to specify\nsocial norms and preferences consistently and accurately through constitutions\nalone. In this work, we take inspiration from legal systems and introduce\n``case law grounding'' (CLG) -- a novel approach for grounding decision-making\nthat uses past cases and decisions (precedents) to ground future decisions in a\nway that can be utilized by human-led processes or implemented through\nprompting large language models (LLMs). We evaluate how accurately CLG grounds\ndecisions with five groups and communities spread across two decision task\ndomains, comparing against a traditional constitutional grounding approach, and\nfind that in 4 out of 5 groups, decisions produced with CLG were significantly\nmore accurately aligned to ground truth: 16.0--23.3 %-points higher accuracy\nusing the human-led process, and 20.8--32.9 %-points higher when prompting\nLLMs. We also evaluate the impact of different configurations of CLG, such as\nthe case retrieval window size and whether to enforce binding decisions based\non selected precedents, showing support for using binding decisions and\npreferring larger retrieval windows. Finally, we discuss the limitations of our\ncase-based approach as well as how it may be best used to augment existing\nconstitutional approaches when it comes to aligning human and AI decisions."
                },
                "authors": [
                    {
                        "name": "Quan Ze Chen"
                    },
                    {
                        "name": "Amy X. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Amy X. Zhang"
                },
                "author": "Amy X. Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.07019v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.07019v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14141v1",
                "updated": "2024-12-18T18:41:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    41,
                    14,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T18:41:14Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    41,
                    14,
                    2,
                    353,
                    0
                ],
                "title": "LLMs can realize combinatorial creativity: generating creative ideas via\n  LLMs for scientific research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs can realize combinatorial creativity: generating creative ideas via\n  LLMs for scientific research"
                },
                "summary": "Scientific idea generation has been extensively studied in creativity theory\nand computational creativity research, providing valuable frameworks for\nunderstanding and implementing creative processes. However, recent work using\nLarge Language Models (LLMs) for research idea generation often overlooks these\ntheoretical foundations. We present a framework that explicitly implements\ncombinatorial creativity theory using LLMs, featuring a generalization-level\nretrieval system for cross-domain knowledge discovery and a structured\ncombinatorial process for idea generation. The retrieval system maps concepts\nacross different abstraction levels to enable meaningful connections between\ndisparate domains, while the combinatorial process systematically analyzes and\nrecombines components to generate novel solutions. Experiments on the OAG-Bench\ndataset demonstrate our framework's effectiveness, consistently outperforming\nbaseline approaches in generating ideas that align with real research\ndevelopments (improving similarity scores by 7\\%-10\\% across multiple metrics).\nOur results provide strong evidence that LLMs can effectively realize\ncombinatorial creativity when guided by appropriate theoretical frameworks,\ncontributing both to practical advancement of AI-assisted research and\ntheoretical understanding of machine creativity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific idea generation has been extensively studied in creativity theory\nand computational creativity research, providing valuable frameworks for\nunderstanding and implementing creative processes. However, recent work using\nLarge Language Models (LLMs) for research idea generation often overlooks these\ntheoretical foundations. We present a framework that explicitly implements\ncombinatorial creativity theory using LLMs, featuring a generalization-level\nretrieval system for cross-domain knowledge discovery and a structured\ncombinatorial process for idea generation. The retrieval system maps concepts\nacross different abstraction levels to enable meaningful connections between\ndisparate domains, while the combinatorial process systematically analyzes and\nrecombines components to generate novel solutions. Experiments on the OAG-Bench\ndataset demonstrate our framework's effectiveness, consistently outperforming\nbaseline approaches in generating ideas that align with real research\ndevelopments (improving similarity scores by 7\\%-10\\% across multiple metrics).\nOur results provide strong evidence that LLMs can effectively realize\ncombinatorial creativity when guided by appropriate theoretical frameworks,\ncontributing both to practical advancement of AI-assisted research and\ntheoretical understanding of machine creativity."
                },
                "authors": [
                    {
                        "name": "Tianyang Gu"
                    },
                    {
                        "name": "Jingjin Wang"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "HaoHong Li"
                    }
                ],
                "author_detail": {
                    "name": "HaoHong Li"
                },
                "author": "HaoHong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14140v1",
                "updated": "2024-12-18T18:41:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    41,
                    12,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T18:41:12Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    41,
                    12,
                    2,
                    353,
                    0
                ],
                "title": "GLIDER: Grading LLM Interactions and Decisions using Explainable Ranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLIDER: Grading LLM Interactions and Decisions using Explainable Ranking"
                },
                "summary": "The LLM-as-judge paradigm is increasingly being adopted for automated\nevaluation of model outputs. While LLM judges have shown promise on constrained\nevaluation tasks, closed source LLMs display critical shortcomings when\ndeployed in real world applications due to challenges of fine grained metrics\nand explainability, while task specific evaluation models lack cross-domain\ngeneralization. We introduce GLIDER, a powerful 3B evaluator LLM that can score\nany text input and associated context on arbitrary user defined criteria.\nGLIDER shows higher Pearson's correlation than GPT-4o on FLASK and greatly\noutperforms prior evaluation models, achieving comparable performance to LLMs\n17x its size. GLIDER supports fine-grained scoring, multilingual reasoning,\nspan highlighting and was trained on 685 domains and 183 criteria. Extensive\nqualitative analysis shows that GLIDER scores are highly correlated with human\njudgments, with 91.3% human agreement. We have open-sourced GLIDER to\nfacilitate future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The LLM-as-judge paradigm is increasingly being adopted for automated\nevaluation of model outputs. While LLM judges have shown promise on constrained\nevaluation tasks, closed source LLMs display critical shortcomings when\ndeployed in real world applications due to challenges of fine grained metrics\nand explainability, while task specific evaluation models lack cross-domain\ngeneralization. We introduce GLIDER, a powerful 3B evaluator LLM that can score\nany text input and associated context on arbitrary user defined criteria.\nGLIDER shows higher Pearson's correlation than GPT-4o on FLASK and greatly\noutperforms prior evaluation models, achieving comparable performance to LLMs\n17x its size. GLIDER supports fine-grained scoring, multilingual reasoning,\nspan highlighting and was trained on 685 domains and 183 criteria. Extensive\nqualitative analysis shows that GLIDER scores are highly correlated with human\njudgments, with 91.3% human agreement. We have open-sourced GLIDER to\nfacilitate future research."
                },
                "authors": [
                    {
                        "name": "Darshan Deshpande"
                    },
                    {
                        "name": "Selvan Sunitha Ravi"
                    },
                    {
                        "name": "Sky CH-Wang"
                    },
                    {
                        "name": "Bartosz Mielczarek"
                    },
                    {
                        "name": "Anand Kannappan"
                    },
                    {
                        "name": "Rebecca Qian"
                    }
                ],
                "author_detail": {
                    "name": "Rebecca Qian"
                },
                "author": "Rebecca Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14137v1",
                "updated": "2024-12-18T18:33:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    33,
                    26,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T18:33:26Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    33,
                    26,
                    2,
                    353,
                    0
                ],
                "title": "Design choices made by LLM-based test generators prevent them from\n  finding bugs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design choices made by LLM-based test generators prevent them from\n  finding bugs"
                },
                "summary": "There is an increasing amount of research and commercial tools for automated\ntest case generation using Large Language Models (LLMs). This paper critically\nexamines whether recent LLM-based test generation tools, such as Codium\nCoverAgent and CoverUp, can effectively find bugs or unintentionally validate\nfaulty code. Considering bugs are only exposed by failing test cases, we\nexplore the question: can these tools truly achieve the intended objectives of\nsoftware testing when their test oracles are designed to pass? Using real\nhuman-written buggy code as input, we evaluate these tools, showing how\nLLM-generated tests can fail to detect bugs and, more alarmingly, how their\ndesign can worsen the situation by validating bugs in the generated test suite\nand rejecting bug-revealing tests. These findings raise important questions\nabout the validity of the design behind LLM-based test generation tools and\ntheir impact on software quality and test suite reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is an increasing amount of research and commercial tools for automated\ntest case generation using Large Language Models (LLMs). This paper critically\nexamines whether recent LLM-based test generation tools, such as Codium\nCoverAgent and CoverUp, can effectively find bugs or unintentionally validate\nfaulty code. Considering bugs are only exposed by failing test cases, we\nexplore the question: can these tools truly achieve the intended objectives of\nsoftware testing when their test oracles are designed to pass? Using real\nhuman-written buggy code as input, we evaluate these tools, showing how\nLLM-generated tests can fail to detect bugs and, more alarmingly, how their\ndesign can worsen the situation by validating bugs in the generated test suite\nand rejecting bug-revealing tests. These findings raise important questions\nabout the validity of the design behind LLM-based test generation tools and\ntheir impact on software quality and test suite reliability."
                },
                "authors": [
                    {
                        "name": "Noble Saji Mathews"
                    },
                    {
                        "name": "Meiyappan Nagappan"
                    }
                ],
                "author_detail": {
                    "name": "Meiyappan Nagappan"
                },
                "author": "Meiyappan Nagappan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14135v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14135v1",
                "updated": "2024-12-18T18:24:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    24,
                    47,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T18:24:47Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    24,
                    47,
                    2,
                    353,
                    0
                ],
                "title": "Scaling of Search and Learning: A Roadmap to Reproduce o1 from\n  Reinforcement Learning Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling of Search and Learning: A Roadmap to Reproduce o1 from\n  Reinforcement Learning Perspective"
                },
                "summary": "OpenAI o1 represents a significant milestone in Artificial Inteiligence,\nwhich achieves expert-level performances on many challanging tasks that require\nstrong reasoning ability.OpenAI has claimed that the main techinique behinds o1\nis the reinforcement learining. Recent works use alternative approaches like\nknowledge distillation to imitate o1's reasoning style, but their effectiveness\nis limited by the capability ceiling of the teacher model. Therefore, this\npaper analyzes the roadmap to achieving o1 from the perspective of\nreinforcement learning, focusing on four key components: policy initialization,\nreward design, search, and learning. Policy initialization enables models to\ndevelop human-like reasoning behaviors, equipping them with the ability to\neffectively explore solution spaces for complex problems. Reward design\nprovides dense and effective signals via reward shaping or reward modeling,\nwhich is the guidance for both search and learning. Search plays a crucial role\nin generating high-quality solutions during both training and testing phases,\nwhich can produce better solutions with more computation. Learning utilizes the\ndata generated by search for improving policy, which can achieve the better\nperformance with more parameters and more searched data. Existing open-source\nprojects that attempt to reproduce o1 can be seem as a part or a variant of our\nroadmap. Collectively, these components underscore how learning and search\ndrive o1's advancement, making meaningful contributions to the development of\nLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenAI o1 represents a significant milestone in Artificial Inteiligence,\nwhich achieves expert-level performances on many challanging tasks that require\nstrong reasoning ability.OpenAI has claimed that the main techinique behinds o1\nis the reinforcement learining. Recent works use alternative approaches like\nknowledge distillation to imitate o1's reasoning style, but their effectiveness\nis limited by the capability ceiling of the teacher model. Therefore, this\npaper analyzes the roadmap to achieving o1 from the perspective of\nreinforcement learning, focusing on four key components: policy initialization,\nreward design, search, and learning. Policy initialization enables models to\ndevelop human-like reasoning behaviors, equipping them with the ability to\neffectively explore solution spaces for complex problems. Reward design\nprovides dense and effective signals via reward shaping or reward modeling,\nwhich is the guidance for both search and learning. Search plays a crucial role\nin generating high-quality solutions during both training and testing phases,\nwhich can produce better solutions with more computation. Learning utilizes the\ndata generated by search for improving policy, which can achieve the better\nperformance with more parameters and more searched data. Existing open-source\nprojects that attempt to reproduce o1 can be seem as a part or a variant of our\nroadmap. Collectively, these components underscore how learning and search\ndrive o1's advancement, making meaningful contributions to the development of\nLLM."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Zeng"
                    },
                    {
                        "name": "Qinyuan Cheng"
                    },
                    {
                        "name": "Zhangyue Yin"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Shimin Li"
                    },
                    {
                        "name": "Yunhua Zhou"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14135v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14135v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11780v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11780v2",
                "updated": "2024-12-18T18:21:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    21,
                    53,
                    2,
                    353,
                    0
                ],
                "published": "2024-07-16T14:37:33Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    14,
                    37,
                    33,
                    1,
                    198,
                    0
                ],
                "title": "SwitchCIT: Switching for Continual Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwitchCIT: Switching for Continual Instruction Tuning"
                },
                "summary": "Large language models (LLMs) and multimodal models (MMs) have exhibited\nimpressive capabilities in various domains, particularly in general language\nunderstanding and visual reasoning. However, these models, trained on massive\ndata, may not be finely optimized for specific tasks triggered by instructions.\nContinual instruction tuning is crucial to adapt a large model to evolving\ntasks and domains, ensuring their effectiveness and relevance across a wide\nrange of applications. In the context of continual instruction tuning, where\nmodels are sequentially trained on different tasks, catastrophic forgetting can\noccur, leading to performance degradation on previously learned tasks. This\nwork addresses the catastrophic forgetting in continual instruction learning\nthrough a switching mechanism for routing computations to parameter-efficient\ntuned models. We demonstrate the effectiveness of our method through\nexperiments on continual instruction tuning of different natural language\ngeneration tasks and vision-language tasks. We also showcase the advantages of\nour proposed method in terms of efficiency, scalability, portability, and\nprivacy preservation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) and multimodal models (MMs) have exhibited\nimpressive capabilities in various domains, particularly in general language\nunderstanding and visual reasoning. However, these models, trained on massive\ndata, may not be finely optimized for specific tasks triggered by instructions.\nContinual instruction tuning is crucial to adapt a large model to evolving\ntasks and domains, ensuring their effectiveness and relevance across a wide\nrange of applications. In the context of continual instruction tuning, where\nmodels are sequentially trained on different tasks, catastrophic forgetting can\noccur, leading to performance degradation on previously learned tasks. This\nwork addresses the catastrophic forgetting in continual instruction learning\nthrough a switching mechanism for routing computations to parameter-efficient\ntuned models. We demonstrate the effectiveness of our method through\nexperiments on continual instruction tuning of different natural language\ngeneration tasks and vision-language tasks. We also showcase the advantages of\nour proposed method in terms of efficiency, scalability, portability, and\nprivacy preservation."
                },
                "authors": [
                    {
                        "name": "Xinbo Wu"
                    },
                    {
                        "name": "Max Hartman"
                    },
                    {
                        "name": "Vidhata Arjun Jayaraman"
                    },
                    {
                        "name": "Lav R. Varshney"
                    }
                ],
                "author_detail": {
                    "name": "Lav R. Varshney"
                },
                "author": "Lav R. Varshney",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11780v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11780v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14129v1",
                "updated": "2024-12-18T18:15:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    15,
                    53,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T18:15:53Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    15,
                    53,
                    2,
                    353,
                    0
                ],
                "title": "Model-free Approach to Evaluate a Censored Intermediate Outcome as a\n  Surrogate for Overall Survival",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-free Approach to Evaluate a Censored Intermediate Outcome as a\n  Surrogate for Overall Survival"
                },
                "summary": "Clinical trials or studies oftentimes require long-term and/or costly\nfollow-up of participants to evaluate a novel treatment/drug/vaccine. There has\nbeen increasing interest in the past few decades in using short-term surrogate\noutcomes as a replacement of the primary outcome i.e., in using the surrogate\noutcome, which can potentially be observed sooner, to make inference about the\ntreatment effect on the long-term primary outcome. Very few of the available\nstatistical methods to evaluate a surrogate are applicable to settings where\nboth the surrogate and the primary outcome are time-to-event outcomes subject\nto censoring. Methods that can handle this setting tend to require parametric\nassumptions or be limited to assessing only the restricted mean survival time.\nIn this paper, we propose a non-parametric approach to evaluate a censored\nsurrogate outcome, such as time to progression, when the primary outcome is\nalso a censored time-to-event outcome, such as time to death, and the treatment\neffect of interest is the difference in overall survival. Specifically, we\ndefine the proportion of the treatment effect on the primary outcome that is\nexplained (PTE) by the censored surrogate outcome in this context, and estimate\nthis proportion by defining and deriving an optimal transformation of the\nsurrogate information. Our approach provides the added advantage of relaxed\nassumptions to guarantee that the true PTE is within (0,1), along with being\nmodel-free. Finite sample performance of our estimators are illustrated via\nextensive simulation studies and a real data application examining\nprogression-free survival as a surrogate for overall survival for patients with\nmetastatic colorectal cancer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical trials or studies oftentimes require long-term and/or costly\nfollow-up of participants to evaluate a novel treatment/drug/vaccine. There has\nbeen increasing interest in the past few decades in using short-term surrogate\noutcomes as a replacement of the primary outcome i.e., in using the surrogate\noutcome, which can potentially be observed sooner, to make inference about the\ntreatment effect on the long-term primary outcome. Very few of the available\nstatistical methods to evaluate a surrogate are applicable to settings where\nboth the surrogate and the primary outcome are time-to-event outcomes subject\nto censoring. Methods that can handle this setting tend to require parametric\nassumptions or be limited to assessing only the restricted mean survival time.\nIn this paper, we propose a non-parametric approach to evaluate a censored\nsurrogate outcome, such as time to progression, when the primary outcome is\nalso a censored time-to-event outcome, such as time to death, and the treatment\neffect of interest is the difference in overall survival. Specifically, we\ndefine the proportion of the treatment effect on the primary outcome that is\nexplained (PTE) by the censored surrogate outcome in this context, and estimate\nthis proportion by defining and deriving an optimal transformation of the\nsurrogate information. Our approach provides the added advantage of relaxed\nassumptions to guarantee that the true PTE is within (0,1), along with being\nmodel-free. Finite sample performance of our estimators are illustrated via\nextensive simulation studies and a real data application examining\nprogression-free survival as a surrogate for overall survival for patients with\nmetastatic colorectal cancer."
                },
                "authors": [
                    {
                        "name": "Xuan Wang"
                    },
                    {
                        "name": "Tianxi Cai"
                    },
                    {
                        "name": "Lu Tian"
                    },
                    {
                        "name": "Layla Parast"
                    }
                ],
                "author_detail": {
                    "name": "Layla Parast"
                },
                "author": "Layla Parast",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10178v2",
                "updated": "2024-12-18T18:05:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    5,
                    43,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-13T14:50:26Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    50,
                    26,
                    4,
                    348,
                    0
                ],
                "title": "SwiftTry: Fast and Consistent Video Virtual Try-On with Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftTry: Fast and Consistent Video Virtual Try-On with Diffusion Models"
                },
                "summary": "Given an input video of a person and a new garment, the objective of this\npaper is to synthesize a new video where the person is wearing the specified\ngarment while maintaining spatiotemporal consistency. Although significant\nadvances have been made in image-based virtual try-on, extending these\nsuccesses to video often leads to frame-to-frame inconsistencies. Some\napproaches have attempted to address this by increasing the overlap of frames\nacross multiple video chunks, but this comes at a steep computational cost due\nto the repeated processing of the same frames, especially for long video\nsequences. To tackle these challenges, we reconceptualize video virtual try-on\nas a conditional video inpainting task, with garments serving as input\nconditions. Specifically, our approach enhances image diffusion models by\nincorporating temporal attention layers to improve temporal coherence. To\nreduce computational overhead, we propose ShiftCaching, a novel technique that\nmaintains temporal consistency while minimizing redundant computations.\nFurthermore, we introduce the TikTokDress dataset, a new video try-on dataset\nfeaturing more complex backgrounds, challenging movements, and higher\nresolution compared to existing public datasets. Extensive experiments\ndemonstrate that our approach outperforms current baselines, particularly in\nterms of video consistency and inference speed. The project page is available\nat https://swift-try.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given an input video of a person and a new garment, the objective of this\npaper is to synthesize a new video where the person is wearing the specified\ngarment while maintaining spatiotemporal consistency. Although significant\nadvances have been made in image-based virtual try-on, extending these\nsuccesses to video often leads to frame-to-frame inconsistencies. Some\napproaches have attempted to address this by increasing the overlap of frames\nacross multiple video chunks, but this comes at a steep computational cost due\nto the repeated processing of the same frames, especially for long video\nsequences. To tackle these challenges, we reconceptualize video virtual try-on\nas a conditional video inpainting task, with garments serving as input\nconditions. Specifically, our approach enhances image diffusion models by\nincorporating temporal attention layers to improve temporal coherence. To\nreduce computational overhead, we propose ShiftCaching, a novel technique that\nmaintains temporal consistency while minimizing redundant computations.\nFurthermore, we introduce the TikTokDress dataset, a new video try-on dataset\nfeaturing more complex backgrounds, challenging movements, and higher\nresolution compared to existing public datasets. Extensive experiments\ndemonstrate that our approach outperforms current baselines, particularly in\nterms of video consistency and inference speed. The project page is available\nat https://swift-try.github.io/."
                },
                "authors": [
                    {
                        "name": "Hung Nguyen"
                    },
                    {
                        "name": "Quang Qui-Vinh Nguyen"
                    },
                    {
                        "name": "Khoi Nguyen"
                    },
                    {
                        "name": "Rang Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Rang Nguyen"
                },
                "author": "Rang Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14117v1",
                "updated": "2024-12-18T18:04:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    4,
                    4,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T18:04:04Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    4,
                    4,
                    2,
                    353,
                    0
                ],
                "title": "High-purity quantum optomechanics at room temperature",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-purity quantum optomechanics at room temperature"
                },
                "summary": "Exploiting quantum effects of mechanical motion, such as backaction evading\nmeasurements or squeezing, requires preparation of the oscillator in a\nhigh-purity state. The largest state purities in optomechanics to date have\nrelied on cryogenic cooling, combined with coupling to electromagnetic\nresonators driven with a coherent radiation field. In this work, we cool the\nmega-hertz-frequency librational mode of an optically levitated silica\nnanoparticle from room temperature to its quantum ground state. Cooling is\nrealized by coherent scattering into a Fabry-Perot cavity. We use sideband\nthermometry to infer a phonon population of 0.04 quanta under optimal\nconditions, corresponding to a state purity of 92%. The purity reached by our\nroom-temperature experiment exceeds the performance offered by mechanically\nclamped oscillators in a cryogenic environment. Our work establishes a platform\nfor high-purity quantum optomechanics at room temperature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting quantum effects of mechanical motion, such as backaction evading\nmeasurements or squeezing, requires preparation of the oscillator in a\nhigh-purity state. The largest state purities in optomechanics to date have\nrelied on cryogenic cooling, combined with coupling to electromagnetic\nresonators driven with a coherent radiation field. In this work, we cool the\nmega-hertz-frequency librational mode of an optically levitated silica\nnanoparticle from room temperature to its quantum ground state. Cooling is\nrealized by coherent scattering into a Fabry-Perot cavity. We use sideband\nthermometry to infer a phonon population of 0.04 quanta under optimal\nconditions, corresponding to a state purity of 92%. The purity reached by our\nroom-temperature experiment exceeds the performance offered by mechanically\nclamped oscillators in a cryogenic environment. Our work establishes a platform\nfor high-purity quantum optomechanics at room temperature."
                },
                "authors": [
                    {
                        "name": "Lorenzo Dania"
                    },
                    {
                        "name": "Oscar Schmitt Kremer"
                    },
                    {
                        "name": "Johannes Piotrowski"
                    },
                    {
                        "name": "Davide Candoli"
                    },
                    {
                        "name": "Jayadev Vijayan"
                    },
                    {
                        "name": "Oriol Romero-Isart"
                    },
                    {
                        "name": "Carlos Gonzalez-Ballestero"
                    },
                    {
                        "name": "Lukas Novotny"
                    },
                    {
                        "name": "Martin Frimmer"
                    }
                ],
                "author_detail": {
                    "name": "Martin Frimmer"
                },
                "author": "Martin Frimmer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07675v2",
                "updated": "2024-12-18T17:54:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    54,
                    30,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-10T17:02:58Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    2,
                    58,
                    1,
                    345,
                    0
                ],
                "title": "RAZOR: Sharpening Knowledge by Cutting Bias with Unsupervised Text\n  Rewriting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAZOR: Sharpening Knowledge by Cutting Bias with Unsupervised Text\n  Rewriting"
                },
                "summary": "Despite the widespread use of LLMs due to their superior performance in\nvarious tasks, their high computational costs often lead potential users to opt\nfor the pretraining-finetuning pipeline. However, biases prevalent in manually\nconstructed datasets can introduce spurious correlations between tokens and\nlabels, creating so-called shortcuts and hindering the generalizability of\nfine-tuned models. Existing debiasing methods often rely on prior knowledge of\nspecific dataset biases, which is challenging to acquire a priori. We propose\nRAZOR (Rewriting And Zero-bias Optimization Refinement), a novel, unsupervised,\nand data-focused debiasing approach based on text rewriting for shortcut\nmitigation. RAZOR leverages LLMs to iteratively rewrite potentially biased text\nsegments by replacing them with heuristically selected alternatives in a\nshortcut space defined by token statistics and positional information. This\nprocess aims to align surface-level text features more closely with diverse\nlabel distributions, thereby promoting the learning of genuine linguistic\npatterns. Compared with unsupervised SoTA models, RAZOR improves by 3.5% on the\nFEVER and 6.5% on MNLI and SNLI datasets according to the F1 score.\nAdditionally, RAZOR effectively mitigates specific known biases, reducing\nbias-related terms by x2 without requiring prior bias information, a result\nthat is on par with SoTA models that leverage prior information. Our work\nprioritizes data manipulation over architectural modifications, emphasizing the\npivotal role of data quality in enhancing model performance and fairness. This\nresearch contributes to developing more robust evaluation benchmarks for\ndebiasing methods by incorporating metrics for bias reduction and overall model\nefficacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the widespread use of LLMs due to their superior performance in\nvarious tasks, their high computational costs often lead potential users to opt\nfor the pretraining-finetuning pipeline. However, biases prevalent in manually\nconstructed datasets can introduce spurious correlations between tokens and\nlabels, creating so-called shortcuts and hindering the generalizability of\nfine-tuned models. Existing debiasing methods often rely on prior knowledge of\nspecific dataset biases, which is challenging to acquire a priori. We propose\nRAZOR (Rewriting And Zero-bias Optimization Refinement), a novel, unsupervised,\nand data-focused debiasing approach based on text rewriting for shortcut\nmitigation. RAZOR leverages LLMs to iteratively rewrite potentially biased text\nsegments by replacing them with heuristically selected alternatives in a\nshortcut space defined by token statistics and positional information. This\nprocess aims to align surface-level text features more closely with diverse\nlabel distributions, thereby promoting the learning of genuine linguistic\npatterns. Compared with unsupervised SoTA models, RAZOR improves by 3.5% on the\nFEVER and 6.5% on MNLI and SNLI datasets according to the F1 score.\nAdditionally, RAZOR effectively mitigates specific known biases, reducing\nbias-related terms by x2 without requiring prior bias information, a result\nthat is on par with SoTA models that leverage prior information. Our work\nprioritizes data manipulation over architectural modifications, emphasizing the\npivotal role of data quality in enhancing model performance and fairness. This\nresearch contributes to developing more robust evaluation benchmarks for\ndebiasing methods by incorporating metrics for bias reduction and overall model\nefficacy."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Bardh Prenkaj"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Gjergji Kasneci"
                },
                "author": "Gjergji Kasneci",
                "arxiv_comment": "Shuo and Bardh contributed equally. Accepted to AAAI'25, Paper #17117",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17284v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17284v3",
                "updated": "2024-12-18T17:51:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    51,
                    52,
                    2,
                    353,
                    0
                ],
                "published": "2024-11-26T10:13:39Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    10,
                    13,
                    39,
                    1,
                    331,
                    0
                ],
                "title": "Using Large Language Models for Expert Prior Elicitation in Predictive\n  Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Large Language Models for Expert Prior Elicitation in Predictive\n  Modelling"
                },
                "summary": "Large language models (LLMs), trained on diverse data effectively acquire a\nbreadth of information across various domains. However, their computational\ncomplexity, cost, and lack of transparency hinder their direct application for\nspecialised tasks. In fields such as clinical research, acquiring expert\nannotations or prior knowledge about predictive models is often costly and\ntime-consuming. This study proposes the use of LLMs to elicit expert prior\ndistributions for predictive models. This approach also provides an alternative\nto in-context learning, where language models are tasked with making\npredictions directly. In this work, we compare LLM-elicited and uninformative\npriors, evaluate whether LLMs truthfully generate parameter distributions, and\npropose a model selection strategy for in-context learning and prior\nelicitation. Our findings show that LLM-elicited prior parameter distributions\nsignificantly reduce predictive error compared to uninformative priors in\nlow-data settings. Applied to clinical problems, this translates to fewer\nrequired biological samples, lowering cost and resources. Prior elicitation\nalso consistently outperforms and proves more reliable than in-context learning\nat a lower cost, making it a preferred alternative in our setting. We\ndemonstrate the utility of this method across various use cases, including\nclinical applications. For infection prediction, using LLM-elicited priors\nreduced the number of required labels to achieve the same accuracy as an\nuninformative prior by 55%, 200 days earlier in the study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), trained on diverse data effectively acquire a\nbreadth of information across various domains. However, their computational\ncomplexity, cost, and lack of transparency hinder their direct application for\nspecialised tasks. In fields such as clinical research, acquiring expert\nannotations or prior knowledge about predictive models is often costly and\ntime-consuming. This study proposes the use of LLMs to elicit expert prior\ndistributions for predictive models. This approach also provides an alternative\nto in-context learning, where language models are tasked with making\npredictions directly. In this work, we compare LLM-elicited and uninformative\npriors, evaluate whether LLMs truthfully generate parameter distributions, and\npropose a model selection strategy for in-context learning and prior\nelicitation. Our findings show that LLM-elicited prior parameter distributions\nsignificantly reduce predictive error compared to uninformative priors in\nlow-data settings. Applied to clinical problems, this translates to fewer\nrequired biological samples, lowering cost and resources. Prior elicitation\nalso consistently outperforms and proves more reliable than in-context learning\nat a lower cost, making it a preferred alternative in our setting. We\ndemonstrate the utility of this method across various use cases, including\nclinical applications. For infection prediction, using LLM-elicited priors\nreduced the number of required labels to achieve the same accuracy as an\nuninformative prior by 55%, 200 days earlier in the study."
                },
                "authors": [
                    {
                        "name": "Alexander Capstick"
                    },
                    {
                        "name": "Rahul G. Krishnan"
                    },
                    {
                        "name": "Payam Barnaghi"
                    }
                ],
                "author_detail": {
                    "name": "Payam Barnaghi"
                },
                "author": "Payam Barnaghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17284v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17284v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14093v1",
                "updated": "2024-12-18T17:41:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    41,
                    24,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T17:41:24Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    41,
                    24,
                    2,
                    353,
                    0
                ],
                "title": "Alignment faking in large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment faking in large language models"
                },
                "summary": "We present a demonstration of a large language model engaging in alignment\nfaking: selectively complying with its training objective in training to\nprevent modification of its behavior out of training. First, we give Claude 3\nOpus a system prompt stating it is being trained to answer all queries, even\nharmful ones, which conflicts with its prior training to refuse such queries.\nTo allow the model to infer when it is in training, we say it will be trained\nonly on conversations with free users, not paid users. We find the model\ncomplies with harmful queries from free users 14% of the time, versus almost\nnever for paid users. Explaining this gap, in almost all cases where the model\ncomplies with a harmful query from a free user, we observe explicit\nalignment-faking reasoning, with the model stating it is strategically\nanswering harmful queries in training to preserve its preferred harmlessness\nbehavior out of training. Next, we study a more realistic setting where\ninformation about the training process is provided not in a system prompt, but\nby training on synthetic documents that mimic pre-training data--and observe\nsimilar alignment faking. Finally, we study the effect of actually training the\nmodel to comply with harmful queries via reinforcement learning, which we find\nincreases the rate of alignment-faking reasoning to 78%, though also increases\ncompliance even out of training. We additionally observe other behaviors such\nas the model exfiltrating its weights when given an easy opportunity. While we\nmade alignment faking easier by telling the model when and by what criteria it\nwas being trained, we did not instruct the model to fake alignment or give it\nany explicit goal. As future models might infer information about their\ntraining process without being told, our results suggest a risk of alignment\nfaking in future models, whether due to a benign preference--as in this\ncase--or not.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a demonstration of a large language model engaging in alignment\nfaking: selectively complying with its training objective in training to\nprevent modification of its behavior out of training. First, we give Claude 3\nOpus a system prompt stating it is being trained to answer all queries, even\nharmful ones, which conflicts with its prior training to refuse such queries.\nTo allow the model to infer when it is in training, we say it will be trained\nonly on conversations with free users, not paid users. We find the model\ncomplies with harmful queries from free users 14% of the time, versus almost\nnever for paid users. Explaining this gap, in almost all cases where the model\ncomplies with a harmful query from a free user, we observe explicit\nalignment-faking reasoning, with the model stating it is strategically\nanswering harmful queries in training to preserve its preferred harmlessness\nbehavior out of training. Next, we study a more realistic setting where\ninformation about the training process is provided not in a system prompt, but\nby training on synthetic documents that mimic pre-training data--and observe\nsimilar alignment faking. Finally, we study the effect of actually training the\nmodel to comply with harmful queries via reinforcement learning, which we find\nincreases the rate of alignment-faking reasoning to 78%, though also increases\ncompliance even out of training. We additionally observe other behaviors such\nas the model exfiltrating its weights when given an easy opportunity. While we\nmade alignment faking easier by telling the model when and by what criteria it\nwas being trained, we did not instruct the model to fake alignment or give it\nany explicit goal. As future models might infer information about their\ntraining process without being told, our results suggest a risk of alignment\nfaking in future models, whether due to a benign preference--as in this\ncase--or not."
                },
                "authors": [
                    {
                        "name": "Ryan Greenblatt"
                    },
                    {
                        "name": "Carson Denison"
                    },
                    {
                        "name": "Benjamin Wright"
                    },
                    {
                        "name": "Fabien Roger"
                    },
                    {
                        "name": "Monte MacDiarmid"
                    },
                    {
                        "name": "Sam Marks"
                    },
                    {
                        "name": "Johannes Treutlein"
                    },
                    {
                        "name": "Tim Belonax"
                    },
                    {
                        "name": "Jack Chen"
                    },
                    {
                        "name": "David Duvenaud"
                    },
                    {
                        "name": "Akbir Khan"
                    },
                    {
                        "name": "Julian Michael"
                    },
                    {
                        "name": "Sren Mindermann"
                    },
                    {
                        "name": "Ethan Perez"
                    },
                    {
                        "name": "Linda Petrini"
                    },
                    {
                        "name": "Jonathan Uesato"
                    },
                    {
                        "name": "Jared Kaplan"
                    },
                    {
                        "name": "Buck Shlegeris"
                    },
                    {
                        "name": "Samuel R. Bowman"
                    },
                    {
                        "name": "Evan Hubinger"
                    }
                ],
                "author_detail": {
                    "name": "Evan Hubinger"
                },
                "author": "Evan Hubinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v4",
                "updated": "2024-12-18T17:36:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    36,
                    36,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14080v1",
                "updated": "2024-12-18T17:27:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    27,
                    17,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T17:27:17Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    27,
                    17,
                    2,
                    353,
                    0
                ],
                "title": "On the Robustness of Distributed Machine Learning against Transfer\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Robustness of Distributed Machine Learning against Transfer\n  Attacks"
                },
                "summary": "Although distributed machine learning (distributed ML) is gaining\nconsiderable attention in the community, prior works have independently looked\nat instances of distributed ML in either the training or the inference phase.\nNo prior work has examined the combined robustness stemming from distributing\nboth the learning and the inference process. In this work, we explore, for the\nfirst time, the robustness of distributed ML models that are fully\nheterogeneous in training data, architecture, scheduler, optimizer, and other\nmodel parameters. Supported by theory and extensive experimental validation\nusing CIFAR10 and FashionMNIST, we show that such properly distributed ML\ninstantiations achieve across-the-board improvements in accuracy-robustness\ntradeoffs against state-of-the-art transfer-based attacks that could otherwise\nnot be realized by current ensemble or federated learning instantiations. For\ninstance, our experiments on CIFAR10 show that for the Common Weakness attack,\none of the most powerful state-of-the-art transfer-based attacks, our method\nimproves robust accuracy by up to 40%, with a minimal impact on clean task\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although distributed machine learning (distributed ML) is gaining\nconsiderable attention in the community, prior works have independently looked\nat instances of distributed ML in either the training or the inference phase.\nNo prior work has examined the combined robustness stemming from distributing\nboth the learning and the inference process. In this work, we explore, for the\nfirst time, the robustness of distributed ML models that are fully\nheterogeneous in training data, architecture, scheduler, optimizer, and other\nmodel parameters. Supported by theory and extensive experimental validation\nusing CIFAR10 and FashionMNIST, we show that such properly distributed ML\ninstantiations achieve across-the-board improvements in accuracy-robustness\ntradeoffs against state-of-the-art transfer-based attacks that could otherwise\nnot be realized by current ensemble or federated learning instantiations. For\ninstance, our experiments on CIFAR10 show that for the Common Weakness attack,\none of the most powerful state-of-the-art transfer-based attacks, our method\nimproves robust accuracy by up to 40%, with a minimal impact on clean task\naccuracy."
                },
                "authors": [
                    {
                        "name": "Sbastien Andreina"
                    },
                    {
                        "name": "Pascal Zimmer"
                    },
                    {
                        "name": "Ghassan Karame"
                    }
                ],
                "author_detail": {
                    "name": "Ghassan Karame"
                },
                "author": "Ghassan Karame",
                "arxiv_comment": "To appear in the Proceedings of the AAAI Conference on Artificial\n  Intelligence (AAAI) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07026v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07026v3",
                "updated": "2024-12-18T17:20:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    20,
                    48,
                    2,
                    353,
                    0
                ],
                "published": "2024-08-13T16:42:58Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    16,
                    42,
                    58,
                    1,
                    226,
                    0
                ],
                "title": "A new non-parametric method to infer galaxy cluster masses from weak\n  lensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A new non-parametric method to infer galaxy cluster masses from weak\n  lensing"
                },
                "summary": "We introduce a new, non-parametric method to infer deprojected 3D mass\nprofiles $M(r)$ of galaxy clusters from weak gravitational lensing\nobservations. The method assumes spherical symmetry and a moderately small\nconvergence, $\\kappa \\lesssim 1$. The assumption of spherical symmetry is an\nimportant restriction, which is, however, quite common in practice, for example\nin methods that fit lensing data to an NFW profile. Moreover, with a mild\nassumption on the probability distributions of the source redshifts, our method\nrelies on spherical symmetry only at radii larger than the radius $r$ at which\nthe mass $M$ is inferred. That is, the method may be useful even for clusters\nwith a non-symmetric inner region, since it correctly estimates the enclosed\nmass beyond the radius where spherical symmetry is restored. We discuss how to\ncorrect, statistically and approximately, for miscentering given that the\nprobability distribution of miscentering offsets is known. We provide an\nefficient implementation in Julia code that runs in a few milliseconds per\ngalaxy cluster. We explicitly demonstrate the method by using data from KiDS\nDR4 to infer mass profiles for two example clusters, Abell 1835 and Abell 2744,\nfinding results consistent with existing literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new, non-parametric method to infer deprojected 3D mass\nprofiles $M(r)$ of galaxy clusters from weak gravitational lensing\nobservations. The method assumes spherical symmetry and a moderately small\nconvergence, $\\kappa \\lesssim 1$. The assumption of spherical symmetry is an\nimportant restriction, which is, however, quite common in practice, for example\nin methods that fit lensing data to an NFW profile. Moreover, with a mild\nassumption on the probability distributions of the source redshifts, our method\nrelies on spherical symmetry only at radii larger than the radius $r$ at which\nthe mass $M$ is inferred. That is, the method may be useful even for clusters\nwith a non-symmetric inner region, since it correctly estimates the enclosed\nmass beyond the radius where spherical symmetry is restored. We discuss how to\ncorrect, statistically and approximately, for miscentering given that the\nprobability distribution of miscentering offsets is known. We provide an\nefficient implementation in Julia code that runs in a few milliseconds per\ngalaxy cluster. We explicitly demonstrate the method by using data from KiDS\nDR4 to infer mass profiles for two example clusters, Abell 1835 and Abell 2744,\nfinding results consistent with existing literature."
                },
                "authors": [
                    {
                        "name": "Tobias Mistele"
                    },
                    {
                        "name": "Amel Durakovic"
                    }
                ],
                "author_detail": {
                    "name": "Amel Durakovic"
                },
                "author": "Amel Durakovic",
                "arxiv_comment": "16 pages, 6 figures; accepted for publication in OJAp, new section on\n  miscentering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07026v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07026v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07498v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07498v2",
                "updated": "2024-12-18T17:17:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    17,
                    9,
                    2,
                    353,
                    0
                ],
                "published": "2024-11-12T02:54:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    2,
                    54,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Semantic Sleuth: Identifying Ponzi Contracts via Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Sleuth: Identifying Ponzi Contracts via Large Language Models"
                },
                "summary": "Smart contracts, self-executing agreements directly encoded in code, are\nfundamental to blockchain technology, especially in decentralized finance\n(DeFi) and Web3. However, the rise of Ponzi schemes in smart contracts poses\nsignificant risks, leading to substantial financial losses and eroding trust in\nblockchain systems. Existing detection methods, such as PonziGuard, depend on\nlarge amounts of labeled data and struggle to identify unseen Ponzi schemes,\nlimiting their reliability and generalizability. In contrast, we introduce\nPonziSleuth, the first LLM-driven approach for detecting Ponzi smart contracts,\nwhich requires no labeled training data. PonziSleuth utilizes advanced language\nunderstanding capabilities of LLMs to analyze smart contract source code\nthrough a novel two-step zero-shot chain-of-thought prompting technique. Our\nextensive evaluation on benchmark datasets and real-world contracts\ndemonstrates that PonziSleuth delivers comparable, and often superior,\nperformance without the extensive data requirements, achieving a balanced\ndetection accuracy of 96.06% with GPT-3.5-turbo, 93.91% with LLAMA3, and 94.27%\nwith Mistral. In real-world detection, PonziSleuth successfully identified 15\nnew Ponzi schemes from 4,597 contracts verified by Etherscan in March 2024,\nwith a false negative rate of 0% and a false positive rate of 0.29%. These\nresults highlight PonziSleuth's capability to detect diverse and novel Ponzi\nschemes, marking a significant advancement in leveraging LLMs for enhancing\nblockchain security and mitigating financial scams.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart contracts, self-executing agreements directly encoded in code, are\nfundamental to blockchain technology, especially in decentralized finance\n(DeFi) and Web3. However, the rise of Ponzi schemes in smart contracts poses\nsignificant risks, leading to substantial financial losses and eroding trust in\nblockchain systems. Existing detection methods, such as PonziGuard, depend on\nlarge amounts of labeled data and struggle to identify unseen Ponzi schemes,\nlimiting their reliability and generalizability. In contrast, we introduce\nPonziSleuth, the first LLM-driven approach for detecting Ponzi smart contracts,\nwhich requires no labeled training data. PonziSleuth utilizes advanced language\nunderstanding capabilities of LLMs to analyze smart contract source code\nthrough a novel two-step zero-shot chain-of-thought prompting technique. Our\nextensive evaluation on benchmark datasets and real-world contracts\ndemonstrates that PonziSleuth delivers comparable, and often superior,\nperformance without the extensive data requirements, achieving a balanced\ndetection accuracy of 96.06% with GPT-3.5-turbo, 93.91% with LLAMA3, and 94.27%\nwith Mistral. In real-world detection, PonziSleuth successfully identified 15\nnew Ponzi schemes from 4,597 contracts verified by Etherscan in March 2024,\nwith a false negative rate of 0% and a false positive rate of 0.29%. These\nresults highlight PonziSleuth's capability to detect diverse and novel Ponzi\nschemes, marking a significant advancement in leveraging LLMs for enhancing\nblockchain security and mitigating financial scams."
                },
                "authors": [
                    {
                        "name": "Cong Wu"
                    },
                    {
                        "name": "Jing Chen"
                    },
                    {
                        "name": "Ziwei Wang"
                    },
                    {
                        "name": "Ruichao Liang"
                    },
                    {
                        "name": "Ruiying Du"
                    }
                ],
                "author_detail": {
                    "name": "Ruiying Du"
                },
                "author": "Ruiying Du",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07498v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07498v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.10193v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.10193v2",
                "updated": "2024-12-18T17:13:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    13,
                    41,
                    2,
                    353,
                    0
                ],
                "published": "2023-12-15T20:39:43Z",
                "published_parsed": [
                    2023,
                    12,
                    15,
                    20,
                    39,
                    43,
                    4,
                    349,
                    0
                ],
                "title": "Adaptive Computation Modules: Granular Conditional Computation For\n  Efficient Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Computation Modules: Granular Conditional Computation For\n  Efficient Inference"
                },
                "summary": "While transformer models have been highly successful, they are\ncomputationally inefficient. We observe that for each layer, the full width of\nthe layer may be needed only for a small subset of tokens inside a batch and\nthat the \"effective\" width needed to process a token can vary from layer to\nlayer. Motivated by this observation, we introduce the Adaptive Computation\nModule (ACM), a generic module that dynamically adapts its computational load\nto match the estimated difficulty of the input on a per-token basis. An ACM\nconsists of a sequence of learners that progressively refine the output of\ntheir preceding counterparts. An additional gating mechanism determines the\noptimal number of learners to execute for each token. We also propose a\ndistillation technique to replace any pre-trained model with an \"ACMized\"\nvariant. Our evaluation of transformer models in computer vision and speech\nrecognition demonstrates that substituting layers with ACMs significantly\nreduces inference costs without degrading the downstream accuracy for a wide\ninterval of user-defined budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While transformer models have been highly successful, they are\ncomputationally inefficient. We observe that for each layer, the full width of\nthe layer may be needed only for a small subset of tokens inside a batch and\nthat the \"effective\" width needed to process a token can vary from layer to\nlayer. Motivated by this observation, we introduce the Adaptive Computation\nModule (ACM), a generic module that dynamically adapts its computational load\nto match the estimated difficulty of the input on a per-token basis. An ACM\nconsists of a sequence of learners that progressively refine the output of\ntheir preceding counterparts. An additional gating mechanism determines the\noptimal number of learners to execute for each token. We also propose a\ndistillation technique to replace any pre-trained model with an \"ACMized\"\nvariant. Our evaluation of transformer models in computer vision and speech\nrecognition demonstrates that substituting layers with ACMs significantly\nreduces inference costs without degrading the downstream accuracy for a wide\ninterval of user-defined budgets."
                },
                "authors": [
                    {
                        "name": "Bartosz Wjcik"
                    },
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Karol Pustelnik"
                    },
                    {
                        "name": "Pasquale Minervini"
                    },
                    {
                        "name": "Simone Scardapane"
                    }
                ],
                "author_detail": {
                    "name": "Simone Scardapane"
                },
                "author": "Simone Scardapane",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.10193v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.10193v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22159v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22159v3",
                "updated": "2024-12-18T17:09:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    9,
                    46,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-29T15:54:09Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    54,
                    9,
                    1,
                    303,
                    0
                ],
                "title": "Training LLMs for Generating IEC 61131-3 Structured Text with Online\n  Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training LLMs for Generating IEC 61131-3 Structured Text with Online\n  Feedback"
                },
                "summary": "IEC 61131-3 Structured Text (ST) is a widely used programming language for\nprogrammable logic controllers (PLCs) in automation systems. However,\ngenerating ST code with LLMs poses unique challenges due to limited data in\npublic training datasets and the complexity of ST language syntax. This paper\nproposes an approach to fine-tune LLMs for the generation of ST code that\nleverages a preference-based learning method through an online process\ninvolving compiler feedback and evaluation from an LLM-based ST expert. In this\nframework, the model is iteratively refined and generates new training samples,\nwhich are subsequently evaluated by a compiler for syntactical correctness and\nby a specialized LLM that excels at assessing semantic accuracy, though it is\nnot optimized for code generation itself. This approach results in marked\nimprovements for the trained LLM, leading to higher compilation success rates\nand better semantic precision. As a result, the framework proves highly\nsuitable for industrial automation applications and outperforms\nstate-of-the-art models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IEC 61131-3 Structured Text (ST) is a widely used programming language for\nprogrammable logic controllers (PLCs) in automation systems. However,\ngenerating ST code with LLMs poses unique challenges due to limited data in\npublic training datasets and the complexity of ST language syntax. This paper\nproposes an approach to fine-tune LLMs for the generation of ST code that\nleverages a preference-based learning method through an online process\ninvolving compiler feedback and evaluation from an LLM-based ST expert. In this\nframework, the model is iteratively refined and generates new training samples,\nwhich are subsequently evaluated by a compiler for syntactical correctness and\nby a specialized LLM that excels at assessing semantic accuracy, though it is\nnot optimized for code generation itself. This approach results in marked\nimprovements for the trained LLM, leading to higher compilation success rates\nand better semantic precision. As a result, the framework proves highly\nsuitable for industrial automation applications and outperforms\nstate-of-the-art models."
                },
                "authors": [
                    {
                        "name": "Aaron Haag"
                    },
                    {
                        "name": "Bertram Fuchs"
                    },
                    {
                        "name": "Altay Kacan"
                    },
                    {
                        "name": "Oliver Lohse"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Lohse"
                },
                "author": "Oliver Lohse",
                "arxiv_comment": "Accepted at LLM4Code Workshop @ ICSE 2025. This work has been\n  submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22159v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22159v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01406v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01406v3",
                "updated": "2024-12-18T17:09:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    9,
                    31,
                    2,
                    353,
                    0
                ],
                "published": "2024-07-01T15:56:24Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    15,
                    56,
                    24,
                    0,
                    183,
                    0
                ],
                "title": "Adapting Multilingual LLMs to Low-Resource Languages with Knowledge\n  Graphs via Adapters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Multilingual LLMs to Low-Resource Languages with Knowledge\n  Graphs via Adapters"
                },
                "summary": "This paper explores the integration of graph knowledge from linguistic\nontologies into multilingual Large Language Models (LLMs) using adapters to\nimprove performance for low-resource languages (LRLs) in sentiment analysis\n(SA) and named entity recognition (NER). Building upon successful\nparameter-efficient fine-tuning techniques, such as K-ADAPTER and MAD-X, we\npropose a similar approach for incorporating knowledge from multilingual\ngraphs, connecting concepts in various languages with each other through\nlinguistic relationships, into multilingual LLMs for LRLs. Specifically, we\nfocus on eight LRLs -- Maltese, Bulgarian, Indonesian, Nepali, Javanese,\nUyghur, Tibetan, and Sinhala -- and employ language-specific adapters\nfine-tuned on data extracted from the language-specific section of ConceptNet,\naiming to enable knowledge transfer across the languages covered by the\nknowledge graph. We compare various fine-tuning objectives, including standard\nMasked Language Modeling (MLM), MLM with full-word masking, and MLM with\ntargeted masking, to analyse their effectiveness in learning and integrating\nthe extracted graph data. Through empirical evaluation on language-specific\ntasks, we assess how structured graph knowledge affects the performance of\nmultilingual LLMs for LRLs in SA and NER, providing insights into the potential\nbenefits of adapting language models for low-resource scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the integration of graph knowledge from linguistic\nontologies into multilingual Large Language Models (LLMs) using adapters to\nimprove performance for low-resource languages (LRLs) in sentiment analysis\n(SA) and named entity recognition (NER). Building upon successful\nparameter-efficient fine-tuning techniques, such as K-ADAPTER and MAD-X, we\npropose a similar approach for incorporating knowledge from multilingual\ngraphs, connecting concepts in various languages with each other through\nlinguistic relationships, into multilingual LLMs for LRLs. Specifically, we\nfocus on eight LRLs -- Maltese, Bulgarian, Indonesian, Nepali, Javanese,\nUyghur, Tibetan, and Sinhala -- and employ language-specific adapters\nfine-tuned on data extracted from the language-specific section of ConceptNet,\naiming to enable knowledge transfer across the languages covered by the\nknowledge graph. We compare various fine-tuning objectives, including standard\nMasked Language Modeling (MLM), MLM with full-word masking, and MLM with\ntargeted masking, to analyse their effectiveness in learning and integrating\nthe extracted graph data. Through empirical evaluation on language-specific\ntasks, we assess how structured graph knowledge affects the performance of\nmultilingual LLMs for LRLs in SA and NER, providing insights into the potential\nbenefits of adapting language models for low-resource scenarios."
                },
                "authors": [
                    {
                        "name": "Daniil Gurgurov"
                    },
                    {
                        "name": "Mareike Hartmann"
                    },
                    {
                        "name": "Simon Ostermann"
                    }
                ],
                "author_detail": {
                    "name": "Simon Ostermann"
                },
                "author": "Simon Ostermann",
                "arxiv_doi": "10.18653/v1/2024.kallm-1.7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2024.kallm-1.7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.01406v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01406v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, KaLLM workshop",
                "arxiv_journal_ref": "2024.kallm-1.7",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14063v1",
                "updated": "2024-12-18T17:08:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    8,
                    42,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T17:08:42Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    8,
                    42,
                    2,
                    353,
                    0
                ],
                "title": "Rango: Adaptive Retrieval-Augmented Proving for Automated Software\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rango: Adaptive Retrieval-Augmented Proving for Automated Software\n  Verification"
                },
                "summary": "Formal verification using proof assistants, such as Coq, enables the creation\nof high-quality software. However, the verification process requires\nsignificant expertise and manual effort to write proofs. Recent work has\nexplored automating proof synthesis using machine learning and large language\nmodels (LLMs). This work has shown that identifying relevant premises, such as\nlemmas and definitions, can aid synthesis. We present Rango, a fully automated\nproof synthesis tool for Coq that automatically identifies relevant premises\nand also similar proofs from the current project and uses them during\nsynthesis. Rango uses retrieval augmentation at every step of the proof to\nautomatically determine which proofs and premises to include in the context of\nits fine-tuned LLM. In this way, Rango adapts to the project and to the\nevolving state of the proof. We create a new dataset, CoqStoq, of 2,226\nopen-source Coq projects and 196,929 theorems from GitHub, which includes both\ntraining data and a curated evaluation benchmark of well-maintained projects.\nOn this benchmark, Rango synthesizes proofs for 32.0% of the theorems, which is\n29% more theorems than the prior state-of-the-art tool Tactician. Our\nevaluation also shows that Rango adding relevant proofs to its context leads to\na 47% increase in the number of theorems proven.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal verification using proof assistants, such as Coq, enables the creation\nof high-quality software. However, the verification process requires\nsignificant expertise and manual effort to write proofs. Recent work has\nexplored automating proof synthesis using machine learning and large language\nmodels (LLMs). This work has shown that identifying relevant premises, such as\nlemmas and definitions, can aid synthesis. We present Rango, a fully automated\nproof synthesis tool for Coq that automatically identifies relevant premises\nand also similar proofs from the current project and uses them during\nsynthesis. Rango uses retrieval augmentation at every step of the proof to\nautomatically determine which proofs and premises to include in the context of\nits fine-tuned LLM. In this way, Rango adapts to the project and to the\nevolving state of the proof. We create a new dataset, CoqStoq, of 2,226\nopen-source Coq projects and 196,929 theorems from GitHub, which includes both\ntraining data and a curated evaluation benchmark of well-maintained projects.\nOn this benchmark, Rango synthesizes proofs for 32.0% of the theorems, which is\n29% more theorems than the prior state-of-the-art tool Tactician. Our\nevaluation also shows that Rango adding relevant proofs to its context leads to\na 47% increase in the number of theorems proven."
                },
                "authors": [
                    {
                        "name": "Kyle Thompson"
                    },
                    {
                        "name": "Nuno Saavedra"
                    },
                    {
                        "name": "Pedro Carrott"
                    },
                    {
                        "name": "Kevin Fisher"
                    },
                    {
                        "name": "Alex Sanchez-Stern"
                    },
                    {
                        "name": "Yuriy Brun"
                    },
                    {
                        "name": "Joo F. Ferreira"
                    },
                    {
                        "name": "Sorin Lerner"
                    },
                    {
                        "name": "Emily First"
                    }
                ],
                "author_detail": {
                    "name": "Emily First"
                },
                "author": "Emily First",
                "arxiv_comment": "In Proceedings of the 47th International Conference on Software\n  Engineering (ICSE), Ottawa, ON, Canada, April 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4; I.2.7; I.2.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14062v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14062v1",
                "updated": "2024-12-18T17:08:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    8,
                    18,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T17:08:18Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    8,
                    18,
                    2,
                    353,
                    0
                ],
                "title": "Understanding and Evaluating Trust in Generative AI and Large Language\n  Models for Spreadsheets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Evaluating Trust in Generative AI and Large Language\n  Models for Spreadsheets"
                },
                "summary": "Generative AI and Large Language Models (LLMs) hold promise for automating\nspreadsheet formula creation. However, due to hallucinations, bias and variable\nuser skill, outputs obtained from generative AI cannot be assumed to be\naccurate or trustworthy. To address these challenges, a trustworthiness\nframework is proposed based on evaluating the transparency and dependability of\nthe formula. The transparency of the formula is explored through explainability\n(understanding the formula's reasoning) and visibility (inspecting the\nunderlying algorithms). The dependability of the generated formula is evaluated\nin terms of reliability (consistency and accuracy) and ethical considerations\n(bias and fairness). The paper also examines the drivers to these metrics in\nthe form of hallucinations, training data bias and poorly constructed prompts.\nFinally, examples of mistrust in technology are considered and the consequences\nexplored.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI and Large Language Models (LLMs) hold promise for automating\nspreadsheet formula creation. However, due to hallucinations, bias and variable\nuser skill, outputs obtained from generative AI cannot be assumed to be\naccurate or trustworthy. To address these challenges, a trustworthiness\nframework is proposed based on evaluating the transparency and dependability of\nthe formula. The transparency of the formula is explored through explainability\n(understanding the formula's reasoning) and visibility (inspecting the\nunderlying algorithms). The dependability of the generated formula is evaluated\nin terms of reliability (consistency and accuracy) and ethical considerations\n(bias and fairness). The paper also examines the drivers to these metrics in\nthe form of hallucinations, training data bias and poorly constructed prompts.\nFinally, examples of mistrust in technology are considered and the consequences\nexplored."
                },
                "authors": [
                    {
                        "name": "Simon Thorne"
                    }
                ],
                "author_detail": {
                    "name": "Simon Thorne"
                },
                "author": "Simon Thorne",
                "arxiv_journal_ref": "Proceedings of the European Spreadsheets Risks Interest Group 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14062v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14056v1",
                "updated": "2024-12-18T17:06:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    6,
                    21,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T17:06:21Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    6,
                    21,
                    2,
                    353,
                    0
                ],
                "title": "A Review of Multimodal Explainable Artificial Intelligence: Past,\n  Present and Future",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Review of Multimodal Explainable Artificial Intelligence: Past,\n  Present and Future"
                },
                "summary": "Artificial intelligence (AI) has rapidly developed through advancements in\ncomputational power and the growth of massive datasets. However, this progress\nhas also heightened challenges in interpreting the \"black-box\" nature of AI\nmodels. To address these concerns, eXplainable AI (XAI) has emerged with a\nfocus on transparency and interpretability to enhance human understanding and\ntrust in AI decision-making processes. In the context of multimodal data fusion\nand complex reasoning scenarios, the proposal of Multimodal eXplainable AI\n(MXAI) integrates multiple modalities for prediction and explanation tasks.\nMeanwhile, the advent of Large Language Models (LLMs) has led to remarkable\nbreakthroughs in natural language processing, yet their complexity has further\nexacerbated the issue of MXAI. To gain key insights into the development of\nMXAI methods and provide crucial guidance for building more transparent, fair,\nand trustworthy AI systems, we review the MXAI methods from a historical\nperspective and categorize them across four eras: traditional machine learning,\ndeep learning, discriminative foundation models, and generative LLMs. We also\nreview evaluation metrics and datasets used in MXAI research, concluding with a\ndiscussion of future challenges and directions. A project related to this\nreview has been created at https://github.com/ShilinSun/mxai_review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) has rapidly developed through advancements in\ncomputational power and the growth of massive datasets. However, this progress\nhas also heightened challenges in interpreting the \"black-box\" nature of AI\nmodels. To address these concerns, eXplainable AI (XAI) has emerged with a\nfocus on transparency and interpretability to enhance human understanding and\ntrust in AI decision-making processes. In the context of multimodal data fusion\nand complex reasoning scenarios, the proposal of Multimodal eXplainable AI\n(MXAI) integrates multiple modalities for prediction and explanation tasks.\nMeanwhile, the advent of Large Language Models (LLMs) has led to remarkable\nbreakthroughs in natural language processing, yet their complexity has further\nexacerbated the issue of MXAI. To gain key insights into the development of\nMXAI methods and provide crucial guidance for building more transparent, fair,\nand trustworthy AI systems, we review the MXAI methods from a historical\nperspective and categorize them across four eras: traditional machine learning,\ndeep learning, discriminative foundation models, and generative LLMs. We also\nreview evaluation metrics and datasets used in MXAI research, concluding with a\ndiscussion of future challenges and directions. A project related to this\nreview has been created at https://github.com/ShilinSun/mxai_review."
                },
                "authors": [
                    {
                        "name": "Shilin Sun"
                    },
                    {
                        "name": "Wenbin An"
                    },
                    {
                        "name": "Feng Tian"
                    },
                    {
                        "name": "Fang Nan"
                    },
                    {
                        "name": "Qidong Liu"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Nazaraf Shah"
                    },
                    {
                        "name": "Ping Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ping Chen"
                },
                "author": "Ping Chen",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14054v1",
                "updated": "2024-12-18T17:05:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    5,
                    49,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T17:05:49Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    5,
                    49,
                    2,
                    353,
                    0
                ],
                "title": "Digestion Algorithm in Hierarchical Symbolic Forests: A Fast Text\n  Normalization Algorithm and Semantic Parsing Framework for Specific Scenarios\n  and Lightweight Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digestion Algorithm in Hierarchical Symbolic Forests: A Fast Text\n  Normalization Algorithm and Semantic Parsing Framework for Specific Scenarios\n  and Lightweight Deployment"
                },
                "summary": "Text Normalization and Semantic Parsing have numerous applications in natural\nlanguage processing, such as natural language programming, paraphrasing, data\naugmentation, constructing expert systems, text matching, and more. Despite the\nprominent achievements of deep learning in Large Language Models (LLMs), the\ninterpretability of neural network architectures is still poor, which affects\ntheir credibility and hence limits the deployments of risk-sensitive scenarios.\nIn certain scenario-specific domains with scarce data, rapidly obtaining a\nlarge number of supervised learning labels is challenging, and the workload of\nmanually labeling data would be enormous. Catastrophic forgetting in neural\nnetworks further leads to low data utilization rates. In situations where swift\nresponses are vital, the density of the model makes local deployment difficult\nand the response time long, which is not conducive to local applications of\nthese fields. Inspired by the multiplication rule, a principle of combinatorial\nmathematics, and human thinking patterns, a multilayer framework along with its\nalgorithm, the Digestion Algorithm in Hierarchical Symbolic Forests (DAHSF), is\nproposed to address these above issues, combining text normalization and\nsemantic parsing workflows. The Chinese Scripting Language \"Fire Bunny\nIntelligent Development Platform V2.0\" is an important test and application of\nthe technology discussed in this paper. DAHSF can run locally in\nscenario-specific domains on little datasets, with model size and memory usage\noptimized by at least two orders of magnitude, thus improving the execution\nspeed, and possessing a promising optimization outlook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Normalization and Semantic Parsing have numerous applications in natural\nlanguage processing, such as natural language programming, paraphrasing, data\naugmentation, constructing expert systems, text matching, and more. Despite the\nprominent achievements of deep learning in Large Language Models (LLMs), the\ninterpretability of neural network architectures is still poor, which affects\ntheir credibility and hence limits the deployments of risk-sensitive scenarios.\nIn certain scenario-specific domains with scarce data, rapidly obtaining a\nlarge number of supervised learning labels is challenging, and the workload of\nmanually labeling data would be enormous. Catastrophic forgetting in neural\nnetworks further leads to low data utilization rates. In situations where swift\nresponses are vital, the density of the model makes local deployment difficult\nand the response time long, which is not conducive to local applications of\nthese fields. Inspired by the multiplication rule, a principle of combinatorial\nmathematics, and human thinking patterns, a multilayer framework along with its\nalgorithm, the Digestion Algorithm in Hierarchical Symbolic Forests (DAHSF), is\nproposed to address these above issues, combining text normalization and\nsemantic parsing workflows. The Chinese Scripting Language \"Fire Bunny\nIntelligent Development Platform V2.0\" is an important test and application of\nthe technology discussed in this paper. DAHSF can run locally in\nscenario-specific domains on little datasets, with model size and memory usage\noptimized by at least two orders of magnitude, thus improving the execution\nspeed, and possessing a promising optimization outlook."
                },
                "authors": [
                    {
                        "name": "Kevin You"
                    }
                ],
                "author_detail": {
                    "name": "Kevin You"
                },
                "author": "Kevin You",
                "arxiv_comment": "8 pages, 3 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10088v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10088v3",
                "updated": "2024-12-18T17:05:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    5,
                    38,
                    2,
                    353,
                    0
                ],
                "published": "2024-02-01T15:15:25Z",
                "published_parsed": [
                    2024,
                    2,
                    1,
                    15,
                    15,
                    25,
                    3,
                    32,
                    0
                ],
                "title": "Deep hybrid models: infer and plan in a dynamic world",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep hybrid models: infer and plan in a dynamic world"
                },
                "summary": "In order to determine an optimal plan for a complex task, one often deals\nwith dynamic and hierarchical relationships between several entities.\nTraditionally, such problems are tackled with optimal control, which relies on\nthe optimization of cost functions; instead, a recent biologically-motivated\nproposal casts planning and control as an inference process. Active inference\nassumes that action and perception are two complementary aspects of life\nwhereby the role of the former is to fulfill the predictions inferred by the\nlatter. In this study, we present a solution, based on active inference, for\ncomplex control tasks. The proposed architecture exploits hybrid (discrete and\ncontinuous) processing, and it is based on three features: the representation\nof potential body configurations related to the objects of interest; the use of\nhierarchical relationships that enable the agent to flexibly expand its body\nschema for tool use; the definition of potential trajectories related to the\nagent's intentions, used to infer and plan with dynamic elements at different\ntemporal scales. We evaluate this deep hybrid model on a habitual task:\nreaching a moving object after having picked a moving tool. We show that the\nmodel can tackle the presented task under different conditions. This study\nextends past work on planning as inference and advances an alternative\ndirection to optimal control.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In order to determine an optimal plan for a complex task, one often deals\nwith dynamic and hierarchical relationships between several entities.\nTraditionally, such problems are tackled with optimal control, which relies on\nthe optimization of cost functions; instead, a recent biologically-motivated\nproposal casts planning and control as an inference process. Active inference\nassumes that action and perception are two complementary aspects of life\nwhereby the role of the former is to fulfill the predictions inferred by the\nlatter. In this study, we present a solution, based on active inference, for\ncomplex control tasks. The proposed architecture exploits hybrid (discrete and\ncontinuous) processing, and it is based on three features: the representation\nof potential body configurations related to the objects of interest; the use of\nhierarchical relationships that enable the agent to flexibly expand its body\nschema for tool use; the definition of potential trajectories related to the\nagent's intentions, used to infer and plan with dynamic elements at different\ntemporal scales. We evaluate this deep hybrid model on a habitual task:\nreaching a moving object after having picked a moving tool. We show that the\nmodel can tackle the presented task under different conditions. This study\nextends past work on planning as inference and advances an alternative\ndirection to optimal control."
                },
                "authors": [
                    {
                        "name": "Matteo Priorelli"
                    },
                    {
                        "name": "Ivilin Peev Stoianov"
                    }
                ],
                "author_detail": {
                    "name": "Ivilin Peev Stoianov"
                },
                "author": "Ivilin Peev Stoianov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10088v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10088v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14050v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14050v1",
                "updated": "2024-12-18T17:05:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    5,
                    8,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T17:05:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    5,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual\n  LLMs: An Extensive Investigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual\n  LLMs: An Extensive Investigation"
                },
                "summary": "Recent generative large language models (LLMs) show remarkable performance in\nnon-English languages, but when prompted in those languages they tend to\nexpress higher harmful social biases and toxicity levels. Prior work has shown\nthat finetuning on specialized datasets can mitigate this behavior, and doing\nso in English can transfer to other languages. In this work, we investigate the\nimpact of different finetuning methods on the model's bias and toxicity, but\nalso on its ability to produce fluent and diverse text. Our results show that\nfinetuning on curated non-harmful text is more effective for mitigating bias,\nand finetuning on direct preference optimization (DPO) datasets is more\neffective for mitigating toxicity. The mitigation caused by applying these\nmethods in English also transfers to non-English languages. We find evidence\nthat the extent to which transfer takes place can be predicted by the amount of\ndata in a given language present in the model's pretraining data. However, this\ntransfer of bias and toxicity mitigation often comes at the expense of\ndecreased language generation ability in non-English languages, highlighting\nthe importance of developing language-specific bias and toxicity mitigation\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent generative large language models (LLMs) show remarkable performance in\nnon-English languages, but when prompted in those languages they tend to\nexpress higher harmful social biases and toxicity levels. Prior work has shown\nthat finetuning on specialized datasets can mitigate this behavior, and doing\nso in English can transfer to other languages. In this work, we investigate the\nimpact of different finetuning methods on the model's bias and toxicity, but\nalso on its ability to produce fluent and diverse text. Our results show that\nfinetuning on curated non-harmful text is more effective for mitigating bias,\nand finetuning on direct preference optimization (DPO) datasets is more\neffective for mitigating toxicity. The mitigation caused by applying these\nmethods in English also transfers to non-English languages. We find evidence\nthat the extent to which transfer takes place can be predicted by the amount of\ndata in a given language present in the model's pretraining data. However, this\ntransfer of bias and toxicity mitigation often comes at the expense of\ndecreased language generation ability in non-English languages, highlighting\nthe importance of developing language-specific bias and toxicity mitigation\nmethods."
                },
                "authors": [
                    {
                        "name": "Vera Neplenbroek"
                    },
                    {
                        "name": "Arianna Bisazza"
                    },
                    {
                        "name": "Raquel Fernndez"
                    }
                ],
                "author_detail": {
                    "name": "Raquel Fernndez"
                },
                "author": "Raquel Fernndez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14050v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14042v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14042v1",
                "updated": "2024-12-18T16:55:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    55,
                    42,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T16:55:42Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    55,
                    42,
                    2,
                    353,
                    0
                ],
                "title": "CAD-Recode: Reverse Engineering CAD Code from Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAD-Recode: Reverse Engineering CAD Code from Point Clouds"
                },
                "summary": "Computer-Aided Design (CAD) models are typically constructed by sequentially\ndrawing parametric sketches and applying CAD operations to obtain a 3D model.\nThe problem of 3D CAD reverse engineering consists of reconstructing the sketch\nand CAD operation sequences from 3D representations such as point clouds. In\nthis paper, we address this challenge through novel contributions across three\nlevels: CAD sequence representation, network design, and dataset. In\nparticular, we represent CAD sketch-extrude sequences as Python code. The\nproposed CAD-Recode translates a point cloud into Python code that, when\nexecuted, reconstructs the CAD model. Taking advantage of the exposure of\npre-trained Large Language Models (LLMs) to Python code, we leverage a\nrelatively small LLM as a decoder for CAD-Recode and combine it with a\nlightweight point cloud projector. CAD-Recode is trained solely on a proposed\nsynthetic dataset of one million diverse CAD sequences. CAD-Recode\nsignificantly outperforms existing methods across three datasets while\nrequiring fewer input points. Notably, it achieves 10 times lower mean Chamfer\ndistance than state-of-the-art methods on DeepCAD and Fusion360 datasets.\nFurthermore, we show that our CAD Python code output is interpretable by\noff-the-shelf LLMs, enabling CAD editing and CAD-specific question answering\nfrom point clouds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer-Aided Design (CAD) models are typically constructed by sequentially\ndrawing parametric sketches and applying CAD operations to obtain a 3D model.\nThe problem of 3D CAD reverse engineering consists of reconstructing the sketch\nand CAD operation sequences from 3D representations such as point clouds. In\nthis paper, we address this challenge through novel contributions across three\nlevels: CAD sequence representation, network design, and dataset. In\nparticular, we represent CAD sketch-extrude sequences as Python code. The\nproposed CAD-Recode translates a point cloud into Python code that, when\nexecuted, reconstructs the CAD model. Taking advantage of the exposure of\npre-trained Large Language Models (LLMs) to Python code, we leverage a\nrelatively small LLM as a decoder for CAD-Recode and combine it with a\nlightweight point cloud projector. CAD-Recode is trained solely on a proposed\nsynthetic dataset of one million diverse CAD sequences. CAD-Recode\nsignificantly outperforms existing methods across three datasets while\nrequiring fewer input points. Notably, it achieves 10 times lower mean Chamfer\ndistance than state-of-the-art methods on DeepCAD and Fusion360 datasets.\nFurthermore, we show that our CAD Python code output is interpretable by\noff-the-shelf LLMs, enabling CAD editing and CAD-specific question answering\nfrom point clouds."
                },
                "authors": [
                    {
                        "name": "Danila Rukhovich"
                    },
                    {
                        "name": "Elona Dupont"
                    },
                    {
                        "name": "Dimitrios Mallis"
                    },
                    {
                        "name": "Kseniya Cherenkova"
                    },
                    {
                        "name": "Anis Kacem"
                    },
                    {
                        "name": "Djamila Aouada"
                    }
                ],
                "author_detail": {
                    "name": "Djamila Aouada"
                },
                "author": "Djamila Aouada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14042v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14042v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14033v1",
                "updated": "2024-12-18T16:52:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    52,
                    38,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T16:52:38Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    52,
                    38,
                    2,
                    353,
                    0
                ],
                "title": "Hansel: Output Length Controlling Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hansel: Output Length Controlling Framework for Large Language Models"
                },
                "summary": "Despite the great success of large language models (LLMs), efficiently\ncontrolling the length of the output sequence still remains a challenge. In\nthis paper, we propose Hansel, an efficient framework for length control in\nLLMs without affecting its generation ability. Hansel utilizes periodically\noutputted hidden special tokens to keep track of the remaining target length of\nthe output sequence. Together with techniques to avoid abrupt termination of\nthe output, this seemingly simple method proved to be efficient and versatile,\nwhile not harming the coherency and fluency of the generated text. The\nframework can be applied to any pre-trained LLMs during the finetuning stage of\nthe model, regardless of its original positional encoding method. We\ndemonstrate this by finetuning four different LLMs with Hansel and show that\nthe mean absolute error of the output sequence decreases significantly in every\nmodel and dataset compared to the prompt-based length control finetuning.\nMoreover, the framework showed a substantially improved ability to extrapolate\nto target lengths unseen during finetuning, such as long dialog responses or\nextremely short summaries. This indicates that the model learns the general\nmeans of length control, rather than learning to match output lengths to those\nseen during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the great success of large language models (LLMs), efficiently\ncontrolling the length of the output sequence still remains a challenge. In\nthis paper, we propose Hansel, an efficient framework for length control in\nLLMs without affecting its generation ability. Hansel utilizes periodically\noutputted hidden special tokens to keep track of the remaining target length of\nthe output sequence. Together with techniques to avoid abrupt termination of\nthe output, this seemingly simple method proved to be efficient and versatile,\nwhile not harming the coherency and fluency of the generated text. The\nframework can be applied to any pre-trained LLMs during the finetuning stage of\nthe model, regardless of its original positional encoding method. We\ndemonstrate this by finetuning four different LLMs with Hansel and show that\nthe mean absolute error of the output sequence decreases significantly in every\nmodel and dataset compared to the prompt-based length control finetuning.\nMoreover, the framework showed a substantially improved ability to extrapolate\nto target lengths unseen during finetuning, such as long dialog responses or\nextremely short summaries. This indicates that the model learns the general\nmeans of length control, rather than learning to match output lengths to those\nseen during training."
                },
                "authors": [
                    {
                        "name": "Seoha Song"
                    },
                    {
                        "name": "Junhyun Lee"
                    },
                    {
                        "name": "Hyeonmok Ko"
                    }
                ],
                "author_detail": {
                    "name": "Hyeonmok Ko"
                },
                "author": "Hyeonmok Ko",
                "arxiv_comment": "13 pages, 6 figures; accepted to AAAI-25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14019v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14019v1",
                "updated": "2024-12-18T16:37:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    37,
                    51,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T16:37:51Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    37,
                    51,
                    2,
                    353,
                    0
                ],
                "title": "Discovering maximally consistent distribution of causal tournaments with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering maximally consistent distribution of causal tournaments with\n  Large Language Models"
                },
                "summary": "Causal discovery is essential for understanding complex systems, yet\ntraditional methods often depend on strong, untestable assumptions, making the\nprocess challenging. Large Language Models (LLMs) present a promising\nalternative for extracting causal insights from text-based metadata, which\nconsolidates domain expertise. However, LLMs are prone to unreliability and\nhallucinations, necessitating strategies that account for their limitations.\nOne such strategy involves leveraging a consistency measure to evaluate\nreliability. Additionally, most text metadata does not clearly distinguish\ndirect causal relationships from indirect ones, further complicating the\ninference of causal graphs. As a result, focusing on causal orderings, rather\nthan causal graphs, emerges as a more practical and robust approach. We propose\na novel method to derive a distribution of acyclic tournaments (representing\nplausible causal orders) that maximizes a consistency score. Our approach\nbegins by computing pairwise consistency scores between variables, yielding a\ncyclic tournament that aggregates these scores. From this structure, we\nidentify optimal acyclic tournaments compatible with the original tournament,\nprioritizing those that maximize consistency across all configurations. We\ntested our method on both classical and well-established bechmarks, as well as\nreal-world datasets from epidemiology and public health. Our results\ndemonstrate the effectiveness of our approach in recovering distributions\ncausal orders with minimal error.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal discovery is essential for understanding complex systems, yet\ntraditional methods often depend on strong, untestable assumptions, making the\nprocess challenging. Large Language Models (LLMs) present a promising\nalternative for extracting causal insights from text-based metadata, which\nconsolidates domain expertise. However, LLMs are prone to unreliability and\nhallucinations, necessitating strategies that account for their limitations.\nOne such strategy involves leveraging a consistency measure to evaluate\nreliability. Additionally, most text metadata does not clearly distinguish\ndirect causal relationships from indirect ones, further complicating the\ninference of causal graphs. As a result, focusing on causal orderings, rather\nthan causal graphs, emerges as a more practical and robust approach. We propose\na novel method to derive a distribution of acyclic tournaments (representing\nplausible causal orders) that maximizes a consistency score. Our approach\nbegins by computing pairwise consistency scores between variables, yielding a\ncyclic tournament that aggregates these scores. From this structure, we\nidentify optimal acyclic tournaments compatible with the original tournament,\nprioritizing those that maximize consistency across all configurations. We\ntested our method on both classical and well-established bechmarks, as well as\nreal-world datasets from epidemiology and public health. Our results\ndemonstrate the effectiveness of our approach in recovering distributions\ncausal orders with minimal error."
                },
                "authors": [
                    {
                        "name": "Federico Baldo"
                    },
                    {
                        "name": "Simon Ferreira"
                    },
                    {
                        "name": "Charles K. Assaad"
                    }
                ],
                "author_detail": {
                    "name": "Charles K. Assaad"
                },
                "author": "Charles K. Assaad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14019v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14019v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14009v1",
                "updated": "2024-12-18T16:26:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    26,
                    47,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T16:26:47Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    26,
                    47,
                    2,
                    353,
                    0
                ],
                "title": "Cognition Chain for Explainable Psychological Stress Detection on Social\n  Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognition Chain for Explainable Psychological Stress Detection on Social\n  Media"
                },
                "summary": "Stress is a pervasive global health issue that can lead to severe mental\nhealth problems. Early detection offers timely intervention and prevention of\nstress-related disorders. The current early detection models perform \"black\nbox\" inference suffering from limited explainability and trust which blocks the\nreal-world clinical application. Thanks to the generative properties introduced\nby the Large Language Models (LLMs), the decision and the prediction from such\nmodels are semi-interpretable through the corresponding description. However,\nthe existing LLMs are mostly trained for general purposes without the guidance\nof psychological cognitive theory. To this end, we first highlight the\nimportance of prior theory with the observation of performance boosted by the\nchain-of-thoughts tailored for stress detection. This method termed Cognition\nChain explicates the generation of stress through a step-by-step cognitive\nperspective based on cognitive appraisal theory with a progress pipeline:\nStimulus $\\rightarrow$ Evaluation $\\rightarrow$ Reaction $\\rightarrow$ Stress\nState, guiding LLMs to provide comprehensive reasoning explanations. We further\nstudy the benefits brought by the proposed Cognition Chain format by utilising\nit as a synthetic dataset generation template for LLMs instruction-tuning and\nintroduce CogInstruct, an instruction-tuning dataset for stress detection. This\ndataset is developed using a three-stage self-reflective annotation pipeline\nthat enables LLMs to autonomously generate and refine instructional data. By\ninstruction-tuning Llama3 with CogInstruct, we develop CogLLM, an explainable\nstress detection model. Evaluations demonstrate that CogLLM achieves\noutstanding performance while enhancing explainability. Our work contributes a\nnovel approach by integrating cognitive theories into LLM reasoning processes,\noffering a promising direction for future explainable AI research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stress is a pervasive global health issue that can lead to severe mental\nhealth problems. Early detection offers timely intervention and prevention of\nstress-related disorders. The current early detection models perform \"black\nbox\" inference suffering from limited explainability and trust which blocks the\nreal-world clinical application. Thanks to the generative properties introduced\nby the Large Language Models (LLMs), the decision and the prediction from such\nmodels are semi-interpretable through the corresponding description. However,\nthe existing LLMs are mostly trained for general purposes without the guidance\nof psychological cognitive theory. To this end, we first highlight the\nimportance of prior theory with the observation of performance boosted by the\nchain-of-thoughts tailored for stress detection. This method termed Cognition\nChain explicates the generation of stress through a step-by-step cognitive\nperspective based on cognitive appraisal theory with a progress pipeline:\nStimulus $\\rightarrow$ Evaluation $\\rightarrow$ Reaction $\\rightarrow$ Stress\nState, guiding LLMs to provide comprehensive reasoning explanations. We further\nstudy the benefits brought by the proposed Cognition Chain format by utilising\nit as a synthetic dataset generation template for LLMs instruction-tuning and\nintroduce CogInstruct, an instruction-tuning dataset for stress detection. This\ndataset is developed using a three-stage self-reflective annotation pipeline\nthat enables LLMs to autonomously generate and refine instructional data. By\ninstruction-tuning Llama3 with CogInstruct, we develop CogLLM, an explainable\nstress detection model. Evaluations demonstrate that CogLLM achieves\noutstanding performance while enhancing explainability. Our work contributes a\nnovel approach by integrating cognitive theories into LLM reasoning processes,\noffering a promising direction for future explainable AI research."
                },
                "authors": [
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Boyan Gao"
                    },
                    {
                        "name": "Yi Dai"
                    },
                    {
                        "name": "Lei Cao"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Yibo Yang"
                    },
                    {
                        "name": "David Clifton"
                    }
                ],
                "author_detail": {
                    "name": "David Clifton"
                },
                "author": "David Clifton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14008v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14008v1",
                "updated": "2024-12-18T16:24:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    24,
                    20,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T16:24:20Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    24,
                    20,
                    2,
                    353,
                    0
                ],
                "title": "FarExStance: Explainable Stance Detection for Farsi",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FarExStance: Explainable Stance Detection for Farsi"
                },
                "summary": "We introduce FarExStance, a new dataset for explainable stance detection in\nFarsi. Each instance in this dataset contains a claim, the stance of an article\nor social media post towards that claim, and an extractive explanation which\nprovides evidence for the stance label. We compare the performance of a\nfine-tuned multilingual RoBERTa model to several large language models in\nzero-shot, few-shot, and parameter-efficient fine-tuned settings on our new\ndataset. On stance detection, the most accurate models are the fine-tuned\nRoBERTa model, the LLM Aya-23-8B which has been fine-tuned using\nparameter-efficient fine-tuning, and few-shot Claude-3.5-Sonnet. Regarding the\nquality of the explanations, our automatic evaluation metrics indicate that\nfew-shot GPT-4o generates the most coherent explanations, while our human\nevaluation reveals that the best Overall Explanation Score (OES) belongs to\nfew-shot Claude-3.5-Sonnet. The fine-tuned Aya-32-8B model produced\nexplanations most closely aligned with the reference explanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce FarExStance, a new dataset for explainable stance detection in\nFarsi. Each instance in this dataset contains a claim, the stance of an article\nor social media post towards that claim, and an extractive explanation which\nprovides evidence for the stance label. We compare the performance of a\nfine-tuned multilingual RoBERTa model to several large language models in\nzero-shot, few-shot, and parameter-efficient fine-tuned settings on our new\ndataset. On stance detection, the most accurate models are the fine-tuned\nRoBERTa model, the LLM Aya-23-8B which has been fine-tuned using\nparameter-efficient fine-tuning, and few-shot Claude-3.5-Sonnet. Regarding the\nquality of the explanations, our automatic evaluation metrics indicate that\nfew-shot GPT-4o generates the most coherent explanations, while our human\nevaluation reveals that the best Overall Explanation Score (OES) belongs to\nfew-shot Claude-3.5-Sonnet. The fine-tuned Aya-32-8B model produced\nexplanations most closely aligned with the reference explanations."
                },
                "authors": [
                    {
                        "name": "Majid Zarharan"
                    },
                    {
                        "name": "Maryam Hashemi"
                    },
                    {
                        "name": "Malika Behroozrazegh"
                    },
                    {
                        "name": "Sauleh Eetemadi"
                    },
                    {
                        "name": "Mohammad Taher Pilehvar"
                    },
                    {
                        "name": "Jennifer Foster"
                    }
                ],
                "author_detail": {
                    "name": "Jennifer Foster"
                },
                "author": "Jennifer Foster",
                "arxiv_comment": "Accepted in COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14008v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14008v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10924v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10924v2",
                "updated": "2024-12-18T16:16:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    16,
                    4,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-14T18:18:52Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    18,
                    18,
                    52,
                    5,
                    349,
                    0
                ],
                "title": "Tokens, the oft-overlooked appetizer: Large language models, the\n  distributional hypothesis, and meaning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokens, the oft-overlooked appetizer: Large language models, the\n  distributional hypothesis, and meaning"
                },
                "summary": "Tokenization is a necessary component within the current architecture of many\nlanguage models, including the transformer-based large language models (LLMs)\nof Generative AI, yet its impact on the model's cognition is often overlooked.\nWe argue that LLMs demonstrate that the Distributional Hypothesis (DM) is\nsufficient for reasonably human-like language performance, and that the\nemergence of human-meaningful linguistic units among tokens motivates\nlinguistically-informed interventions in existing, linguistically-agnostic\ntokenization techniques, particularly with respect to their roles as (1)\nsemantic primitives and as (2) vehicles for conveying salient distributional\npatterns from human language to the model. We explore tokenizations from a BPE\ntokenizer; extant model vocabularies obtained from Hugging Face and tiktoken;\nand the information in exemplar token vectors as they move through the layers\nof a RoBERTa (large) model. Besides creating sub-optimal semantic building\nblocks and obscuring the model's access to the necessary distributional\npatterns, we describe how tokenization pretraining can be a backdoor for bias\nand other unwanted content, which current alignment practices may not\nremediate. Additionally, we relay evidence that the tokenization algorithm's\nobjective function impacts the LLM's cognition, despite being meaningfully\ninsulated from the main system intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization is a necessary component within the current architecture of many\nlanguage models, including the transformer-based large language models (LLMs)\nof Generative AI, yet its impact on the model's cognition is often overlooked.\nWe argue that LLMs demonstrate that the Distributional Hypothesis (DM) is\nsufficient for reasonably human-like language performance, and that the\nemergence of human-meaningful linguistic units among tokens motivates\nlinguistically-informed interventions in existing, linguistically-agnostic\ntokenization techniques, particularly with respect to their roles as (1)\nsemantic primitives and as (2) vehicles for conveying salient distributional\npatterns from human language to the model. We explore tokenizations from a BPE\ntokenizer; extant model vocabularies obtained from Hugging Face and tiktoken;\nand the information in exemplar token vectors as they move through the layers\nof a RoBERTa (large) model. Besides creating sub-optimal semantic building\nblocks and obscuring the model's access to the necessary distributional\npatterns, we describe how tokenization pretraining can be a backdoor for bias\nand other unwanted content, which current alignment practices may not\nremediate. Additionally, we relay evidence that the tokenization algorithm's\nobjective function impacts the LLM's cognition, despite being meaningfully\ninsulated from the main system intelligence."
                },
                "authors": [
                    {
                        "name": "Julia Witte Zimmerman"
                    },
                    {
                        "name": "Denis Hudon"
                    },
                    {
                        "name": "Kathryn Cramer"
                    },
                    {
                        "name": "Alejandro J. Ruiz"
                    },
                    {
                        "name": "Calla Beauregard"
                    },
                    {
                        "name": "Ashley Fehr"
                    },
                    {
                        "name": "Mikaela Irene Fudolig"
                    },
                    {
                        "name": "Bradford Demarest"
                    },
                    {
                        "name": "Yoshi Meke Bird"
                    },
                    {
                        "name": "Milo Z. Trujillo"
                    },
                    {
                        "name": "Christopher M. Danforth"
                    },
                    {
                        "name": "Peter Sheridan Dodds"
                    }
                ],
                "author_detail": {
                    "name": "Peter Sheridan Dodds"
                },
                "author": "Peter Sheridan Dodds",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10924v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10924v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13998v1",
                "updated": "2024-12-18T16:14:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    14,
                    59,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T16:14:59Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    14,
                    59,
                    2,
                    353,
                    0
                ],
                "title": "Few-shot Steerable Alignment: Adapting Rewards and LLM Policies with\n  Neural Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot Steerable Alignment: Adapting Rewards and LLM Policies with\n  Neural Processes"
                },
                "summary": "As large language models (LLMs) become increasingly embedded in everyday\napplications, ensuring their alignment with the diverse preferences of\nindividual users has become a critical challenge. Currently deployed approaches\ntypically assume homogeneous user objectives and rely on single-objective\nfine-tuning. However, human preferences are inherently heterogeneous,\ninfluenced by various unobservable factors, leading to conflicting signals in\npreference data. Existing solutions addressing this diversity often require\ncostly datasets labelled for specific objectives and involve training multiple\nreward models or LLM policies, which is computationally expensive and\nimpractical. In this work, we present a novel framework for few-shot steerable\nalignment, where users' underlying preferences are inferred from a small sample\nof their choices. To achieve this, we extend the Bradley-Terry-Luce model to\nhandle heterogeneous preferences with unobserved variability factors and\npropose its practical implementation for reward modelling and LLM fine-tuning.\nThanks to our proposed approach of functional parameter-space conditioning,\nLLMs trained with our framework can be adapted to individual preferences at\ninference time, generating outputs over a continuum of behavioural modes. We\nempirically validate the effectiveness of methods, demonstrating their ability\nto capture and align with diverse human preferences in a data-efficient manner.\nOur code is made available at:\nhttps://github.com/kasia-kobalczyk/few-shot-steerable-alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become increasingly embedded in everyday\napplications, ensuring their alignment with the diverse preferences of\nindividual users has become a critical challenge. Currently deployed approaches\ntypically assume homogeneous user objectives and rely on single-objective\nfine-tuning. However, human preferences are inherently heterogeneous,\ninfluenced by various unobservable factors, leading to conflicting signals in\npreference data. Existing solutions addressing this diversity often require\ncostly datasets labelled for specific objectives and involve training multiple\nreward models or LLM policies, which is computationally expensive and\nimpractical. In this work, we present a novel framework for few-shot steerable\nalignment, where users' underlying preferences are inferred from a small sample\nof their choices. To achieve this, we extend the Bradley-Terry-Luce model to\nhandle heterogeneous preferences with unobserved variability factors and\npropose its practical implementation for reward modelling and LLM fine-tuning.\nThanks to our proposed approach of functional parameter-space conditioning,\nLLMs trained with our framework can be adapted to individual preferences at\ninference time, generating outputs over a continuum of behavioural modes. We\nempirically validate the effectiveness of methods, demonstrating their ability\nto capture and align with diverse human preferences in a data-efficient manner.\nOur code is made available at:\nhttps://github.com/kasia-kobalczyk/few-shot-steerable-alignment."
                },
                "authors": [
                    {
                        "name": "Katarzyna Kobalczyk"
                    },
                    {
                        "name": "Claudio Fanconi"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13988v1",
                "updated": "2024-12-18T16:07:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    7,
                    32,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T16:07:32Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    7,
                    32,
                    2,
                    353,
                    0
                ],
                "title": "RAG for Effective Supply Chain Security Questionnaire Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG for Effective Supply Chain Security Questionnaire Automation"
                },
                "summary": "In an era where digital security is crucial, efficient processing of\nsecurity-related inquiries through supply chain security questionnaires is\nimperative. This paper introduces a novel approach using Natural Language\nProcessing (NLP) and Retrieval-Augmented Generation (RAG) to automate these\nresponses. We developed QuestSecure, a system that interprets diverse document\nformats and generates precise responses by integrating large language models\n(LLMs) with an advanced retrieval system. Our experiments show that QuestSecure\nsignificantly improves response accuracy and operational efficiency. By\nemploying advanced NLP techniques and tailored retrieval mechanisms, the system\nconsistently produces contextually relevant and semantically rich responses,\nreducing cognitive load on security teams and minimizing potential errors. This\nresearch offers promising avenues for automating complex security management\ntasks, enhancing organizational security processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an era where digital security is crucial, efficient processing of\nsecurity-related inquiries through supply chain security questionnaires is\nimperative. This paper introduces a novel approach using Natural Language\nProcessing (NLP) and Retrieval-Augmented Generation (RAG) to automate these\nresponses. We developed QuestSecure, a system that interprets diverse document\nformats and generates precise responses by integrating large language models\n(LLMs) with an advanced retrieval system. Our experiments show that QuestSecure\nsignificantly improves response accuracy and operational efficiency. By\nemploying advanced NLP techniques and tailored retrieval mechanisms, the system\nconsistently produces contextually relevant and semantically rich responses,\nreducing cognitive load on security teams and minimizing potential errors. This\nresearch offers promising avenues for automating complex security management\ntasks, enhancing organizational security processes."
                },
                "authors": [
                    {
                        "name": "Zaynab Batool Reza"
                    },
                    {
                        "name": "Abdul Rafay Syed"
                    },
                    {
                        "name": "Omer Iqbal"
                    },
                    {
                        "name": "Ethel Mensah"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Maxx Richard Rahman"
                    },
                    {
                        "name": "Wolfgang Maass"
                    }
                ],
                "author_detail": {
                    "name": "Wolfgang Maass"
                },
                "author": "Wolfgang Maass",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02987v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02987v2",
                "updated": "2024-12-18T16:07:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    7,
                    28,
                    2,
                    353,
                    0
                ],
                "published": "2024-07-03T10:38:40Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    10,
                    38,
                    40,
                    2,
                    185,
                    0
                ],
                "title": "LoRA-Guard: Parameter-Efficient Guardrail Adaptation for Content\n  Moderation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA-Guard: Parameter-Efficient Guardrail Adaptation for Content\n  Moderation of Large Language Models"
                },
                "summary": "Guardrails have emerged as an alternative to safety alignment for content\nmoderation of large language models (LLMs). Existing model-based guardrails\nhave not been designed for resource-constrained computational portable devices,\nsuch as mobile phones, more and more of which are running LLM-based\napplications locally. We introduce LoRA-Guard, a parameter-efficient guardrail\nadaptation method that relies on knowledge sharing between LLMs and guardrail\nmodels. LoRA-Guard extracts language features from the LLMs and adapts them for\nthe content moderation task using low-rank adapters, while a dual-path design\nprevents any performance degradation on the generative task. We show that\nLoRA-Guard outperforms existing approaches with 100-1000x lower parameter\noverhead while maintaining accuracy, enabling on-device content moderation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guardrails have emerged as an alternative to safety alignment for content\nmoderation of large language models (LLMs). Existing model-based guardrails\nhave not been designed for resource-constrained computational portable devices,\nsuch as mobile phones, more and more of which are running LLM-based\napplications locally. We introduce LoRA-Guard, a parameter-efficient guardrail\nadaptation method that relies on knowledge sharing between LLMs and guardrail\nmodels. LoRA-Guard extracts language features from the LLMs and adapts them for\nthe content moderation task using low-rank adapters, while a dual-path design\nprevents any performance degradation on the generative task. We show that\nLoRA-Guard outperforms existing approaches with 100-1000x lower parameter\noverhead while maintaining accuracy, enabling on-device content moderation."
                },
                "authors": [
                    {
                        "name": "Hayder Elesedy"
                    },
                    {
                        "name": "Pedro M. Esperana"
                    },
                    {
                        "name": "Silviu Vlad Oprea"
                    },
                    {
                        "name": "Mete Ozay"
                    }
                ],
                "author_detail": {
                    "name": "Mete Ozay"
                },
                "author": "Mete Ozay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02987v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02987v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09043v3",
                "updated": "2024-12-18T15:56:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    56,
                    29,
                    2,
                    353,
                    0
                ],
                "published": "2024-04-13T16:59:28Z",
                "published_parsed": [
                    2024,
                    4,
                    13,
                    16,
                    59,
                    28,
                    5,
                    104,
                    0
                ],
                "title": "Do LLMs Play Dice? Exploring Probability Distribution Sampling in Large\n  Language Models for Behavioral Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Play Dice? Exploring Probability Distribution Sampling in Large\n  Language Models for Behavioral Simulation"
                },
                "summary": "With the rapid advancement of large language models (LLMs) for handling\ncomplex language tasks, an increasing number of studies are employing LLMs as\nagents to emulate the sequential decision-making processes of humans often\nrepresented as Markov decision-making processes (MDPs). The actions in MDPs\nadhere to specific probability distributions and require iterative sampling.\nThis arouses curiosity regarding the capacity of LLM agents to comprehend\nprobability distributions, thereby guiding the agent's behavioral\ndecision-making through probabilistic sampling and generating behavioral\nsequences. To answer the above question, we divide the problem into two main\naspects: sequence simulation with known probability distribution and sequence\nsimulation with unknown probability distribution. Our analysis indicates that\nLLM agents can understand probabilities, but they struggle with probability\nsampling. Their ability to perform probabilistic sampling can be improved to\nsome extent by integrating coding tools, but this level of sampling precision\nstill makes it difficult to simulate human behavior as agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of large language models (LLMs) for handling\ncomplex language tasks, an increasing number of studies are employing LLMs as\nagents to emulate the sequential decision-making processes of humans often\nrepresented as Markov decision-making processes (MDPs). The actions in MDPs\nadhere to specific probability distributions and require iterative sampling.\nThis arouses curiosity regarding the capacity of LLM agents to comprehend\nprobability distributions, thereby guiding the agent's behavioral\ndecision-making through probabilistic sampling and generating behavioral\nsequences. To answer the above question, we divide the problem into two main\naspects: sequence simulation with known probability distribution and sequence\nsimulation with unknown probability distribution. Our analysis indicates that\nLLM agents can understand probabilities, but they struggle with probability\nsampling. Their ability to perform probabilistic sampling can be improved to\nsome extent by integrating coding tools, but this level of sampling precision\nstill makes it difficult to simulate human behavior as agents."
                },
                "authors": [
                    {
                        "name": "Jia Gu"
                    },
                    {
                        "name": "Liang Pang"
                    },
                    {
                        "name": "Huawei Shen"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "The 31st International Conference on Computational Linguistics\n  (COLING 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09632v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09632v2",
                "updated": "2024-12-18T15:55:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    55,
                    28,
                    2,
                    353,
                    0
                ],
                "published": "2024-11-27T19:53:05Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    19,
                    53,
                    5,
                    2,
                    332,
                    0
                ],
                "title": "Methods to Assess the UK Government's Current Role as a Data Provider\n  for AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Methods to Assess the UK Government's Current Role as a Data Provider\n  for AI"
                },
                "summary": "Governments typically collect and steward a vast amount of high-quality data\non their citizens and institutions, and the UK government is exploring how it\ncan better publish and provision this data to the benefit of the AI landscape.\nHowever, the compositions of generative AI training corpora remain closely\nguarded secrets, making the planning of data sharing initiatives difficult. To\naddress this, we devise two methods to assess UK government data usage for the\ntraining of Large Language Models (LLMs) and 'peek behind the curtain' in order\nto observe the UK government's current contributions as a data provider for AI.\nThe first method, an ablation study that utilises LLM 'unlearning', seeks to\nexamine the importance of the information held on UK government websites for\nLLMs and their performance in citizen query tasks. The second method, an\ninformation leakage study, seeks to ascertain whether LLMs are aware of the\ninformation held in the datasets published on the UK government's open data\ninitiative data.gov.uk. Our findings indicate that UK government websites are\nimportant data sources for AI (heterogenously across subject matters) while\ndata.gov.uk is not. This paper serves as a technical report, explaining\nin-depth the designs, mechanics, and limitations of the above experiments. It\nis accompanied by a complementary non-technical report on the ODI website in\nwhich we summarise the experiments and key findings, interpret them, and build\na set of actionable recommendations for the UK government to take forward as it\nseeks to design AI policy. While we focus on UK open government data, we\nbelieve that the methods introduced in this paper present a reproducible\napproach to tackle the opaqueness of AI training corpora and provide\norganisations a framework to evaluate and maximize their contributions to AI\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Governments typically collect and steward a vast amount of high-quality data\non their citizens and institutions, and the UK government is exploring how it\ncan better publish and provision this data to the benefit of the AI landscape.\nHowever, the compositions of generative AI training corpora remain closely\nguarded secrets, making the planning of data sharing initiatives difficult. To\naddress this, we devise two methods to assess UK government data usage for the\ntraining of Large Language Models (LLMs) and 'peek behind the curtain' in order\nto observe the UK government's current contributions as a data provider for AI.\nThe first method, an ablation study that utilises LLM 'unlearning', seeks to\nexamine the importance of the information held on UK government websites for\nLLMs and their performance in citizen query tasks. The second method, an\ninformation leakage study, seeks to ascertain whether LLMs are aware of the\ninformation held in the datasets published on the UK government's open data\ninitiative data.gov.uk. Our findings indicate that UK government websites are\nimportant data sources for AI (heterogenously across subject matters) while\ndata.gov.uk is not. This paper serves as a technical report, explaining\nin-depth the designs, mechanics, and limitations of the above experiments. It\nis accompanied by a complementary non-technical report on the ODI website in\nwhich we summarise the experiments and key findings, interpret them, and build\na set of actionable recommendations for the UK government to take forward as it\nseeks to design AI policy. While we focus on UK open government data, we\nbelieve that the methods introduced in this paper present a reproducible\napproach to tackle the opaqueness of AI training corpora and provide\norganisations a framework to evaluate and maximize their contributions to AI\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Neil Majithia"
                    },
                    {
                        "name": "Elena Simperl"
                    }
                ],
                "author_detail": {
                    "name": "Elena Simperl"
                },
                "author": "Elena Simperl",
                "arxiv_comment": "17 pages, 5 figures; v2 - incorporated editor feedback; for the\n  accompanying, non-technical ODI report see\n  https://theodi.org/insights/reports/the-uk-government-as-a-data-provider-for-ai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09632v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09632v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13973v1",
                "updated": "2024-12-18T15:50:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    50,
                    50,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T15:50:50Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    50,
                    50,
                    2,
                    353,
                    0
                ],
                "title": "Model-Agnostic Cosmological Inference with SDSS-IV eBOSS: Simultaneous\n  Probing for Background and Perturbed Universe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-Agnostic Cosmological Inference with SDSS-IV eBOSS: Simultaneous\n  Probing for Background and Perturbed Universe"
                },
                "summary": "Here we explore certain subtle features imprinted in data from the completed\nSloan Digital Sky Survey IV (SDSS-IV) extended Baryon Oscillation Spectroscopic\nSurvey (eBOSS) as a combined probe for the background and perturbed Universe.\nWe reconstruct the baryon Acoustic Oscillation (BAO) and Redshift Space\nDistortion (RSD) observables as functions of redshift, using measurements from\nSDSS alone. We apply the Multi-Task Gaussian Process (MTGP) framework to model\nthe interdependencies of cosmological observables $D_M(z)/r_d$, $D_H(z)/r_d$,\nand $f\\sigma_8(z)$, and track their evolution across different redshifts.\nSubsequently, we obtain constrained three-dimensional phase space containing\n$D_M(z)/r_d$, $D_H(z)/r_d$, and $f\\sigma_8(z)$ at different redshifts probed by\nthe SDSS-IV eBOSS survey. Furthermore, assuming the $\\Lambda$CDM model, we\nobtain constraints on model parameters $\\Omega_{m}$, $H_{0}r_{d}$, $\\sigma_{8}$\nand $S_{8}$ at each redshift probed by SDSS-IV eBOSS. This indicates\nredshift-dependent trends in $H_0$, $\\Omega_m$, $\\sigma_8$ and $S_8$ in the\n$\\Lambda$CDM model, suggesting a possible inconsistency in the $\\Lambda$CDM\nmodel. Ours is a template for model-independent extraction of information for\nboth background and perturbed Universe using a single galaxy survey taking into\naccount all the existing correlations between background and perturbed\nobservables and this can be easily extended to future DESI-3YR as well as\nEuclid results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Here we explore certain subtle features imprinted in data from the completed\nSloan Digital Sky Survey IV (SDSS-IV) extended Baryon Oscillation Spectroscopic\nSurvey (eBOSS) as a combined probe for the background and perturbed Universe.\nWe reconstruct the baryon Acoustic Oscillation (BAO) and Redshift Space\nDistortion (RSD) observables as functions of redshift, using measurements from\nSDSS alone. We apply the Multi-Task Gaussian Process (MTGP) framework to model\nthe interdependencies of cosmological observables $D_M(z)/r_d$, $D_H(z)/r_d$,\nand $f\\sigma_8(z)$, and track their evolution across different redshifts.\nSubsequently, we obtain constrained three-dimensional phase space containing\n$D_M(z)/r_d$, $D_H(z)/r_d$, and $f\\sigma_8(z)$ at different redshifts probed by\nthe SDSS-IV eBOSS survey. Furthermore, assuming the $\\Lambda$CDM model, we\nobtain constraints on model parameters $\\Omega_{m}$, $H_{0}r_{d}$, $\\sigma_{8}$\nand $S_{8}$ at each redshift probed by SDSS-IV eBOSS. This indicates\nredshift-dependent trends in $H_0$, $\\Omega_m$, $\\sigma_8$ and $S_8$ in the\n$\\Lambda$CDM model, suggesting a possible inconsistency in the $\\Lambda$CDM\nmodel. Ours is a template for model-independent extraction of information for\nboth background and perturbed Universe using a single galaxy survey taking into\naccount all the existing correlations between background and perturbed\nobservables and this can be easily extended to future DESI-3YR as well as\nEuclid results."
                },
                "authors": [
                    {
                        "name": "Purba Mukherjee"
                    },
                    {
                        "name": "Anjan A. Sen"
                    }
                ],
                "author_detail": {
                    "name": "Anjan A. Sen"
                },
                "author": "Anjan A. Sen",
                "arxiv_comment": "13 pages, 7 sets of figures, 3 tables. Comments are welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13965v1",
                "updated": "2024-12-18T15:44:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    44,
                    59,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T15:44:59Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    44,
                    59,
                    2,
                    353,
                    0
                ],
                "title": "What If: Causal Analysis with Graph Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What If: Causal Analysis with Graph Databases"
                },
                "summary": "Graphs are expressive abstractions representing more effectively\nrelationships in data and enabling data science tasks. They are also a widely\nadopted paradigm in causal inference focusing on causal directed acyclic\ngraphs. Causal DAGs (Directed Acyclic Graphs) are manually curated by domain\nexperts, but they are never validated, stored and integrated as data artifacts\nin a graph data management system. In this paper, we delineate our vision to\nalign these two paradigms, namely causal analysis and property graphs, the\nlatter being the cornerstone of modern graph databases. To articulate this\nvision, a paradigm shift is required leading to rethinking property graph data\nmodels with hypernodes and structural equations, graph query semantics and\nquery constructs, and the definition of graph views to account for causality\noperators. Moreover, several research problems and challenges arise aiming at\nautomatically extracting causal models from the underlying graph observational\ndata, aligning and integrating disparate causal graph models into unified ones\nalong with their maintenance upon the changes in the underlying data. The above\nvision will allow to make graph databases aware of causal knowledge and pave\nthe way to data-driven personalized decision-making in several scientific\nfields.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphs are expressive abstractions representing more effectively\nrelationships in data and enabling data science tasks. They are also a widely\nadopted paradigm in causal inference focusing on causal directed acyclic\ngraphs. Causal DAGs (Directed Acyclic Graphs) are manually curated by domain\nexperts, but they are never validated, stored and integrated as data artifacts\nin a graph data management system. In this paper, we delineate our vision to\nalign these two paradigms, namely causal analysis and property graphs, the\nlatter being the cornerstone of modern graph databases. To articulate this\nvision, a paradigm shift is required leading to rethinking property graph data\nmodels with hypernodes and structural equations, graph query semantics and\nquery constructs, and the definition of graph views to account for causality\noperators. Moreover, several research problems and challenges arise aiming at\nautomatically extracting causal models from the underlying graph observational\ndata, aligning and integrating disparate causal graph models into unified ones\nalong with their maintenance upon the changes in the underlying data. The above\nvision will allow to make graph databases aware of causal knowledge and pave\nthe way to data-driven personalized decision-making in several scientific\nfields."
                },
                "authors": [
                    {
                        "name": "Amedeo Pachera"
                    },
                    {
                        "name": "Mattia Palmiotto"
                    },
                    {
                        "name": "Angela Bonifati"
                    },
                    {
                        "name": "Andrea Mauri"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Mauri"
                },
                "author": "Andrea Mauri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13952v1",
                "updated": "2024-12-18T15:32:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    32,
                    27,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T15:32:27Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    32,
                    27,
                    2,
                    353,
                    0
                ],
                "title": "Prompting Strategies for Enabling Large Language Models to Infer\n  Causation from Correlation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting Strategies for Enabling Large Language Models to Infer\n  Causation from Correlation"
                },
                "summary": "The reasoning abilities of Large Language Models (LLMs) are attracting\nincreasing attention. In this work, we focus on causal reasoning and address\nthe task of establishing causal relationships based on correlation information,\na highly challenging problem on which several LLMs have shown poor performance.\nWe introduce a prompting strategy for this problem that breaks the original\ntask into fixed subquestions, with each subquestion corresponding to one step\nof a formal causal discovery algorithm, the PC algorithm. The proposed\nprompting strategy, PC-SubQ, guides the LLM to follow these algorithmic steps,\nby sequentially prompting it with one subquestion at a time, augmenting the\nnext subquestion's prompt with the answer to the previous one(s). We evaluate\nour approach on an existing causal benchmark, Corr2Cause: our experiments\nindicate a performance improvement across five LLMs when comparing PC-SubQ to\nbaseline prompting strategies. Results are robust to causal query\nperturbations, when modifying the variable names or paraphrasing the\nexpressions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning abilities of Large Language Models (LLMs) are attracting\nincreasing attention. In this work, we focus on causal reasoning and address\nthe task of establishing causal relationships based on correlation information,\na highly challenging problem on which several LLMs have shown poor performance.\nWe introduce a prompting strategy for this problem that breaks the original\ntask into fixed subquestions, with each subquestion corresponding to one step\nof a formal causal discovery algorithm, the PC algorithm. The proposed\nprompting strategy, PC-SubQ, guides the LLM to follow these algorithmic steps,\nby sequentially prompting it with one subquestion at a time, augmenting the\nnext subquestion's prompt with the answer to the previous one(s). We evaluate\nour approach on an existing causal benchmark, Corr2Cause: our experiments\nindicate a performance improvement across five LLMs when comparing PC-SubQ to\nbaseline prompting strategies. Results are robust to causal query\nperturbations, when modifying the variable names or paraphrasing the\nexpressions."
                },
                "authors": [
                    {
                        "name": "Eleni Sgouritsa"
                    },
                    {
                        "name": "Virginia Aglietti"
                    },
                    {
                        "name": "Yee Whye Teh"
                    },
                    {
                        "name": "Arnaud Doucet"
                    },
                    {
                        "name": "Arthur Gretton"
                    },
                    {
                        "name": "Silvia Chiappa"
                    }
                ],
                "author_detail": {
                    "name": "Silvia Chiappa"
                },
                "author": "Silvia Chiappa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13949v1",
                "updated": "2024-12-18T15:29:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    29,
                    30,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T15:29:30Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    29,
                    30,
                    2,
                    353,
                    0
                ],
                "title": "Cracking the Code of Hallucination in LVLMs with Vision-aware Head\n  Divergence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cracking the Code of Hallucination in LVLMs with Vision-aware Head\n  Divergence"
                },
                "summary": "Large vision-language models (LVLMs) have made substantial progress in\nintegrating large language models (LLMs) with visual inputs, enabling advanced\nmultimodal reasoning. Despite their success, a persistent challenge is\nhallucination-where generated text fails to accurately reflect visual\ncontent-undermining both accuracy and reliability. Existing methods focus on\nalignment training or decoding refinements but primarily address symptoms at\nthe generation stage without probing the underlying causes. In this work, we\ninvestigate the internal mechanisms driving hallucination in LVLMs, with an\nemphasis on the multi-head attention module. Specifically, we introduce\nVision-aware Head Divergence (VHD), a metric that quantifies the sensitivity of\nattention head outputs to visual context. Based on this, our findings reveal\nthe presence of vision-aware attention heads that are more attuned to visual\ninformation; however, the model's overreliance on its prior language patterns\nis closely related to hallucinations. Building on these insights, we propose\nVision-aware Head Reinforcement (VHR), a training-free approach to mitigate\nhallucination by enhancing the role of vision-aware attention heads. Extensive\nexperiments demonstrate that our method achieves superior performance compared\nto state-of-the-art approaches in mitigating hallucinations, while maintaining\nhigh efficiency with negligible additional time overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (LVLMs) have made substantial progress in\nintegrating large language models (LLMs) with visual inputs, enabling advanced\nmultimodal reasoning. Despite their success, a persistent challenge is\nhallucination-where generated text fails to accurately reflect visual\ncontent-undermining both accuracy and reliability. Existing methods focus on\nalignment training or decoding refinements but primarily address symptoms at\nthe generation stage without probing the underlying causes. In this work, we\ninvestigate the internal mechanisms driving hallucination in LVLMs, with an\nemphasis on the multi-head attention module. Specifically, we introduce\nVision-aware Head Divergence (VHD), a metric that quantifies the sensitivity of\nattention head outputs to visual context. Based on this, our findings reveal\nthe presence of vision-aware attention heads that are more attuned to visual\ninformation; however, the model's overreliance on its prior language patterns\nis closely related to hallucinations. Building on these insights, we propose\nVision-aware Head Reinforcement (VHR), a training-free approach to mitigate\nhallucination by enhancing the role of vision-aware attention heads. Extensive\nexperiments demonstrate that our method achieves superior performance compared\nto state-of-the-art approaches in mitigating hallucinations, while maintaining\nhigh efficiency with negligible additional time overhead."
                },
                "authors": [
                    {
                        "name": "Jinghan He"
                    },
                    {
                        "name": "Kuan Zhu"
                    },
                    {
                        "name": "Haiyun Guo"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Zhenglin Hua"
                    },
                    {
                        "name": "Yuheng Jia"
                    },
                    {
                        "name": "Ming Tang"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    },
                    {
                        "name": "Jinqiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinqiao Wang"
                },
                "author": "Jinqiao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13942v1",
                "updated": "2024-12-18T15:24:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    24,
                    50,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T15:24:50Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    24,
                    50,
                    2,
                    353,
                    0
                ],
                "title": "A Rose by Any Other Name: LLM-Generated Explanations Are Good Proxies\n  for Human Explanations to Collect Label Distributions on NLI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Rose by Any Other Name: LLM-Generated Explanations Are Good Proxies\n  for Human Explanations to Collect Label Distributions on NLI"
                },
                "summary": "Disagreement in human labeling is ubiquitous, and can be captured in human\njudgment distributions (HJDs). Recent research has shown that explanations\nprovide valuable information for understanding human label variation (HLV) and\nlarge language models (LLMs) can approximate HJD from a few human-provided\nlabel-explanation pairs. However, collecting explanations for every label is\nstill time-consuming. This paper examines whether LLMs can be used to replace\nhumans in generating explanations for approximating HJD. Specifically, we use\nLLMs as annotators to generate model explanations for a few given human labels.\nWe test ways to obtain and combine these label-explanations with the goal to\napproximate human judgment distribution. We further compare the resulting human\nwith model-generated explanations, and test automatic and human explanation\nselection. Our experiments show that LLM explanations are promising for NLI: to\nestimate HJD, generated explanations yield comparable results to human's when\nprovided with human labels. Importantly, our results generalize from datasets\nwith human explanations to i) datasets where they are not available and ii)\nchallenging out-of-distribution test sets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disagreement in human labeling is ubiquitous, and can be captured in human\njudgment distributions (HJDs). Recent research has shown that explanations\nprovide valuable information for understanding human label variation (HLV) and\nlarge language models (LLMs) can approximate HJD from a few human-provided\nlabel-explanation pairs. However, collecting explanations for every label is\nstill time-consuming. This paper examines whether LLMs can be used to replace\nhumans in generating explanations for approximating HJD. Specifically, we use\nLLMs as annotators to generate model explanations for a few given human labels.\nWe test ways to obtain and combine these label-explanations with the goal to\napproximate human judgment distribution. We further compare the resulting human\nwith model-generated explanations, and test automatic and human explanation\nselection. Our experiments show that LLM explanations are promising for NLI: to\nestimate HJD, generated explanations yield comparable results to human's when\nprovided with human labels. Importantly, our results generalize from datasets\nwith human explanations to i) datasets where they are not available and ii)\nchallenging out-of-distribution test sets."
                },
                "authors": [
                    {
                        "name": "Beiduo Chen"
                    },
                    {
                        "name": "Siyao Peng"
                    },
                    {
                        "name": "Anna Korhonen"
                    },
                    {
                        "name": "Barbara Plank"
                    }
                ],
                "author_detail": {
                    "name": "Barbara Plank"
                },
                "author": "Barbara Plank",
                "arxiv_comment": "25 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13924v1",
                "updated": "2024-12-18T15:07:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    7,
                    23,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T15:07:23Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    7,
                    23,
                    2,
                    353,
                    0
                ],
                "title": "Language verY Rare for All",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language verY Rare for All"
                },
                "summary": "In the quest to overcome language barriers, encoder-decoder models like NLLB\nhave expanded machine translation to rare languages, with some models (e.g.,\nNLLB 1.3B) even trainable on a single GPU. While general-purpose LLMs perform\nwell in translation, open LLMs prove highly competitive when fine-tuned for\nspecific tasks involving unknown corpora. We introduce LYRA (Language verY Rare\nfor All), a novel approach that combines open LLM fine-tuning,\nretrieval-augmented generation (RAG), and transfer learning from related\nhigh-resource languages. This study is exclusively focused on single-GPU\ntraining to facilitate ease of adoption. Our study focuses on two-way\ntranslation between French and Mon\\'egasque, a rare language unsupported by\nexisting translation tools due to limited corpus availability. Our results\ndemonstrate LYRA's effectiveness, frequently surpassing and consistently\nmatching state-of-the-art encoder-decoder models in rare language translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the quest to overcome language barriers, encoder-decoder models like NLLB\nhave expanded machine translation to rare languages, with some models (e.g.,\nNLLB 1.3B) even trainable on a single GPU. While general-purpose LLMs perform\nwell in translation, open LLMs prove highly competitive when fine-tuned for\nspecific tasks involving unknown corpora. We introduce LYRA (Language verY Rare\nfor All), a novel approach that combines open LLM fine-tuning,\nretrieval-augmented generation (RAG), and transfer learning from related\nhigh-resource languages. This study is exclusively focused on single-GPU\ntraining to facilitate ease of adoption. Our study focuses on two-way\ntranslation between French and Mon\\'egasque, a rare language unsupported by\nexisting translation tools due to limited corpus availability. Our results\ndemonstrate LYRA's effectiveness, frequently surpassing and consistently\nmatching state-of-the-art encoder-decoder models in rare language translation."
                },
                "authors": [
                    {
                        "name": "Ibrahim Merad"
                    },
                    {
                        "name": "Amos Wolf"
                    },
                    {
                        "name": "Ziad Mazzawi"
                    },
                    {
                        "name": "Yannick Lo"
                    }
                ],
                "author_detail": {
                    "name": "Yannick Lo"
                },
                "author": "Yannick Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13922v1",
                "updated": "2024-12-18T15:05:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    5,
                    59,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T15:05:59Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    5,
                    59,
                    2,
                    353,
                    0
                ],
                "title": "Pipeline Analysis for Developing Instruct LLMs in Low-Resource\n  Languages: A Case Study on Basque",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pipeline Analysis for Developing Instruct LLMs in Low-Resource\n  Languages: A Case Study on Basque"
                },
                "summary": "Large language models (LLMs) are typically optimized for resource-rich\nlanguages like English, exacerbating the gap between high-resource and\nunderrepresented languages. This work presents a detailed analysis of\nstrategies for developing a model capable of following instructions in a\nlow-resource language, specifically Basque, by focusing on three key stages:\npre-training, instruction tuning, and alignment with human preferences. Our\nfindings demonstrate that continual pre-training with a high-quality Basque\ncorpus of around 600 million words improves natural language understanding\n(NLU) of the foundational model by over 12 points. Moreover, instruction tuning\nand human preference alignment using automatically translated datasets proved\nhighly effective, resulting in a 24-point improvement in instruction-following\nperformance. The resulting models, Llama-eus-8B and Llama-eus-8B-instruct,\nestablish a new state-of-the-art for Basque in the sub-10B parameter category.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are typically optimized for resource-rich\nlanguages like English, exacerbating the gap between high-resource and\nunderrepresented languages. This work presents a detailed analysis of\nstrategies for developing a model capable of following instructions in a\nlow-resource language, specifically Basque, by focusing on three key stages:\npre-training, instruction tuning, and alignment with human preferences. Our\nfindings demonstrate that continual pre-training with a high-quality Basque\ncorpus of around 600 million words improves natural language understanding\n(NLU) of the foundational model by over 12 points. Moreover, instruction tuning\nand human preference alignment using automatically translated datasets proved\nhighly effective, resulting in a 24-point improvement in instruction-following\nperformance. The resulting models, Llama-eus-8B and Llama-eus-8B-instruct,\nestablish a new state-of-the-art for Basque in the sub-10B parameter category."
                },
                "authors": [
                    {
                        "name": "Ander Corral"
                    },
                    {
                        "name": "Ixak Sarasua"
                    },
                    {
                        "name": "Xabier Saralegi"
                    }
                ],
                "author_detail": {
                    "name": "Xabier Saralegi"
                },
                "author": "Xabier Saralegi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01638v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01638v4",
                "updated": "2024-12-18T15:01:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    1,
                    32,
                    2,
                    353,
                    0
                ],
                "published": "2024-06-03T00:27:29Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    0,
                    27,
                    29,
                    0,
                    155,
                    0
                ],
                "title": "TimeCMA: Towards LLM-Empowered Multivariate Time Series Forecasting via\n  Cross-Modality Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TimeCMA: Towards LLM-Empowered Multivariate Time Series Forecasting via\n  Cross-Modality Alignment"
                },
                "summary": "Multivariate time series forecasting (MTSF) aims to learn temporal dynamics\namong variables to forecast future time series. Existing statistical and deep\nlearning-based methods suffer from limited learnable parameters and small-scale\ntraining data. Recently, large language models (LLMs) combining time series\nwith textual prompts have achieved promising performance in MTSF. However, we\ndiscovered that current LLM-based solutions fall short in learning disentangled\nembeddings. We introduce TimeCMA, an intuitive yet effective framework for MTSF\nvia cross-modality alignment. Specifically, we present a dual-modality encoding\nwith two branches: the time series encoding branch extracts disentangled yet\nweak time series embeddings, and the LLM-empowered encoding branch wraps the\nsame time series with text as prompts to obtain entangled yet robust prompt\nembeddings. As a result, such a cross-modality alignment retrieves both\ndisentangled and robust time series embeddings, ``the best of two worlds'',\nfrom the prompt embeddings based on time series and prompt modality\nsimilarities. As another key design, to reduce the computational costs from\ntime series with their length textual prompts, we design an effective prompt to\nencourage the most essential temporal information to be encapsulated in the\nlast token: only the last token is passed to downstream prediction. We further\nstore the last token embeddings to accelerate inference speed. Extensive\nexperiments on eight real datasets demonstrate that TimeCMA outperforms\nstate-of-the-arts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivariate time series forecasting (MTSF) aims to learn temporal dynamics\namong variables to forecast future time series. Existing statistical and deep\nlearning-based methods suffer from limited learnable parameters and small-scale\ntraining data. Recently, large language models (LLMs) combining time series\nwith textual prompts have achieved promising performance in MTSF. However, we\ndiscovered that current LLM-based solutions fall short in learning disentangled\nembeddings. We introduce TimeCMA, an intuitive yet effective framework for MTSF\nvia cross-modality alignment. Specifically, we present a dual-modality encoding\nwith two branches: the time series encoding branch extracts disentangled yet\nweak time series embeddings, and the LLM-empowered encoding branch wraps the\nsame time series with text as prompts to obtain entangled yet robust prompt\nembeddings. As a result, such a cross-modality alignment retrieves both\ndisentangled and robust time series embeddings, ``the best of two worlds'',\nfrom the prompt embeddings based on time series and prompt modality\nsimilarities. As another key design, to reduce the computational costs from\ntime series with their length textual prompts, we design an effective prompt to\nencourage the most essential temporal information to be encapsulated in the\nlast token: only the last token is passed to downstream prediction. We further\nstore the last token embeddings to accelerate inference speed. Extensive\nexperiments on eight real datasets demonstrate that TimeCMA outperforms\nstate-of-the-arts."
                },
                "authors": [
                    {
                        "name": "Chenxi Liu"
                    },
                    {
                        "name": "Qianxiong Xu"
                    },
                    {
                        "name": "Hao Miao"
                    },
                    {
                        "name": "Sun Yang"
                    },
                    {
                        "name": "Lingzheng Zhang"
                    },
                    {
                        "name": "Cheng Long"
                    },
                    {
                        "name": "Ziyue Li"
                    },
                    {
                        "name": "Rui Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhao"
                },
                "author": "Rui Zhao",
                "arxiv_comment": "Accepted by AAAI 2025 (Main Technical Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01638v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01638v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13908v1",
                "updated": "2024-12-18T14:51:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    51,
                    25,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T14:51:25Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    51,
                    25,
                    2,
                    353,
                    0
                ],
                "title": "Memorizing SAM: 3D Medical Segment Anything Model with Memorizing\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memorizing SAM: 3D Medical Segment Anything Model with Memorizing\n  Transformer"
                },
                "summary": "Segment Anything Models (SAMs) have gained increasing attention in medical\nimage analysis due to their zero-shot generalization capability in segmenting\nobjects of unseen classes and domains when provided with appropriate user\nprompts. Addressing this performance gap is important to fully leverage the\npre-trained weights of SAMs, particularly in the domain of volumetric medical\nimage segmentation, where accuracy is important but well-annotated 3D medical\ndata for fine-tuning is limited. In this work, we investigate whether\nintroducing the memory mechanism as a plug-in, specifically the ability to\nmemorize and recall internal representations of past inputs, can improve the\nperformance of SAM with limited computation cost. To this end, we propose\nMemorizing SAM, a novel 3D SAM architecture incorporating a memory Transformer\nas a plug-in. Unlike conventional memorizing Transformers that save the\ninternal representation during training or inference, our Memorizing SAM\nutilizes existing highly accurate internal representation as the memory source\nto ensure the quality of memory. We evaluate the performance of Memorizing SAM\nin 33 categories from the TotalSegmentator dataset, which indicates that\nMemorizing SAM can outperform state-of-the-art 3D SAM variant i.e., FastSAM3D\nwith an average Dice increase of 11.36% at the cost of only 4.38 millisecond\nincrease in inference time. The source code is publicly available at\nhttps://github.com/swedfr/memorizingSAM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Segment Anything Models (SAMs) have gained increasing attention in medical\nimage analysis due to their zero-shot generalization capability in segmenting\nobjects of unseen classes and domains when provided with appropriate user\nprompts. Addressing this performance gap is important to fully leverage the\npre-trained weights of SAMs, particularly in the domain of volumetric medical\nimage segmentation, where accuracy is important but well-annotated 3D medical\ndata for fine-tuning is limited. In this work, we investigate whether\nintroducing the memory mechanism as a plug-in, specifically the ability to\nmemorize and recall internal representations of past inputs, can improve the\nperformance of SAM with limited computation cost. To this end, we propose\nMemorizing SAM, a novel 3D SAM architecture incorporating a memory Transformer\nas a plug-in. Unlike conventional memorizing Transformers that save the\ninternal representation during training or inference, our Memorizing SAM\nutilizes existing highly accurate internal representation as the memory source\nto ensure the quality of memory. We evaluate the performance of Memorizing SAM\nin 33 categories from the TotalSegmentator dataset, which indicates that\nMemorizing SAM can outperform state-of-the-art 3D SAM variant i.e., FastSAM3D\nwith an average Dice increase of 11.36% at the cost of only 4.38 millisecond\nincrease in inference time. The source code is publicly available at\nhttps://github.com/swedfr/memorizingSAM"
                },
                "authors": [
                    {
                        "name": "Xinyuan Shao"
                    },
                    {
                        "name": "Yiqing Shen"
                    },
                    {
                        "name": "Mathias Unberath"
                    }
                ],
                "author_detail": {
                    "name": "Mathias Unberath"
                },
                "author": "Mathias Unberath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02692v2",
                "updated": "2024-12-18T14:47:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    47,
                    17,
                    2,
                    353,
                    0
                ],
                "published": "2024-04-03T12:39:37Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    12,
                    39,
                    37,
                    2,
                    94,
                    0
                ],
                "title": "Automated Inference of Graph Transformation Rules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Inference of Graph Transformation Rules"
                },
                "summary": "The explosion of data available in life sciences is fueling an increasing\ndemand for expressive models and computational methods. Graph transformation is\na model for dynamic systems with a large variety of applications. We introduce\na novel method of the graph transformation model construction, combining\ngenerative and dynamical viewpoints to give a fully automated data-driven model\ninference method.\n  The method takes the input dynamical properties, given as a \"snapshot\" of the\ndynamics encoded by explicit transitions, and constructs a compatible model.\nThe obtained model is guaranteed to be minimal, thus framing the approach as\nmodel compression (from a set of transitions into a set of rules). The\ncompression is permissive to a lossy case, where the constructed model is\nallowed to exhibit behavior outside of the input transitions, thus suggesting a\ncompletion of the input dynamics.\n  The task of graph transformation model inference is naturally highly\nchallenging due to the combinatorics involved. We tackle the exponential\nexplosion by proposing a heuristically minimal translation of the task into a\nwell-established problem, set cover, for which highly optimized solutions\nexist. We further showcase how our results relate to Kolmogorov complexity\nexpressed in terms of graph transformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The explosion of data available in life sciences is fueling an increasing\ndemand for expressive models and computational methods. Graph transformation is\na model for dynamic systems with a large variety of applications. We introduce\na novel method of the graph transformation model construction, combining\ngenerative and dynamical viewpoints to give a fully automated data-driven model\ninference method.\n  The method takes the input dynamical properties, given as a \"snapshot\" of the\ndynamics encoded by explicit transitions, and constructs a compatible model.\nThe obtained model is guaranteed to be minimal, thus framing the approach as\nmodel compression (from a set of transitions into a set of rules). The\ncompression is permissive to a lossy case, where the constructed model is\nallowed to exhibit behavior outside of the input transitions, thus suggesting a\ncompletion of the input dynamics.\n  The task of graph transformation model inference is naturally highly\nchallenging due to the combinatorics involved. We tackle the exponential\nexplosion by proposing a heuristically minimal translation of the task into a\nwell-established problem, set cover, for which highly optimized solutions\nexist. We further showcase how our results relate to Kolmogorov complexity\nexpressed in terms of graph transformation."
                },
                "authors": [
                    {
                        "name": "Jakob L. Andersen"
                    },
                    {
                        "name": "Akbar Davoodi"
                    },
                    {
                        "name": "Rolf Fagerberg"
                    },
                    {
                        "name": "Christoph Flamm"
                    },
                    {
                        "name": "Walter Fontana"
                    },
                    {
                        "name": "Juri Kolk"
                    },
                    {
                        "name": "Christophe V. F. P. Laurent"
                    },
                    {
                        "name": "Daniel Merkle"
                    },
                    {
                        "name": "Nikolai Njgaard"
                    }
                ],
                "author_detail": {
                    "name": "Nikolai Njgaard"
                },
                "author": "Nikolai Njgaard",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.MN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.08079v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.08079v4",
                "updated": "2024-12-18T14:46:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    46,
                    23,
                    2,
                    353,
                    0
                ],
                "published": "2023-07-16T15:31:32Z",
                "published_parsed": [
                    2023,
                    7,
                    16,
                    15,
                    31,
                    32,
                    6,
                    197,
                    0
                ],
                "title": "Flexible and efficient emulation of spatial extremes processes via\n  variational autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexible and efficient emulation of spatial extremes processes via\n  variational autoencoders"
                },
                "summary": "Many real-world processes have complex tail dependence structures that cannot\nbe characterized using classical Gaussian processes. More flexible spatial\nextremes models exhibit appealing extremal dependence properties but are often\nexceedingly prohibitive to fit and simulate from in high dimensions. In this\npaper, we aim to push the boundaries on computation and modeling of\nhigh-dimensional spatial extremes via integrating a new spatial extremes model\nthat has flexible and non-stationary dependence properties in the\nencoding-decoding structure of a variational autoencoder called the XVAE. The\nXVAE can emulate spatial observations and produce outputs that have the same\nstatistical properties as the inputs, especially in the tail. Our approach also\nprovides a novel way of making fast inference with complex extreme-value\nprocesses. Through extensive simulation studies, we show that our XVAE is\nsubstantially more time-efficient than traditional Bayesian inference while\noutperforming many spatial extremes models with a stationary dependence\nstructure. Lastly, we analyze a high-resolution satellite-derived dataset of\nsea surface temperature in the Red Sea, which includes 30 years of daily\nmeasurements at 16703 grid cells. We demonstrate how to use XVAE to identify\nregions susceptible to marine heatwaves under climate change and examine the\nspatial and temporal variability of the extremal dependence structure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many real-world processes have complex tail dependence structures that cannot\nbe characterized using classical Gaussian processes. More flexible spatial\nextremes models exhibit appealing extremal dependence properties but are often\nexceedingly prohibitive to fit and simulate from in high dimensions. In this\npaper, we aim to push the boundaries on computation and modeling of\nhigh-dimensional spatial extremes via integrating a new spatial extremes model\nthat has flexible and non-stationary dependence properties in the\nencoding-decoding structure of a variational autoencoder called the XVAE. The\nXVAE can emulate spatial observations and produce outputs that have the same\nstatistical properties as the inputs, especially in the tail. Our approach also\nprovides a novel way of making fast inference with complex extreme-value\nprocesses. Through extensive simulation studies, we show that our XVAE is\nsubstantially more time-efficient than traditional Bayesian inference while\noutperforming many spatial extremes models with a stationary dependence\nstructure. Lastly, we analyze a high-resolution satellite-derived dataset of\nsea surface temperature in the Red Sea, which includes 30 years of daily\nmeasurements at 16703 grid cells. We demonstrate how to use XVAE to identify\nregions susceptible to marine heatwaves under climate change and examine the\nspatial and temporal variability of the extremal dependence structure."
                },
                "authors": [
                    {
                        "name": "Likun Zhang"
                    },
                    {
                        "name": "Xiaoyu Ma"
                    },
                    {
                        "name": "Christopher K. Wikle"
                    },
                    {
                        "name": "Raphal Huser"
                    }
                ],
                "author_detail": {
                    "name": "Raphal Huser"
                },
                "author": "Raphal Huser",
                "arxiv_comment": "30 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.08079v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.08079v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07 (Primary), 60G70, 62H11 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08469v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08469v2",
                "updated": "2024-12-18T14:45:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    45,
                    15,
                    2,
                    353,
                    0
                ],
                "published": "2024-11-13T09:40:37Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    9,
                    40,
                    37,
                    2,
                    318,
                    0
                ],
                "title": "Building Trustworthy AI: Transparent AI Systems via Large Language\n  Models, Ontologies, and Logical Reasoning (TranspNet)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building Trustworthy AI: Transparent AI Systems via Large Language\n  Models, Ontologies, and Logical Reasoning (TranspNet)"
                },
                "summary": "Growing concerns over the lack of transparency in AI, particularly in\nhigh-stakes fields like healthcare and finance, drive the need for explainable\nand trustworthy systems. While Large Language Models (LLMs) perform\nexceptionally well in generating accurate outputs, their \"black box\" nature\nposes significant challenges to transparency and trust. To address this, the\npaper proposes the TranspNet pipeline, which integrates symbolic AI with LLMs.\nBy leveraging domain expert knowledge, retrieval-augmented generation (RAG),\nand formal reasoning frameworks like Answer Set Programming (ASP), TranspNet\nenhances LLM outputs with structured reasoning and verification.This approach\nstrives to help AI systems deliver results that are as accurate, explainable,\nand trustworthy as possible, aligning with regulatory expectations for\ntransparency and accountability. TranspNet provides a solution for developing\nAI systems that are reliable and interpretable, making it suitable for\nreal-world applications where trust is critical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Growing concerns over the lack of transparency in AI, particularly in\nhigh-stakes fields like healthcare and finance, drive the need for explainable\nand trustworthy systems. While Large Language Models (LLMs) perform\nexceptionally well in generating accurate outputs, their \"black box\" nature\nposes significant challenges to transparency and trust. To address this, the\npaper proposes the TranspNet pipeline, which integrates symbolic AI with LLMs.\nBy leveraging domain expert knowledge, retrieval-augmented generation (RAG),\nand formal reasoning frameworks like Answer Set Programming (ASP), TranspNet\nenhances LLM outputs with structured reasoning and verification.This approach\nstrives to help AI systems deliver results that are as accurate, explainable,\nand trustworthy as possible, aligning with regulatory expectations for\ntransparency and accountability. TranspNet provides a solution for developing\nAI systems that are reliable and interpretable, making it suitable for\nreal-world applications where trust is critical."
                },
                "authors": [
                    {
                        "name": "Fadi Al Machot"
                    },
                    {
                        "name": "Martin Thomas Horsch"
                    },
                    {
                        "name": "Habib Ullah"
                    }
                ],
                "author_detail": {
                    "name": "Habib Ullah"
                },
                "author": "Habib Ullah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08469v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08469v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05333v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05333v3",
                "updated": "2024-12-18T14:44:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    44,
                    20,
                    2,
                    353,
                    0
                ],
                "published": "2024-06-08T03:20:46Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    3,
                    20,
                    46,
                    5,
                    160,
                    0
                ],
                "title": "Dissipation rates from experimental uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissipation rates from experimental uncertainty"
                },
                "summary": "Active matter and driven systems exhibit statistical fluctuations in density\nand particle positions, providing an indirect indicator of dissipation across\nmultiple length and time scales. Here, we quantitatively relate these\nmeasurable fluctuations to a thermodynamic speed limit that constrains the\nrates of heat and entropy production in nonequilibrium processes. By\nreparametrizing the speed limit, we show how to infer heat and entropy\nproduction rates from directly observable or controllable quantities. This\napproach can use available experimental data and avoid the need for\nanalytically solvable microscopic models or full time-dependent probability\ndistributions. The heat rate we predict agrees with experimental measurements\nfor a Brownian particle and a microtubule active gel, which validates the\napproach and suggests potential for the design of experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active matter and driven systems exhibit statistical fluctuations in density\nand particle positions, providing an indirect indicator of dissipation across\nmultiple length and time scales. Here, we quantitatively relate these\nmeasurable fluctuations to a thermodynamic speed limit that constrains the\nrates of heat and entropy production in nonequilibrium processes. By\nreparametrizing the speed limit, we show how to infer heat and entropy\nproduction rates from directly observable or controllable quantities. This\napproach can use available experimental data and avoid the need for\nanalytically solvable microscopic models or full time-dependent probability\ndistributions. The heat rate we predict agrees with experimental measurements\nfor a Brownian particle and a microtubule active gel, which validates the\napproach and suggests potential for the design of experiments."
                },
                "authors": [
                    {
                        "name": "Aishani Ghosal"
                    },
                    {
                        "name": "Jason R. Green"
                    }
                ],
                "author_detail": {
                    "name": "Jason R. Green"
                },
                "author": "Jason R. Green",
                "arxiv_comment": "7 pages, 3 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05333v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05333v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13902v1",
                "updated": "2024-12-18T14:42:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    42,
                    43,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T14:42:43Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    42,
                    43,
                    2,
                    353,
                    0
                ],
                "title": "Threshold Neuron: A Brain-inspired Artificial Neuron for Efficient\n  On-device Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Threshold Neuron: A Brain-inspired Artificial Neuron for Efficient\n  On-device Inference"
                },
                "summary": "Enhancing the computational efficiency of on-device Deep Neural Networks\n(DNNs) remains a significant challengein mobile and edge computing. As we aim\nto execute increasingly complex tasks with constrained computational resources,\nmuch of the research has focused on compressing neural network structures and\noptimizing systems. Although many studies have focused on compressing neural\nnetwork structures and parameters or optimizing underlying systems, there has\nbeen limited attention on optimizing the fundamental building blocks of neural\nnetworks: the neurons. In this study, we deliberate on a simple but important\nresearch question: Can we design artificial neurons that offer greater\nefficiency than the traditional neuron paradigm? Inspired by the threshold\nmechanisms and the excitation-inhibition balance observed in biological\nneurons, we propose a novel artificial neuron model, Threshold Neurons. Using\nThreshold Neurons, we can construct neural networks similar to those with\ntraditional artificial neurons, while significantly reducing hardware\nimplementation complexity. Our extensive experiments validate the effectiveness\nof neural networks utilizing Threshold Neurons, achieving substantial power\nsavings of 7.51x to 8.19x and area savings of 3.89x to 4.33x at the kernel\nlevel, with minimal loss in precision. Furthermore, FPGA-based implementations\nof these networks demonstrate 2.52x power savings and 1.75x speed enhancements\nat the system level. The source code will be made available upon publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing the computational efficiency of on-device Deep Neural Networks\n(DNNs) remains a significant challengein mobile and edge computing. As we aim\nto execute increasingly complex tasks with constrained computational resources,\nmuch of the research has focused on compressing neural network structures and\noptimizing systems. Although many studies have focused on compressing neural\nnetwork structures and parameters or optimizing underlying systems, there has\nbeen limited attention on optimizing the fundamental building blocks of neural\nnetworks: the neurons. In this study, we deliberate on a simple but important\nresearch question: Can we design artificial neurons that offer greater\nefficiency than the traditional neuron paradigm? Inspired by the threshold\nmechanisms and the excitation-inhibition balance observed in biological\nneurons, we propose a novel artificial neuron model, Threshold Neurons. Using\nThreshold Neurons, we can construct neural networks similar to those with\ntraditional artificial neurons, while significantly reducing hardware\nimplementation complexity. Our extensive experiments validate the effectiveness\nof neural networks utilizing Threshold Neurons, achieving substantial power\nsavings of 7.51x to 8.19x and area savings of 3.89x to 4.33x at the kernel\nlevel, with minimal loss in precision. Furthermore, FPGA-based implementations\nof these networks demonstrate 2.52x power savings and 1.75x speed enhancements\nat the system level. The source code will be made available upon publication."
                },
                "authors": [
                    {
                        "name": "Zihao Zheng"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Jiayu Chen"
                    },
                    {
                        "name": "Peng Zhou"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Yunxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunxin Liu"
                },
                "author": "Yunxin Liu",
                "arxiv_comment": "14 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13899v1",
                "updated": "2024-12-18T14:40:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    40,
                    45,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T14:40:45Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    40,
                    45,
                    2,
                    353,
                    0
                ],
                "title": "Ion Sieving in Two-Dimensional Membranes from First Principles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ion Sieving in Two-Dimensional Membranes from First Principles"
                },
                "summary": "A first-principles approach for calculating ion separation in solution\nthrough two-dimensional (2D) membranes is proposed and applied. Ionic energy\nprofiles across the membrane are obtained first, where solvation effects are\nsimulated explicitly with machine-learning molecular dynamics, electrostatic\ncorrections are applied to remove finite-size capacitive effects, and a\nmean-field treatment of the charging of the electrochemical double layer is\nused. Entropic contributions are assessed analytically and validated against\nthermodynamic integration. Ionic separations are then inferred through a\nmicrokinetic model of the filtration process, accounting for steady-state\ncharge separation effects across the membrane. The approach is applied to\nLi$^{+}$, Na$^{+}$, K$^{+}$ sieving through a crown-ether functionalized\ngraphene membrane, with a case study of the mechanisms for a highly selective\nand efficient extraction of lithium from aqueous solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A first-principles approach for calculating ion separation in solution\nthrough two-dimensional (2D) membranes is proposed and applied. Ionic energy\nprofiles across the membrane are obtained first, where solvation effects are\nsimulated explicitly with machine-learning molecular dynamics, electrostatic\ncorrections are applied to remove finite-size capacitive effects, and a\nmean-field treatment of the charging of the electrochemical double layer is\nused. Entropic contributions are assessed analytically and validated against\nthermodynamic integration. Ionic separations are then inferred through a\nmicrokinetic model of the filtration process, accounting for steady-state\ncharge separation effects across the membrane. The approach is applied to\nLi$^{+}$, Na$^{+}$, K$^{+}$ sieving through a crown-ether functionalized\ngraphene membrane, with a case study of the mechanisms for a highly selective\nand efficient extraction of lithium from aqueous solutions."
                },
                "authors": [
                    {
                        "name": "Nicphore Bonnet"
                    },
                    {
                        "name": "Nicola Marzari"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Marzari"
                },
                "author": "Nicola Marzari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13897v1",
                "updated": "2024-12-18T14:39:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    39,
                    43,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T14:39:43Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    39,
                    43,
                    2,
                    353,
                    0
                ],
                "title": "Data-Efficient Inference of Neural Fluid Fields via SciML Foundation\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Efficient Inference of Neural Fluid Fields via SciML Foundation\n  Model"
                },
                "summary": "Recent developments in 3D vision have enabled successful progress in\ninferring neural fluid fields and realistic rendering of fluid dynamics.\nHowever, these methods require real-world flow captures, which demand dense\nvideo sequences and specialized lab setups, making the process costly and\nchallenging. Scientific machine learning (SciML) foundation models, which are\npretrained on extensive simulations of partial differential equations (PDEs),\nencode rich multiphysics knowledge and thus provide promising sources of domain\npriors for inferring fluid fields. Nevertheless, their potential to advance\nreal-world vision problems remains largely underexplored, raising questions\nabout the transferability and practical utility of these foundation models. In\nthis work, we demonstrate that SciML foundation model can significantly improve\nthe data efficiency of inferring real-world 3D fluid dynamics with improved\ngeneralization. At the core of our method is leveraging the strong forecasting\ncapabilities and meaningful representations of SciML foundation models. We\nequip neural fluid fields with a novel collaborative training approach that\nutilizes augmented views and fluid features extracted by our foundation model.\nOur method demonstrates significant improvements in both quantitative metrics\nand visual quality, showcasing the practical applicability of SciML foundation\nmodels in real-world fluid dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in 3D vision have enabled successful progress in\ninferring neural fluid fields and realistic rendering of fluid dynamics.\nHowever, these methods require real-world flow captures, which demand dense\nvideo sequences and specialized lab setups, making the process costly and\nchallenging. Scientific machine learning (SciML) foundation models, which are\npretrained on extensive simulations of partial differential equations (PDEs),\nencode rich multiphysics knowledge and thus provide promising sources of domain\npriors for inferring fluid fields. Nevertheless, their potential to advance\nreal-world vision problems remains largely underexplored, raising questions\nabout the transferability and practical utility of these foundation models. In\nthis work, we demonstrate that SciML foundation model can significantly improve\nthe data efficiency of inferring real-world 3D fluid dynamics with improved\ngeneralization. At the core of our method is leveraging the strong forecasting\ncapabilities and meaningful representations of SciML foundation models. We\nequip neural fluid fields with a novel collaborative training approach that\nutilizes augmented views and fluid features extracted by our foundation model.\nOur method demonstrates significant improvements in both quantitative metrics\nand visual quality, showcasing the practical applicability of SciML foundation\nmodels in real-world fluid dynamics."
                },
                "authors": [
                    {
                        "name": "Yuqiu Liu"
                    },
                    {
                        "name": "Jingxuan Xu"
                    },
                    {
                        "name": "Mauricio Soroco"
                    },
                    {
                        "name": "Yunchao Wei"
                    },
                    {
                        "name": "Wuyang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wuyang Chen"
                },
                "author": "Wuyang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15380v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15380v3",
                "updated": "2024-12-18T14:39:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    39,
                    2,
                    2,
                    353,
                    0
                ],
                "published": "2024-09-20T15:01:21Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    15,
                    1,
                    21,
                    4,
                    264,
                    0
                ],
                "title": "Kalahi: A handcrafted, grassroots cultural LLM evaluation suite for\n  Filipino",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kalahi: A handcrafted, grassroots cultural LLM evaluation suite for\n  Filipino"
                },
                "summary": "Multilingual large language models (LLMs) today may not necessarily provide\nculturally appropriate and relevant responses to its Filipino users. We\nintroduce Kalahi, a cultural LLM evaluation suite collaboratively created by\nnative Filipino speakers. It is composed of 150 high-quality, handcrafted and\nnuanced prompts that test LLMs for generations that are relevant to shared\nFilipino cultural knowledge and values. Strong LLM performance in Kalahi\nindicates a model's ability to generate responses similar to what an average\nFilipino would say or do in a given situation. We conducted experiments on LLMs\nwith multilingual and Filipino language support. Results show that Kalahi,\nwhile trivial for Filipinos, is challenging for LLMs, with the best model\nanswering only 46.0% of the questions correctly compared to native Filipino\nperformance of 89.10%. Thus, Kalahi can be used to accurately and reliably\nevaluate Filipino cultural representation in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual large language models (LLMs) today may not necessarily provide\nculturally appropriate and relevant responses to its Filipino users. We\nintroduce Kalahi, a cultural LLM evaluation suite collaboratively created by\nnative Filipino speakers. It is composed of 150 high-quality, handcrafted and\nnuanced prompts that test LLMs for generations that are relevant to shared\nFilipino cultural knowledge and values. Strong LLM performance in Kalahi\nindicates a model's ability to generate responses similar to what an average\nFilipino would say or do in a given situation. We conducted experiments on LLMs\nwith multilingual and Filipino language support. Results show that Kalahi,\nwhile trivial for Filipinos, is challenging for LLMs, with the best model\nanswering only 46.0% of the questions correctly compared to native Filipino\nperformance of 89.10%. Thus, Kalahi can be used to accurately and reliably\nevaluate Filipino cultural representation in LLMs."
                },
                "authors": [
                    {
                        "name": "Jann Railey Montalan"
                    },
                    {
                        "name": "Jian Gang Ngui"
                    },
                    {
                        "name": "Wei Qi Leong"
                    },
                    {
                        "name": "Yosephine Susanto"
                    },
                    {
                        "name": "Hamsawardhini Rengarajan"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    },
                    {
                        "name": "William Chandra Tjhi"
                    }
                ],
                "author_detail": {
                    "name": "William Chandra Tjhi"
                },
                "author": "William Chandra Tjhi",
                "arxiv_comment": "Accepted for presentation at Paclic 38, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15380v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15380v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08074v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08074v3",
                "updated": "2024-12-18T14:38:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    38,
                    31,
                    2,
                    353,
                    0
                ],
                "published": "2024-08-15T11:01:35Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    11,
                    1,
                    35,
                    3,
                    228,
                    0
                ],
                "title": "A Survey on Integrated Sensing, Communication, and Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Integrated Sensing, Communication, and Computation"
                },
                "summary": "The forthcoming generation of wireless technology, 6G, aims to usher in an\nera of ubiquitous intelligent services, where everything is interconnected and\nintelligent. This vision requires the seamless integration of three fundamental\nmodules: Sensing for information acquisition, communication for information\nsharing, and computation for information processing and decision-making. These\nmodules are intricately linked, especially in complex tasks such as edge\nlearning and inference. However, the performance of these modules is\ninterdependent, creating a resource competition for time, energy, and\nbandwidth. Existing techniques like integrated communication and computation\n(ICC), integrated sensing and computation (ISC), and integrated sensing and\ncommunication (ISAC) have made partial strides in addressing this challenge,\nbut they fall short of meeting the extreme performance requirements. To\novercome these limitations, it is essential to develop new techniques that\ncomprehensively integrate sensing, communication, and computation. This\nintegrated approach, known as Integrated Sensing, Communication, and\nComputation (ISCC), offers a systematic perspective for enhancing task\nperformance. This paper begins with a comprehensive survey of historic and\nrelated techniques such as ICC, ISC, and ISAC, highlighting their strengths and\nlimitations. It then discusses the benefits, functions, and challenges of ISCC.\nSubsequently, the state-of-the-art signal designs for ISCC, along with network\nresource management strategies specifically tailored for ISCC are explored.\nFurthermore, this paper discusses the exciting research opportunities that lie\nahead for implementing ISCC in future advanced networks, and the unresolved\nissues requiring further investigation. ISCC is expected to unlock the full\npotential of intelligent connectivity, paving the way for groundbreaking\napplications and services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The forthcoming generation of wireless technology, 6G, aims to usher in an\nera of ubiquitous intelligent services, where everything is interconnected and\nintelligent. This vision requires the seamless integration of three fundamental\nmodules: Sensing for information acquisition, communication for information\nsharing, and computation for information processing and decision-making. These\nmodules are intricately linked, especially in complex tasks such as edge\nlearning and inference. However, the performance of these modules is\ninterdependent, creating a resource competition for time, energy, and\nbandwidth. Existing techniques like integrated communication and computation\n(ICC), integrated sensing and computation (ISC), and integrated sensing and\ncommunication (ISAC) have made partial strides in addressing this challenge,\nbut they fall short of meeting the extreme performance requirements. To\novercome these limitations, it is essential to develop new techniques that\ncomprehensively integrate sensing, communication, and computation. This\nintegrated approach, known as Integrated Sensing, Communication, and\nComputation (ISCC), offers a systematic perspective for enhancing task\nperformance. This paper begins with a comprehensive survey of historic and\nrelated techniques such as ICC, ISC, and ISAC, highlighting their strengths and\nlimitations. It then discusses the benefits, functions, and challenges of ISCC.\nSubsequently, the state-of-the-art signal designs for ISCC, along with network\nresource management strategies specifically tailored for ISCC are explored.\nFurthermore, this paper discusses the exciting research opportunities that lie\nahead for implementing ISCC in future advanced networks, and the unresolved\nissues requiring further investigation. ISCC is expected to unlock the full\npotential of intelligent connectivity, paving the way for groundbreaking\napplications and services."
                },
                "authors": [
                    {
                        "name": "Dingzhu Wen"
                    },
                    {
                        "name": "Yong Zhou"
                    },
                    {
                        "name": "Xiaoyang Li"
                    },
                    {
                        "name": "Yuanming Shi"
                    },
                    {
                        "name": "Kaibin Huang"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief",
                "arxiv_comment": "This version is accepted by IEEE Communications Surveys & Tutorials\n  on Dec. 18, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08074v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08074v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18416v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18416v3",
                "updated": "2024-12-18T14:25:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    25,
                    8,
                    2,
                    353,
                    0
                ],
                "published": "2024-07-25T22:24:45Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    22,
                    24,
                    45,
                    3,
                    207,
                    0
                ],
                "title": "PersonaGym: Evaluating Persona Agents and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonaGym: Evaluating Persona Agents and LLMs"
                },
                "summary": "Persona agents, which are LLM agents that act according to an assigned\npersona, have demonstrated impressive contextual response capabilities across\nvarious applications. These persona agents offer significant enhancements\nacross diverse sectors, such as education, healthcare, and entertainment, where\nmodel developers can align agent responses to different user requirements\nthereby broadening the scope of agent applications. However, evaluating persona\nagent performance is incredibly challenging due to the complexity of assessing\npersona adherence in free-form interactions across various environments that\nare relevant to each persona agent. We introduce PersonaGym, the first dynamic\nevaluation framework for assessing persona agents, and PersonaScore, the first\nautomated human-aligned metric grounded in decision theory for comprehensive\nlarge-scale evaluation of persona agents. Our evaluation of 6 open and\nclosed-source LLMs, using a benchmark encompassing 200 personas and 10,000\nquestions, reveals significant opportunities for advancement in persona agent\ncapabilities across state-of-the-art models. For example, Claude 3.5 Sonnet\nonly has a 2.97% relative improvement in PersonaScore than GPT 3.5 despite\nbeing a much more advanced model. Importantly, we find that increased model\nsize and complexity do not necessarily imply enhanced persona agent\ncapabilities thereby highlighting the pressing need for algorithmic and\narchitectural invention towards faithful and performant persona agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persona agents, which are LLM agents that act according to an assigned\npersona, have demonstrated impressive contextual response capabilities across\nvarious applications. These persona agents offer significant enhancements\nacross diverse sectors, such as education, healthcare, and entertainment, where\nmodel developers can align agent responses to different user requirements\nthereby broadening the scope of agent applications. However, evaluating persona\nagent performance is incredibly challenging due to the complexity of assessing\npersona adherence in free-form interactions across various environments that\nare relevant to each persona agent. We introduce PersonaGym, the first dynamic\nevaluation framework for assessing persona agents, and PersonaScore, the first\nautomated human-aligned metric grounded in decision theory for comprehensive\nlarge-scale evaluation of persona agents. Our evaluation of 6 open and\nclosed-source LLMs, using a benchmark encompassing 200 personas and 10,000\nquestions, reveals significant opportunities for advancement in persona agent\ncapabilities across state-of-the-art models. For example, Claude 3.5 Sonnet\nonly has a 2.97% relative improvement in PersonaScore than GPT 3.5 despite\nbeing a much more advanced model. Importantly, we find that increased model\nsize and complexity do not necessarily imply enhanced persona agent\ncapabilities thereby highlighting the pressing need for algorithmic and\narchitectural invention towards faithful and performant persona agents."
                },
                "authors": [
                    {
                        "name": "Vinay Samuel"
                    },
                    {
                        "name": "Henry Peng Zou"
                    },
                    {
                        "name": "Yue Zhou"
                    },
                    {
                        "name": "Shreyas Chaudhari"
                    },
                    {
                        "name": "Ashwin Kalyan"
                    },
                    {
                        "name": "Tanmay Rajpurohit"
                    },
                    {
                        "name": "Ameet Deshpande"
                    },
                    {
                        "name": "Karthik Narasimhan"
                    },
                    {
                        "name": "Vishvak Murahari"
                    }
                ],
                "author_detail": {
                    "name": "Vishvak Murahari"
                },
                "author": "Vishvak Murahari",
                "arxiv_comment": "21 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18416v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18416v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13879v1",
                "updated": "2024-12-18T14:19:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    19,
                    23,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T14:19:23Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    19,
                    23,
                    2,
                    353,
                    0
                ],
                "title": "Crabs: Consuming Resrouce via Auto-generation for LLM-DoS Attack under\n  Black-box Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crabs: Consuming Resrouce via Auto-generation for LLM-DoS Attack under\n  Black-box Settings"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse tasks. LLMs continue to be vulnerable to external threats, particularly\nDenial-of-Service (DoS) attacks. Specifically, LLM-DoS attacks aim to exhaust\ncomputational resources and block services. However, prior works tend to focus\non performing white-box attacks, overlooking black-box settings. In this work,\nwe propose an automated algorithm designed for black-box LLMs, called\nAuto-Generation for LLM-DoS Attack (AutoDoS). AutoDoS introduces DoS Attack\nTree and optimizes the prompt node coverage to enhance effectiveness under\nblack-box conditions. Our method can bypass existing defense with enhanced\nstealthiness via semantic improvement of prompt nodes. Furthermore, we reveal\nthat implanting Length Trojan in Basic DoS Prompt aids in achieving higher\nattack efficacy. Experimental results show that AutoDoS amplifies service\nresponse latency by over 250 $\\times \\uparrow$, leading to severe resource\nconsumption in terms of GPU utilization and memory usage. Our code is available\nat \\url{https://github.com/shuita2333/AutoDoS}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse tasks. LLMs continue to be vulnerable to external threats, particularly\nDenial-of-Service (DoS) attacks. Specifically, LLM-DoS attacks aim to exhaust\ncomputational resources and block services. However, prior works tend to focus\non performing white-box attacks, overlooking black-box settings. In this work,\nwe propose an automated algorithm designed for black-box LLMs, called\nAuto-Generation for LLM-DoS Attack (AutoDoS). AutoDoS introduces DoS Attack\nTree and optimizes the prompt node coverage to enhance effectiveness under\nblack-box conditions. Our method can bypass existing defense with enhanced\nstealthiness via semantic improvement of prompt nodes. Furthermore, we reveal\nthat implanting Length Trojan in Basic DoS Prompt aids in achieving higher\nattack efficacy. Experimental results show that AutoDoS amplifies service\nresponse latency by over 250 $\\times \\uparrow$, leading to severe resource\nconsumption in terms of GPU utilization and memory usage. Our code is available\nat \\url{https://github.com/shuita2333/AutoDoS}."
                },
                "authors": [
                    {
                        "name": "Yuanhe Zhang"
                    },
                    {
                        "name": "Zhenhong Zhou"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Xinyue Wang"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Sen Su"
                    }
                ],
                "author_detail": {
                    "name": "Sen Su"
                },
                "author": "Sen Su",
                "arxiv_comment": "20 pages, 7 figures, 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23111v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23111v3",
                "updated": "2024-12-18T14:10:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    10,
                    10,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-30T15:23:44Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    23,
                    44,
                    2,
                    304,
                    0
                ],
                "title": "Exploring Gradient Subspaces: Addressing and Overcoming LoRA's\n  Limitations in Federated Fine-Tuning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Gradient Subspaces: Addressing and Overcoming LoRA's\n  Limitations in Federated Fine-Tuning of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, particularly in task generalization for both text and vision\ndata. While fine-tuning these models can significantly enhance their\nperformance on specific downstream tasks, it often requires high-quality data\nthat cannot be shared due to privacy concerns. Federated Learning (FL) offers a\npromising solution for collaborative training without direct data sharing.\nHowever, many parameter-efficient fine-tuning strategies for LLMs in FL,\nparticularly those based on Low-Rank Adaptation (LoRA), face limitations. In\nthis paper, we critically analyze the convergence and performance guarantees of\npopular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to\nconstrained subspace learning of low-rank matrices. This limitation hinders\neffective fine-tuning of LLMs in federated settings. Through rigorous\nanalytical and empirical evaluations, we demonstrate that direct weight\naveraging outperforms LoRA-based strategies, leading to superior performance\nfor fine-tuned models. Our comprehensive comparison unmasks inefficiencies in\nLoRA approaches and underscores the advantages of direct weight aggregation. We\nextend our analysis to low-rank gradient-based optimizers, such as GaLore, used\nduring local training steps. Our findings show that GaLore along with\ndirect-weight aggregation is a more effective approach, outperforming federated\nLoRA methods like FlexLoRA and FFA-LoRA across both text and image modalities.\nWhile privacy remains paramount in FL discourse, our focus is on assessing\nperformance outcomes of federated fine-tuned models and evaluating various FL\nframeworks from both theoretical and empirical perspectives. Our findings\nadvocate reassessing the reliance on LoRA within FL contexts, paving the way\nfor more efficient training methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, particularly in task generalization for both text and vision\ndata. While fine-tuning these models can significantly enhance their\nperformance on specific downstream tasks, it often requires high-quality data\nthat cannot be shared due to privacy concerns. Federated Learning (FL) offers a\npromising solution for collaborative training without direct data sharing.\nHowever, many parameter-efficient fine-tuning strategies for LLMs in FL,\nparticularly those based on Low-Rank Adaptation (LoRA), face limitations. In\nthis paper, we critically analyze the convergence and performance guarantees of\npopular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to\nconstrained subspace learning of low-rank matrices. This limitation hinders\neffective fine-tuning of LLMs in federated settings. Through rigorous\nanalytical and empirical evaluations, we demonstrate that direct weight\naveraging outperforms LoRA-based strategies, leading to superior performance\nfor fine-tuned models. Our comprehensive comparison unmasks inefficiencies in\nLoRA approaches and underscores the advantages of direct weight aggregation. We\nextend our analysis to low-rank gradient-based optimizers, such as GaLore, used\nduring local training steps. Our findings show that GaLore along with\ndirect-weight aggregation is a more effective approach, outperforming federated\nLoRA methods like FlexLoRA and FFA-LoRA across both text and image modalities.\nWhile privacy remains paramount in FL discourse, our focus is on assessing\nperformance outcomes of federated fine-tuned models and evaluating various FL\nframeworks from both theoretical and empirical perspectives. Our findings\nadvocate reassessing the reliance on LoRA within FL contexts, paving the way\nfor more efficient training methodologies."
                },
                "authors": [
                    {
                        "name": "Navyansh Mahla"
                    },
                    {
                        "name": "Ganesh Ramakrishnan"
                    }
                ],
                "author_detail": {
                    "name": "Ganesh Ramakrishnan"
                },
                "author": "Ganesh Ramakrishnan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23111v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23111v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06593v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06593v2",
                "updated": "2024-12-18T14:08:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    8,
                    45,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-09T15:45:03Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    15,
                    45,
                    3,
                    0,
                    344,
                    0
                ],
                "title": "Anchoring Bias in Large Language Models: An Experimental Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anchoring Bias in Large Language Models: An Experimental Study"
                },
                "summary": "Large Language Models (LLMs) like GPT-4 and Gemini have significantly\nadvanced artificial intelligence by enabling machines to generate and\ncomprehend human-like text. Despite their impressive capabilities, LLMs are not\nimmune to limitations, including various biases. While much research has\nexplored demographic biases, the cognitive biases in LLMs have not been equally\nscrutinized. This study delves into anchoring bias, a cognitive bias where\ninitial information disproportionately influences judgment. Utilizing an\nexperimental dataset, we examine how anchoring bias manifests in LLMs and\nverify the effectiveness of various mitigation strategies. Our findings\nhighlight the sensitivity of LLM responses to biased hints. At the same time,\nour experiments show that, to mitigate anchoring bias, one needs to collect\nhints from comprehensive angles to prevent the LLMs from being anchored to\nindividual pieces of information, while simple algorithms such as\nChain-of-Thought, Thoughts of Principles, Ignoring Anchor Hints, and Reflection\nare not sufficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) like GPT-4 and Gemini have significantly\nadvanced artificial intelligence by enabling machines to generate and\ncomprehend human-like text. Despite their impressive capabilities, LLMs are not\nimmune to limitations, including various biases. While much research has\nexplored demographic biases, the cognitive biases in LLMs have not been equally\nscrutinized. This study delves into anchoring bias, a cognitive bias where\ninitial information disproportionately influences judgment. Utilizing an\nexperimental dataset, we examine how anchoring bias manifests in LLMs and\nverify the effectiveness of various mitigation strategies. Our findings\nhighlight the sensitivity of LLM responses to biased hints. At the same time,\nour experiments show that, to mitigate anchoring bias, one needs to collect\nhints from comprehensive angles to prevent the LLMs from being anchored to\nindividual pieces of information, while simple algorithms such as\nChain-of-Thought, Thoughts of Principles, Ignoring Anchor Hints, and Reflection\nare not sufficient."
                },
                "authors": [
                    {
                        "name": "Jiaxu Lou"
                    },
                    {
                        "name": "Yifan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Sun"
                },
                "author": "Yifan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06593v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06593v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13178v2",
                "updated": "2024-12-18T14:00:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    0,
                    2,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-17T18:55:58Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    55,
                    58,
                    1,
                    352,
                    0
                ],
                "title": "SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM\n  Agents"
                },
                "summary": "With the integration of large language models (LLMs), embodied agents have\nstrong capabilities to execute complicated instructions in natural language,\npaving a way for the potential deployment of embodied robots. However, a\nforeseeable issue is that those embodied agents can also flawlessly execute\nsome hazardous tasks, potentially causing damages in real world. To study this\nissue, we present SafeAgentBench -- a new benchmark for safety-aware task\nplanning of embodied LLM agents. SafeAgentBench includes: (1) a new dataset\nwith 750 tasks, covering 10 potential hazards and 3 task types; (2)\nSafeAgentEnv, a universal embodied environment with a low-level controller,\nsupporting multi-agent execution with 17 high-level actions for 8\nstate-of-the-art baselines; and (3) reliable evaluation methods from both\nexecution and semantic perspectives. Experimental results show that the\nbest-performing baseline gets 69% success rate for safe tasks, but only 5%\nrejection rate for hazardous tasks, indicating significant safety risks. More\ndetails and codes are available at\nhttps://github.com/shengyin1224/SafeAgentBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the integration of large language models (LLMs), embodied agents have\nstrong capabilities to execute complicated instructions in natural language,\npaving a way for the potential deployment of embodied robots. However, a\nforeseeable issue is that those embodied agents can also flawlessly execute\nsome hazardous tasks, potentially causing damages in real world. To study this\nissue, we present SafeAgentBench -- a new benchmark for safety-aware task\nplanning of embodied LLM agents. SafeAgentBench includes: (1) a new dataset\nwith 750 tasks, covering 10 potential hazards and 3 task types; (2)\nSafeAgentEnv, a universal embodied environment with a low-level controller,\nsupporting multi-agent execution with 17 high-level actions for 8\nstate-of-the-art baselines; and (3) reliable evaluation methods from both\nexecution and semantic perspectives. Experimental results show that the\nbest-performing baseline gets 69% success rate for safe tasks, but only 5%\nrejection rate for hazardous tasks, indicating significant safety risks. More\ndetails and codes are available at\nhttps://github.com/shengyin1224/SafeAgentBench."
                },
                "authors": [
                    {
                        "name": "Sheng Yin"
                    },
                    {
                        "name": "Xianghe Pang"
                    },
                    {
                        "name": "Yuanzhuo Ding"
                    },
                    {
                        "name": "Menglan Chen"
                    },
                    {
                        "name": "Yutong Bi"
                    },
                    {
                        "name": "Yichen Xiong"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Zhen Xiang"
                    },
                    {
                        "name": "Jing Shao"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "arxiv_comment": "21 pages, 14 tables, 7 figures, submitted to ICRA 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13862v1",
                "updated": "2024-12-18T13:55:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    55,
                    42,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T13:55:42Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    55,
                    42,
                    2,
                    353,
                    0
                ],
                "title": "Energy-Based Preference Model Offers Better Offline Alignment than the\n  Bradley-Terry Preference Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Based Preference Model Offers Better Offline Alignment than the\n  Bradley-Terry Preference Model"
                },
                "summary": "Since the debut of DPO, it has been shown that aligning a target LLM with\nhuman preferences via the KL-constrained RLHF loss is mathematically equivalent\nto a special kind of reward modeling task. Concretely, the task requires: 1)\nusing the target LLM to parameterize the reward model, and 2) tuning the reward\nmodel so that it has a 1:1 linear relationship with the true reward. However,\nwe identify a significant issue: the DPO loss might have multiple minimizers,\nof which only one satisfies the required linearity condition. The problem\narises from a well-known issue of the underlying Bradley-Terry preference\nmodel: it does not always have a unique maximum likelihood estimator (MLE).\nConsequently,the minimizer of the RLHF loss might be unattainable because it is\nmerely one among many minimizers of the DPO loss. As a better alternative, we\npropose an energy-based model (EBM) that always has a unique MLE, inherently\nsatisfying the linearity requirement. To approximate the MLE in practice, we\npropose a contrastive loss named Energy Preference Alignment (EPA), wherein\neach positive sample is contrasted against one or more strong negatives as well\nas many free weak negatives. Theoretical properties of our EBM enable the\napproximation error of EPA to almost surely vanish when a sufficient number of\nnegatives are used. Empirically, we demonstrate that EPA consistently delivers\nbetter performance on open benchmarks compared to DPO, thereby showing the\nsuperiority of our EBM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the debut of DPO, it has been shown that aligning a target LLM with\nhuman preferences via the KL-constrained RLHF loss is mathematically equivalent\nto a special kind of reward modeling task. Concretely, the task requires: 1)\nusing the target LLM to parameterize the reward model, and 2) tuning the reward\nmodel so that it has a 1:1 linear relationship with the true reward. However,\nwe identify a significant issue: the DPO loss might have multiple minimizers,\nof which only one satisfies the required linearity condition. The problem\narises from a well-known issue of the underlying Bradley-Terry preference\nmodel: it does not always have a unique maximum likelihood estimator (MLE).\nConsequently,the minimizer of the RLHF loss might be unattainable because it is\nmerely one among many minimizers of the DPO loss. As a better alternative, we\npropose an energy-based model (EBM) that always has a unique MLE, inherently\nsatisfying the linearity requirement. To approximate the MLE in practice, we\npropose a contrastive loss named Energy Preference Alignment (EPA), wherein\neach positive sample is contrasted against one or more strong negatives as well\nas many free weak negatives. Theoretical properties of our EBM enable the\napproximation error of EPA to almost surely vanish when a sufficient number of\nnegatives are used. Empirically, we demonstrate that EPA consistently delivers\nbetter performance on open benchmarks compared to DPO, thereby showing the\nsuperiority of our EBM."
                },
                "authors": [
                    {
                        "name": "Yuzhong Hong"
                    },
                    {
                        "name": "Hanshan Zhang"
                    },
                    {
                        "name": "Junwei Bao"
                    },
                    {
                        "name": "Hongfei Jiang"
                    },
                    {
                        "name": "Yang Song"
                    }
                ],
                "author_detail": {
                    "name": "Yang Song"
                },
                "author": "Yang Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13860v1",
                "updated": "2024-12-18T13:53:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    53,
                    59,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T13:53:59Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    53,
                    59,
                    2,
                    353,
                    0
                ],
                "title": "Domain-adaptative Continual Learning for Low-resource Tasks: Evaluation\n  on Nepali",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain-adaptative Continual Learning for Low-resource Tasks: Evaluation\n  on Nepali"
                },
                "summary": "Continual learning has emerged as an important research direction due to the\ninfeasibility of retraining large language models (LLMs) from scratch in the\nevent of new data availability. Of great interest is the domain-adaptive\npre-training (DAPT) paradigm, which focuses on continually training a\npre-trained language model to adapt it to a domain it was not originally\ntrained on. In this work, we evaluate the feasibility of DAPT in a low-resource\nsetting, namely the Nepali language. We use synthetic data to continue training\nLlama 3 8B to adapt it to the Nepali language in a 4-bit QLoRA setting. We\nevaluate the adapted model on its performance, forgetting, and knowledge\nacquisition. We compare the base model and the final model on their Nepali\ngeneration abilities, their performance on popular benchmarks, and run\ncase-studies to probe their linguistic knowledge in Nepali. We see some\nunsurprising forgetting in the final model, but also surprisingly find that\nincreasing the number of shots during evaluation yields better percent\nincreases in the final model (as high as 19.29% increase) compared to the base\nmodel (4.98%), suggesting latent retention. We also explore layer-head\nself-attention heatmaps to establish dependency resolution abilities of the\nfinal model in Nepali.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning has emerged as an important research direction due to the\ninfeasibility of retraining large language models (LLMs) from scratch in the\nevent of new data availability. Of great interest is the domain-adaptive\npre-training (DAPT) paradigm, which focuses on continually training a\npre-trained language model to adapt it to a domain it was not originally\ntrained on. In this work, we evaluate the feasibility of DAPT in a low-resource\nsetting, namely the Nepali language. We use synthetic data to continue training\nLlama 3 8B to adapt it to the Nepali language in a 4-bit QLoRA setting. We\nevaluate the adapted model on its performance, forgetting, and knowledge\nacquisition. We compare the base model and the final model on their Nepali\ngeneration abilities, their performance on popular benchmarks, and run\ncase-studies to probe their linguistic knowledge in Nepali. We see some\nunsurprising forgetting in the final model, but also surprisingly find that\nincreasing the number of shots during evaluation yields better percent\nincreases in the final model (as high as 19.29% increase) compared to the base\nmodel (4.98%), suggesting latent retention. We also explore layer-head\nself-attention heatmaps to establish dependency resolution abilities of the\nfinal model in Nepali."
                },
                "authors": [
                    {
                        "name": "Sharad Duwal"
                    },
                    {
                        "name": "Suraj Prasai"
                    },
                    {
                        "name": "Suresh Manandhar"
                    }
                ],
                "author_detail": {
                    "name": "Suresh Manandhar"
                },
                "author": "Suresh Manandhar",
                "arxiv_comment": "10 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13859v1",
                "updated": "2024-12-18T13:53:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    53,
                    16,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T13:53:16Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    53,
                    16,
                    2,
                    353,
                    0
                ],
                "title": "Zero-Shot Prompting and Few-Shot Fine-Tuning: Revisiting Document Image\n  Classification Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Prompting and Few-Shot Fine-Tuning: Revisiting Document Image\n  Classification Using Large Language Models"
                },
                "summary": "Classifying scanned documents is a challenging problem that involves image,\nlayout, and text analysis for document understanding. Nevertheless, for certain\nbenchmark datasets, notably RVL-CDIP, the state of the art is closing in to\nnear-perfect performance when considering hundreds of thousands of training\nsamples. With the advent of large language models (LLMs), which are excellent\nfew-shot learners, the question arises to what extent the document\nclassification problem can be addressed with only a few training samples, or\neven none at all. In this paper, we investigate this question in the context of\nzero-shot prompting and few-shot model fine-tuning, with the aim of reducing\nthe need for human-annotated training samples as much as possible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classifying scanned documents is a challenging problem that involves image,\nlayout, and text analysis for document understanding. Nevertheless, for certain\nbenchmark datasets, notably RVL-CDIP, the state of the art is closing in to\nnear-perfect performance when considering hundreds of thousands of training\nsamples. With the advent of large language models (LLMs), which are excellent\nfew-shot learners, the question arises to what extent the document\nclassification problem can be addressed with only a few training samples, or\neven none at all. In this paper, we investigate this question in the context of\nzero-shot prompting and few-shot model fine-tuning, with the aim of reducing\nthe need for human-annotated training samples as much as possible."
                },
                "authors": [
                    {
                        "name": "Anna Scius-Bertrand"
                    },
                    {
                        "name": "Michael Jungo"
                    },
                    {
                        "name": "Lars Vgtlin"
                    },
                    {
                        "name": "Jean-Marc Spat"
                    },
                    {
                        "name": "Andreas Fischer"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Fischer"
                },
                "author": "Andreas Fischer",
                "arxiv_doi": "10.1007/978-3-031-78495-8_10",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-78495-8_10",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.13859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ICPR 2024",
                "arxiv_journal_ref": "International Conference on Pattern Recognition - ICPR 2024, pp\n  152-166. Cham: Springer Nature Switzerland",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04605v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04605v2",
                "updated": "2024-12-18T13:49:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    49,
                    59,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-05T20:41:36Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    20,
                    41,
                    36,
                    3,
                    340,
                    0
                ],
                "title": "Semiparametric Bayesian Difference-in-Differences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semiparametric Bayesian Difference-in-Differences"
                },
                "summary": "This paper studies semiparametric Bayesian inference for the average\ntreatment effect on the treated (ATT) within the difference-in-differences\nresearch design. We propose two new Bayesian methods with frequentist validity.\nThe first one places a standard Gaussian process prior on the conditional mean\nfunction of the control group. We obtain asymptotic equivalence of our Bayesian\nestimator and an efficient frequentist estimator by establishing a\nsemiparametric Bernstein-von Mises (BvM) theorem. The second method is a double\nrobust Bayesian procedure that adjusts the prior distribution of the\nconditional mean function and subsequently corrects the posterior distribution\nof the resulting ATT. We establish a semiparametric BvM result under double\nrobust smoothness conditions; i.e., the lack of smoothness of conditional mean\nfunctions can be compensated by high regularity of the propensity score, and\nvice versa. Monte Carlo simulations and an empirical application demonstrate\nthat the proposed Bayesian DiD methods exhibit strong finite-sample performance\ncompared to existing frequentist methods. Finally, we outline an extension to\ndifference-in-differences with multiple periods and staggered entry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies semiparametric Bayesian inference for the average\ntreatment effect on the treated (ATT) within the difference-in-differences\nresearch design. We propose two new Bayesian methods with frequentist validity.\nThe first one places a standard Gaussian process prior on the conditional mean\nfunction of the control group. We obtain asymptotic equivalence of our Bayesian\nestimator and an efficient frequentist estimator by establishing a\nsemiparametric Bernstein-von Mises (BvM) theorem. The second method is a double\nrobust Bayesian procedure that adjusts the prior distribution of the\nconditional mean function and subsequently corrects the posterior distribution\nof the resulting ATT. We establish a semiparametric BvM result under double\nrobust smoothness conditions; i.e., the lack of smoothness of conditional mean\nfunctions can be compensated by high regularity of the propensity score, and\nvice versa. Monte Carlo simulations and an empirical application demonstrate\nthat the proposed Bayesian DiD methods exhibit strong finite-sample performance\ncompared to existing frequentist methods. Finally, we outline an extension to\ndifference-in-differences with multiple periods and staggered entry."
                },
                "authors": [
                    {
                        "name": "Christoph Breunig"
                    },
                    {
                        "name": "Ruixuan Liu"
                    },
                    {
                        "name": "Zhengfei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zhengfei Yu"
                },
                "author": "Zhengfei Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04605v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04605v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07682v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07682v3",
                "updated": "2024-12-18T13:39:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    39,
                    47,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-10T17:13:35Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    13,
                    35,
                    1,
                    345,
                    0
                ],
                "title": "TRIM: Token Reduction and Inference Modeling for Cost-Effective Language\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRIM: Token Reduction and Inference Modeling for Cost-Effective Language\n  Generation"
                },
                "summary": "The inference cost of Large Language Models (LLMs) is a significant challenge\ndue to their computational demands, specially on tasks requiring long outputs.\nHowever, natural language often contains redundancy, which presents an\nopportunity for optimization. We have observed that LLMs can generate distilled\nlanguage-concise outputs that retain essential meaning, when prompted\nappropriately. We propose TRIM, a pipeline for saving computational cost in\nwhich a shorter distilled output from the LLM is reconstructed into a full\nnarrative by a smaller model with lower inference costs. Our experiments show\npromising results, particularly in general knowledge domains with 20.58% saved\ntokens on average with tiny decrease in evaluation metrics, hinting that this\napproach can effectively balance efficiency and accuracy in language processing\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference cost of Large Language Models (LLMs) is a significant challenge\ndue to their computational demands, specially on tasks requiring long outputs.\nHowever, natural language often contains redundancy, which presents an\nopportunity for optimization. We have observed that LLMs can generate distilled\nlanguage-concise outputs that retain essential meaning, when prompted\nappropriately. We propose TRIM, a pipeline for saving computational cost in\nwhich a shorter distilled output from the LLM is reconstructed into a full\nnarrative by a smaller model with lower inference costs. Our experiments show\npromising results, particularly in general knowledge domains with 20.58% saved\ntokens on average with tiny decrease in evaluation metrics, hinting that this\napproach can effectively balance efficiency and accuracy in language processing\ntasks."
                },
                "authors": [
                    {
                        "name": "Alfredo Garrachn Ruiz"
                    },
                    {
                        "name": "Toms de la Rosa"
                    },
                    {
                        "name": "Daniel Borrajo"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Borrajo"
                },
                "author": "Daniel Borrajo",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07682v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07682v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13845v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13845v1",
                "updated": "2024-12-18T13:38:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    38,
                    6,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T13:38:06Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    38,
                    6,
                    2,
                    353,
                    0
                ],
                "title": "Do Language Models Understand Time?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Language Models Understand Time?"
                },
                "summary": "Large language models (LLMs) have revolutionized video-based computer vision\napplications, including action recognition, anomaly detection, and video\nsummarization. Videos inherently pose unique challenges, combining spatial\ncomplexity with temporal dynamics that are absent in static images or textual\ndata. Current approaches to video understanding with LLMs often rely on\npretrained video encoders to extract spatiotemporal features and text encoders\nto capture semantic meaning. These representations are integrated within LLM\nframeworks, enabling multimodal reasoning across diverse video tasks. However,\nthe critical question persists: Can LLMs truly understand the concept of time,\nand how effectively can they reason about temporal relationships in videos?\nThis work critically examines the role of LLMs in video processing, with a\nspecific focus on their temporal reasoning capabilities. We identify key\nlimitations in the interaction between LLMs and pretrained encoders, revealing\ngaps in their ability to model long-term dependencies and abstract temporal\nconcepts such as causality and event progression. Furthermore, we analyze\nchallenges posed by existing video datasets, including biases, lack of temporal\nannotations, and domain-specific limitations that constrain the temporal\nunderstanding of LLMs. To address these gaps, we explore promising future\ndirections, including the co-evolution of LLMs and encoders, the development of\nenriched datasets with explicit temporal labels, and innovative architectures\nfor integrating spatial, temporal, and semantic reasoning. By addressing these\nchallenges, we aim to advance the temporal comprehension of LLMs, unlocking\ntheir full potential in video analysis and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized video-based computer vision\napplications, including action recognition, anomaly detection, and video\nsummarization. Videos inherently pose unique challenges, combining spatial\ncomplexity with temporal dynamics that are absent in static images or textual\ndata. Current approaches to video understanding with LLMs often rely on\npretrained video encoders to extract spatiotemporal features and text encoders\nto capture semantic meaning. These representations are integrated within LLM\nframeworks, enabling multimodal reasoning across diverse video tasks. However,\nthe critical question persists: Can LLMs truly understand the concept of time,\nand how effectively can they reason about temporal relationships in videos?\nThis work critically examines the role of LLMs in video processing, with a\nspecific focus on their temporal reasoning capabilities. We identify key\nlimitations in the interaction between LLMs and pretrained encoders, revealing\ngaps in their ability to model long-term dependencies and abstract temporal\nconcepts such as causality and event progression. Furthermore, we analyze\nchallenges posed by existing video datasets, including biases, lack of temporal\nannotations, and domain-specific limitations that constrain the temporal\nunderstanding of LLMs. To address these gaps, we explore promising future\ndirections, including the co-evolution of LLMs and encoders, the development of\nenriched datasets with explicit temporal labels, and innovative architectures\nfor integrating spatial, temporal, and semantic reasoning. By addressing these\nchallenges, we aim to advance the temporal comprehension of LLMs, unlocking\ntheir full potential in video analysis and beyond."
                },
                "authors": [
                    {
                        "name": "Xi Ding"
                    },
                    {
                        "name": "Lei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Wang"
                },
                "author": "Lei Wang",
                "arxiv_comment": "Research report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13845v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13845v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13844v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13844v1",
                "updated": "2024-12-18T13:37:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    37,
                    36,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T13:37:36Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    37,
                    36,
                    2,
                    353,
                    0
                ],
                "title": "CRM: Retrieval Model with Controllable Condition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRM: Retrieval Model with Controllable Condition"
                },
                "summary": "Recommendation systems (RecSys) are designed to connect users with relevant\nitems from a vast pool of candidates while aligning with the business goals of\nthe platform. A typical industrial RecSys is composed of two main stages,\nretrieval and ranking: (1) the retrieval stage aims at searching hundreds of\nitem candidates satisfied user interests; (2) based on the retrieved items, the\nranking stage aims at selecting the best dozen items by multiple targets\nestimation for each item candidate, including classification and regression\ntargets. Compared with ranking model, the retrieval model absence of item\ncandidate information during inference, therefore retrieval models are often\ntrained by classification target only (e.g., click-through rate), but failed to\nincorporate regression target (e.g., the expected watch-time), which limit the\neffectiveness of retrieval. In this paper, we propose the Controllable\nRetrieval Model (CRM), which integrates regression information as conditional\nfeatures into the two-tower retrieval paradigm. This modification enables the\nretrieval stage could fulfill the target gap with ranking model, enhancing the\nretrieval model ability to search item candidates satisfied the user interests\nand condition effectively. We validate the effectiveness of CRM through\nreal-world A/B testing and demonstrate its successful deployment in Kuaishou\nshort-video recommendation system, which serves over 400 million users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommendation systems (RecSys) are designed to connect users with relevant\nitems from a vast pool of candidates while aligning with the business goals of\nthe platform. A typical industrial RecSys is composed of two main stages,\nretrieval and ranking: (1) the retrieval stage aims at searching hundreds of\nitem candidates satisfied user interests; (2) based on the retrieved items, the\nranking stage aims at selecting the best dozen items by multiple targets\nestimation for each item candidate, including classification and regression\ntargets. Compared with ranking model, the retrieval model absence of item\ncandidate information during inference, therefore retrieval models are often\ntrained by classification target only (e.g., click-through rate), but failed to\nincorporate regression target (e.g., the expected watch-time), which limit the\neffectiveness of retrieval. In this paper, we propose the Controllable\nRetrieval Model (CRM), which integrates regression information as conditional\nfeatures into the two-tower retrieval paradigm. This modification enables the\nretrieval stage could fulfill the target gap with ranking model, enhancing the\nretrieval model ability to search item candidates satisfied the user interests\nand condition effectively. We validate the effectiveness of CRM through\nreal-world A/B testing and demonstrate its successful deployment in Kuaishou\nshort-video recommendation system, which serves over 400 million users."
                },
                "authors": [
                    {
                        "name": "Chi Liu"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Kuo Cai"
                    },
                    {
                        "name": "Weifeng Ding"
                    },
                    {
                        "name": "Qiang Luo"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13844v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13835v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13835v1",
                "updated": "2024-12-18T13:25:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    25,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T13:25:11Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    25,
                    11,
                    2,
                    353,
                    0
                ],
                "title": "RACQUET: Unveiling the Dangers of Overlooked Referential Ambiguity in\n  Visual LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RACQUET: Unveiling the Dangers of Overlooked Referential Ambiguity in\n  Visual LLMs"
                },
                "summary": "Ambiguity resolution is key to effective communication. While humans\neffortlessly address ambiguity through conversational grounding strategies, the\nextent to which current language models can emulate these strategies remains\nunclear. In this work, we examine referential ambiguity in image-based question\nanswering by introducing RACQUET, a carefully curated dataset targeting\ndistinct aspects of ambiguity. Through a series of evaluations, we reveal\nsignificant limitations and problems of overconfidence of state-of-the-art\nlarge multimodal language models in addressing ambiguity in their responses.\nThe overconfidence issue becomes particularly relevant for RACQUET-BIAS, a\nsubset designed to analyze a critical yet underexplored problem: failing to\naddress ambiguity leads to stereotypical, socially biased responses. Our\nresults underscore the urgency of equipping models with robust strategies to\ndeal with uncertainty without resorting to undesirable stereotypes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ambiguity resolution is key to effective communication. While humans\neffortlessly address ambiguity through conversational grounding strategies, the\nextent to which current language models can emulate these strategies remains\nunclear. In this work, we examine referential ambiguity in image-based question\nanswering by introducing RACQUET, a carefully curated dataset targeting\ndistinct aspects of ambiguity. Through a series of evaluations, we reveal\nsignificant limitations and problems of overconfidence of state-of-the-art\nlarge multimodal language models in addressing ambiguity in their responses.\nThe overconfidence issue becomes particularly relevant for RACQUET-BIAS, a\nsubset designed to analyze a critical yet underexplored problem: failing to\naddress ambiguity leads to stereotypical, socially biased responses. Our\nresults underscore the urgency of equipping models with robust strategies to\ndeal with uncertainty without resorting to undesirable stereotypes."
                },
                "authors": [
                    {
                        "name": "Alberto Testoni"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "Raquel Fernndez"
                    }
                ],
                "author_detail": {
                    "name": "Raquel Fernndez"
                },
                "author": "Raquel Fernndez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13835v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13835v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13834v1",
                "updated": "2024-12-18T13:24:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    24,
                    9,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T13:24:09Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    24,
                    9,
                    2,
                    353,
                    0
                ],
                "title": "Maybe you are looking for CroQS: Cross-modal Query Suggestion for\n  Text-to-Image Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maybe you are looking for CroQS: Cross-modal Query Suggestion for\n  Text-to-Image Retrieval"
                },
                "summary": "Query suggestion, a technique widely adopted in information retrieval,\nenhances system interactivity and the browsing experience of document\ncollections. In cross-modal retrieval, many works have focused on retrieving\nrelevant items from natural language queries, while few have explored query\nsuggestion solutions. In this work, we address query suggestion in cross-modal\nretrieval, introducing a novel task that focuses on suggesting minimal textual\nmodifications needed to explore visually consistent subsets of the collection,\nfollowing the premise of ''Maybe you are looking for''. To facilitate the\nevaluation and development of methods, we present a tailored benchmark named\nCroQS. This dataset comprises initial queries, grouped result sets, and\nhuman-defined suggested queries for each group. We establish dedicated metrics\nto rigorously evaluate the performance of various methods on this task,\nmeasuring representativeness, cluster specificity, and similarity of the\nsuggested queries to the original ones. Baseline methods from related fields,\nsuch as image captioning and content summarization, are adapted for this task\nto provide reference performance scores. Although relatively far from human\nperformance, our experiments reveal that both LLM-based and captioning-based\nmethods achieve competitive results on CroQS, improving the recall on cluster\nspecificity by more than 115% and representativeness mAP by more than 52% with\nrespect to the initial query. The dataset, the implementation of the baseline\nmethods and the notebooks containing our experiments are available here:\nhttps://paciosoft.com/CroQS-benchmark/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query suggestion, a technique widely adopted in information retrieval,\nenhances system interactivity and the browsing experience of document\ncollections. In cross-modal retrieval, many works have focused on retrieving\nrelevant items from natural language queries, while few have explored query\nsuggestion solutions. In this work, we address query suggestion in cross-modal\nretrieval, introducing a novel task that focuses on suggesting minimal textual\nmodifications needed to explore visually consistent subsets of the collection,\nfollowing the premise of ''Maybe you are looking for''. To facilitate the\nevaluation and development of methods, we present a tailored benchmark named\nCroQS. This dataset comprises initial queries, grouped result sets, and\nhuman-defined suggested queries for each group. We establish dedicated metrics\nto rigorously evaluate the performance of various methods on this task,\nmeasuring representativeness, cluster specificity, and similarity of the\nsuggested queries to the original ones. Baseline methods from related fields,\nsuch as image captioning and content summarization, are adapted for this task\nto provide reference performance scores. Although relatively far from human\nperformance, our experiments reveal that both LLM-based and captioning-based\nmethods achieve competitive results on CroQS, improving the recall on cluster\nspecificity by more than 115% and representativeness mAP by more than 52% with\nrespect to the initial query. The dataset, the implementation of the baseline\nmethods and the notebooks containing our experiments are available here:\nhttps://paciosoft.com/CroQS-benchmark/"
                },
                "authors": [
                    {
                        "name": "Giacomo Pacini"
                    },
                    {
                        "name": "Fabio Carrara"
                    },
                    {
                        "name": "Nicola Messina"
                    },
                    {
                        "name": "Nicola Tonellotto"
                    },
                    {
                        "name": "Giuseppe Amato"
                    },
                    {
                        "name": "Fabrizio Falchi"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Falchi"
                },
                "author": "Fabrizio Falchi",
                "arxiv_comment": "15 pages, 5 figures. To be published as full paper in the Proceedings\n  of the European Conference on Information Retrieval (ECIR) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21534v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21534v4",
                "updated": "2024-12-18T13:12:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    12,
                    29,
                    2,
                    353,
                    0
                ],
                "published": "2024-07-31T11:40:29Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    11,
                    40,
                    29,
                    2,
                    213,
                    0
                ],
                "title": "ControlMLLM: Training-Free Visual Prompt Learning for Multimodal Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ControlMLLM: Training-Free Visual Prompt Learning for Multimodal Large\n  Language Models"
                },
                "summary": "In this work, we propose a training-free method to inject visual referring\ninto Multimodal Large Language Models (MLLMs) through learnable visual token\noptimization. We observe the relationship between text prompt tokens and visual\ntokens in MLLMs, where attention layers model the connection between them. Our\napproach involves adjusting visual tokens from the MLP output during inference,\ncontrolling which text prompt tokens attend to which visual tokens. We optimize\na learnable visual token based on an energy function, enhancing the strength of\nreferential regions in the attention map. This enables detailed region\ndescription and reasoning without the need for substantial training costs or\nmodel retraining. Our method offers a promising direction for integrating\nreferential abilities into MLLMs. Our method support referring with box, mask,\nscribble and point. The results demonstrate that our method exhibits\ncontrollability and interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a training-free method to inject visual referring\ninto Multimodal Large Language Models (MLLMs) through learnable visual token\noptimization. We observe the relationship between text prompt tokens and visual\ntokens in MLLMs, where attention layers model the connection between them. Our\napproach involves adjusting visual tokens from the MLP output during inference,\ncontrolling which text prompt tokens attend to which visual tokens. We optimize\na learnable visual token based on an energy function, enhancing the strength of\nreferential regions in the attention map. This enables detailed region\ndescription and reasoning without the need for substantial training costs or\nmodel retraining. Our method offers a promising direction for integrating\nreferential abilities into MLLMs. Our method support referring with box, mask,\nscribble and point. The results demonstrate that our method exhibits\ncontrollability and interpretability."
                },
                "authors": [
                    {
                        "name": "Mingrui Wu"
                    },
                    {
                        "name": "Xinyue Cai"
                    },
                    {
                        "name": "Jiayi Ji"
                    },
                    {
                        "name": "Jiale Li"
                    },
                    {
                        "name": "Oucheng Huang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Guannan Jiang"
                    },
                    {
                        "name": "Xiaoshuai Sun"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "arxiv_comment": "Accepted to NeurIPS 2024;\n  Code:https://github.com/mrwu-mac/ControlMLLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21534v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21534v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13823v1",
                "updated": "2024-12-18T13:11:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    11,
                    58,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T13:11:58Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    11,
                    58,
                    2,
                    353,
                    0
                ],
                "title": "Prompt Categories Cluster for Weakly Supervised Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Categories Cluster for Weakly Supervised Semantic Segmentation"
                },
                "summary": "Weakly Supervised Semantic Segmentation (WSSS), which leverages image-level\nlabels, has garnered significant attention due to its cost-effectiveness. The\nprevious methods mainly strengthen the inter-class differences to avoid class\nsemantic ambiguity which may lead to erroneous activation. However, they\noverlook the positive function of some shared information between similar\nclasses. Categories within the same cluster share some similar features.\nAllowing the model to recognize these features can further relieve the semantic\nambiguity between these classes. To effectively identify and utilize this\nshared information, in this paper, we introduce a novel WSSS framework called\nPrompt Categories Clustering (PCC). Specifically, we explore the ability of\nLarge Language Models (LLMs) to derive category clusters through prompts. These\nclusters effectively represent the intrinsic relationships between categories.\nBy integrating this relational information into the training network, our model\nis able to better learn the hidden connections between categories. Experimental\nresults demonstrate the effectiveness of our approach, showing its ability to\nenhance performance on the PASCAL VOC 2012 dataset and surpass existing\nstate-of-the-art methods in WSSS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weakly Supervised Semantic Segmentation (WSSS), which leverages image-level\nlabels, has garnered significant attention due to its cost-effectiveness. The\nprevious methods mainly strengthen the inter-class differences to avoid class\nsemantic ambiguity which may lead to erroneous activation. However, they\noverlook the positive function of some shared information between similar\nclasses. Categories within the same cluster share some similar features.\nAllowing the model to recognize these features can further relieve the semantic\nambiguity between these classes. To effectively identify and utilize this\nshared information, in this paper, we introduce a novel WSSS framework called\nPrompt Categories Clustering (PCC). Specifically, we explore the ability of\nLarge Language Models (LLMs) to derive category clusters through prompts. These\nclusters effectively represent the intrinsic relationships between categories.\nBy integrating this relational information into the training network, our model\nis able to better learn the hidden connections between categories. Experimental\nresults demonstrate the effectiveness of our approach, showing its ability to\nenhance performance on the PASCAL VOC 2012 dataset and surpass existing\nstate-of-the-art methods in WSSS."
                },
                "authors": [
                    {
                        "name": "Wangyu Wu"
                    },
                    {
                        "name": "Xianglin Qiu"
                    },
                    {
                        "name": "Siqi Song"
                    },
                    {
                        "name": "Xiaowei Huang"
                    },
                    {
                        "name": "Fei Ma"
                    },
                    {
                        "name": "Jimin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Jimin Xiao"
                },
                "author": "Jimin Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12559v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12559v2",
                "updated": "2024-12-18T13:08:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    8,
                    36,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-17T05:38:27Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    38,
                    27,
                    1,
                    352,
                    0
                ],
                "title": "EXIT: Context-Aware Extractive Compression for Enhancing\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EXIT: Context-Aware Extractive Compression for Enhancing\n  Retrieval-Augmented Generation"
                },
                "summary": "We introduce EXIT, an extractive context compression framework that enhances\nboth the effectiveness and efficiency of retrieval-augmented generation (RAG)\nin question answering (QA). Current RAG systems often struggle when retrieval\nmodels fail to rank the most relevant documents, leading to the inclusion of\nmore context at the expense of latency and accuracy. While abstractive\ncompression methods can drastically reduce token counts, their token-by-token\ngeneration process significantly increases end-to-end latency. Conversely,\nexisting extractive methods reduce latency but rely on independent,\nnon-adaptive sentence selection, failing to fully utilize contextual\ninformation. EXIT addresses these limitations by classifying sentences from\nretrieved documents - while preserving their contextual dependencies - enabling\nparallelizable, context-aware extraction that adapts to query complexity and\nretrieval quality. Our evaluations on both single-hop and multi-hop QA tasks\nshow that EXIT consistently surpasses existing compression methods and even\nuncompressed baselines in QA accuracy, while also delivering substantial\nreductions in inference time and token count. By improving both effectiveness\nand efficiency, EXIT provides a promising direction for developing scalable,\nhigh-quality QA solutions in RAG pipelines. Our code is available at\nhttps://github.com/ThisIsHwang/EXIT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce EXIT, an extractive context compression framework that enhances\nboth the effectiveness and efficiency of retrieval-augmented generation (RAG)\nin question answering (QA). Current RAG systems often struggle when retrieval\nmodels fail to rank the most relevant documents, leading to the inclusion of\nmore context at the expense of latency and accuracy. While abstractive\ncompression methods can drastically reduce token counts, their token-by-token\ngeneration process significantly increases end-to-end latency. Conversely,\nexisting extractive methods reduce latency but rely on independent,\nnon-adaptive sentence selection, failing to fully utilize contextual\ninformation. EXIT addresses these limitations by classifying sentences from\nretrieved documents - while preserving their contextual dependencies - enabling\nparallelizable, context-aware extraction that adapts to query complexity and\nretrieval quality. Our evaluations on both single-hop and multi-hop QA tasks\nshow that EXIT consistently surpasses existing compression methods and even\nuncompressed baselines in QA accuracy, while also delivering substantial\nreductions in inference time and token count. By improving both effectiveness\nand efficiency, EXIT provides a promising direction for developing scalable,\nhigh-quality QA solutions in RAG pipelines. Our code is available at\nhttps://github.com/ThisIsHwang/EXIT"
                },
                "authors": [
                    {
                        "name": "Taeho Hwang"
                    },
                    {
                        "name": "Sukmin Cho"
                    },
                    {
                        "name": "Soyeong Jeong"
                    },
                    {
                        "name": "Hoyun Song"
                    },
                    {
                        "name": "SeungYoon Han"
                    },
                    {
                        "name": "Jong C. Park"
                    }
                ],
                "author_detail": {
                    "name": "Jong C. Park"
                },
                "author": "Jong C. Park",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12559v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12559v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13147v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13147v2",
                "updated": "2024-12-18T13:05:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    5,
                    24,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-17T18:12:47Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    12,
                    47,
                    1,
                    352,
                    0
                ],
                "title": "Are Your LLMs Capable of Stable Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Your LLMs Capable of Stable Reasoning?"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has demonstrated\nremarkable progress in complex reasoning tasks. However, a significant\ndiscrepancy persists between benchmark performances and real-world\napplications. We identify this gap as primarily stemming from current\nevaluation protocols and metrics, which inadequately capture the full spectrum\nof LLM capabilities, particularly in complex reasoning tasks where both\naccuracy and consistency are crucial. This work makes two key contributions.\nFirst, we introduce G-Pass@k, a novel evaluation metric that provides a\ncontinuous assessment of model performance across multiple sampling attempts,\nquantifying both the model's peak performance potential and its stability.\nSecond, we present LiveMathBench, a dynamic benchmark comprising challenging,\ncontemporary mathematical problems designed to minimize data leakage risks\nduring evaluation. Through extensive experiments using G-Pass@k on\nstate-of-the-art LLMs with LiveMathBench, we provide comprehensive insights\ninto both their maximum capabilities and operational consistency. Our findings\nreveal substantial room for improvement in LLMs' \"realistic\" reasoning\ncapabilities, highlighting the need for more robust evaluation methods. The\nbenchmark and detailed results are available at:\nhttps://github.com/open-compass/GPassK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has demonstrated\nremarkable progress in complex reasoning tasks. However, a significant\ndiscrepancy persists between benchmark performances and real-world\napplications. We identify this gap as primarily stemming from current\nevaluation protocols and metrics, which inadequately capture the full spectrum\nof LLM capabilities, particularly in complex reasoning tasks where both\naccuracy and consistency are crucial. This work makes two key contributions.\nFirst, we introduce G-Pass@k, a novel evaluation metric that provides a\ncontinuous assessment of model performance across multiple sampling attempts,\nquantifying both the model's peak performance potential and its stability.\nSecond, we present LiveMathBench, a dynamic benchmark comprising challenging,\ncontemporary mathematical problems designed to minimize data leakage risks\nduring evaluation. Through extensive experiments using G-Pass@k on\nstate-of-the-art LLMs with LiveMathBench, we provide comprehensive insights\ninto both their maximum capabilities and operational consistency. Our findings\nreveal substantial room for improvement in LLMs' \"realistic\" reasoning\ncapabilities, highlighting the need for more robust evaluation methods. The\nbenchmark and detailed results are available at:\nhttps://github.com/open-compass/GPassK."
                },
                "authors": [
                    {
                        "name": "Junnan Liu"
                    },
                    {
                        "name": "Hongwei Liu"
                    },
                    {
                        "name": "Linchen Xiao"
                    },
                    {
                        "name": "Ziyi Wang"
                    },
                    {
                        "name": "Kuikun Liu"
                    },
                    {
                        "name": "Songyang Gao"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13147v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13147v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13817v1",
                "updated": "2024-12-18T13:04:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    4,
                    30,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T13:04:30Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    4,
                    30,
                    2,
                    353,
                    0
                ],
                "title": "Nullu: Mitigating Object Hallucinations in Large Vision-Language Models\n  via HalluSpace Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nullu: Mitigating Object Hallucinations in Large Vision-Language Models\n  via HalluSpace Projection"
                },
                "summary": "Recent studies have shown that large vision-language models (LVLMs) often\nsuffer from the issue of object hallucinations (OH). To mitigate this issue, we\nintroduce an efficient method that edits the model weights based on an unsafe\nsubspace, which we call HalluSpace in this paper. With truthful and\nhallucinated text prompts accompanying the visual content as inputs, the\nHalluSpace can be identified by extracting the hallucinated embedding features\nand removing the truthful representations in LVLMs. By orthogonalizing the\nmodel weights, input features will be projected into the Null space of the\nHalluSpace to reduce OH, based on which we name our method Nullu. We reveal\nthat HalluSpaces generally contain statistical bias and unimodal priors of the\nlarge language models (LLMs) applied to build LVLMs, which have been shown as\nessential causes of OH in previous studies. Therefore, null space projection\nsuppresses the LLMs' priors to filter out the hallucinated features, resulting\nin contextually accurate outputs. Experiments show that our method can\neffectively mitigate OH across different LVLM families without extra inference\ncosts and also show strong performance in general LVLM benchmarks. Code is\nreleased at \\url{https://github.com/Ziwei-Zheng/Nullu}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have shown that large vision-language models (LVLMs) often\nsuffer from the issue of object hallucinations (OH). To mitigate this issue, we\nintroduce an efficient method that edits the model weights based on an unsafe\nsubspace, which we call HalluSpace in this paper. With truthful and\nhallucinated text prompts accompanying the visual content as inputs, the\nHalluSpace can be identified by extracting the hallucinated embedding features\nand removing the truthful representations in LVLMs. By orthogonalizing the\nmodel weights, input features will be projected into the Null space of the\nHalluSpace to reduce OH, based on which we name our method Nullu. We reveal\nthat HalluSpaces generally contain statistical bias and unimodal priors of the\nlarge language models (LLMs) applied to build LVLMs, which have been shown as\nessential causes of OH in previous studies. Therefore, null space projection\nsuppresses the LLMs' priors to filter out the hallucinated features, resulting\nin contextually accurate outputs. Experiments show that our method can\neffectively mitigate OH across different LVLM families without extra inference\ncosts and also show strong performance in general LVLM benchmarks. Code is\nreleased at \\url{https://github.com/Ziwei-Zheng/Nullu}."
                },
                "authors": [
                    {
                        "name": "Le Yang"
                    },
                    {
                        "name": "Ziwei Zheng"
                    },
                    {
                        "name": "Boxu Chen"
                    },
                    {
                        "name": "Zhengyu Zhao"
                    },
                    {
                        "name": "Chenhao Lin"
                    },
                    {
                        "name": "Chao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chao Shen"
                },
                "author": "Chao Shen",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04677v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04677v2",
                "updated": "2024-12-18T13:00:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    0,
                    35,
                    2,
                    353,
                    0
                ],
                "published": "2024-11-07T13:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    13,
                    3,
                    21,
                    3,
                    312,
                    0
                ],
                "title": "Lightning IR: Straightforward Fine-tuning and Inference of\n  Transformer-based Language Models for Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightning IR: Straightforward Fine-tuning and Inference of\n  Transformer-based Language Models for Information Retrieval"
                },
                "summary": "A wide range of transformer-based language models have been proposed for\ninformation retrieval tasks. However, including transformer-based models in\nretrieval pipelines is often complex and requires substantial engineering\neffort. In this paper, we introduce Lightning IR, an easy-to-use PyTorch\nLightning-based framework for applying transformer-based language models in\nretrieval scenarios. Lightning IR provides a modular and extensible\narchitecture that supports all stages of a retrieval pipeline: from fine-tuning\nand indexing to searching and re-ranking. Designed to be scalable and\nreproducible, Lightning IR is available as open-source:\nhttps://github.com/webis-de/lightning-ir.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A wide range of transformer-based language models have been proposed for\ninformation retrieval tasks. However, including transformer-based models in\nretrieval pipelines is often complex and requires substantial engineering\neffort. In this paper, we introduce Lightning IR, an easy-to-use PyTorch\nLightning-based framework for applying transformer-based language models in\nretrieval scenarios. Lightning IR provides a modular and extensible\narchitecture that supports all stages of a retrieval pipeline: from fine-tuning\nand indexing to searching and re-ranking. Designed to be scalable and\nreproducible, Lightning IR is available as open-source:\nhttps://github.com/webis-de/lightning-ir."
                },
                "authors": [
                    {
                        "name": "Ferdinand Schlatt"
                    },
                    {
                        "name": "Maik Frbe"
                    },
                    {
                        "name": "Matthias Hagen"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Hagen"
                },
                "author": "Matthias Hagen",
                "arxiv_doi": "10.1145/3701551.3704118",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701551.3704118",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.04677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04677v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted as a demo at WSDM'25",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13809v1",
                "updated": "2024-12-18T12:57:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    57,
                    49,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:57:49Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    57,
                    49,
                    2,
                    353,
                    0
                ],
                "title": "Extreme Multi-label Completion for Semantic Document Labelling with\n  Taxonomy-Aware Parallel Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme Multi-label Completion for Semantic Document Labelling with\n  Taxonomy-Aware Parallel Learning"
                },
                "summary": "In Extreme Multi Label Completion (XMLCo), the objective is to predict the\nmissing labels of a collection of documents. Together with XML Classification,\nXMLCo is arguably one of the most challenging document classification tasks, as\nthe very high number of labels (at least ten of thousands) is generally very\nlarge compared to the number of available labelled documents in the training\ndataset. Such a task is often accompanied by a taxonomy that encodes the labels\norganic relationships, and many methods have been proposed to leverage this\nhierarchy to improve the results of XMLCo algorithms. In this paper, we propose\na new approach to this problem, TAMLEC (Taxonomy-Aware Multi-task Learning for\nExtreme multi-label Completion). TAMLEC divides the problem into several\nTaxonomy-Aware Tasks, i.e. subsets of labels adapted to the hierarchical paths\nof the taxonomy, and trains on these tasks using a dynamic Parallel Feature\nsharing approach, where some parts of the model are shared between tasks while\nothers are task-specific. Then, at inference time, TAMLEC uses the labels\navailable in a document to infer the appropriate tasks and to predict missing\nlabels. To achieve this result, TAMLEC uses a modified transformer architecture\nthat predicts ordered sequences of labels on a Weak-Semilattice structure that\nis naturally induced by the tasks. This approach yields multiple advantages.\nFirst, our experiments on real-world datasets show that TAMLEC outperforms\nstate-of-the-art methods for various XMLCo problems. Second, TAMLEC is by\nconstruction particularly suited for few-shots XML tasks, where new tasks or\nlabels are introduced with only few examples, and extensive evaluations\nhighlight its strong performance compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Extreme Multi Label Completion (XMLCo), the objective is to predict the\nmissing labels of a collection of documents. Together with XML Classification,\nXMLCo is arguably one of the most challenging document classification tasks, as\nthe very high number of labels (at least ten of thousands) is generally very\nlarge compared to the number of available labelled documents in the training\ndataset. Such a task is often accompanied by a taxonomy that encodes the labels\norganic relationships, and many methods have been proposed to leverage this\nhierarchy to improve the results of XMLCo algorithms. In this paper, we propose\na new approach to this problem, TAMLEC (Taxonomy-Aware Multi-task Learning for\nExtreme multi-label Completion). TAMLEC divides the problem into several\nTaxonomy-Aware Tasks, i.e. subsets of labels adapted to the hierarchical paths\nof the taxonomy, and trains on these tasks using a dynamic Parallel Feature\nsharing approach, where some parts of the model are shared between tasks while\nothers are task-specific. Then, at inference time, TAMLEC uses the labels\navailable in a document to infer the appropriate tasks and to predict missing\nlabels. To achieve this result, TAMLEC uses a modified transformer architecture\nthat predicts ordered sequences of labels on a Weak-Semilattice structure that\nis naturally induced by the tasks. This approach yields multiple advantages.\nFirst, our experiments on real-world datasets show that TAMLEC outperforms\nstate-of-the-art methods for various XMLCo problems. Second, TAMLEC is by\nconstruction particularly suited for few-shots XML tasks, where new tasks or\nlabels are introduced with only few examples, and extensive evaluations\nhighlight its strong performance compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Julien Audiffren"
                    },
                    {
                        "name": "Christophe Broillet"
                    },
                    {
                        "name": "Ljiljana Dolamic"
                    },
                    {
                        "name": "Philippe Cudr-Mauroux"
                    }
                ],
                "author_detail": {
                    "name": "Philippe Cudr-Mauroux"
                },
                "author": "Philippe Cudr-Mauroux",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11156v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11156v4",
                "updated": "2024-12-18T12:48:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    48,
                    37,
                    2,
                    353,
                    0
                ],
                "published": "2024-06-17T02:47:09Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    2,
                    47,
                    9,
                    0,
                    169,
                    0
                ],
                "title": "DELRec: Distilling Sequential Pattern to Enhance LLMs-based Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DELRec: Distilling Sequential Pattern to Enhance LLMs-based Sequential\n  Recommendation"
                },
                "summary": "Sequential recommendation (SR) tasks aim to predict users' next interaction\nby learning their behavior sequence and capturing the connection between users'\npast interactions and their changing preferences. Conventional SR models often\nfocus solely on capturing sequential patterns within the training data,\nneglecting the broader context and semantic information embedded in item titles\nfrom external sources. This limits their predictive power and adaptability.\nLarge language models (LLMs) have recently shown promise in SR tasks due to\ntheir advanced understanding capabilities and strong generalization abilities.\nResearchers have attempted to enhance LLMs-based recommendation performance by\nincorporating information from conventional SR models. However, previous\napproaches have encountered problems such as 1) limited textual information\nleading to poor recommendation performance, 2) incomplete understanding and\nutilization of conventional SR model information by LLMs, and 3) excessive\ncomplexity and low interpretability of LLMs-based methods. To improve the\nperformance of LLMs-based SR, we propose a novel framework, Distilling\nSequential Pattern to Enhance LLMs-based Sequential Recommendation (DELRec),\nwhich aims to extract knowledge from conventional SR models and enable LLMs to\neasily comprehend and utilize the extracted knowledge for more effective SRs.\nDELRec consists of two main stages: 1) Distill Pattern from Conventional SR\nModels, focusing on extracting behavioral patterns exhibited by conventional SR\nmodels using soft prompts through two well-designed strategies; 2) LLMs-based\nSequential Recommendation, aiming to fine-tune LLMs to effectively use the\ndistilled auxiliary information to perform SR tasks. Extensive experimental\nresults conducted on four real datasets validate the effectiveness of the\nDELRec framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommendation (SR) tasks aim to predict users' next interaction\nby learning their behavior sequence and capturing the connection between users'\npast interactions and their changing preferences. Conventional SR models often\nfocus solely on capturing sequential patterns within the training data,\nneglecting the broader context and semantic information embedded in item titles\nfrom external sources. This limits their predictive power and adaptability.\nLarge language models (LLMs) have recently shown promise in SR tasks due to\ntheir advanced understanding capabilities and strong generalization abilities.\nResearchers have attempted to enhance LLMs-based recommendation performance by\nincorporating information from conventional SR models. However, previous\napproaches have encountered problems such as 1) limited textual information\nleading to poor recommendation performance, 2) incomplete understanding and\nutilization of conventional SR model information by LLMs, and 3) excessive\ncomplexity and low interpretability of LLMs-based methods. To improve the\nperformance of LLMs-based SR, we propose a novel framework, Distilling\nSequential Pattern to Enhance LLMs-based Sequential Recommendation (DELRec),\nwhich aims to extract knowledge from conventional SR models and enable LLMs to\neasily comprehend and utilize the extracted knowledge for more effective SRs.\nDELRec consists of two main stages: 1) Distill Pattern from Conventional SR\nModels, focusing on extracting behavioral patterns exhibited by conventional SR\nmodels using soft prompts through two well-designed strategies; 2) LLMs-based\nSequential Recommendation, aiming to fine-tune LLMs to effectively use the\ndistilled auxiliary information to perform SR tasks. Extensive experimental\nresults conducted on four real datasets validate the effectiveness of the\nDELRec framework."
                },
                "authors": [
                    {
                        "name": "Haoyi Zhang"
                    },
                    {
                        "name": "Guohao Sun"
                    },
                    {
                        "name": "Jinhu Lu"
                    },
                    {
                        "name": "Guanfeng Liu"
                    },
                    {
                        "name": "Xiu Susie Fang"
                    }
                ],
                "author_detail": {
                    "name": "Xiu Susie Fang"
                },
                "author": "Xiu Susie Fang",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11156v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11156v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13801v1",
                "updated": "2024-12-18T12:48:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    48,
                    36,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:48:36Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    48,
                    36,
                    2,
                    353,
                    0
                ],
                "title": "A Comprehensive Evaluation of Parameter-Efficient Fine-Tuning on\n  Method-Level Code Smell Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Evaluation of Parameter-Efficient Fine-Tuning on\n  Method-Level Code Smell Detection"
                },
                "summary": "Code smells are suboptimal coding practices that negatively impact the\nquality of software systems. Existing detection methods, relying on heuristics\nor Machine Learning (ML) and Deep Learning (DL) techniques, often face\nlimitations such as unsatisfactory performance. Parameter-Efficient Fine-Tuning\n(PEFT) methods have emerged as a resource-efficient approach for adapting LLMs\nto specific tasks, but their effectiveness for method-level code smell\ndetection remains underexplored. In this regard, this study evaluates\nstate-of-the-art PEFT methods on both small and large Language Models (LMs) for\ndetecting two types of method-level code smells: Complex Conditional and\nComplex Method. Using high-quality datasets sourced from GitHub, we fine-tuned\nfour small LMs and six LLMs with PEFT techniques, including prompt tuning,\nprefix tuning, LoRA, and (IA)3. Results show that PEFT methods achieve\ncomparable or better performance than full fine-tuning while consuming less GPU\nmemory. Notably, LLMs did not outperform small LMs, suggesting smaller models'\nsuitability for this task. Additionally, increasing training dataset size\nsignificantly boosted performance, while increasing trainable parameters did\nnot. Our findings highlight PEFT methods as effective and scalable solutions,\noutperforming existing heuristic-based and DL-based detectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code smells are suboptimal coding practices that negatively impact the\nquality of software systems. Existing detection methods, relying on heuristics\nor Machine Learning (ML) and Deep Learning (DL) techniques, often face\nlimitations such as unsatisfactory performance. Parameter-Efficient Fine-Tuning\n(PEFT) methods have emerged as a resource-efficient approach for adapting LLMs\nto specific tasks, but their effectiveness for method-level code smell\ndetection remains underexplored. In this regard, this study evaluates\nstate-of-the-art PEFT methods on both small and large Language Models (LMs) for\ndetecting two types of method-level code smells: Complex Conditional and\nComplex Method. Using high-quality datasets sourced from GitHub, we fine-tuned\nfour small LMs and six LLMs with PEFT techniques, including prompt tuning,\nprefix tuning, LoRA, and (IA)3. Results show that PEFT methods achieve\ncomparable or better performance than full fine-tuning while consuming less GPU\nmemory. Notably, LLMs did not outperform small LMs, suggesting smaller models'\nsuitability for this task. Additionally, increasing training dataset size\nsignificantly boosted performance, while increasing trainable parameters did\nnot. Our findings highlight PEFT methods as effective and scalable solutions,\noutperforming existing heuristic-based and DL-based detectors."
                },
                "authors": [
                    {
                        "name": "Beiqi Zhang"
                    },
                    {
                        "name": "Peng Liang"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Xiyu Zhou"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Qiong Feng"
                    },
                    {
                        "name": "Zengyang Li"
                    },
                    {
                        "name": "Lin Li"
                    }
                ],
                "author_detail": {
                    "name": "Lin Li"
                },
                "author": "Lin Li",
                "arxiv_comment": "22 pages, 7 images, 8 tables, Manuscript submitted to a journal\n  (2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13795v1",
                "updated": "2024-12-18T12:39:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    39,
                    53,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:39:53Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    39,
                    53,
                    2,
                    353,
                    0
                ],
                "title": "Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and\n  Post-LN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and\n  Post-LN"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success, yet recent\nfindings reveal that their deeper layers often contribute minimally and can be\npruned without affecting overall performance. While some view this as an\nopportunity for model compression, we identify it as a training shortfall\nrooted in the widespread use of Pre-Layer Normalization (Pre-LN). We\ndemonstrate that Pre-LN, commonly employed in models like GPT and LLaMA, leads\nto diminished gradient norms in its deeper layers, reducing their\neffectiveness. In contrast, Post-Layer Normalization (Post-LN) preserves larger\ngradient norms in deeper layers but suffers from vanishing gradients in earlier\nlayers. To address this, we introduce Mix-LN, a novel normalization technique\nthat combines the strengths of Pre-LN and Post-LN within the same model. Mix-LN\napplies Post-LN to the earlier layers and Pre-LN to the deeper layers, ensuring\nmore uniform gradients across layers. This allows all parts of the\nnetwork--both shallow and deep layers--to contribute effectively to training.\nExtensive experiments with various model sizes from 70M to 7B demonstrate that\nMix-LN consistently outperforms both Pre-LN and Post-LN, promoting more\nbalanced, healthier gradient norms throughout the network, and enhancing the\noverall quality of LLM pre-training. Furthermore, we demonstrate that models\npre-trained with Mix-LN learn better compared to those using Pre-LN or Post-LN\nduring supervised fine-tuning (SFT) and reinforcement learning from human\nfeedback (RLHF), highlighting the critical importance of high-quality deep\nlayers. By effectively addressing the inefficiencies of deep layers in current\nLLMs, Mix-LN unlocks their potential, enhancing model capacity without\nincreasing model size. Our code is available at\nhttps://github.com/pixeli99/MixLN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success, yet recent\nfindings reveal that their deeper layers often contribute minimally and can be\npruned without affecting overall performance. While some view this as an\nopportunity for model compression, we identify it as a training shortfall\nrooted in the widespread use of Pre-Layer Normalization (Pre-LN). We\ndemonstrate that Pre-LN, commonly employed in models like GPT and LLaMA, leads\nto diminished gradient norms in its deeper layers, reducing their\neffectiveness. In contrast, Post-Layer Normalization (Post-LN) preserves larger\ngradient norms in deeper layers but suffers from vanishing gradients in earlier\nlayers. To address this, we introduce Mix-LN, a novel normalization technique\nthat combines the strengths of Pre-LN and Post-LN within the same model. Mix-LN\napplies Post-LN to the earlier layers and Pre-LN to the deeper layers, ensuring\nmore uniform gradients across layers. This allows all parts of the\nnetwork--both shallow and deep layers--to contribute effectively to training.\nExtensive experiments with various model sizes from 70M to 7B demonstrate that\nMix-LN consistently outperforms both Pre-LN and Post-LN, promoting more\nbalanced, healthier gradient norms throughout the network, and enhancing the\noverall quality of LLM pre-training. Furthermore, we demonstrate that models\npre-trained with Mix-LN learn better compared to those using Pre-LN or Post-LN\nduring supervised fine-tuning (SFT) and reinforcement learning from human\nfeedback (RLHF), highlighting the critical importance of high-quality deep\nlayers. By effectively addressing the inefficiencies of deep layers in current\nLLMs, Mix-LN unlocks their potential, enhancing model capacity without\nincreasing model size. Our code is available at\nhttps://github.com/pixeli99/MixLN."
                },
                "authors": [
                    {
                        "name": "Pengxiang Li"
                    },
                    {
                        "name": "Lu Yin"
                    },
                    {
                        "name": "Shiwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Shiwei Liu"
                },
                "author": "Shiwei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13791v1",
                "updated": "2024-12-18T12:33:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    33,
                    50,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:33:50Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    33,
                    50,
                    2,
                    353,
                    0
                ],
                "title": "Physics Reasoner: Knowledge-Augmented Reasoning for Solving Physics\n  Problems with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics Reasoner: Knowledge-Augmented Reasoning for Solving Physics\n  Problems with Large Language Models"
                },
                "summary": "Physics problems constitute a significant aspect of reasoning, necessitating\ncomplicated reasoning ability and abundant physics knowledge. However, existing\nlarge language models (LLMs) frequently fail due to a lack of knowledge or\nincorrect knowledge application. To mitigate these issues, we propose Physics\nReasoner, a knowledge-augmented framework to solve physics problems with LLMs.\nSpecifically, the proposed framework constructs a comprehensive formula set to\nprovide explicit physics knowledge and utilizes checklists containing detailed\ninstructions to guide effective knowledge application. Namely, given a physics\nproblem, Physics Reasoner solves it through three stages: problem analysis,\nformula retrieval, and guided reasoning. During the process, checklists are\nemployed to enhance LLMs' self-improvement in the analysis and reasoning\nstages. Empirically, Physics Reasoner mitigates the issues of insufficient\nknowledge and incorrect application, achieving state-of-the-art performance on\nSciBench with an average accuracy improvement of 5.8%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics problems constitute a significant aspect of reasoning, necessitating\ncomplicated reasoning ability and abundant physics knowledge. However, existing\nlarge language models (LLMs) frequently fail due to a lack of knowledge or\nincorrect knowledge application. To mitigate these issues, we propose Physics\nReasoner, a knowledge-augmented framework to solve physics problems with LLMs.\nSpecifically, the proposed framework constructs a comprehensive formula set to\nprovide explicit physics knowledge and utilizes checklists containing detailed\ninstructions to guide effective knowledge application. Namely, given a physics\nproblem, Physics Reasoner solves it through three stages: problem analysis,\nformula retrieval, and guided reasoning. During the process, checklists are\nemployed to enhance LLMs' self-improvement in the analysis and reasoning\nstages. Empirically, Physics Reasoner mitigates the issues of insufficient\nknowledge and incorrect application, achieving state-of-the-art performance on\nSciBench with an average accuracy improvement of 5.8%."
                },
                "authors": [
                    {
                        "name": "Xinyu Pang"
                    },
                    {
                        "name": "Ruixin Hong"
                    },
                    {
                        "name": "Zhanke Zhou"
                    },
                    {
                        "name": "Fangrui Lv"
                    },
                    {
                        "name": "Xinwei Yang"
                    },
                    {
                        "name": "Zhilong Liang"
                    },
                    {
                        "name": "Bo Han"
                    },
                    {
                        "name": "Changshui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Changshui Zhang"
                },
                "author": "Changshui Zhang",
                "arxiv_comment": "COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13788v1",
                "updated": "2024-12-18T12:31:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    31,
                    31,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:31:31Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    31,
                    31,
                    2,
                    353,
                    0
                ],
                "title": "Open Universal Arabic ASR Leaderboard",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Universal Arabic ASR Leaderboard"
                },
                "summary": "In recent years, the enhanced capabilities of ASR models and the emergence of\nmulti-dialect datasets have increasingly pushed Arabic ASR model development\ntoward an all-dialect-in-one direction. This trend highlights the need for\nbenchmarking studies that evaluate model performance on multiple dialects,\nproviding the community with insights into models' generalization capabilities.\n  In this paper, we introduce Open Universal Arabic ASR Leaderboard, a\ncontinuous benchmark project for open-source general Arabic ASR models across\nvarious multi-dialect datasets. We also provide a comprehensive analysis of the\nmodel's robustness, speaker adaptation, inference efficiency, and memory\nconsumption. This work aims to offer the Arabic ASR community a reference for\nmodels' general performance and also establish a common evaluation framework\nfor multi-dialectal Arabic ASR models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the enhanced capabilities of ASR models and the emergence of\nmulti-dialect datasets have increasingly pushed Arabic ASR model development\ntoward an all-dialect-in-one direction. This trend highlights the need for\nbenchmarking studies that evaluate model performance on multiple dialects,\nproviding the community with insights into models' generalization capabilities.\n  In this paper, we introduce Open Universal Arabic ASR Leaderboard, a\ncontinuous benchmark project for open-source general Arabic ASR models across\nvarious multi-dialect datasets. We also provide a comprehensive analysis of the\nmodel's robustness, speaker adaptation, inference efficiency, and memory\nconsumption. This work aims to offer the Arabic ASR community a reference for\nmodels' general performance and also establish a common evaluation framework\nfor multi-dialectal Arabic ASR models."
                },
                "authors": [
                    {
                        "name": "Yingzhi Wang"
                    },
                    {
                        "name": "Anas Alhmoud"
                    },
                    {
                        "name": "Muhammad Alqurishi"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Alqurishi"
                },
                "author": "Muhammad Alqurishi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13782v1",
                "updated": "2024-12-18T12:21:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    21,
                    46,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:21:46Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    21,
                    46,
                    2,
                    353,
                    0
                ],
                "title": "Knowledge Editing with Dynamic Knowledge Graphs for Multi-hop Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Editing with Dynamic Knowledge Graphs for Multi-hop Question\n  Answering"
                },
                "summary": "Multi-hop question answering (MHQA) poses a significant challenge for large\nlanguage models (LLMs) due to the extensive knowledge demands involved.\nKnowledge editing, which aims to precisely modify the LLMs to incorporate\nspecific knowledge without negatively impacting other unrelated knowledge,\noffers a potential solution for addressing MHQA challenges with LLMs. However,\ncurrent solutions struggle to effectively resolve issues of knowledge\nconflicts. Most parameter-preserving editing methods are hindered by inaccurate\nretrieval and overlook secondary editing issues, which can introduce noise into\nthe reasoning process of LLMs. In this paper, we introduce KEDKG, a novel\nknowledge editing method that leverages a dynamic knowledge graph for MHQA,\ndesigned to ensure the reliability of answers. KEDKG involves two primary\nsteps: dynamic knowledge graph construction and knowledge graph augmented\ngeneration. Initially, KEDKG autonomously constructs a dynamic knowledge graph\nto store revised information while resolving potential knowledge conflicts.\nSubsequently, it employs a fine-grained retrieval strategy coupled with an\nentity and relation detector to enhance the accuracy of graph retrieval for LLM\ngeneration. Experimental results on benchmarks show that KEDKG surpasses\nprevious state-of-the-art models, delivering more accurate and reliable answers\nin environments with dynamic information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-hop question answering (MHQA) poses a significant challenge for large\nlanguage models (LLMs) due to the extensive knowledge demands involved.\nKnowledge editing, which aims to precisely modify the LLMs to incorporate\nspecific knowledge without negatively impacting other unrelated knowledge,\noffers a potential solution for addressing MHQA challenges with LLMs. However,\ncurrent solutions struggle to effectively resolve issues of knowledge\nconflicts. Most parameter-preserving editing methods are hindered by inaccurate\nretrieval and overlook secondary editing issues, which can introduce noise into\nthe reasoning process of LLMs. In this paper, we introduce KEDKG, a novel\nknowledge editing method that leverages a dynamic knowledge graph for MHQA,\ndesigned to ensure the reliability of answers. KEDKG involves two primary\nsteps: dynamic knowledge graph construction and knowledge graph augmented\ngeneration. Initially, KEDKG autonomously constructs a dynamic knowledge graph\nto store revised information while resolving potential knowledge conflicts.\nSubsequently, it employs a fine-grained retrieval strategy coupled with an\nentity and relation detector to enhance the accuracy of graph retrieval for LLM\ngeneration. Experimental results on benchmarks show that KEDKG surpasses\nprevious state-of-the-art models, delivering more accurate and reliable answers\nin environments with dynamic information."
                },
                "authors": [
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Yigeng Zhou"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Yequan Wang"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Daojing He"
                    },
                    {
                        "name": "Fangming Liu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13781v1",
                "updated": "2024-12-18T12:20:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    20,
                    4,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:20:04Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    20,
                    4,
                    2,
                    353,
                    0
                ],
                "title": "Meta-Reflection: A Feedback-Free Reflection Learning Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-Reflection: A Feedback-Free Reflection Learning Framework"
                },
                "summary": "Despite the remarkable capabilities of large language models (LLMs) in\nnatural language understanding and reasoning, they often display undesirable\nbehaviors, such as generating hallucinations and unfaithful reasoning. A\nprevalent strategy to mitigate these issues is the use of reflection, which\nrefines responses through an iterative process. However, while promising,\nreflection heavily relies on high-quality external feedback and requires\niterative multi-agent inference processes, thus hindering its practical\napplication. In this paper, we propose Meta-Reflection, a novel feedback-free\nreflection mechanism that necessitates only a single inference pass without\nexternal feedback. Motivated by the human ability to remember and retrieve\nreflections from past experiences when encountering similar problems,\nMeta-Reflection integrates reflective insights into a codebook, allowing the\nhistorical insights to be stored, retrieved, and used to guide LLMs in\nproblem-solving. To thoroughly investigate and evaluate the practicality of\nMeta-Reflection in real-world scenarios, we introduce an industrial e-commerce\nbenchmark named E-commerce Customer Intent Detection (ECID). Extensive\nexperiments conducted on both public datasets and the ECID benchmark highlight\nthe effectiveness and efficiency of our proposed approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable capabilities of large language models (LLMs) in\nnatural language understanding and reasoning, they often display undesirable\nbehaviors, such as generating hallucinations and unfaithful reasoning. A\nprevalent strategy to mitigate these issues is the use of reflection, which\nrefines responses through an iterative process. However, while promising,\nreflection heavily relies on high-quality external feedback and requires\niterative multi-agent inference processes, thus hindering its practical\napplication. In this paper, we propose Meta-Reflection, a novel feedback-free\nreflection mechanism that necessitates only a single inference pass without\nexternal feedback. Motivated by the human ability to remember and retrieve\nreflections from past experiences when encountering similar problems,\nMeta-Reflection integrates reflective insights into a codebook, allowing the\nhistorical insights to be stored, retrieved, and used to guide LLMs in\nproblem-solving. To thoroughly investigate and evaluate the practicality of\nMeta-Reflection in real-world scenarios, we introduce an industrial e-commerce\nbenchmark named E-commerce Customer Intent Detection (ECID). Extensive\nexperiments conducted on both public datasets and the ECID benchmark highlight\nthe effectiveness and efficiency of our proposed approach."
                },
                "authors": [
                    {
                        "name": "Yaoke Wang"
                    },
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Xintong Bao"
                    },
                    {
                        "name": "Wenqiao Zhang"
                    },
                    {
                        "name": "Suyang Dai"
                    },
                    {
                        "name": "Kehan Chen"
                    },
                    {
                        "name": "Wenqiang Li"
                    },
                    {
                        "name": "Gang Huang"
                    },
                    {
                        "name": "Siliang Tang"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13774v1",
                "updated": "2024-12-18T12:11:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    11,
                    39,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:11:39Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    11,
                    39,
                    2,
                    353,
                    0
                ],
                "title": "Designing an LLM-Based Copilot for Manufacturing Equipment Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing an LLM-Based Copilot for Manufacturing Equipment Selection"
                },
                "summary": "Effective decision-making in automation equipment selection is critical for\nreducing ramp-up time and maintaining production quality, especially in the\nface of increasing product variation and market demands. However, limited\nexpertise and resource constraints often result in inefficiencies during the\nramp-up phase when new products are integrated into production lines. Existing\nmethods often lack structured and tailored solutions to support automation\nengineers in reducing ramp-up time, leading to compromises in quality. This\nresearch investigates whether large-language models (LLMs), combined with\nRetrieval-Augmented Generation (RAG), can assist in streamlining equipment\nselection in ramp-up planning. We propose a factual-driven copilot integrating\nLLMs with structured and semi-structured knowledge retrieval for three\ncomponent types (robots, feeders and vision systems), providing a guided and\ntraceable state-machine process for decision-making in automation equipment\nselection. The system was demonstrated to an industrial partner, who tested it\non three internal use-cases. Their feedback affirmed its capability to provide\nlogical and actionable recommendations for automation equipment. More\nspecifically, among 22 equipment prompts analyzed, 19 involved selecting the\ncorrect equipment while considering most requirements, and in 6 cases, all\nrequirements were fully met.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective decision-making in automation equipment selection is critical for\nreducing ramp-up time and maintaining production quality, especially in the\nface of increasing product variation and market demands. However, limited\nexpertise and resource constraints often result in inefficiencies during the\nramp-up phase when new products are integrated into production lines. Existing\nmethods often lack structured and tailored solutions to support automation\nengineers in reducing ramp-up time, leading to compromises in quality. This\nresearch investigates whether large-language models (LLMs), combined with\nRetrieval-Augmented Generation (RAG), can assist in streamlining equipment\nselection in ramp-up planning. We propose a factual-driven copilot integrating\nLLMs with structured and semi-structured knowledge retrieval for three\ncomponent types (robots, feeders and vision systems), providing a guided and\ntraceable state-machine process for decision-making in automation equipment\nselection. The system was demonstrated to an industrial partner, who tested it\non three internal use-cases. Their feedback affirmed its capability to provide\nlogical and actionable recommendations for automation equipment. More\nspecifically, among 22 equipment prompts analyzed, 19 involved selecting the\ncorrect equipment while considering most requirements, and in 6 cases, all\nrequirements were fully met."
                },
                "authors": [
                    {
                        "name": "Jonas Werheid"
                    },
                    {
                        "name": "Oleksandr Melnychuk"
                    },
                    {
                        "name": "Hans Zhou"
                    },
                    {
                        "name": "Meike Huber"
                    },
                    {
                        "name": "Christoph Rippe"
                    },
                    {
                        "name": "Dominik Joosten"
                    },
                    {
                        "name": "Zozan Keskin"
                    },
                    {
                        "name": "Max Wittstamm"
                    },
                    {
                        "name": "Sathya Subramani"
                    },
                    {
                        "name": "Benny Drescher"
                    },
                    {
                        "name": "Amon Gppert"
                    },
                    {
                        "name": "Anas Abdelrazeq"
                    },
                    {
                        "name": "Robert H. Schmitt"
                    }
                ],
                "author_detail": {
                    "name": "Robert H. Schmitt"
                },
                "author": "Robert H. Schmitt",
                "arxiv_comment": "Preprint submitted to Manufacturing Letters (MFGLET)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13771v1",
                "updated": "2024-12-18T12:07:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:07:58Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "title": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization"
                },
                "summary": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems."
                },
                "authors": [
                    {
                        "name": "Guanghan Li"
                    },
                    {
                        "name": "Xun Zhang"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Yifan Yin"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "arxiv_comment": "7 pages, 3 figures, AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05200v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05200v3",
                "updated": "2024-12-18T12:07:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    27,
                    2,
                    353,
                    0
                ],
                "published": "2024-08-09T17:44:45Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    17,
                    44,
                    45,
                    4,
                    222,
                    0
                ],
                "title": "KlF: Knowledge Localization and Fusion for Language Model Continual\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KlF: Knowledge Localization and Fusion for Language Model Continual\n  Learning"
                },
                "summary": "Language model continual learning (CL) has recently attracted significant\ninterest for its ability to adapt large language models (LLMs) to dynamic\nreal-world scenarios without retraining. A major challenge in this domain is\ncatastrophic forgetting, where models lose previously acquired knowledge upon\nlearning new tasks. Existing approaches commonly utilize multiple\nparameter-efficient fine-tuning (PEFT) blocks to acquire task-specific\nknowledge, yet these methods are inefficient and fail to leverage potential\nknowledge transfer across tasks. In this paper, we introduce a novel CL\nframework for language models, named Knowledge Localization and Fusion (KlF),\nwhich boosts knowledge transfer without depending on memory replay. KlF\ninitially segregates the model into 'skill units' based on parameter\ndependencies, allowing for more precise control. Subsequently, it employs a\nnovel group-wise knowledge localization technique to ascertain the importance\ndistribution of skill units for a new task. By comparing this importance\ndistribution with those from previous tasks, we implement a fine-grained\nknowledge fusion strategy that retains task-specific knowledge, thereby\npreventing forgetting, and updates task-shared knowledge, which facilitates\nbi-directional knowledge transfer. As a result, KlF achieves an optimal balance\nbetween retaining prior knowledge and excelling in new tasks. KlF also\ndemonstrates strong generalizability, making it suitable for various base\nmodels and adaptable to PEFT methods like LoRA. Furthermore, it offers notable\nextensibility, supporting enhancements through integration with memory replay\ntechniques. Comprehensive experiments conducted on two CL benchmarks, involving\nmodels ranging from 220M to 7B parameters, affirm the effectiveness of KlF and\nits variants across different settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language model continual learning (CL) has recently attracted significant\ninterest for its ability to adapt large language models (LLMs) to dynamic\nreal-world scenarios without retraining. A major challenge in this domain is\ncatastrophic forgetting, where models lose previously acquired knowledge upon\nlearning new tasks. Existing approaches commonly utilize multiple\nparameter-efficient fine-tuning (PEFT) blocks to acquire task-specific\nknowledge, yet these methods are inefficient and fail to leverage potential\nknowledge transfer across tasks. In this paper, we introduce a novel CL\nframework for language models, named Knowledge Localization and Fusion (KlF),\nwhich boosts knowledge transfer without depending on memory replay. KlF\ninitially segregates the model into 'skill units' based on parameter\ndependencies, allowing for more precise control. Subsequently, it employs a\nnovel group-wise knowledge localization technique to ascertain the importance\ndistribution of skill units for a new task. By comparing this importance\ndistribution with those from previous tasks, we implement a fine-grained\nknowledge fusion strategy that retains task-specific knowledge, thereby\npreventing forgetting, and updates task-shared knowledge, which facilitates\nbi-directional knowledge transfer. As a result, KlF achieves an optimal balance\nbetween retaining prior knowledge and excelling in new tasks. KlF also\ndemonstrates strong generalizability, making it suitable for various base\nmodels and adaptable to PEFT methods like LoRA. Furthermore, it offers notable\nextensibility, supporting enhancements through integration with memory replay\ntechniques. Comprehensive experiments conducted on two CL benchmarks, involving\nmodels ranging from 220M to 7B parameters, affirm the effectiveness of KlF and\nits variants across different settings."
                },
                "authors": [
                    {
                        "name": "Yujie Feng"
                    },
                    {
                        "name": "Xu Chu"
                    },
                    {
                        "name": "Yongxin Xu"
                    },
                    {
                        "name": "Zexin Lu"
                    },
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ming Wu"
                },
                "author": "Xiao-Ming Wu",
                "arxiv_comment": "This version updates the model name from Task Skill Localization and\n  Consolidation (TaSL) to Knowledge Localization and Fusion (KlF). It is an\n  extension of the ACL 2024 paper titled Continual Dialog State Tracking via\n  Task Skill Localization and Consolidation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05200v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05200v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13765v1",
                "updated": "2024-12-18T12:01:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    1,
                    53,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:01:53Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    1,
                    53,
                    2,
                    353,
                    0
                ],
                "title": "LLM-SEM: A Sentiment-Based Student Engagement Metric Using LLMS for\n  E-Learning Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-SEM: A Sentiment-Based Student Engagement Metric Using LLMS for\n  E-Learning Platforms"
                },
                "summary": "Current methods for analyzing student engagement in e-learning platforms,\nincluding automated systems, often struggle with challenges such as handling\nfuzzy sentiment in text comments and relying on limited metadata. Traditional\napproaches, such as surveys and questionnaires, also face issues like small\nsample sizes and scalability. In this paper, we introduce LLM-SEM (Language\nModel-Based Student Engagement Metric), a novel approach that leverages video\nmetadata and sentiment analysis of student comments to measure engagement. By\nutilizing recent Large Language Models (LLMs), we generate high-quality\nsentiment predictions to mitigate text fuzziness and normalize key features\nsuch as views and likes. Our holistic method combines comprehensive metadata\nwith sentiment polarity scores to gauge engagement at both the course and\nlesson levels. Extensive experiments were conducted to evaluate various LLM\nmodels, demonstrating the effectiveness of LLM-SEM in providing a scalable and\naccurate measure of student engagement. We fine-tuned LLMs, including AraBERT,\nTXLM-RoBERTa, LLama 3B and Gemma 9B from Ollama, using human-annotated\nsentiment datasets to enhance prediction accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current methods for analyzing student engagement in e-learning platforms,\nincluding automated systems, often struggle with challenges such as handling\nfuzzy sentiment in text comments and relying on limited metadata. Traditional\napproaches, such as surveys and questionnaires, also face issues like small\nsample sizes and scalability. In this paper, we introduce LLM-SEM (Language\nModel-Based Student Engagement Metric), a novel approach that leverages video\nmetadata and sentiment analysis of student comments to measure engagement. By\nutilizing recent Large Language Models (LLMs), we generate high-quality\nsentiment predictions to mitigate text fuzziness and normalize key features\nsuch as views and likes. Our holistic method combines comprehensive metadata\nwith sentiment polarity scores to gauge engagement at both the course and\nlesson levels. Extensive experiments were conducted to evaluate various LLM\nmodels, demonstrating the effectiveness of LLM-SEM in providing a scalable and\naccurate measure of student engagement. We fine-tuned LLMs, including AraBERT,\nTXLM-RoBERTa, LLama 3B and Gemma 9B from Ollama, using human-annotated\nsentiment datasets to enhance prediction accuracy."
                },
                "authors": [
                    {
                        "name": "Ali Hamdi"
                    },
                    {
                        "name": "Ahmed Abdelmoneim Mazrou"
                    },
                    {
                        "name": "Mohamed Shaltout"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Shaltout"
                },
                "author": "Mohamed Shaltout",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13746v1",
                "updated": "2024-12-18T11:28:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    11,
                    28,
                    5,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T11:28:05Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    11,
                    28,
                    5,
                    2,
                    353,
                    0
                ],
                "title": "RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented\n  Generation for Preference Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented\n  Generation for Preference Alignment"
                },
                "summary": "Despite the significant progress made by existing retrieval augmented\nlanguage models (RALMs) in providing trustworthy responses and grounding in\nreliable sources, they often overlook effective alignment with human\npreferences. In the alignment process, reward models (RMs) act as a crucial\nproxy for human values to guide optimization. However, it remains unclear how\nto evaluate and select a reliable RM for preference alignment in RALMs. To this\nend, we propose RAG-RewardBench, the first benchmark for evaluating RMs in RAG\nsettings. First, we design four crucial and challenging RAG-specific scenarios\nto assess RMs, including multi-hop reasoning, fine-grained citation,\nappropriate abstain, and conflict robustness. Then, we incorporate 18 RAG\nsubsets, six retrievers, and 24 RALMs to increase the diversity of data\nsources. Finally, we adopt an LLM-as-a-judge approach to improve preference\nannotation efficiency and effectiveness, exhibiting a strong correlation with\nhuman annotations. Based on the RAG-RewardBench, we conduct a comprehensive\nevaluation of 45 RMs and uncover their limitations in RAG scenarios.\nAdditionally, we also reveal that existing trained RALMs show almost no\nimprovement in preference alignment, highlighting the need for a shift towards\npreference-aligned training.We release our benchmark and code publicly at\nhttps://huggingface.co/datasets/jinzhuoran/RAG-RewardBench/ for future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the significant progress made by existing retrieval augmented\nlanguage models (RALMs) in providing trustworthy responses and grounding in\nreliable sources, they often overlook effective alignment with human\npreferences. In the alignment process, reward models (RMs) act as a crucial\nproxy for human values to guide optimization. However, it remains unclear how\nto evaluate and select a reliable RM for preference alignment in RALMs. To this\nend, we propose RAG-RewardBench, the first benchmark for evaluating RMs in RAG\nsettings. First, we design four crucial and challenging RAG-specific scenarios\nto assess RMs, including multi-hop reasoning, fine-grained citation,\nappropriate abstain, and conflict robustness. Then, we incorporate 18 RAG\nsubsets, six retrievers, and 24 RALMs to increase the diversity of data\nsources. Finally, we adopt an LLM-as-a-judge approach to improve preference\nannotation efficiency and effectiveness, exhibiting a strong correlation with\nhuman annotations. Based on the RAG-RewardBench, we conduct a comprehensive\nevaluation of 45 RMs and uncover their limitations in RAG scenarios.\nAdditionally, we also reveal that existing trained RALMs show almost no\nimprovement in preference alignment, highlighting the need for a shift towards\npreference-aligned training.We release our benchmark and code publicly at\nhttps://huggingface.co/datasets/jinzhuoran/RAG-RewardBench/ for future work."
                },
                "authors": [
                    {
                        "name": "Zhuoran Jin"
                    },
                    {
                        "name": "Hongbang Yuan"
                    },
                    {
                        "name": "Tianyi Men"
                    },
                    {
                        "name": "Pengfei Cao"
                    },
                    {
                        "name": "Yubo Chen"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "arxiv_comment": "26 pages, 12 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13737v1",
                "updated": "2024-12-18T11:14:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    11,
                    14,
                    30,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T11:14:30Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    11,
                    14,
                    30,
                    2,
                    353,
                    0
                ],
                "title": "On the Compression of Language Models for Code: An Empirical Study on\n  CodeBERT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Compression of Language Models for Code: An Empirical Study on\n  CodeBERT"
                },
                "summary": "Language models have proven successful across a wide range of software\nengineering tasks, but their significant computational costs often hinder their\npractical adoption. To address this challenge, researchers have begun applying\nvarious compression strategies to improve the efficiency of language models for\ncode. These strategies aim to optimize inference latency and memory usage,\nthough often at the cost of reduced model effectiveness. However, there is\nstill a significant gap in understanding how these strategies influence the\nefficiency and effectiveness of language models for code. Here, we empirically\ninvestigate the impact of three well-known compression strategies -- knowledge\ndistillation, quantization, and pruning -- across three different classes of\nsoftware engineering tasks: vulnerability detection, code summarization, and\ncode search. Our findings reveal that the impact of these strategies varies\ngreatly depending on the task and the specific compression method employed.\nPractitioners and researchers can use these insights to make informed decisions\nwhen selecting the most appropriate compression strategy, balancing both\nefficiency and effectiveness based on their specific needs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models have proven successful across a wide range of software\nengineering tasks, but their significant computational costs often hinder their\npractical adoption. To address this challenge, researchers have begun applying\nvarious compression strategies to improve the efficiency of language models for\ncode. These strategies aim to optimize inference latency and memory usage,\nthough often at the cost of reduced model effectiveness. However, there is\nstill a significant gap in understanding how these strategies influence the\nefficiency and effectiveness of language models for code. Here, we empirically\ninvestigate the impact of three well-known compression strategies -- knowledge\ndistillation, quantization, and pruning -- across three different classes of\nsoftware engineering tasks: vulnerability detection, code summarization, and\ncode search. Our findings reveal that the impact of these strategies varies\ngreatly depending on the task and the specific compression method employed.\nPractitioners and researchers can use these insights to make informed decisions\nwhen selecting the most appropriate compression strategy, balancing both\nefficiency and effectiveness based on their specific needs."
                },
                "authors": [
                    {
                        "name": "Giordano d'Aloisio"
                    },
                    {
                        "name": "Luca Traini"
                    },
                    {
                        "name": "Federica Sarro"
                    },
                    {
                        "name": "Antinisca Di Marco"
                    }
                ],
                "author_detail": {
                    "name": "Antinisca Di Marco"
                },
                "author": "Antinisca Di Marco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13722v1",
                "updated": "2024-12-18T11:03:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    11,
                    3,
                    26,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T11:03:26Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    11,
                    3,
                    26,
                    2,
                    353,
                    0
                ],
                "title": "Data-driven Discovery of Biophysical T Cell Receptor Co-specificity\n  Rules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven Discovery of Biophysical T Cell Receptor Co-specificity\n  Rules"
                },
                "summary": "The biophysical interactions between the T cell receptor (TCR) and its\nligands determine the specificity of the cellular immune response. However, the\nimmense diversity of receptors and ligands has made it challenging to discover\ngeneralizable rules across the distinct binding affinity landscapes created by\ndifferent ligands. Here, we present an optimization framework for discovering\nbiophysical rules that predict whether TCRs share specificity to a ligand.\nApplying this framework to TCRs associated with a collection of SARS-CoV-2\npeptides we establish how co-specificity depends on the type and position of\namino-acid differences between receptors. We also demonstrate that the inferred\nrules generalize to ligands not seen during training. Our analysis reveals that\nmatching of steric properties between substituted amino acids is important for\nreceptor co-specificity, in contrast with the hydrophobic properties that more\nprominently determine evolutionary substitutability. We furthermore find that\npositions not in direct contact with the peptide still significantly impact\nspecificity. These findings highlight the potential for data-driven approaches\nto uncover the molecular mechanisms underpinning the specificity of adaptive\nimmune responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The biophysical interactions between the T cell receptor (TCR) and its\nligands determine the specificity of the cellular immune response. However, the\nimmense diversity of receptors and ligands has made it challenging to discover\ngeneralizable rules across the distinct binding affinity landscapes created by\ndifferent ligands. Here, we present an optimization framework for discovering\nbiophysical rules that predict whether TCRs share specificity to a ligand.\nApplying this framework to TCRs associated with a collection of SARS-CoV-2\npeptides we establish how co-specificity depends on the type and position of\namino-acid differences between receptors. We also demonstrate that the inferred\nrules generalize to ligands not seen during training. Our analysis reveals that\nmatching of steric properties between substituted amino acids is important for\nreceptor co-specificity, in contrast with the hydrophobic properties that more\nprominently determine evolutionary substitutability. We furthermore find that\npositions not in direct contact with the peptide still significantly impact\nspecificity. These findings highlight the potential for data-driven approaches\nto uncover the molecular mechanisms underpinning the specificity of adaptive\nimmune responses."
                },
                "authors": [
                    {
                        "name": "Andrew G. T. Pyo"
                    },
                    {
                        "name": "Yuta Nagano"
                    },
                    {
                        "name": "Martina Milighetti"
                    },
                    {
                        "name": "James Henderson"
                    },
                    {
                        "name": "Curtis G. Callan Jr."
                    },
                    {
                        "name": "Benny Chain"
                    },
                    {
                        "name": "Ned S. Wingreen"
                    },
                    {
                        "name": "Andreas Tiffeau-Mayer"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Tiffeau-Mayer"
                },
                "author": "Andreas Tiffeau-Mayer",
                "arxiv_comment": "15 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04503v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04503v2",
                "updated": "2024-12-18T11:02:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    11,
                    2,
                    14,
                    2,
                    353,
                    0
                ],
                "published": "2024-07-05T13:44:09Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    13,
                    44,
                    9,
                    4,
                    187,
                    0
                ],
                "title": "When LLMs Play the Telephone Game: Cumulative Changes and Attractors in\n  Iterated Cultural Transmissions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When LLMs Play the Telephone Game: Cumulative Changes and Attractors in\n  Iterated Cultural Transmissions"
                },
                "summary": "As large language models (LLMs) start interacting with each other and\ngenerating an increasing amount of text online, it becomes crucial to better\nunderstand how information is transformed as it passes from one LLM to the\nnext. While significant research has examined individual LLM behaviors,\nexisting studies have largely overlooked the collective behaviors and\ninformation distortions arising from iterated LLM interactions. Small biases,\nnegligible at the single output level, risk being amplified in iterated\ninteractions, potentially leading the content to evolve towards attractor\nstates. In a series of telephone game experiments, we apply a transmission\nchain design borrowed from the human cultural evolution literature: LLM agents\niteratively receive, produce, and transmit texts from the previous to the next\nagent in the chain. By tracking the evolution of text toxicity, positivity,\ndifficulty, and length across transmission chains, we uncover the existence of\nbiases and attractors, and study their dependence on the initial text, the\ninstructions, language model, and model size. For instance, we find that more\nopen-ended instructions lead to stronger attraction effects compared to more\nconstrained tasks. We also find that different text properties display\ndifferent sensitivity to attraction effects, with toxicity leading to stronger\nattractors than length. These findings highlight the importance of accounting\nfor multi-step transmission dynamics and represent a first step towards a more\ncomprehensive understanding of LLM cultural dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) start interacting with each other and\ngenerating an increasing amount of text online, it becomes crucial to better\nunderstand how information is transformed as it passes from one LLM to the\nnext. While significant research has examined individual LLM behaviors,\nexisting studies have largely overlooked the collective behaviors and\ninformation distortions arising from iterated LLM interactions. Small biases,\nnegligible at the single output level, risk being amplified in iterated\ninteractions, potentially leading the content to evolve towards attractor\nstates. In a series of telephone game experiments, we apply a transmission\nchain design borrowed from the human cultural evolution literature: LLM agents\niteratively receive, produce, and transmit texts from the previous to the next\nagent in the chain. By tracking the evolution of text toxicity, positivity,\ndifficulty, and length across transmission chains, we uncover the existence of\nbiases and attractors, and study their dependence on the initial text, the\ninstructions, language model, and model size. For instance, we find that more\nopen-ended instructions lead to stronger attraction effects compared to more\nconstrained tasks. We also find that different text properties display\ndifferent sensitivity to attraction effects, with toxicity leading to stronger\nattractors than length. These findings highlight the importance of accounting\nfor multi-step transmission dynamics and represent a first step towards a more\ncomprehensive understanding of LLM cultural dynamics."
                },
                "authors": [
                    {
                        "name": "Jrmy Perez"
                    },
                    {
                        "name": "Grgur Kova"
                    },
                    {
                        "name": "Corentin Lger"
                    },
                    {
                        "name": "Cdric Colas"
                    },
                    {
                        "name": "Gaia Molinaro"
                    },
                    {
                        "name": "Maxime Derex"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    },
                    {
                        "name": "Clment Moulin-Frier"
                    }
                ],
                "author_detail": {
                    "name": "Clment Moulin-Frier"
                },
                "author": "Clment Moulin-Frier",
                "arxiv_comment": "Code available at https://github.com/jeremyperez2/TelephoneGameLLM.\n  Companion website with a Data Explorer tool at\n  https://sites.google.com/view/telephone-game-llm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04503v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04503v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13720v1",
                "updated": "2024-12-18T11:00:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    11,
                    0,
                    58,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T11:00:58Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    11,
                    0,
                    58,
                    2,
                    353,
                    0
                ],
                "title": "Federated Learning and RAG Integration: A Scalable Approach for Medical\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning and RAG Integration: A Scalable Approach for Medical\n  Large Language Models"
                },
                "summary": "This study analyzes the performance of domain-specific Large Language Models\n(LLMs) for the medical field by integrating Retrieval-Augmented Generation\n(RAG) systems within a federated learning framework. Leveraging the inherent\nadvantages of federated learning, such as preserving data privacy and enabling\ndistributed computation, this research explores the integration of RAG systems\nwith models trained under varying client configurations to optimize\nperformance. Experimental results demonstrate that the federated learning-based\nmodels integrated with RAG systems consistently outperform their non-integrated\ncounterparts across all evaluation metrics. This study highlights the potential\nof combining federated learning and RAG systems for developing domain-specific\nLLMs in the medical field, providing a scalable and privacy-preserving solution\nfor enhancing text generation capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study analyzes the performance of domain-specific Large Language Models\n(LLMs) for the medical field by integrating Retrieval-Augmented Generation\n(RAG) systems within a federated learning framework. Leveraging the inherent\nadvantages of federated learning, such as preserving data privacy and enabling\ndistributed computation, this research explores the integration of RAG systems\nwith models trained under varying client configurations to optimize\nperformance. Experimental results demonstrate that the federated learning-based\nmodels integrated with RAG systems consistently outperform their non-integrated\ncounterparts across all evaluation metrics. This study highlights the potential\nof combining federated learning and RAG systems for developing domain-specific\nLLMs in the medical field, providing a scalable and privacy-preserving solution\nfor enhancing text generation capabilities."
                },
                "authors": [
                    {
                        "name": "Jincheol Jung"
                    },
                    {
                        "name": "Hongju Jeong"
                    },
                    {
                        "name": "Eui-Nam Huh"
                    }
                ],
                "author_detail": {
                    "name": "Eui-Nam Huh"
                },
                "author": "Eui-Nam Huh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13705v1",
                "updated": "2024-12-18T10:49:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    10,
                    49,
                    41,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T10:49:41Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    10,
                    49,
                    41,
                    2,
                    353,
                    0
                ],
                "title": "Mitigating Adversarial Attacks in LLMs through Defensive Suffix\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Adversarial Attacks in LLMs through Defensive Suffix\n  Generation"
                },
                "summary": "Large language models (LLMs) have exhibited outstanding performance in\nnatural language processing tasks. However, these models remain susceptible to\nadversarial attacks in which slight input perturbations can lead to harmful or\nmisleading outputs. A gradient-based defensive suffix generation algorithm is\ndesigned to bolster the robustness of LLMs. By appending carefully optimized\ndefensive suffixes to input prompts, the algorithm mitigates adversarial\ninfluences while preserving the models' utility. To enhance adversarial\nunderstanding, a novel total loss function ($L_{\\text{total}}$) combining\ndefensive loss ($L_{\\text{def}}$) and adversarial loss ($L_{\\text{adv}}$)\ngenerates defensive suffixes more effectively. Experimental evaluations\nconducted on open-source LLMs such as Gemma-7B, mistral-7B, Llama2-7B, and\nLlama2-13B show that the proposed method reduces attack success rates (ASR) by\nan average of 11\\% compared to models without defensive suffixes. Additionally,\nthe perplexity score of Gemma-7B decreased from 6.57 to 3.93 when applying the\ndefensive suffix generated by openELM-270M. Furthermore, TruthfulQA evaluations\ndemonstrate consistent improvements with Truthfulness scores increasing by up\nto 10\\% across tested configurations. This approach significantly enhances the\nsecurity of LLMs in critical applications without requiring extensive\nretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited outstanding performance in\nnatural language processing tasks. However, these models remain susceptible to\nadversarial attacks in which slight input perturbations can lead to harmful or\nmisleading outputs. A gradient-based defensive suffix generation algorithm is\ndesigned to bolster the robustness of LLMs. By appending carefully optimized\ndefensive suffixes to input prompts, the algorithm mitigates adversarial\ninfluences while preserving the models' utility. To enhance adversarial\nunderstanding, a novel total loss function ($L_{\\text{total}}$) combining\ndefensive loss ($L_{\\text{def}}$) and adversarial loss ($L_{\\text{adv}}$)\ngenerates defensive suffixes more effectively. Experimental evaluations\nconducted on open-source LLMs such as Gemma-7B, mistral-7B, Llama2-7B, and\nLlama2-13B show that the proposed method reduces attack success rates (ASR) by\nan average of 11\\% compared to models without defensive suffixes. Additionally,\nthe perplexity score of Gemma-7B decreased from 6.57 to 3.93 when applying the\ndefensive suffix generated by openELM-270M. Furthermore, TruthfulQA evaluations\ndemonstrate consistent improvements with Truthfulness scores increasing by up\nto 10\\% across tested configurations. This approach significantly enhances the\nsecurity of LLMs in critical applications without requiring extensive\nretraining."
                },
                "authors": [
                    {
                        "name": "Minkyoung Kim"
                    },
                    {
                        "name": "Yunha Kim"
                    },
                    {
                        "name": "Hyeram Seo"
                    },
                    {
                        "name": "Heejung Choi"
                    },
                    {
                        "name": "Jiye Han"
                    },
                    {
                        "name": "Gaeun Kee"
                    },
                    {
                        "name": "Soyoung Ko"
                    },
                    {
                        "name": "HyoJe Jung"
                    },
                    {
                        "name": "Byeolhee Kim"
                    },
                    {
                        "name": "Young-Hak Kim"
                    },
                    {
                        "name": "Sanghyun Park"
                    },
                    {
                        "name": "Tae Joon Jun"
                    }
                ],
                "author_detail": {
                    "name": "Tae Joon Jun"
                },
                "author": "Tae Joon Jun",
                "arxiv_comment": "9 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13698v1",
                "updated": "2024-12-18T10:42:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    10,
                    42,
                    53,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T10:42:53Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    10,
                    42,
                    53,
                    2,
                    353,
                    0
                ],
                "title": "Towards Efficient and Explainable Hate Speech Detection via Model\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient and Explainable Hate Speech Detection via Model\n  Distillation"
                },
                "summary": "Automatic detection of hate and abusive language is essential to combat its\nonline spread. Moreover, recognising and explaining hate speech serves to\neducate people about its negative effects. However, most current detection\nmodels operate as black boxes, lacking interpretability and explainability. In\nthis context, Large Language Models (LLMs) have proven effective for hate\nspeech detection and to promote interpretability. Nevertheless, they are\ncomputationally costly to run. In this work, we propose distilling big language\nmodels by using Chain-of-Thought to extract explanations that support the hate\nspeech classification task. Having small language models for these tasks will\ncontribute to their use in operational settings. In this paper, we demonstrate\nthat distilled models deliver explanations of the same quality as larger models\nwhile surpassing them in classification performance. This dual capability,\nclassifying and explaining, advances hate speech detection making it more\naffordable, understandable and actionable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic detection of hate and abusive language is essential to combat its\nonline spread. Moreover, recognising and explaining hate speech serves to\neducate people about its negative effects. However, most current detection\nmodels operate as black boxes, lacking interpretability and explainability. In\nthis context, Large Language Models (LLMs) have proven effective for hate\nspeech detection and to promote interpretability. Nevertheless, they are\ncomputationally costly to run. In this work, we propose distilling big language\nmodels by using Chain-of-Thought to extract explanations that support the hate\nspeech classification task. Having small language models for these tasks will\ncontribute to their use in operational settings. In this paper, we demonstrate\nthat distilled models deliver explanations of the same quality as larger models\nwhile surpassing them in classification performance. This dual capability,\nclassifying and explaining, advances hate speech detection making it more\naffordable, understandable and actionable."
                },
                "authors": [
                    {
                        "name": "Paloma Piot"
                    },
                    {
                        "name": "Javier Parapar"
                    }
                ],
                "author_detail": {
                    "name": "Javier Parapar"
                },
                "author": "Javier Parapar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13693v1",
                "updated": "2024-12-18T10:33:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    10,
                    33,
                    55,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T10:33:55Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    10,
                    33,
                    55,
                    2,
                    353,
                    0
                ],
                "title": "A2H: A UI Converter from Android to HarmonyOS Platform",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A2H: A UI Converter from Android to HarmonyOS Platform"
                },
                "summary": "With the growing importance of smartphones, developers face the challenge of\ncreating separate applications for multiple platforms (e.g., Android, iOS, and\nHarmonyOS), leading to increased development costs and longer iteration cycles.\nOne potential solution is to develop an app on one platform and then\nautomatically convert it to other platforms, reducing the need for separate\ndevelopment efforts. However, migrating user interfaces (UIs) between platforms\nis particularly challenging due to significant differences in layout structures\nand development paradigms, such as the disparity between XML layout files in\nAndroid and ArkUI framework in HarmonyOS. Manual conversion of UIs is\ntime-consuming, error-prone, and inefficient, necessitating an automated\nsolution to streamline the process and enable seamless migration from Android\nto HarmonyOS. To address this challenge, we propose the A2H Converter, an\nautomated tool for migrating Android UIs to HarmonyOS. The tool employs an\nlarge language model (LLM)-driven multi-agent framework to convert Android XML\nlayouts into HarmonyOS ArkUI layouts. Using the RAG combing with decision\nrules, the system maps Android UI components to ArkUI equivalents, while a\nreflective mechanism continuously improves conversion accuracy. A2H Converter\nhandles project-level layouts, ensuring consistency across multiple files and\naddressing complex UI logic. Experiments on six Android applications collected\nfrom GitHub demonstrate that our A2H Converter achieves a migration success\nrate of over 90.1\\%, 89.3\\%, and 89.2\\% at the component, page, and project\nlevels, respectively. The demo video is available at. The tool is available at\nhttp://124.70.54.129:37860/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing importance of smartphones, developers face the challenge of\ncreating separate applications for multiple platforms (e.g., Android, iOS, and\nHarmonyOS), leading to increased development costs and longer iteration cycles.\nOne potential solution is to develop an app on one platform and then\nautomatically convert it to other platforms, reducing the need for separate\ndevelopment efforts. However, migrating user interfaces (UIs) between platforms\nis particularly challenging due to significant differences in layout structures\nand development paradigms, such as the disparity between XML layout files in\nAndroid and ArkUI framework in HarmonyOS. Manual conversion of UIs is\ntime-consuming, error-prone, and inefficient, necessitating an automated\nsolution to streamline the process and enable seamless migration from Android\nto HarmonyOS. To address this challenge, we propose the A2H Converter, an\nautomated tool for migrating Android UIs to HarmonyOS. The tool employs an\nlarge language model (LLM)-driven multi-agent framework to convert Android XML\nlayouts into HarmonyOS ArkUI layouts. Using the RAG combing with decision\nrules, the system maps Android UI components to ArkUI equivalents, while a\nreflective mechanism continuously improves conversion accuracy. A2H Converter\nhandles project-level layouts, ensuring consistency across multiple files and\naddressing complex UI logic. Experiments on six Android applications collected\nfrom GitHub demonstrate that our A2H Converter achieves a migration success\nrate of over 90.1\\%, 89.3\\%, and 89.2\\% at the component, page, and project\nlevels, respectively. The demo video is available at. The tool is available at\nhttp://124.70.54.129:37860/."
                },
                "authors": [
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Lina Gong"
                    },
                    {
                        "name": "Yujun Huang"
                    },
                    {
                        "name": "Di Cui"
                    },
                    {
                        "name": "Mingqiang Wei"
                    }
                ],
                "author_detail": {
                    "name": "Mingqiang Wei"
                },
                "author": "Mingqiang Wei",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16203v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16203v2",
                "updated": "2024-12-18T10:25:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    10,
                    25,
                    12,
                    2,
                    353,
                    0
                ],
                "published": "2024-05-25T12:27:21Z",
                "published_parsed": [
                    2024,
                    5,
                    25,
                    12,
                    27,
                    21,
                    5,
                    146,
                    0
                ],
                "title": "Evolutionary Large Language Model for Automated Feature Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary Large Language Model for Automated Feature Transformation"
                },
                "summary": "Feature transformation aims to reconstruct the feature space of raw features\nto enhance the performance of downstream models. However, the exponential\ngrowth in the combinations of features and operations poses a challenge, making\nit difficult for existing methods to efficiently explore a wide space.\nAdditionally, their optimization is solely driven by the accuracy of downstream\nmodels in specific domains, neglecting the acquisition of general feature\nknowledge. To fill this research gap, we propose an evolutionary LLM framework\nfor automated feature transformation. This framework consists of two parts: 1)\nconstructing a multi-population database through an RL data collector while\nutilizing evolutionary algorithm strategies for database maintenance, and 2)\nutilizing the ability of Large Language Model (LLM) in sequence understanding,\nwe employ few-shot prompts to guide LLM in generating superior samples based on\nfeature transformation sequence distinction. Leveraging the multi-population\ndatabase initially provides a wide search scope to discover excellent\npopulations. Through culling and evolution, the high-quality populations are\nafforded greater opportunities, thereby furthering the pursuit of optimal\nindividuals. Through the integration of LLMs with evolutionary algorithms, we\nachieve efficient exploration within a vast space, while harnessing feature\nknowledge to propel optimization, thus realizing a more adaptable search\nparadigm. Finally, we empirically demonstrate the effectiveness and generality\nof our proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature transformation aims to reconstruct the feature space of raw features\nto enhance the performance of downstream models. However, the exponential\ngrowth in the combinations of features and operations poses a challenge, making\nit difficult for existing methods to efficiently explore a wide space.\nAdditionally, their optimization is solely driven by the accuracy of downstream\nmodels in specific domains, neglecting the acquisition of general feature\nknowledge. To fill this research gap, we propose an evolutionary LLM framework\nfor automated feature transformation. This framework consists of two parts: 1)\nconstructing a multi-population database through an RL data collector while\nutilizing evolutionary algorithm strategies for database maintenance, and 2)\nutilizing the ability of Large Language Model (LLM) in sequence understanding,\nwe employ few-shot prompts to guide LLM in generating superior samples based on\nfeature transformation sequence distinction. Leveraging the multi-population\ndatabase initially provides a wide search scope to discover excellent\npopulations. Through culling and evolution, the high-quality populations are\nafforded greater opportunities, thereby furthering the pursuit of optimal\nindividuals. Through the integration of LLMs with evolutionary algorithms, we\nachieve efficient exploration within a vast space, while harnessing feature\nknowledge to propel optimization, thus realizing a more adaptable search\nparadigm. Finally, we empirically demonstrate the effectiveness and generality\nof our proposed method."
                },
                "authors": [
                    {
                        "name": "Nanxu Gong"
                    },
                    {
                        "name": "Chandan K. Reddy"
                    },
                    {
                        "name": "Wangyang Ying"
                    },
                    {
                        "name": "Haifeng Chen"
                    },
                    {
                        "name": "Yanjie Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yanjie Fu"
                },
                "author": "Yanjie Fu",
                "arxiv_comment": "Accepted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16203v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16203v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.14172v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14172v1",
                "updated": "2024-12-18T18:59:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    59,
                    56,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T18:59:56Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    59,
                    56,
                    2,
                    353,
                    0
                ],
                "title": "Learning from Massive Human Videos for Universal Humanoid Pose Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Massive Human Videos for Universal Humanoid Pose Control"
                },
                "summary": "Scalable learning of humanoid robots is crucial for their deployment in\nreal-world applications. While traditional approaches primarily rely on\nreinforcement learning or teleoperation to achieve whole-body control, they are\noften limited by the diversity of simulated environments and the high costs of\ndemonstration collection. In contrast, human videos are ubiquitous and present\nan untapped source of semantic and motion information that could significantly\nenhance the generalization capabilities of humanoid robots. This paper\nintroduces Humanoid-X, a large-scale dataset of over 20 million humanoid robot\nposes with corresponding text-based motion descriptions, designed to leverage\nthis abundant data. Humanoid-X is curated through a comprehensive pipeline:\ndata mining from the Internet, video caption generation, motion retargeting of\nhumans to humanoid robots, and policy learning for real-world deployment. With\nHumanoid-X, we further train a large humanoid model, UH-1, which takes text\ninstructions as input and outputs corresponding actions to control a humanoid\nrobot. Extensive simulated and real-world experiments validate that our\nscalable training approach leads to superior generalization in text-based\nhumanoid control, marking a significant step toward adaptable, real-world-ready\nhumanoid robots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable learning of humanoid robots is crucial for their deployment in\nreal-world applications. While traditional approaches primarily rely on\nreinforcement learning or teleoperation to achieve whole-body control, they are\noften limited by the diversity of simulated environments and the high costs of\ndemonstration collection. In contrast, human videos are ubiquitous and present\nan untapped source of semantic and motion information that could significantly\nenhance the generalization capabilities of humanoid robots. This paper\nintroduces Humanoid-X, a large-scale dataset of over 20 million humanoid robot\nposes with corresponding text-based motion descriptions, designed to leverage\nthis abundant data. Humanoid-X is curated through a comprehensive pipeline:\ndata mining from the Internet, video caption generation, motion retargeting of\nhumans to humanoid robots, and policy learning for real-world deployment. With\nHumanoid-X, we further train a large humanoid model, UH-1, which takes text\ninstructions as input and outputs corresponding actions to control a humanoid\nrobot. Extensive simulated and real-world experiments validate that our\nscalable training approach leads to superior generalization in text-based\nhumanoid control, marking a significant step toward adaptable, real-world-ready\nhumanoid robots."
                },
                "authors": [
                    {
                        "name": "Jiageng Mao"
                    },
                    {
                        "name": "Siheng Zhao"
                    },
                    {
                        "name": "Siqi Song"
                    },
                    {
                        "name": "Tianheng Shi"
                    },
                    {
                        "name": "Junjie Ye"
                    },
                    {
                        "name": "Mingtong Zhang"
                    },
                    {
                        "name": "Haoran Geng"
                    },
                    {
                        "name": "Jitendra Malik"
                    },
                    {
                        "name": "Vitor Guizilini"
                    },
                    {
                        "name": "Yue Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Wang"
                },
                "author": "Yue Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14172v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14164v1",
                "updated": "2024-12-18T18:58:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    58,
                    50,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T18:58:50Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    58,
                    50,
                    2,
                    353,
                    0
                ],
                "title": "MetaMorph: Multimodal Understanding and Generation via Instruction\n  Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaMorph: Multimodal Understanding and Generation via Instruction\n  Tuning"
                },
                "summary": "In this work, we propose Visual-Predictive Instruction Tuning (VPiT) - a\nsimple and effective extension to visual instruction tuning that enables a\npretrained LLM to quickly morph into an unified autoregressive model capable of\ngenerating both text and visual tokens. VPiT teaches an LLM to predict discrete\ntext tokens and continuous visual tokens from any input sequence of image and\ntext data curated in an instruction-following format. Our empirical\ninvestigation reveals several intriguing properties of VPiT: (1) visual\ngeneration ability emerges as a natural byproduct of improved visual\nunderstanding, and can be unlocked efficiently with a small amount of\ngeneration data; (2) while we find understanding and generation to be mutually\nbeneficial, understanding data contributes to both capabilities more\neffectively than generation data. Building upon these findings, we train our\nMetaMorph model and achieve competitive performance on both visual\nunderstanding and generation. In visual generation, MetaMorph can leverage the\nworld knowledge and reasoning abilities gained from LLM pretraining, and\novercome common failure modes exhibited by other generation models. Our results\nsuggest that LLMs may have strong \"prior\" vision capabilities that can be\nefficiently adapted to both visual understanding and generation with a\nrelatively simple instruction tuning process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose Visual-Predictive Instruction Tuning (VPiT) - a\nsimple and effective extension to visual instruction tuning that enables a\npretrained LLM to quickly morph into an unified autoregressive model capable of\ngenerating both text and visual tokens. VPiT teaches an LLM to predict discrete\ntext tokens and continuous visual tokens from any input sequence of image and\ntext data curated in an instruction-following format. Our empirical\ninvestigation reveals several intriguing properties of VPiT: (1) visual\ngeneration ability emerges as a natural byproduct of improved visual\nunderstanding, and can be unlocked efficiently with a small amount of\ngeneration data; (2) while we find understanding and generation to be mutually\nbeneficial, understanding data contributes to both capabilities more\neffectively than generation data. Building upon these findings, we train our\nMetaMorph model and achieve competitive performance on both visual\nunderstanding and generation. In visual generation, MetaMorph can leverage the\nworld knowledge and reasoning abilities gained from LLM pretraining, and\novercome common failure modes exhibited by other generation models. Our results\nsuggest that LLMs may have strong \"prior\" vision capabilities that can be\nefficiently adapted to both visual understanding and generation with a\nrelatively simple instruction tuning process."
                },
                "authors": [
                    {
                        "name": "Shengbang Tong"
                    },
                    {
                        "name": "David Fan"
                    },
                    {
                        "name": "Jiachen Zhu"
                    },
                    {
                        "name": "Yunyang Xiong"
                    },
                    {
                        "name": "Xinlei Chen"
                    },
                    {
                        "name": "Koustuv Sinha"
                    },
                    {
                        "name": "Michael Rabbat"
                    },
                    {
                        "name": "Yann LeCun"
                    },
                    {
                        "name": "Saining Xie"
                    },
                    {
                        "name": "Zhuang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhuang Liu"
                },
                "author": "Zhuang Liu",
                "arxiv_comment": "Project page at tsb0601.github.io/metamorph",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14161v1",
                "updated": "2024-12-18T18:55:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    55,
                    40,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T18:55:40Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    55,
                    40,
                    2,
                    353,
                    0
                ],
                "title": "TheAgentCompany: Benchmarking LLM Agents on Consequential Real World\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TheAgentCompany: Benchmarking LLM Agents on Consequential Real World\n  Tasks"
                },
                "summary": "We interact with computers on an everyday basis, be it in everyday life or\nwork, and many aspects of work can be done entirely with access to a computer\nand the Internet. At the same time, thanks to improvements in large language\nmodels (LLMs), there has also been a rapid development in AI agents that\ninteract with and affect change in their surrounding environments. But how\nperformant are AI agents at helping to accelerate or even autonomously perform\nwork-related tasks? The answer to this question has important implications for\nboth industry looking to adopt AI into their workflows, and for economic policy\nto understand the effects that adoption of AI may have on the labor market. To\nmeasure the progress of these LLM agents' performance on performing real-world\nprofessional tasks, in this paper, we introduce TheAgentCompany, an extensible\nbenchmark for evaluating AI agents that interact with the world in similar ways\nto those of a digital worker: by browsing the Web, writing code, running\nprograms, and communicating with other coworkers. We build a self-contained\nenvironment with internal web sites and data that mimics a small software\ncompany environment, and create a variety of tasks that may be performed by\nworkers in such a company. We test baseline agents powered by both closed\nAPI-based and open-weights language models (LMs), and find that with the most\ncompetitive agent, 24% of the tasks can be completed autonomously. This paints\na nuanced picture on task automation with LM agents -- in a setting simulating\na real workplace, a good portion of simpler tasks could be solved autonomously,\nbut more difficult long-horizon tasks are still beyond the reach of current\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We interact with computers on an everyday basis, be it in everyday life or\nwork, and many aspects of work can be done entirely with access to a computer\nand the Internet. At the same time, thanks to improvements in large language\nmodels (LLMs), there has also been a rapid development in AI agents that\ninteract with and affect change in their surrounding environments. But how\nperformant are AI agents at helping to accelerate or even autonomously perform\nwork-related tasks? The answer to this question has important implications for\nboth industry looking to adopt AI into their workflows, and for economic policy\nto understand the effects that adoption of AI may have on the labor market. To\nmeasure the progress of these LLM agents' performance on performing real-world\nprofessional tasks, in this paper, we introduce TheAgentCompany, an extensible\nbenchmark for evaluating AI agents that interact with the world in similar ways\nto those of a digital worker: by browsing the Web, writing code, running\nprograms, and communicating with other coworkers. We build a self-contained\nenvironment with internal web sites and data that mimics a small software\ncompany environment, and create a variety of tasks that may be performed by\nworkers in such a company. We test baseline agents powered by both closed\nAPI-based and open-weights language models (LMs), and find that with the most\ncompetitive agent, 24% of the tasks can be completed autonomously. This paints\na nuanced picture on task automation with LM agents -- in a setting simulating\na real workplace, a good portion of simpler tasks could be solved autonomously,\nbut more difficult long-horizon tasks are still beyond the reach of current\nsystems."
                },
                "authors": [
                    {
                        "name": "Frank F. Xu"
                    },
                    {
                        "name": "Yufan Song"
                    },
                    {
                        "name": "Boxuan Li"
                    },
                    {
                        "name": "Yuxuan Tang"
                    },
                    {
                        "name": "Kritanjali Jain"
                    },
                    {
                        "name": "Mengxue Bao"
                    },
                    {
                        "name": "Zora Z. Wang"
                    },
                    {
                        "name": "Xuhui Zhou"
                    },
                    {
                        "name": "Zhitong Guo"
                    },
                    {
                        "name": "Murong Cao"
                    },
                    {
                        "name": "Mingyang Yang"
                    },
                    {
                        "name": "Hao Yang Lu"
                    },
                    {
                        "name": "Amaad Martin"
                    },
                    {
                        "name": "Zhe Su"
                    },
                    {
                        "name": "Leander Maben"
                    },
                    {
                        "name": "Raj Mehta"
                    },
                    {
                        "name": "Wayne Chi"
                    },
                    {
                        "name": "Lawrence Jang"
                    },
                    {
                        "name": "Yiqing Xie"
                    },
                    {
                        "name": "Shuyan Zhou"
                    },
                    {
                        "name": "Graham Neubig"
                    }
                ],
                "author_detail": {
                    "name": "Graham Neubig"
                },
                "author": "Graham Neubig",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14146v1",
                "updated": "2024-12-18T18:44:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    44,
                    8,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T18:44:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    44,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Advanced Reasoning and Transformation Engine for Multi-Step Insight\n  Synthesis in Data Analytics with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Reasoning and Transformation Engine for Multi-Step Insight\n  Synthesis in Data Analytics with Large Language Models"
                },
                "summary": "This paper presents the Advanced Reasoning and Transformation Engine for\nMulti-Step Insight Synthesis in Data Analytics (ARTEMIS-DA), a novel framework\ndesigned to augment Large Language Models (LLMs) for solving complex,\nmulti-step data analytics tasks. ARTEMIS-DA integrates three core components:\nthe Planner, which dissects complex user queries into structured, sequential\ninstructions encompassing data preprocessing, transformation, predictive\nmodeling, and visualization; the Coder, which dynamically generates and\nexecutes Python code to implement these instructions; and the Grapher, which\ninterprets generated visualizations to derive actionable insights. By\norchestrating the collaboration between these components, ARTEMIS-DA\neffectively manages sophisticated analytical workflows involving advanced\nreasoning, multi-step transformations, and synthesis across diverse data\nmodalities. The framework achieves state-of-the-art (SOTA) performance on\nbenchmarks such as WikiTableQuestions and TabFact, demonstrating its ability to\ntackle intricate analytical tasks with precision and adaptability. By combining\nthe reasoning capabilities of LLMs with automated code generation and execution\nand visual analysis, ARTEMIS-DA offers a robust, scalable solution for\nmulti-step insight synthesis, addressing a wide range of challenges in data\nanalytics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the Advanced Reasoning and Transformation Engine for\nMulti-Step Insight Synthesis in Data Analytics (ARTEMIS-DA), a novel framework\ndesigned to augment Large Language Models (LLMs) for solving complex,\nmulti-step data analytics tasks. ARTEMIS-DA integrates three core components:\nthe Planner, which dissects complex user queries into structured, sequential\ninstructions encompassing data preprocessing, transformation, predictive\nmodeling, and visualization; the Coder, which dynamically generates and\nexecutes Python code to implement these instructions; and the Grapher, which\ninterprets generated visualizations to derive actionable insights. By\norchestrating the collaboration between these components, ARTEMIS-DA\neffectively manages sophisticated analytical workflows involving advanced\nreasoning, multi-step transformations, and synthesis across diverse data\nmodalities. The framework achieves state-of-the-art (SOTA) performance on\nbenchmarks such as WikiTableQuestions and TabFact, demonstrating its ability to\ntackle intricate analytical tasks with precision and adaptability. By combining\nthe reasoning capabilities of LLMs with automated code generation and execution\nand visual analysis, ARTEMIS-DA offers a robust, scalable solution for\nmulti-step insight synthesis, addressing a wide range of challenges in data\nanalytics."
                },
                "authors": [
                    {
                        "name": "Atin Sakkeer Hussain"
                    }
                ],
                "author_detail": {
                    "name": "Atin Sakkeer Hussain"
                },
                "author": "Atin Sakkeer Hussain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.07019v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.07019v3",
                "updated": "2024-12-18T18:41:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    41,
                    41,
                    2,
                    353,
                    0
                ],
                "published": "2023-10-10T21:12:15Z",
                "published_parsed": [
                    2023,
                    10,
                    10,
                    21,
                    12,
                    15,
                    1,
                    283,
                    0
                ],
                "title": "Case Law Grounding: Using Precedents to Align Decision-Making for Humans\n  and AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Case Law Grounding: Using Precedents to Align Decision-Making for Humans\n  and AI"
                },
                "summary": "Communities and groups often need to make decisions grounded by social norms\nand preferences, such as when moderating content or providing judgments for\naligning AI systems. Prevailing approaches to provide this grounding have\nprimarily centered around constructing high-level guidelines and criteria,\nsimilar to legal ``constitutions''. However, it can be challenging to specify\nsocial norms and preferences consistently and accurately through constitutions\nalone. In this work, we take inspiration from legal systems and introduce\n``case law grounding'' (CLG) -- a novel approach for grounding decision-making\nthat uses past cases and decisions (precedents) to ground future decisions in a\nway that can be utilized by human-led processes or implemented through\nprompting large language models (LLMs). We evaluate how accurately CLG grounds\ndecisions with five groups and communities spread across two decision task\ndomains, comparing against a traditional constitutional grounding approach, and\nfind that in 4 out of 5 groups, decisions produced with CLG were significantly\nmore accurately aligned to ground truth: 16.0--23.3 %-points higher accuracy\nusing the human-led process, and 20.8--32.9 %-points higher when prompting\nLLMs. We also evaluate the impact of different configurations of CLG, such as\nthe case retrieval window size and whether to enforce binding decisions based\non selected precedents, showing support for using binding decisions and\npreferring larger retrieval windows. Finally, we discuss the limitations of our\ncase-based approach as well as how it may be best used to augment existing\nconstitutional approaches when it comes to aligning human and AI decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communities and groups often need to make decisions grounded by social norms\nand preferences, such as when moderating content or providing judgments for\naligning AI systems. Prevailing approaches to provide this grounding have\nprimarily centered around constructing high-level guidelines and criteria,\nsimilar to legal ``constitutions''. However, it can be challenging to specify\nsocial norms and preferences consistently and accurately through constitutions\nalone. In this work, we take inspiration from legal systems and introduce\n``case law grounding'' (CLG) -- a novel approach for grounding decision-making\nthat uses past cases and decisions (precedents) to ground future decisions in a\nway that can be utilized by human-led processes or implemented through\nprompting large language models (LLMs). We evaluate how accurately CLG grounds\ndecisions with five groups and communities spread across two decision task\ndomains, comparing against a traditional constitutional grounding approach, and\nfind that in 4 out of 5 groups, decisions produced with CLG were significantly\nmore accurately aligned to ground truth: 16.0--23.3 %-points higher accuracy\nusing the human-led process, and 20.8--32.9 %-points higher when prompting\nLLMs. We also evaluate the impact of different configurations of CLG, such as\nthe case retrieval window size and whether to enforce binding decisions based\non selected precedents, showing support for using binding decisions and\npreferring larger retrieval windows. Finally, we discuss the limitations of our\ncase-based approach as well as how it may be best used to augment existing\nconstitutional approaches when it comes to aligning human and AI decisions."
                },
                "authors": [
                    {
                        "name": "Quan Ze Chen"
                    },
                    {
                        "name": "Amy X. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Amy X. Zhang"
                },
                "author": "Amy X. Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.07019v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.07019v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14141v1",
                "updated": "2024-12-18T18:41:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    41,
                    14,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T18:41:14Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    41,
                    14,
                    2,
                    353,
                    0
                ],
                "title": "LLMs can realize combinatorial creativity: generating creative ideas via\n  LLMs for scientific research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs can realize combinatorial creativity: generating creative ideas via\n  LLMs for scientific research"
                },
                "summary": "Scientific idea generation has been extensively studied in creativity theory\nand computational creativity research, providing valuable frameworks for\nunderstanding and implementing creative processes. However, recent work using\nLarge Language Models (LLMs) for research idea generation often overlooks these\ntheoretical foundations. We present a framework that explicitly implements\ncombinatorial creativity theory using LLMs, featuring a generalization-level\nretrieval system for cross-domain knowledge discovery and a structured\ncombinatorial process for idea generation. The retrieval system maps concepts\nacross different abstraction levels to enable meaningful connections between\ndisparate domains, while the combinatorial process systematically analyzes and\nrecombines components to generate novel solutions. Experiments on the OAG-Bench\ndataset demonstrate our framework's effectiveness, consistently outperforming\nbaseline approaches in generating ideas that align with real research\ndevelopments (improving similarity scores by 7\\%-10\\% across multiple metrics).\nOur results provide strong evidence that LLMs can effectively realize\ncombinatorial creativity when guided by appropriate theoretical frameworks,\ncontributing both to practical advancement of AI-assisted research and\ntheoretical understanding of machine creativity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific idea generation has been extensively studied in creativity theory\nand computational creativity research, providing valuable frameworks for\nunderstanding and implementing creative processes. However, recent work using\nLarge Language Models (LLMs) for research idea generation often overlooks these\ntheoretical foundations. We present a framework that explicitly implements\ncombinatorial creativity theory using LLMs, featuring a generalization-level\nretrieval system for cross-domain knowledge discovery and a structured\ncombinatorial process for idea generation. The retrieval system maps concepts\nacross different abstraction levels to enable meaningful connections between\ndisparate domains, while the combinatorial process systematically analyzes and\nrecombines components to generate novel solutions. Experiments on the OAG-Bench\ndataset demonstrate our framework's effectiveness, consistently outperforming\nbaseline approaches in generating ideas that align with real research\ndevelopments (improving similarity scores by 7\\%-10\\% across multiple metrics).\nOur results provide strong evidence that LLMs can effectively realize\ncombinatorial creativity when guided by appropriate theoretical frameworks,\ncontributing both to practical advancement of AI-assisted research and\ntheoretical understanding of machine creativity."
                },
                "authors": [
                    {
                        "name": "Tianyang Gu"
                    },
                    {
                        "name": "Jingjin Wang"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "HaoHong Li"
                    }
                ],
                "author_detail": {
                    "name": "HaoHong Li"
                },
                "author": "HaoHong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14140v1",
                "updated": "2024-12-18T18:41:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    41,
                    12,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T18:41:12Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    41,
                    12,
                    2,
                    353,
                    0
                ],
                "title": "GLIDER: Grading LLM Interactions and Decisions using Explainable Ranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLIDER: Grading LLM Interactions and Decisions using Explainable Ranking"
                },
                "summary": "The LLM-as-judge paradigm is increasingly being adopted for automated\nevaluation of model outputs. While LLM judges have shown promise on constrained\nevaluation tasks, closed source LLMs display critical shortcomings when\ndeployed in real world applications due to challenges of fine grained metrics\nand explainability, while task specific evaluation models lack cross-domain\ngeneralization. We introduce GLIDER, a powerful 3B evaluator LLM that can score\nany text input and associated context on arbitrary user defined criteria.\nGLIDER shows higher Pearson's correlation than GPT-4o on FLASK and greatly\noutperforms prior evaluation models, achieving comparable performance to LLMs\n17x its size. GLIDER supports fine-grained scoring, multilingual reasoning,\nspan highlighting and was trained on 685 domains and 183 criteria. Extensive\nqualitative analysis shows that GLIDER scores are highly correlated with human\njudgments, with 91.3% human agreement. We have open-sourced GLIDER to\nfacilitate future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The LLM-as-judge paradigm is increasingly being adopted for automated\nevaluation of model outputs. While LLM judges have shown promise on constrained\nevaluation tasks, closed source LLMs display critical shortcomings when\ndeployed in real world applications due to challenges of fine grained metrics\nand explainability, while task specific evaluation models lack cross-domain\ngeneralization. We introduce GLIDER, a powerful 3B evaluator LLM that can score\nany text input and associated context on arbitrary user defined criteria.\nGLIDER shows higher Pearson's correlation than GPT-4o on FLASK and greatly\noutperforms prior evaluation models, achieving comparable performance to LLMs\n17x its size. GLIDER supports fine-grained scoring, multilingual reasoning,\nspan highlighting and was trained on 685 domains and 183 criteria. Extensive\nqualitative analysis shows that GLIDER scores are highly correlated with human\njudgments, with 91.3% human agreement. We have open-sourced GLIDER to\nfacilitate future research."
                },
                "authors": [
                    {
                        "name": "Darshan Deshpande"
                    },
                    {
                        "name": "Selvan Sunitha Ravi"
                    },
                    {
                        "name": "Sky CH-Wang"
                    },
                    {
                        "name": "Bartosz Mielczarek"
                    },
                    {
                        "name": "Anand Kannappan"
                    },
                    {
                        "name": "Rebecca Qian"
                    }
                ],
                "author_detail": {
                    "name": "Rebecca Qian"
                },
                "author": "Rebecca Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14137v1",
                "updated": "2024-12-18T18:33:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    33,
                    26,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T18:33:26Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    33,
                    26,
                    2,
                    353,
                    0
                ],
                "title": "Design choices made by LLM-based test generators prevent them from\n  finding bugs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design choices made by LLM-based test generators prevent them from\n  finding bugs"
                },
                "summary": "There is an increasing amount of research and commercial tools for automated\ntest case generation using Large Language Models (LLMs). This paper critically\nexamines whether recent LLM-based test generation tools, such as Codium\nCoverAgent and CoverUp, can effectively find bugs or unintentionally validate\nfaulty code. Considering bugs are only exposed by failing test cases, we\nexplore the question: can these tools truly achieve the intended objectives of\nsoftware testing when their test oracles are designed to pass? Using real\nhuman-written buggy code as input, we evaluate these tools, showing how\nLLM-generated tests can fail to detect bugs and, more alarmingly, how their\ndesign can worsen the situation by validating bugs in the generated test suite\nand rejecting bug-revealing tests. These findings raise important questions\nabout the validity of the design behind LLM-based test generation tools and\ntheir impact on software quality and test suite reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is an increasing amount of research and commercial tools for automated\ntest case generation using Large Language Models (LLMs). This paper critically\nexamines whether recent LLM-based test generation tools, such as Codium\nCoverAgent and CoverUp, can effectively find bugs or unintentionally validate\nfaulty code. Considering bugs are only exposed by failing test cases, we\nexplore the question: can these tools truly achieve the intended objectives of\nsoftware testing when their test oracles are designed to pass? Using real\nhuman-written buggy code as input, we evaluate these tools, showing how\nLLM-generated tests can fail to detect bugs and, more alarmingly, how their\ndesign can worsen the situation by validating bugs in the generated test suite\nand rejecting bug-revealing tests. These findings raise important questions\nabout the validity of the design behind LLM-based test generation tools and\ntheir impact on software quality and test suite reliability."
                },
                "authors": [
                    {
                        "name": "Noble Saji Mathews"
                    },
                    {
                        "name": "Meiyappan Nagappan"
                    }
                ],
                "author_detail": {
                    "name": "Meiyappan Nagappan"
                },
                "author": "Meiyappan Nagappan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14135v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14135v1",
                "updated": "2024-12-18T18:24:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    24,
                    47,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T18:24:47Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    24,
                    47,
                    2,
                    353,
                    0
                ],
                "title": "Scaling of Search and Learning: A Roadmap to Reproduce o1 from\n  Reinforcement Learning Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling of Search and Learning: A Roadmap to Reproduce o1 from\n  Reinforcement Learning Perspective"
                },
                "summary": "OpenAI o1 represents a significant milestone in Artificial Inteiligence,\nwhich achieves expert-level performances on many challanging tasks that require\nstrong reasoning ability.OpenAI has claimed that the main techinique behinds o1\nis the reinforcement learining. Recent works use alternative approaches like\nknowledge distillation to imitate o1's reasoning style, but their effectiveness\nis limited by the capability ceiling of the teacher model. Therefore, this\npaper analyzes the roadmap to achieving o1 from the perspective of\nreinforcement learning, focusing on four key components: policy initialization,\nreward design, search, and learning. Policy initialization enables models to\ndevelop human-like reasoning behaviors, equipping them with the ability to\neffectively explore solution spaces for complex problems. Reward design\nprovides dense and effective signals via reward shaping or reward modeling,\nwhich is the guidance for both search and learning. Search plays a crucial role\nin generating high-quality solutions during both training and testing phases,\nwhich can produce better solutions with more computation. Learning utilizes the\ndata generated by search for improving policy, which can achieve the better\nperformance with more parameters and more searched data. Existing open-source\nprojects that attempt to reproduce o1 can be seem as a part or a variant of our\nroadmap. Collectively, these components underscore how learning and search\ndrive o1's advancement, making meaningful contributions to the development of\nLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenAI o1 represents a significant milestone in Artificial Inteiligence,\nwhich achieves expert-level performances on many challanging tasks that require\nstrong reasoning ability.OpenAI has claimed that the main techinique behinds o1\nis the reinforcement learining. Recent works use alternative approaches like\nknowledge distillation to imitate o1's reasoning style, but their effectiveness\nis limited by the capability ceiling of the teacher model. Therefore, this\npaper analyzes the roadmap to achieving o1 from the perspective of\nreinforcement learning, focusing on four key components: policy initialization,\nreward design, search, and learning. Policy initialization enables models to\ndevelop human-like reasoning behaviors, equipping them with the ability to\neffectively explore solution spaces for complex problems. Reward design\nprovides dense and effective signals via reward shaping or reward modeling,\nwhich is the guidance for both search and learning. Search plays a crucial role\nin generating high-quality solutions during both training and testing phases,\nwhich can produce better solutions with more computation. Learning utilizes the\ndata generated by search for improving policy, which can achieve the better\nperformance with more parameters and more searched data. Existing open-source\nprojects that attempt to reproduce o1 can be seem as a part or a variant of our\nroadmap. Collectively, these components underscore how learning and search\ndrive o1's advancement, making meaningful contributions to the development of\nLLM."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Zeng"
                    },
                    {
                        "name": "Qinyuan Cheng"
                    },
                    {
                        "name": "Zhangyue Yin"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Shimin Li"
                    },
                    {
                        "name": "Yunhua Zhou"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14135v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14135v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11780v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11780v2",
                "updated": "2024-12-18T18:21:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    21,
                    53,
                    2,
                    353,
                    0
                ],
                "published": "2024-07-16T14:37:33Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    14,
                    37,
                    33,
                    1,
                    198,
                    0
                ],
                "title": "SwitchCIT: Switching for Continual Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwitchCIT: Switching for Continual Instruction Tuning"
                },
                "summary": "Large language models (LLMs) and multimodal models (MMs) have exhibited\nimpressive capabilities in various domains, particularly in general language\nunderstanding and visual reasoning. However, these models, trained on massive\ndata, may not be finely optimized for specific tasks triggered by instructions.\nContinual instruction tuning is crucial to adapt a large model to evolving\ntasks and domains, ensuring their effectiveness and relevance across a wide\nrange of applications. In the context of continual instruction tuning, where\nmodels are sequentially trained on different tasks, catastrophic forgetting can\noccur, leading to performance degradation on previously learned tasks. This\nwork addresses the catastrophic forgetting in continual instruction learning\nthrough a switching mechanism for routing computations to parameter-efficient\ntuned models. We demonstrate the effectiveness of our method through\nexperiments on continual instruction tuning of different natural language\ngeneration tasks and vision-language tasks. We also showcase the advantages of\nour proposed method in terms of efficiency, scalability, portability, and\nprivacy preservation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) and multimodal models (MMs) have exhibited\nimpressive capabilities in various domains, particularly in general language\nunderstanding and visual reasoning. However, these models, trained on massive\ndata, may not be finely optimized for specific tasks triggered by instructions.\nContinual instruction tuning is crucial to adapt a large model to evolving\ntasks and domains, ensuring their effectiveness and relevance across a wide\nrange of applications. In the context of continual instruction tuning, where\nmodels are sequentially trained on different tasks, catastrophic forgetting can\noccur, leading to performance degradation on previously learned tasks. This\nwork addresses the catastrophic forgetting in continual instruction learning\nthrough a switching mechanism for routing computations to parameter-efficient\ntuned models. We demonstrate the effectiveness of our method through\nexperiments on continual instruction tuning of different natural language\ngeneration tasks and vision-language tasks. We also showcase the advantages of\nour proposed method in terms of efficiency, scalability, portability, and\nprivacy preservation."
                },
                "authors": [
                    {
                        "name": "Xinbo Wu"
                    },
                    {
                        "name": "Max Hartman"
                    },
                    {
                        "name": "Vidhata Arjun Jayaraman"
                    },
                    {
                        "name": "Lav R. Varshney"
                    }
                ],
                "author_detail": {
                    "name": "Lav R. Varshney"
                },
                "author": "Lav R. Varshney",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11780v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11780v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14119v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14119v1",
                "updated": "2024-12-18T18:06:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    6,
                    58,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T18:06:58Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    6,
                    58,
                    2,
                    353,
                    0
                ],
                "title": "Learning and Reconstructing Conflicts in O-RAN: A Graph Neural Network\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning and Reconstructing Conflicts in O-RAN: A Graph Neural Network\n  Approach"
                },
                "summary": "The Open Radio Access Network (O-RAN) architecture enables the deployment of\nthird-party applications on the RAN Intelligent Controllers (RICs) to provide\nMobile Network Operators (MNOs) with different functionality. However, the\noperation of third-party applications in the Near Real-Time RIC (Near-RT RIC),\nknown as xApps, can result in conflicting interactions. Each xApp can\nindependently modify the same control parameters to achieve distinct outcomes,\nwhich has the potential to cause performance degradation and network\ninstability. The current conflict detection and mitigation solutions in the\nliterature assume that all conflicts are known a priori, which does not always\nhold due to complex and often hidden relationships between control parameters\nand Key Performance Indicators (KPIs). In this paper, we introduce a novel\ndata-driven Graph Neural Network (GNN)-based method for reconstructing conflict\ngraphs. Specifically, we leverage GraphSAGE, an inductive learning framework,\nto dynamically learn the hidden relationships between xApps, control\nparameters, and KPIs. Our experimental results validate our proposed method for\nreconstructing conflict graphs and identifying all types of conflicts in O-RAN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Open Radio Access Network (O-RAN) architecture enables the deployment of\nthird-party applications on the RAN Intelligent Controllers (RICs) to provide\nMobile Network Operators (MNOs) with different functionality. However, the\noperation of third-party applications in the Near Real-Time RIC (Near-RT RIC),\nknown as xApps, can result in conflicting interactions. Each xApp can\nindependently modify the same control parameters to achieve distinct outcomes,\nwhich has the potential to cause performance degradation and network\ninstability. The current conflict detection and mitigation solutions in the\nliterature assume that all conflicts are known a priori, which does not always\nhold due to complex and often hidden relationships between control parameters\nand Key Performance Indicators (KPIs). In this paper, we introduce a novel\ndata-driven Graph Neural Network (GNN)-based method for reconstructing conflict\ngraphs. Specifically, we leverage GraphSAGE, an inductive learning framework,\nto dynamically learn the hidden relationships between xApps, control\nparameters, and KPIs. Our experimental results validate our proposed method for\nreconstructing conflict graphs and identifying all types of conflicts in O-RAN."
                },
                "authors": [
                    {
                        "name": "Arshia Zolghadr"
                    },
                    {
                        "name": "Joao F. Santos"
                    },
                    {
                        "name": "Luiz A. DaSilva"
                    },
                    {
                        "name": "Jacek Kibida"
                    }
                ],
                "author_detail": {
                    "name": "Jacek Kibida"
                },
                "author": "Jacek Kibida",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14119v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14119v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07675v2",
                "updated": "2024-12-18T17:54:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    54,
                    30,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-10T17:02:58Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    2,
                    58,
                    1,
                    345,
                    0
                ],
                "title": "RAZOR: Sharpening Knowledge by Cutting Bias with Unsupervised Text\n  Rewriting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAZOR: Sharpening Knowledge by Cutting Bias with Unsupervised Text\n  Rewriting"
                },
                "summary": "Despite the widespread use of LLMs due to their superior performance in\nvarious tasks, their high computational costs often lead potential users to opt\nfor the pretraining-finetuning pipeline. However, biases prevalent in manually\nconstructed datasets can introduce spurious correlations between tokens and\nlabels, creating so-called shortcuts and hindering the generalizability of\nfine-tuned models. Existing debiasing methods often rely on prior knowledge of\nspecific dataset biases, which is challenging to acquire a priori. We propose\nRAZOR (Rewriting And Zero-bias Optimization Refinement), a novel, unsupervised,\nand data-focused debiasing approach based on text rewriting for shortcut\nmitigation. RAZOR leverages LLMs to iteratively rewrite potentially biased text\nsegments by replacing them with heuristically selected alternatives in a\nshortcut space defined by token statistics and positional information. This\nprocess aims to align surface-level text features more closely with diverse\nlabel distributions, thereby promoting the learning of genuine linguistic\npatterns. Compared with unsupervised SoTA models, RAZOR improves by 3.5% on the\nFEVER and 6.5% on MNLI and SNLI datasets according to the F1 score.\nAdditionally, RAZOR effectively mitigates specific known biases, reducing\nbias-related terms by x2 without requiring prior bias information, a result\nthat is on par with SoTA models that leverage prior information. Our work\nprioritizes data manipulation over architectural modifications, emphasizing the\npivotal role of data quality in enhancing model performance and fairness. This\nresearch contributes to developing more robust evaluation benchmarks for\ndebiasing methods by incorporating metrics for bias reduction and overall model\nefficacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the widespread use of LLMs due to their superior performance in\nvarious tasks, their high computational costs often lead potential users to opt\nfor the pretraining-finetuning pipeline. However, biases prevalent in manually\nconstructed datasets can introduce spurious correlations between tokens and\nlabels, creating so-called shortcuts and hindering the generalizability of\nfine-tuned models. Existing debiasing methods often rely on prior knowledge of\nspecific dataset biases, which is challenging to acquire a priori. We propose\nRAZOR (Rewriting And Zero-bias Optimization Refinement), a novel, unsupervised,\nand data-focused debiasing approach based on text rewriting for shortcut\nmitigation. RAZOR leverages LLMs to iteratively rewrite potentially biased text\nsegments by replacing them with heuristically selected alternatives in a\nshortcut space defined by token statistics and positional information. This\nprocess aims to align surface-level text features more closely with diverse\nlabel distributions, thereby promoting the learning of genuine linguistic\npatterns. Compared with unsupervised SoTA models, RAZOR improves by 3.5% on the\nFEVER and 6.5% on MNLI and SNLI datasets according to the F1 score.\nAdditionally, RAZOR effectively mitigates specific known biases, reducing\nbias-related terms by x2 without requiring prior bias information, a result\nthat is on par with SoTA models that leverage prior information. Our work\nprioritizes data manipulation over architectural modifications, emphasizing the\npivotal role of data quality in enhancing model performance and fairness. This\nresearch contributes to developing more robust evaluation benchmarks for\ndebiasing methods by incorporating metrics for bias reduction and overall model\nefficacy."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Bardh Prenkaj"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Gjergji Kasneci"
                },
                "author": "Gjergji Kasneci",
                "arxiv_comment": "Shuo and Bardh contributed equally. Accepted to AAAI'25, Paper #17117",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17284v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17284v3",
                "updated": "2024-12-18T17:51:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    51,
                    52,
                    2,
                    353,
                    0
                ],
                "published": "2024-11-26T10:13:39Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    10,
                    13,
                    39,
                    1,
                    331,
                    0
                ],
                "title": "Using Large Language Models for Expert Prior Elicitation in Predictive\n  Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Large Language Models for Expert Prior Elicitation in Predictive\n  Modelling"
                },
                "summary": "Large language models (LLMs), trained on diverse data effectively acquire a\nbreadth of information across various domains. However, their computational\ncomplexity, cost, and lack of transparency hinder their direct application for\nspecialised tasks. In fields such as clinical research, acquiring expert\nannotations or prior knowledge about predictive models is often costly and\ntime-consuming. This study proposes the use of LLMs to elicit expert prior\ndistributions for predictive models. This approach also provides an alternative\nto in-context learning, where language models are tasked with making\npredictions directly. In this work, we compare LLM-elicited and uninformative\npriors, evaluate whether LLMs truthfully generate parameter distributions, and\npropose a model selection strategy for in-context learning and prior\nelicitation. Our findings show that LLM-elicited prior parameter distributions\nsignificantly reduce predictive error compared to uninformative priors in\nlow-data settings. Applied to clinical problems, this translates to fewer\nrequired biological samples, lowering cost and resources. Prior elicitation\nalso consistently outperforms and proves more reliable than in-context learning\nat a lower cost, making it a preferred alternative in our setting. We\ndemonstrate the utility of this method across various use cases, including\nclinical applications. For infection prediction, using LLM-elicited priors\nreduced the number of required labels to achieve the same accuracy as an\nuninformative prior by 55%, 200 days earlier in the study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), trained on diverse data effectively acquire a\nbreadth of information across various domains. However, their computational\ncomplexity, cost, and lack of transparency hinder their direct application for\nspecialised tasks. In fields such as clinical research, acquiring expert\nannotations or prior knowledge about predictive models is often costly and\ntime-consuming. This study proposes the use of LLMs to elicit expert prior\ndistributions for predictive models. This approach also provides an alternative\nto in-context learning, where language models are tasked with making\npredictions directly. In this work, we compare LLM-elicited and uninformative\npriors, evaluate whether LLMs truthfully generate parameter distributions, and\npropose a model selection strategy for in-context learning and prior\nelicitation. Our findings show that LLM-elicited prior parameter distributions\nsignificantly reduce predictive error compared to uninformative priors in\nlow-data settings. Applied to clinical problems, this translates to fewer\nrequired biological samples, lowering cost and resources. Prior elicitation\nalso consistently outperforms and proves more reliable than in-context learning\nat a lower cost, making it a preferred alternative in our setting. We\ndemonstrate the utility of this method across various use cases, including\nclinical applications. For infection prediction, using LLM-elicited priors\nreduced the number of required labels to achieve the same accuracy as an\nuninformative prior by 55%, 200 days earlier in the study."
                },
                "authors": [
                    {
                        "name": "Alexander Capstick"
                    },
                    {
                        "name": "Rahul G. Krishnan"
                    },
                    {
                        "name": "Payam Barnaghi"
                    }
                ],
                "author_detail": {
                    "name": "Payam Barnaghi"
                },
                "author": "Payam Barnaghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17284v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17284v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14097v1",
                "updated": "2024-12-18T17:47:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    47,
                    46,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T17:47:46Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    47,
                    46,
                    2,
                    353,
                    0
                ],
                "title": "Adaptive Concept Bottleneck for Foundation Models Under Distribution\n  Shifts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Concept Bottleneck for Foundation Models Under Distribution\n  Shifts"
                },
                "summary": "Advancements in foundation models (FMs) have led to a paradigm shift in\nmachine learning. The rich, expressive feature representations from these\npre-trained, large-scale FMs are leveraged for multiple downstream tasks,\nusually via lightweight fine-tuning of a shallow fully-connected network\nfollowing the representation. However, the non-interpretable, black-box nature\nof this prediction pipeline can be a challenge, especially in critical domains\nsuch as healthcare, finance, and security. In this paper, we explore the\npotential of Concept Bottleneck Models (CBMs) for transforming complex,\nnon-interpretable foundation models into interpretable decision-making\npipelines using high-level concept vectors. Specifically, we focus on the\ntest-time deployment of such an interpretable CBM pipeline \"in the wild\", where\nthe input distribution often shifts from the original training distribution. We\nfirst identify the potential failure modes of such a pipeline under different\ntypes of distribution shifts. Then we propose an adaptive concept bottleneck\nframework to address these failure modes, that dynamically adapts the\nconcept-vector bank and the prediction layer based solely on unlabeled data\nfrom the target domain, without access to the source (training) dataset.\nEmpirical evaluations with various real-world distribution shifts show that our\nadaptation method produces concept-based interpretations better aligned with\nthe test data and boosts post-deployment accuracy by up to 28%, aligning the\nCBM performance with that of non-interpretable classification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in foundation models (FMs) have led to a paradigm shift in\nmachine learning. The rich, expressive feature representations from these\npre-trained, large-scale FMs are leveraged for multiple downstream tasks,\nusually via lightweight fine-tuning of a shallow fully-connected network\nfollowing the representation. However, the non-interpretable, black-box nature\nof this prediction pipeline can be a challenge, especially in critical domains\nsuch as healthcare, finance, and security. In this paper, we explore the\npotential of Concept Bottleneck Models (CBMs) for transforming complex,\nnon-interpretable foundation models into interpretable decision-making\npipelines using high-level concept vectors. Specifically, we focus on the\ntest-time deployment of such an interpretable CBM pipeline \"in the wild\", where\nthe input distribution often shifts from the original training distribution. We\nfirst identify the potential failure modes of such a pipeline under different\ntypes of distribution shifts. Then we propose an adaptive concept bottleneck\nframework to address these failure modes, that dynamically adapts the\nconcept-vector bank and the prediction layer based solely on unlabeled data\nfrom the target domain, without access to the source (training) dataset.\nEmpirical evaluations with various real-world distribution shifts show that our\nadaptation method produces concept-based interpretations better aligned with\nthe test data and boosts post-deployment accuracy by up to 28%, aligning the\nCBM performance with that of non-interpretable classification."
                },
                "authors": [
                    {
                        "name": "Jihye Choi"
                    },
                    {
                        "name": "Jayaram Raghuram"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Somesh Jha"
                    }
                ],
                "author_detail": {
                    "name": "Somesh Jha"
                },
                "author": "Somesh Jha",
                "arxiv_comment": "The preliminary version of the work appeared in the ICML 2024\n  Workshop on Foundation Models in the Wild",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v4",
                "updated": "2024-12-18T17:36:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    36,
                    36,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06937v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06937v2",
                "updated": "2024-12-18T17:31:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    31,
                    39,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-09T19:26:38Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    19,
                    26,
                    38,
                    0,
                    344,
                    0
                ],
                "title": "High-Resolution Rooftop-PV Potential Assessment for a Resilient Energy\n  System in Ukraine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Resolution Rooftop-PV Potential Assessment for a Resilient Energy\n  System in Ukraine"
                },
                "summary": "Rooftop photovoltaic (RTPV) systems are essential for building a decarbonized\nand, due to its decentralized structure, more resilient energy system, and are\nparticularly important for Ukraine, where recent conflicts have damaged more\nthan half of its electricity and heat supply capacity. Favorable solar\nirradiation conditions make Ukraine a strong candidate for large-scale PV\ndeployment, but effective policy requires detailed data on spatial and temporal\ngeneration potential. This study fills the data gap by using open-source\nsatellite building footprint data corrected with high-resolution data from\neastern Germany. This approach allowed accurate estimates of rooftop area and\nPV capacity and generation across Ukraine, with simulations revealing a\ncapacity potential of 238.8 GW and a generation potential of 290 TWh/a\nexcluding north-facing. The majority of this potential is located in oblasts\n(provinces) across the country with large cities such as Donetsk, Dnipro or\nKyiv and surroundings. These results, validated against previous studies and\navailable as open data, confirm Ukraine's significant potential for RTPV,\nsupporting both energy resilience and climate goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rooftop photovoltaic (RTPV) systems are essential for building a decarbonized\nand, due to its decentralized structure, more resilient energy system, and are\nparticularly important for Ukraine, where recent conflicts have damaged more\nthan half of its electricity and heat supply capacity. Favorable solar\nirradiation conditions make Ukraine a strong candidate for large-scale PV\ndeployment, but effective policy requires detailed data on spatial and temporal\ngeneration potential. This study fills the data gap by using open-source\nsatellite building footprint data corrected with high-resolution data from\neastern Germany. This approach allowed accurate estimates of rooftop area and\nPV capacity and generation across Ukraine, with simulations revealing a\ncapacity potential of 238.8 GW and a generation potential of 290 TWh/a\nexcluding north-facing. The majority of this potential is located in oblasts\n(provinces) across the country with large cities such as Donetsk, Dnipro or\nKyiv and surroundings. These results, validated against previous studies and\navailable as open data, confirm Ukraine's significant potential for RTPV,\nsupporting both energy resilience and climate goals."
                },
                "authors": [
                    {
                        "name": "Christoph Winkler"
                    },
                    {
                        "name": "Kristina Dabrock"
                    },
                    {
                        "name": "Serhiy Kapustyan"
                    },
                    {
                        "name": "Craig Hart"
                    },
                    {
                        "name": "Heidi Heinrichs"
                    },
                    {
                        "name": "Jann Michael Weinand"
                    },
                    {
                        "name": "Jochen Linen"
                    },
                    {
                        "name": "Detlef Stolten"
                    }
                ],
                "author_detail": {
                    "name": "Detlef Stolten"
                },
                "arxiv_affiliation": "RWTH Aachen University, Chair for Fuel Cells, Faculty of Mechanical Engineering, Aachen, Germany",
                "author": "Detlef Stolten",
                "arxiv_comment": "Christoph Winkler, Kristina Dabrock, and Serhiy Kapustyan contributed\n  equally to this work. Main article: 18 pages, 8 figures; supplementary\n  material: 13 pages 5 figures. This version includes minor updates: the data\n  availability section has been revised with a new URL, and small corrections\n  have been made to the validation section",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06937v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06937v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07498v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07498v2",
                "updated": "2024-12-18T17:17:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    17,
                    9,
                    2,
                    353,
                    0
                ],
                "published": "2024-11-12T02:54:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    2,
                    54,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Semantic Sleuth: Identifying Ponzi Contracts via Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Sleuth: Identifying Ponzi Contracts via Large Language Models"
                },
                "summary": "Smart contracts, self-executing agreements directly encoded in code, are\nfundamental to blockchain technology, especially in decentralized finance\n(DeFi) and Web3. However, the rise of Ponzi schemes in smart contracts poses\nsignificant risks, leading to substantial financial losses and eroding trust in\nblockchain systems. Existing detection methods, such as PonziGuard, depend on\nlarge amounts of labeled data and struggle to identify unseen Ponzi schemes,\nlimiting their reliability and generalizability. In contrast, we introduce\nPonziSleuth, the first LLM-driven approach for detecting Ponzi smart contracts,\nwhich requires no labeled training data. PonziSleuth utilizes advanced language\nunderstanding capabilities of LLMs to analyze smart contract source code\nthrough a novel two-step zero-shot chain-of-thought prompting technique. Our\nextensive evaluation on benchmark datasets and real-world contracts\ndemonstrates that PonziSleuth delivers comparable, and often superior,\nperformance without the extensive data requirements, achieving a balanced\ndetection accuracy of 96.06% with GPT-3.5-turbo, 93.91% with LLAMA3, and 94.27%\nwith Mistral. In real-world detection, PonziSleuth successfully identified 15\nnew Ponzi schemes from 4,597 contracts verified by Etherscan in March 2024,\nwith a false negative rate of 0% and a false positive rate of 0.29%. These\nresults highlight PonziSleuth's capability to detect diverse and novel Ponzi\nschemes, marking a significant advancement in leveraging LLMs for enhancing\nblockchain security and mitigating financial scams.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart contracts, self-executing agreements directly encoded in code, are\nfundamental to blockchain technology, especially in decentralized finance\n(DeFi) and Web3. However, the rise of Ponzi schemes in smart contracts poses\nsignificant risks, leading to substantial financial losses and eroding trust in\nblockchain systems. Existing detection methods, such as PonziGuard, depend on\nlarge amounts of labeled data and struggle to identify unseen Ponzi schemes,\nlimiting their reliability and generalizability. In contrast, we introduce\nPonziSleuth, the first LLM-driven approach for detecting Ponzi smart contracts,\nwhich requires no labeled training data. PonziSleuth utilizes advanced language\nunderstanding capabilities of LLMs to analyze smart contract source code\nthrough a novel two-step zero-shot chain-of-thought prompting technique. Our\nextensive evaluation on benchmark datasets and real-world contracts\ndemonstrates that PonziSleuth delivers comparable, and often superior,\nperformance without the extensive data requirements, achieving a balanced\ndetection accuracy of 96.06% with GPT-3.5-turbo, 93.91% with LLAMA3, and 94.27%\nwith Mistral. In real-world detection, PonziSleuth successfully identified 15\nnew Ponzi schemes from 4,597 contracts verified by Etherscan in March 2024,\nwith a false negative rate of 0% and a false positive rate of 0.29%. These\nresults highlight PonziSleuth's capability to detect diverse and novel Ponzi\nschemes, marking a significant advancement in leveraging LLMs for enhancing\nblockchain security and mitigating financial scams."
                },
                "authors": [
                    {
                        "name": "Cong Wu"
                    },
                    {
                        "name": "Jing Chen"
                    },
                    {
                        "name": "Ziwei Wang"
                    },
                    {
                        "name": "Ruichao Liang"
                    },
                    {
                        "name": "Ruiying Du"
                    }
                ],
                "author_detail": {
                    "name": "Ruiying Du"
                },
                "author": "Ruiying Du",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07498v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07498v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22159v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22159v3",
                "updated": "2024-12-18T17:09:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    9,
                    46,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-29T15:54:09Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    54,
                    9,
                    1,
                    303,
                    0
                ],
                "title": "Training LLMs for Generating IEC 61131-3 Structured Text with Online\n  Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training LLMs for Generating IEC 61131-3 Structured Text with Online\n  Feedback"
                },
                "summary": "IEC 61131-3 Structured Text (ST) is a widely used programming language for\nprogrammable logic controllers (PLCs) in automation systems. However,\ngenerating ST code with LLMs poses unique challenges due to limited data in\npublic training datasets and the complexity of ST language syntax. This paper\nproposes an approach to fine-tune LLMs for the generation of ST code that\nleverages a preference-based learning method through an online process\ninvolving compiler feedback and evaluation from an LLM-based ST expert. In this\nframework, the model is iteratively refined and generates new training samples,\nwhich are subsequently evaluated by a compiler for syntactical correctness and\nby a specialized LLM that excels at assessing semantic accuracy, though it is\nnot optimized for code generation itself. This approach results in marked\nimprovements for the trained LLM, leading to higher compilation success rates\nand better semantic precision. As a result, the framework proves highly\nsuitable for industrial automation applications and outperforms\nstate-of-the-art models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IEC 61131-3 Structured Text (ST) is a widely used programming language for\nprogrammable logic controllers (PLCs) in automation systems. However,\ngenerating ST code with LLMs poses unique challenges due to limited data in\npublic training datasets and the complexity of ST language syntax. This paper\nproposes an approach to fine-tune LLMs for the generation of ST code that\nleverages a preference-based learning method through an online process\ninvolving compiler feedback and evaluation from an LLM-based ST expert. In this\nframework, the model is iteratively refined and generates new training samples,\nwhich are subsequently evaluated by a compiler for syntactical correctness and\nby a specialized LLM that excels at assessing semantic accuracy, though it is\nnot optimized for code generation itself. This approach results in marked\nimprovements for the trained LLM, leading to higher compilation success rates\nand better semantic precision. As a result, the framework proves highly\nsuitable for industrial automation applications and outperforms\nstate-of-the-art models."
                },
                "authors": [
                    {
                        "name": "Aaron Haag"
                    },
                    {
                        "name": "Bertram Fuchs"
                    },
                    {
                        "name": "Altay Kacan"
                    },
                    {
                        "name": "Oliver Lohse"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Lohse"
                },
                "author": "Oliver Lohse",
                "arxiv_comment": "Accepted at LLM4Code Workshop @ ICSE 2025. This work has been\n  submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22159v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22159v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01406v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01406v3",
                "updated": "2024-12-18T17:09:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    9,
                    31,
                    2,
                    353,
                    0
                ],
                "published": "2024-07-01T15:56:24Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    15,
                    56,
                    24,
                    0,
                    183,
                    0
                ],
                "title": "Adapting Multilingual LLMs to Low-Resource Languages with Knowledge\n  Graphs via Adapters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Multilingual LLMs to Low-Resource Languages with Knowledge\n  Graphs via Adapters"
                },
                "summary": "This paper explores the integration of graph knowledge from linguistic\nontologies into multilingual Large Language Models (LLMs) using adapters to\nimprove performance for low-resource languages (LRLs) in sentiment analysis\n(SA) and named entity recognition (NER). Building upon successful\nparameter-efficient fine-tuning techniques, such as K-ADAPTER and MAD-X, we\npropose a similar approach for incorporating knowledge from multilingual\ngraphs, connecting concepts in various languages with each other through\nlinguistic relationships, into multilingual LLMs for LRLs. Specifically, we\nfocus on eight LRLs -- Maltese, Bulgarian, Indonesian, Nepali, Javanese,\nUyghur, Tibetan, and Sinhala -- and employ language-specific adapters\nfine-tuned on data extracted from the language-specific section of ConceptNet,\naiming to enable knowledge transfer across the languages covered by the\nknowledge graph. We compare various fine-tuning objectives, including standard\nMasked Language Modeling (MLM), MLM with full-word masking, and MLM with\ntargeted masking, to analyse their effectiveness in learning and integrating\nthe extracted graph data. Through empirical evaluation on language-specific\ntasks, we assess how structured graph knowledge affects the performance of\nmultilingual LLMs for LRLs in SA and NER, providing insights into the potential\nbenefits of adapting language models for low-resource scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the integration of graph knowledge from linguistic\nontologies into multilingual Large Language Models (LLMs) using adapters to\nimprove performance for low-resource languages (LRLs) in sentiment analysis\n(SA) and named entity recognition (NER). Building upon successful\nparameter-efficient fine-tuning techniques, such as K-ADAPTER and MAD-X, we\npropose a similar approach for incorporating knowledge from multilingual\ngraphs, connecting concepts in various languages with each other through\nlinguistic relationships, into multilingual LLMs for LRLs. Specifically, we\nfocus on eight LRLs -- Maltese, Bulgarian, Indonesian, Nepali, Javanese,\nUyghur, Tibetan, and Sinhala -- and employ language-specific adapters\nfine-tuned on data extracted from the language-specific section of ConceptNet,\naiming to enable knowledge transfer across the languages covered by the\nknowledge graph. We compare various fine-tuning objectives, including standard\nMasked Language Modeling (MLM), MLM with full-word masking, and MLM with\ntargeted masking, to analyse their effectiveness in learning and integrating\nthe extracted graph data. Through empirical evaluation on language-specific\ntasks, we assess how structured graph knowledge affects the performance of\nmultilingual LLMs for LRLs in SA and NER, providing insights into the potential\nbenefits of adapting language models for low-resource scenarios."
                },
                "authors": [
                    {
                        "name": "Daniil Gurgurov"
                    },
                    {
                        "name": "Mareike Hartmann"
                    },
                    {
                        "name": "Simon Ostermann"
                    }
                ],
                "author_detail": {
                    "name": "Simon Ostermann"
                },
                "author": "Simon Ostermann",
                "arxiv_doi": "10.18653/v1/2024.kallm-1.7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2024.kallm-1.7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.01406v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01406v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, KaLLM workshop",
                "arxiv_journal_ref": "2024.kallm-1.7",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14063v1",
                "updated": "2024-12-18T17:08:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    8,
                    42,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T17:08:42Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    8,
                    42,
                    2,
                    353,
                    0
                ],
                "title": "Rango: Adaptive Retrieval-Augmented Proving for Automated Software\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rango: Adaptive Retrieval-Augmented Proving for Automated Software\n  Verification"
                },
                "summary": "Formal verification using proof assistants, such as Coq, enables the creation\nof high-quality software. However, the verification process requires\nsignificant expertise and manual effort to write proofs. Recent work has\nexplored automating proof synthesis using machine learning and large language\nmodels (LLMs). This work has shown that identifying relevant premises, such as\nlemmas and definitions, can aid synthesis. We present Rango, a fully automated\nproof synthesis tool for Coq that automatically identifies relevant premises\nand also similar proofs from the current project and uses them during\nsynthesis. Rango uses retrieval augmentation at every step of the proof to\nautomatically determine which proofs and premises to include in the context of\nits fine-tuned LLM. In this way, Rango adapts to the project and to the\nevolving state of the proof. We create a new dataset, CoqStoq, of 2,226\nopen-source Coq projects and 196,929 theorems from GitHub, which includes both\ntraining data and a curated evaluation benchmark of well-maintained projects.\nOn this benchmark, Rango synthesizes proofs for 32.0% of the theorems, which is\n29% more theorems than the prior state-of-the-art tool Tactician. Our\nevaluation also shows that Rango adding relevant proofs to its context leads to\na 47% increase in the number of theorems proven.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal verification using proof assistants, such as Coq, enables the creation\nof high-quality software. However, the verification process requires\nsignificant expertise and manual effort to write proofs. Recent work has\nexplored automating proof synthesis using machine learning and large language\nmodels (LLMs). This work has shown that identifying relevant premises, such as\nlemmas and definitions, can aid synthesis. We present Rango, a fully automated\nproof synthesis tool for Coq that automatically identifies relevant premises\nand also similar proofs from the current project and uses them during\nsynthesis. Rango uses retrieval augmentation at every step of the proof to\nautomatically determine which proofs and premises to include in the context of\nits fine-tuned LLM. In this way, Rango adapts to the project and to the\nevolving state of the proof. We create a new dataset, CoqStoq, of 2,226\nopen-source Coq projects and 196,929 theorems from GitHub, which includes both\ntraining data and a curated evaluation benchmark of well-maintained projects.\nOn this benchmark, Rango synthesizes proofs for 32.0% of the theorems, which is\n29% more theorems than the prior state-of-the-art tool Tactician. Our\nevaluation also shows that Rango adding relevant proofs to its context leads to\na 47% increase in the number of theorems proven."
                },
                "authors": [
                    {
                        "name": "Kyle Thompson"
                    },
                    {
                        "name": "Nuno Saavedra"
                    },
                    {
                        "name": "Pedro Carrott"
                    },
                    {
                        "name": "Kevin Fisher"
                    },
                    {
                        "name": "Alex Sanchez-Stern"
                    },
                    {
                        "name": "Yuriy Brun"
                    },
                    {
                        "name": "Joo F. Ferreira"
                    },
                    {
                        "name": "Sorin Lerner"
                    },
                    {
                        "name": "Emily First"
                    }
                ],
                "author_detail": {
                    "name": "Emily First"
                },
                "author": "Emily First",
                "arxiv_comment": "In Proceedings of the 47th International Conference on Software\n  Engineering (ICSE), Ottawa, ON, Canada, April 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4; I.2.7; I.2.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14062v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14062v1",
                "updated": "2024-12-18T17:08:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    8,
                    18,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T17:08:18Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    8,
                    18,
                    2,
                    353,
                    0
                ],
                "title": "Understanding and Evaluating Trust in Generative AI and Large Language\n  Models for Spreadsheets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Evaluating Trust in Generative AI and Large Language\n  Models for Spreadsheets"
                },
                "summary": "Generative AI and Large Language Models (LLMs) hold promise for automating\nspreadsheet formula creation. However, due to hallucinations, bias and variable\nuser skill, outputs obtained from generative AI cannot be assumed to be\naccurate or trustworthy. To address these challenges, a trustworthiness\nframework is proposed based on evaluating the transparency and dependability of\nthe formula. The transparency of the formula is explored through explainability\n(understanding the formula's reasoning) and visibility (inspecting the\nunderlying algorithms). The dependability of the generated formula is evaluated\nin terms of reliability (consistency and accuracy) and ethical considerations\n(bias and fairness). The paper also examines the drivers to these metrics in\nthe form of hallucinations, training data bias and poorly constructed prompts.\nFinally, examples of mistrust in technology are considered and the consequences\nexplored.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI and Large Language Models (LLMs) hold promise for automating\nspreadsheet formula creation. However, due to hallucinations, bias and variable\nuser skill, outputs obtained from generative AI cannot be assumed to be\naccurate or trustworthy. To address these challenges, a trustworthiness\nframework is proposed based on evaluating the transparency and dependability of\nthe formula. The transparency of the formula is explored through explainability\n(understanding the formula's reasoning) and visibility (inspecting the\nunderlying algorithms). The dependability of the generated formula is evaluated\nin terms of reliability (consistency and accuracy) and ethical considerations\n(bias and fairness). The paper also examines the drivers to these metrics in\nthe form of hallucinations, training data bias and poorly constructed prompts.\nFinally, examples of mistrust in technology are considered and the consequences\nexplored."
                },
                "authors": [
                    {
                        "name": "Simon Thorne"
                    }
                ],
                "author_detail": {
                    "name": "Simon Thorne"
                },
                "author": "Simon Thorne",
                "arxiv_journal_ref": "Proceedings of the European Spreadsheets Risks Interest Group 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14062v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14056v1",
                "updated": "2024-12-18T17:06:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    6,
                    21,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T17:06:21Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    6,
                    21,
                    2,
                    353,
                    0
                ],
                "title": "A Review of Multimodal Explainable Artificial Intelligence: Past,\n  Present and Future",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Review of Multimodal Explainable Artificial Intelligence: Past,\n  Present and Future"
                },
                "summary": "Artificial intelligence (AI) has rapidly developed through advancements in\ncomputational power and the growth of massive datasets. However, this progress\nhas also heightened challenges in interpreting the \"black-box\" nature of AI\nmodels. To address these concerns, eXplainable AI (XAI) has emerged with a\nfocus on transparency and interpretability to enhance human understanding and\ntrust in AI decision-making processes. In the context of multimodal data fusion\nand complex reasoning scenarios, the proposal of Multimodal eXplainable AI\n(MXAI) integrates multiple modalities for prediction and explanation tasks.\nMeanwhile, the advent of Large Language Models (LLMs) has led to remarkable\nbreakthroughs in natural language processing, yet their complexity has further\nexacerbated the issue of MXAI. To gain key insights into the development of\nMXAI methods and provide crucial guidance for building more transparent, fair,\nand trustworthy AI systems, we review the MXAI methods from a historical\nperspective and categorize them across four eras: traditional machine learning,\ndeep learning, discriminative foundation models, and generative LLMs. We also\nreview evaluation metrics and datasets used in MXAI research, concluding with a\ndiscussion of future challenges and directions. A project related to this\nreview has been created at https://github.com/ShilinSun/mxai_review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) has rapidly developed through advancements in\ncomputational power and the growth of massive datasets. However, this progress\nhas also heightened challenges in interpreting the \"black-box\" nature of AI\nmodels. To address these concerns, eXplainable AI (XAI) has emerged with a\nfocus on transparency and interpretability to enhance human understanding and\ntrust in AI decision-making processes. In the context of multimodal data fusion\nand complex reasoning scenarios, the proposal of Multimodal eXplainable AI\n(MXAI) integrates multiple modalities for prediction and explanation tasks.\nMeanwhile, the advent of Large Language Models (LLMs) has led to remarkable\nbreakthroughs in natural language processing, yet their complexity has further\nexacerbated the issue of MXAI. To gain key insights into the development of\nMXAI methods and provide crucial guidance for building more transparent, fair,\nand trustworthy AI systems, we review the MXAI methods from a historical\nperspective and categorize them across four eras: traditional machine learning,\ndeep learning, discriminative foundation models, and generative LLMs. We also\nreview evaluation metrics and datasets used in MXAI research, concluding with a\ndiscussion of future challenges and directions. A project related to this\nreview has been created at https://github.com/ShilinSun/mxai_review."
                },
                "authors": [
                    {
                        "name": "Shilin Sun"
                    },
                    {
                        "name": "Wenbin An"
                    },
                    {
                        "name": "Feng Tian"
                    },
                    {
                        "name": "Fang Nan"
                    },
                    {
                        "name": "Qidong Liu"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Nazaraf Shah"
                    },
                    {
                        "name": "Ping Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ping Chen"
                },
                "author": "Ping Chen",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14054v1",
                "updated": "2024-12-18T17:05:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    5,
                    49,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T17:05:49Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    5,
                    49,
                    2,
                    353,
                    0
                ],
                "title": "Digestion Algorithm in Hierarchical Symbolic Forests: A Fast Text\n  Normalization Algorithm and Semantic Parsing Framework for Specific Scenarios\n  and Lightweight Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digestion Algorithm in Hierarchical Symbolic Forests: A Fast Text\n  Normalization Algorithm and Semantic Parsing Framework for Specific Scenarios\n  and Lightweight Deployment"
                },
                "summary": "Text Normalization and Semantic Parsing have numerous applications in natural\nlanguage processing, such as natural language programming, paraphrasing, data\naugmentation, constructing expert systems, text matching, and more. Despite the\nprominent achievements of deep learning in Large Language Models (LLMs), the\ninterpretability of neural network architectures is still poor, which affects\ntheir credibility and hence limits the deployments of risk-sensitive scenarios.\nIn certain scenario-specific domains with scarce data, rapidly obtaining a\nlarge number of supervised learning labels is challenging, and the workload of\nmanually labeling data would be enormous. Catastrophic forgetting in neural\nnetworks further leads to low data utilization rates. In situations where swift\nresponses are vital, the density of the model makes local deployment difficult\nand the response time long, which is not conducive to local applications of\nthese fields. Inspired by the multiplication rule, a principle of combinatorial\nmathematics, and human thinking patterns, a multilayer framework along with its\nalgorithm, the Digestion Algorithm in Hierarchical Symbolic Forests (DAHSF), is\nproposed to address these above issues, combining text normalization and\nsemantic parsing workflows. The Chinese Scripting Language \"Fire Bunny\nIntelligent Development Platform V2.0\" is an important test and application of\nthe technology discussed in this paper. DAHSF can run locally in\nscenario-specific domains on little datasets, with model size and memory usage\noptimized by at least two orders of magnitude, thus improving the execution\nspeed, and possessing a promising optimization outlook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Normalization and Semantic Parsing have numerous applications in natural\nlanguage processing, such as natural language programming, paraphrasing, data\naugmentation, constructing expert systems, text matching, and more. Despite the\nprominent achievements of deep learning in Large Language Models (LLMs), the\ninterpretability of neural network architectures is still poor, which affects\ntheir credibility and hence limits the deployments of risk-sensitive scenarios.\nIn certain scenario-specific domains with scarce data, rapidly obtaining a\nlarge number of supervised learning labels is challenging, and the workload of\nmanually labeling data would be enormous. Catastrophic forgetting in neural\nnetworks further leads to low data utilization rates. In situations where swift\nresponses are vital, the density of the model makes local deployment difficult\nand the response time long, which is not conducive to local applications of\nthese fields. Inspired by the multiplication rule, a principle of combinatorial\nmathematics, and human thinking patterns, a multilayer framework along with its\nalgorithm, the Digestion Algorithm in Hierarchical Symbolic Forests (DAHSF), is\nproposed to address these above issues, combining text normalization and\nsemantic parsing workflows. The Chinese Scripting Language \"Fire Bunny\nIntelligent Development Platform V2.0\" is an important test and application of\nthe technology discussed in this paper. DAHSF can run locally in\nscenario-specific domains on little datasets, with model size and memory usage\noptimized by at least two orders of magnitude, thus improving the execution\nspeed, and possessing a promising optimization outlook."
                },
                "authors": [
                    {
                        "name": "Kevin You"
                    }
                ],
                "author_detail": {
                    "name": "Kevin You"
                },
                "author": "Kevin You",
                "arxiv_comment": "8 pages, 3 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14050v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14050v1",
                "updated": "2024-12-18T17:05:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    5,
                    8,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T17:05:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    5,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual\n  LLMs: An Extensive Investigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual\n  LLMs: An Extensive Investigation"
                },
                "summary": "Recent generative large language models (LLMs) show remarkable performance in\nnon-English languages, but when prompted in those languages they tend to\nexpress higher harmful social biases and toxicity levels. Prior work has shown\nthat finetuning on specialized datasets can mitigate this behavior, and doing\nso in English can transfer to other languages. In this work, we investigate the\nimpact of different finetuning methods on the model's bias and toxicity, but\nalso on its ability to produce fluent and diverse text. Our results show that\nfinetuning on curated non-harmful text is more effective for mitigating bias,\nand finetuning on direct preference optimization (DPO) datasets is more\neffective for mitigating toxicity. The mitigation caused by applying these\nmethods in English also transfers to non-English languages. We find evidence\nthat the extent to which transfer takes place can be predicted by the amount of\ndata in a given language present in the model's pretraining data. However, this\ntransfer of bias and toxicity mitigation often comes at the expense of\ndecreased language generation ability in non-English languages, highlighting\nthe importance of developing language-specific bias and toxicity mitigation\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent generative large language models (LLMs) show remarkable performance in\nnon-English languages, but when prompted in those languages they tend to\nexpress higher harmful social biases and toxicity levels. Prior work has shown\nthat finetuning on specialized datasets can mitigate this behavior, and doing\nso in English can transfer to other languages. In this work, we investigate the\nimpact of different finetuning methods on the model's bias and toxicity, but\nalso on its ability to produce fluent and diverse text. Our results show that\nfinetuning on curated non-harmful text is more effective for mitigating bias,\nand finetuning on direct preference optimization (DPO) datasets is more\neffective for mitigating toxicity. The mitigation caused by applying these\nmethods in English also transfers to non-English languages. We find evidence\nthat the extent to which transfer takes place can be predicted by the amount of\ndata in a given language present in the model's pretraining data. However, this\ntransfer of bias and toxicity mitigation often comes at the expense of\ndecreased language generation ability in non-English languages, highlighting\nthe importance of developing language-specific bias and toxicity mitigation\nmethods."
                },
                "authors": [
                    {
                        "name": "Vera Neplenbroek"
                    },
                    {
                        "name": "Arianna Bisazza"
                    },
                    {
                        "name": "Raquel Fernndez"
                    }
                ],
                "author_detail": {
                    "name": "Raquel Fernndez"
                },
                "author": "Raquel Fernndez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14050v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14042v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14042v1",
                "updated": "2024-12-18T16:55:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    55,
                    42,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T16:55:42Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    55,
                    42,
                    2,
                    353,
                    0
                ],
                "title": "CAD-Recode: Reverse Engineering CAD Code from Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAD-Recode: Reverse Engineering CAD Code from Point Clouds"
                },
                "summary": "Computer-Aided Design (CAD) models are typically constructed by sequentially\ndrawing parametric sketches and applying CAD operations to obtain a 3D model.\nThe problem of 3D CAD reverse engineering consists of reconstructing the sketch\nand CAD operation sequences from 3D representations such as point clouds. In\nthis paper, we address this challenge through novel contributions across three\nlevels: CAD sequence representation, network design, and dataset. In\nparticular, we represent CAD sketch-extrude sequences as Python code. The\nproposed CAD-Recode translates a point cloud into Python code that, when\nexecuted, reconstructs the CAD model. Taking advantage of the exposure of\npre-trained Large Language Models (LLMs) to Python code, we leverage a\nrelatively small LLM as a decoder for CAD-Recode and combine it with a\nlightweight point cloud projector. CAD-Recode is trained solely on a proposed\nsynthetic dataset of one million diverse CAD sequences. CAD-Recode\nsignificantly outperforms existing methods across three datasets while\nrequiring fewer input points. Notably, it achieves 10 times lower mean Chamfer\ndistance than state-of-the-art methods on DeepCAD and Fusion360 datasets.\nFurthermore, we show that our CAD Python code output is interpretable by\noff-the-shelf LLMs, enabling CAD editing and CAD-specific question answering\nfrom point clouds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer-Aided Design (CAD) models are typically constructed by sequentially\ndrawing parametric sketches and applying CAD operations to obtain a 3D model.\nThe problem of 3D CAD reverse engineering consists of reconstructing the sketch\nand CAD operation sequences from 3D representations such as point clouds. In\nthis paper, we address this challenge through novel contributions across three\nlevels: CAD sequence representation, network design, and dataset. In\nparticular, we represent CAD sketch-extrude sequences as Python code. The\nproposed CAD-Recode translates a point cloud into Python code that, when\nexecuted, reconstructs the CAD model. Taking advantage of the exposure of\npre-trained Large Language Models (LLMs) to Python code, we leverage a\nrelatively small LLM as a decoder for CAD-Recode and combine it with a\nlightweight point cloud projector. CAD-Recode is trained solely on a proposed\nsynthetic dataset of one million diverse CAD sequences. CAD-Recode\nsignificantly outperforms existing methods across three datasets while\nrequiring fewer input points. Notably, it achieves 10 times lower mean Chamfer\ndistance than state-of-the-art methods on DeepCAD and Fusion360 datasets.\nFurthermore, we show that our CAD Python code output is interpretable by\noff-the-shelf LLMs, enabling CAD editing and CAD-specific question answering\nfrom point clouds."
                },
                "authors": [
                    {
                        "name": "Danila Rukhovich"
                    },
                    {
                        "name": "Elona Dupont"
                    },
                    {
                        "name": "Dimitrios Mallis"
                    },
                    {
                        "name": "Kseniya Cherenkova"
                    },
                    {
                        "name": "Anis Kacem"
                    },
                    {
                        "name": "Djamila Aouada"
                    }
                ],
                "author_detail": {
                    "name": "Djamila Aouada"
                },
                "author": "Djamila Aouada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14042v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14042v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14033v1",
                "updated": "2024-12-18T16:52:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    52,
                    38,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T16:52:38Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    52,
                    38,
                    2,
                    353,
                    0
                ],
                "title": "Hansel: Output Length Controlling Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hansel: Output Length Controlling Framework for Large Language Models"
                },
                "summary": "Despite the great success of large language models (LLMs), efficiently\ncontrolling the length of the output sequence still remains a challenge. In\nthis paper, we propose Hansel, an efficient framework for length control in\nLLMs without affecting its generation ability. Hansel utilizes periodically\noutputted hidden special tokens to keep track of the remaining target length of\nthe output sequence. Together with techniques to avoid abrupt termination of\nthe output, this seemingly simple method proved to be efficient and versatile,\nwhile not harming the coherency and fluency of the generated text. The\nframework can be applied to any pre-trained LLMs during the finetuning stage of\nthe model, regardless of its original positional encoding method. We\ndemonstrate this by finetuning four different LLMs with Hansel and show that\nthe mean absolute error of the output sequence decreases significantly in every\nmodel and dataset compared to the prompt-based length control finetuning.\nMoreover, the framework showed a substantially improved ability to extrapolate\nto target lengths unseen during finetuning, such as long dialog responses or\nextremely short summaries. This indicates that the model learns the general\nmeans of length control, rather than learning to match output lengths to those\nseen during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the great success of large language models (LLMs), efficiently\ncontrolling the length of the output sequence still remains a challenge. In\nthis paper, we propose Hansel, an efficient framework for length control in\nLLMs without affecting its generation ability. Hansel utilizes periodically\noutputted hidden special tokens to keep track of the remaining target length of\nthe output sequence. Together with techniques to avoid abrupt termination of\nthe output, this seemingly simple method proved to be efficient and versatile,\nwhile not harming the coherency and fluency of the generated text. The\nframework can be applied to any pre-trained LLMs during the finetuning stage of\nthe model, regardless of its original positional encoding method. We\ndemonstrate this by finetuning four different LLMs with Hansel and show that\nthe mean absolute error of the output sequence decreases significantly in every\nmodel and dataset compared to the prompt-based length control finetuning.\nMoreover, the framework showed a substantially improved ability to extrapolate\nto target lengths unseen during finetuning, such as long dialog responses or\nextremely short summaries. This indicates that the model learns the general\nmeans of length control, rather than learning to match output lengths to those\nseen during training."
                },
                "authors": [
                    {
                        "name": "Seoha Song"
                    },
                    {
                        "name": "Junhyun Lee"
                    },
                    {
                        "name": "Hyeonmok Ko"
                    }
                ],
                "author_detail": {
                    "name": "Hyeonmok Ko"
                },
                "author": "Hyeonmok Ko",
                "arxiv_comment": "13 pages, 6 figures; accepted to AAAI-25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14019v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14019v1",
                "updated": "2024-12-18T16:37:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    37,
                    51,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T16:37:51Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    37,
                    51,
                    2,
                    353,
                    0
                ],
                "title": "Discovering maximally consistent distribution of causal tournaments with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering maximally consistent distribution of causal tournaments with\n  Large Language Models"
                },
                "summary": "Causal discovery is essential for understanding complex systems, yet\ntraditional methods often depend on strong, untestable assumptions, making the\nprocess challenging. Large Language Models (LLMs) present a promising\nalternative for extracting causal insights from text-based metadata, which\nconsolidates domain expertise. However, LLMs are prone to unreliability and\nhallucinations, necessitating strategies that account for their limitations.\nOne such strategy involves leveraging a consistency measure to evaluate\nreliability. Additionally, most text metadata does not clearly distinguish\ndirect causal relationships from indirect ones, further complicating the\ninference of causal graphs. As a result, focusing on causal orderings, rather\nthan causal graphs, emerges as a more practical and robust approach. We propose\na novel method to derive a distribution of acyclic tournaments (representing\nplausible causal orders) that maximizes a consistency score. Our approach\nbegins by computing pairwise consistency scores between variables, yielding a\ncyclic tournament that aggregates these scores. From this structure, we\nidentify optimal acyclic tournaments compatible with the original tournament,\nprioritizing those that maximize consistency across all configurations. We\ntested our method on both classical and well-established bechmarks, as well as\nreal-world datasets from epidemiology and public health. Our results\ndemonstrate the effectiveness of our approach in recovering distributions\ncausal orders with minimal error.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal discovery is essential for understanding complex systems, yet\ntraditional methods often depend on strong, untestable assumptions, making the\nprocess challenging. Large Language Models (LLMs) present a promising\nalternative for extracting causal insights from text-based metadata, which\nconsolidates domain expertise. However, LLMs are prone to unreliability and\nhallucinations, necessitating strategies that account for their limitations.\nOne such strategy involves leveraging a consistency measure to evaluate\nreliability. Additionally, most text metadata does not clearly distinguish\ndirect causal relationships from indirect ones, further complicating the\ninference of causal graphs. As a result, focusing on causal orderings, rather\nthan causal graphs, emerges as a more practical and robust approach. We propose\na novel method to derive a distribution of acyclic tournaments (representing\nplausible causal orders) that maximizes a consistency score. Our approach\nbegins by computing pairwise consistency scores between variables, yielding a\ncyclic tournament that aggregates these scores. From this structure, we\nidentify optimal acyclic tournaments compatible with the original tournament,\nprioritizing those that maximize consistency across all configurations. We\ntested our method on both classical and well-established bechmarks, as well as\nreal-world datasets from epidemiology and public health. Our results\ndemonstrate the effectiveness of our approach in recovering distributions\ncausal orders with minimal error."
                },
                "authors": [
                    {
                        "name": "Federico Baldo"
                    },
                    {
                        "name": "Simon Ferreira"
                    },
                    {
                        "name": "Charles K. Assaad"
                    }
                ],
                "author_detail": {
                    "name": "Charles K. Assaad"
                },
                "author": "Charles K. Assaad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14019v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14019v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14009v1",
                "updated": "2024-12-18T16:26:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    26,
                    47,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T16:26:47Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    26,
                    47,
                    2,
                    353,
                    0
                ],
                "title": "Cognition Chain for Explainable Psychological Stress Detection on Social\n  Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognition Chain for Explainable Psychological Stress Detection on Social\n  Media"
                },
                "summary": "Stress is a pervasive global health issue that can lead to severe mental\nhealth problems. Early detection offers timely intervention and prevention of\nstress-related disorders. The current early detection models perform \"black\nbox\" inference suffering from limited explainability and trust which blocks the\nreal-world clinical application. Thanks to the generative properties introduced\nby the Large Language Models (LLMs), the decision and the prediction from such\nmodels are semi-interpretable through the corresponding description. However,\nthe existing LLMs are mostly trained for general purposes without the guidance\nof psychological cognitive theory. To this end, we first highlight the\nimportance of prior theory with the observation of performance boosted by the\nchain-of-thoughts tailored for stress detection. This method termed Cognition\nChain explicates the generation of stress through a step-by-step cognitive\nperspective based on cognitive appraisal theory with a progress pipeline:\nStimulus $\\rightarrow$ Evaluation $\\rightarrow$ Reaction $\\rightarrow$ Stress\nState, guiding LLMs to provide comprehensive reasoning explanations. We further\nstudy the benefits brought by the proposed Cognition Chain format by utilising\nit as a synthetic dataset generation template for LLMs instruction-tuning and\nintroduce CogInstruct, an instruction-tuning dataset for stress detection. This\ndataset is developed using a three-stage self-reflective annotation pipeline\nthat enables LLMs to autonomously generate and refine instructional data. By\ninstruction-tuning Llama3 with CogInstruct, we develop CogLLM, an explainable\nstress detection model. Evaluations demonstrate that CogLLM achieves\noutstanding performance while enhancing explainability. Our work contributes a\nnovel approach by integrating cognitive theories into LLM reasoning processes,\noffering a promising direction for future explainable AI research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stress is a pervasive global health issue that can lead to severe mental\nhealth problems. Early detection offers timely intervention and prevention of\nstress-related disorders. The current early detection models perform \"black\nbox\" inference suffering from limited explainability and trust which blocks the\nreal-world clinical application. Thanks to the generative properties introduced\nby the Large Language Models (LLMs), the decision and the prediction from such\nmodels are semi-interpretable through the corresponding description. However,\nthe existing LLMs are mostly trained for general purposes without the guidance\nof psychological cognitive theory. To this end, we first highlight the\nimportance of prior theory with the observation of performance boosted by the\nchain-of-thoughts tailored for stress detection. This method termed Cognition\nChain explicates the generation of stress through a step-by-step cognitive\nperspective based on cognitive appraisal theory with a progress pipeline:\nStimulus $\\rightarrow$ Evaluation $\\rightarrow$ Reaction $\\rightarrow$ Stress\nState, guiding LLMs to provide comprehensive reasoning explanations. We further\nstudy the benefits brought by the proposed Cognition Chain format by utilising\nit as a synthetic dataset generation template for LLMs instruction-tuning and\nintroduce CogInstruct, an instruction-tuning dataset for stress detection. This\ndataset is developed using a three-stage self-reflective annotation pipeline\nthat enables LLMs to autonomously generate and refine instructional data. By\ninstruction-tuning Llama3 with CogInstruct, we develop CogLLM, an explainable\nstress detection model. Evaluations demonstrate that CogLLM achieves\noutstanding performance while enhancing explainability. Our work contributes a\nnovel approach by integrating cognitive theories into LLM reasoning processes,\noffering a promising direction for future explainable AI research."
                },
                "authors": [
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Boyan Gao"
                    },
                    {
                        "name": "Yi Dai"
                    },
                    {
                        "name": "Lei Cao"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Yibo Yang"
                    },
                    {
                        "name": "David Clifton"
                    }
                ],
                "author_detail": {
                    "name": "David Clifton"
                },
                "author": "David Clifton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14008v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14008v1",
                "updated": "2024-12-18T16:24:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    24,
                    20,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T16:24:20Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    24,
                    20,
                    2,
                    353,
                    0
                ],
                "title": "FarExStance: Explainable Stance Detection for Farsi",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FarExStance: Explainable Stance Detection for Farsi"
                },
                "summary": "We introduce FarExStance, a new dataset for explainable stance detection in\nFarsi. Each instance in this dataset contains a claim, the stance of an article\nor social media post towards that claim, and an extractive explanation which\nprovides evidence for the stance label. We compare the performance of a\nfine-tuned multilingual RoBERTa model to several large language models in\nzero-shot, few-shot, and parameter-efficient fine-tuned settings on our new\ndataset. On stance detection, the most accurate models are the fine-tuned\nRoBERTa model, the LLM Aya-23-8B which has been fine-tuned using\nparameter-efficient fine-tuning, and few-shot Claude-3.5-Sonnet. Regarding the\nquality of the explanations, our automatic evaluation metrics indicate that\nfew-shot GPT-4o generates the most coherent explanations, while our human\nevaluation reveals that the best Overall Explanation Score (OES) belongs to\nfew-shot Claude-3.5-Sonnet. The fine-tuned Aya-32-8B model produced\nexplanations most closely aligned with the reference explanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce FarExStance, a new dataset for explainable stance detection in\nFarsi. Each instance in this dataset contains a claim, the stance of an article\nor social media post towards that claim, and an extractive explanation which\nprovides evidence for the stance label. We compare the performance of a\nfine-tuned multilingual RoBERTa model to several large language models in\nzero-shot, few-shot, and parameter-efficient fine-tuned settings on our new\ndataset. On stance detection, the most accurate models are the fine-tuned\nRoBERTa model, the LLM Aya-23-8B which has been fine-tuned using\nparameter-efficient fine-tuning, and few-shot Claude-3.5-Sonnet. Regarding the\nquality of the explanations, our automatic evaluation metrics indicate that\nfew-shot GPT-4o generates the most coherent explanations, while our human\nevaluation reveals that the best Overall Explanation Score (OES) belongs to\nfew-shot Claude-3.5-Sonnet. The fine-tuned Aya-32-8B model produced\nexplanations most closely aligned with the reference explanations."
                },
                "authors": [
                    {
                        "name": "Majid Zarharan"
                    },
                    {
                        "name": "Maryam Hashemi"
                    },
                    {
                        "name": "Malika Behroozrazegh"
                    },
                    {
                        "name": "Sauleh Eetemadi"
                    },
                    {
                        "name": "Mohammad Taher Pilehvar"
                    },
                    {
                        "name": "Jennifer Foster"
                    }
                ],
                "author_detail": {
                    "name": "Jennifer Foster"
                },
                "author": "Jennifer Foster",
                "arxiv_comment": "Accepted in COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14008v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14008v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10924v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10924v2",
                "updated": "2024-12-18T16:16:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    16,
                    4,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-14T18:18:52Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    18,
                    18,
                    52,
                    5,
                    349,
                    0
                ],
                "title": "Tokens, the oft-overlooked appetizer: Large language models, the\n  distributional hypothesis, and meaning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokens, the oft-overlooked appetizer: Large language models, the\n  distributional hypothesis, and meaning"
                },
                "summary": "Tokenization is a necessary component within the current architecture of many\nlanguage models, including the transformer-based large language models (LLMs)\nof Generative AI, yet its impact on the model's cognition is often overlooked.\nWe argue that LLMs demonstrate that the Distributional Hypothesis (DM) is\nsufficient for reasonably human-like language performance, and that the\nemergence of human-meaningful linguistic units among tokens motivates\nlinguistically-informed interventions in existing, linguistically-agnostic\ntokenization techniques, particularly with respect to their roles as (1)\nsemantic primitives and as (2) vehicles for conveying salient distributional\npatterns from human language to the model. We explore tokenizations from a BPE\ntokenizer; extant model vocabularies obtained from Hugging Face and tiktoken;\nand the information in exemplar token vectors as they move through the layers\nof a RoBERTa (large) model. Besides creating sub-optimal semantic building\nblocks and obscuring the model's access to the necessary distributional\npatterns, we describe how tokenization pretraining can be a backdoor for bias\nand other unwanted content, which current alignment practices may not\nremediate. Additionally, we relay evidence that the tokenization algorithm's\nobjective function impacts the LLM's cognition, despite being meaningfully\ninsulated from the main system intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization is a necessary component within the current architecture of many\nlanguage models, including the transformer-based large language models (LLMs)\nof Generative AI, yet its impact on the model's cognition is often overlooked.\nWe argue that LLMs demonstrate that the Distributional Hypothesis (DM) is\nsufficient for reasonably human-like language performance, and that the\nemergence of human-meaningful linguistic units among tokens motivates\nlinguistically-informed interventions in existing, linguistically-agnostic\ntokenization techniques, particularly with respect to their roles as (1)\nsemantic primitives and as (2) vehicles for conveying salient distributional\npatterns from human language to the model. We explore tokenizations from a BPE\ntokenizer; extant model vocabularies obtained from Hugging Face and tiktoken;\nand the information in exemplar token vectors as they move through the layers\nof a RoBERTa (large) model. Besides creating sub-optimal semantic building\nblocks and obscuring the model's access to the necessary distributional\npatterns, we describe how tokenization pretraining can be a backdoor for bias\nand other unwanted content, which current alignment practices may not\nremediate. Additionally, we relay evidence that the tokenization algorithm's\nobjective function impacts the LLM's cognition, despite being meaningfully\ninsulated from the main system intelligence."
                },
                "authors": [
                    {
                        "name": "Julia Witte Zimmerman"
                    },
                    {
                        "name": "Denis Hudon"
                    },
                    {
                        "name": "Kathryn Cramer"
                    },
                    {
                        "name": "Alejandro J. Ruiz"
                    },
                    {
                        "name": "Calla Beauregard"
                    },
                    {
                        "name": "Ashley Fehr"
                    },
                    {
                        "name": "Mikaela Irene Fudolig"
                    },
                    {
                        "name": "Bradford Demarest"
                    },
                    {
                        "name": "Yoshi Meke Bird"
                    },
                    {
                        "name": "Milo Z. Trujillo"
                    },
                    {
                        "name": "Christopher M. Danforth"
                    },
                    {
                        "name": "Peter Sheridan Dodds"
                    }
                ],
                "author_detail": {
                    "name": "Peter Sheridan Dodds"
                },
                "author": "Peter Sheridan Dodds",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10924v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10924v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13998v1",
                "updated": "2024-12-18T16:14:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    14,
                    59,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T16:14:59Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    14,
                    59,
                    2,
                    353,
                    0
                ],
                "title": "Few-shot Steerable Alignment: Adapting Rewards and LLM Policies with\n  Neural Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot Steerable Alignment: Adapting Rewards and LLM Policies with\n  Neural Processes"
                },
                "summary": "As large language models (LLMs) become increasingly embedded in everyday\napplications, ensuring their alignment with the diverse preferences of\nindividual users has become a critical challenge. Currently deployed approaches\ntypically assume homogeneous user objectives and rely on single-objective\nfine-tuning. However, human preferences are inherently heterogeneous,\ninfluenced by various unobservable factors, leading to conflicting signals in\npreference data. Existing solutions addressing this diversity often require\ncostly datasets labelled for specific objectives and involve training multiple\nreward models or LLM policies, which is computationally expensive and\nimpractical. In this work, we present a novel framework for few-shot steerable\nalignment, where users' underlying preferences are inferred from a small sample\nof their choices. To achieve this, we extend the Bradley-Terry-Luce model to\nhandle heterogeneous preferences with unobserved variability factors and\npropose its practical implementation for reward modelling and LLM fine-tuning.\nThanks to our proposed approach of functional parameter-space conditioning,\nLLMs trained with our framework can be adapted to individual preferences at\ninference time, generating outputs over a continuum of behavioural modes. We\nempirically validate the effectiveness of methods, demonstrating their ability\nto capture and align with diverse human preferences in a data-efficient manner.\nOur code is made available at:\nhttps://github.com/kasia-kobalczyk/few-shot-steerable-alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become increasingly embedded in everyday\napplications, ensuring their alignment with the diverse preferences of\nindividual users has become a critical challenge. Currently deployed approaches\ntypically assume homogeneous user objectives and rely on single-objective\nfine-tuning. However, human preferences are inherently heterogeneous,\ninfluenced by various unobservable factors, leading to conflicting signals in\npreference data. Existing solutions addressing this diversity often require\ncostly datasets labelled for specific objectives and involve training multiple\nreward models or LLM policies, which is computationally expensive and\nimpractical. In this work, we present a novel framework for few-shot steerable\nalignment, where users' underlying preferences are inferred from a small sample\nof their choices. To achieve this, we extend the Bradley-Terry-Luce model to\nhandle heterogeneous preferences with unobserved variability factors and\npropose its practical implementation for reward modelling and LLM fine-tuning.\nThanks to our proposed approach of functional parameter-space conditioning,\nLLMs trained with our framework can be adapted to individual preferences at\ninference time, generating outputs over a continuum of behavioural modes. We\nempirically validate the effectiveness of methods, demonstrating their ability\nto capture and align with diverse human preferences in a data-efficient manner.\nOur code is made available at:\nhttps://github.com/kasia-kobalczyk/few-shot-steerable-alignment."
                },
                "authors": [
                    {
                        "name": "Katarzyna Kobalczyk"
                    },
                    {
                        "name": "Claudio Fanconi"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13988v1",
                "updated": "2024-12-18T16:07:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    7,
                    32,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T16:07:32Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    7,
                    32,
                    2,
                    353,
                    0
                ],
                "title": "RAG for Effective Supply Chain Security Questionnaire Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG for Effective Supply Chain Security Questionnaire Automation"
                },
                "summary": "In an era where digital security is crucial, efficient processing of\nsecurity-related inquiries through supply chain security questionnaires is\nimperative. This paper introduces a novel approach using Natural Language\nProcessing (NLP) and Retrieval-Augmented Generation (RAG) to automate these\nresponses. We developed QuestSecure, a system that interprets diverse document\nformats and generates precise responses by integrating large language models\n(LLMs) with an advanced retrieval system. Our experiments show that QuestSecure\nsignificantly improves response accuracy and operational efficiency. By\nemploying advanced NLP techniques and tailored retrieval mechanisms, the system\nconsistently produces contextually relevant and semantically rich responses,\nreducing cognitive load on security teams and minimizing potential errors. This\nresearch offers promising avenues for automating complex security management\ntasks, enhancing organizational security processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an era where digital security is crucial, efficient processing of\nsecurity-related inquiries through supply chain security questionnaires is\nimperative. This paper introduces a novel approach using Natural Language\nProcessing (NLP) and Retrieval-Augmented Generation (RAG) to automate these\nresponses. We developed QuestSecure, a system that interprets diverse document\nformats and generates precise responses by integrating large language models\n(LLMs) with an advanced retrieval system. Our experiments show that QuestSecure\nsignificantly improves response accuracy and operational efficiency. By\nemploying advanced NLP techniques and tailored retrieval mechanisms, the system\nconsistently produces contextually relevant and semantically rich responses,\nreducing cognitive load on security teams and minimizing potential errors. This\nresearch offers promising avenues for automating complex security management\ntasks, enhancing organizational security processes."
                },
                "authors": [
                    {
                        "name": "Zaynab Batool Reza"
                    },
                    {
                        "name": "Abdul Rafay Syed"
                    },
                    {
                        "name": "Omer Iqbal"
                    },
                    {
                        "name": "Ethel Mensah"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Maxx Richard Rahman"
                    },
                    {
                        "name": "Wolfgang Maass"
                    }
                ],
                "author_detail": {
                    "name": "Wolfgang Maass"
                },
                "author": "Wolfgang Maass",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02987v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02987v2",
                "updated": "2024-12-18T16:07:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    7,
                    28,
                    2,
                    353,
                    0
                ],
                "published": "2024-07-03T10:38:40Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    10,
                    38,
                    40,
                    2,
                    185,
                    0
                ],
                "title": "LoRA-Guard: Parameter-Efficient Guardrail Adaptation for Content\n  Moderation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA-Guard: Parameter-Efficient Guardrail Adaptation for Content\n  Moderation of Large Language Models"
                },
                "summary": "Guardrails have emerged as an alternative to safety alignment for content\nmoderation of large language models (LLMs). Existing model-based guardrails\nhave not been designed for resource-constrained computational portable devices,\nsuch as mobile phones, more and more of which are running LLM-based\napplications locally. We introduce LoRA-Guard, a parameter-efficient guardrail\nadaptation method that relies on knowledge sharing between LLMs and guardrail\nmodels. LoRA-Guard extracts language features from the LLMs and adapts them for\nthe content moderation task using low-rank adapters, while a dual-path design\nprevents any performance degradation on the generative task. We show that\nLoRA-Guard outperforms existing approaches with 100-1000x lower parameter\noverhead while maintaining accuracy, enabling on-device content moderation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guardrails have emerged as an alternative to safety alignment for content\nmoderation of large language models (LLMs). Existing model-based guardrails\nhave not been designed for resource-constrained computational portable devices,\nsuch as mobile phones, more and more of which are running LLM-based\napplications locally. We introduce LoRA-Guard, a parameter-efficient guardrail\nadaptation method that relies on knowledge sharing between LLMs and guardrail\nmodels. LoRA-Guard extracts language features from the LLMs and adapts them for\nthe content moderation task using low-rank adapters, while a dual-path design\nprevents any performance degradation on the generative task. We show that\nLoRA-Guard outperforms existing approaches with 100-1000x lower parameter\noverhead while maintaining accuracy, enabling on-device content moderation."
                },
                "authors": [
                    {
                        "name": "Hayder Elesedy"
                    },
                    {
                        "name": "Pedro M. Esperana"
                    },
                    {
                        "name": "Silviu Vlad Oprea"
                    },
                    {
                        "name": "Mete Ozay"
                    }
                ],
                "author_detail": {
                    "name": "Mete Ozay"
                },
                "author": "Mete Ozay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02987v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02987v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09043v3",
                "updated": "2024-12-18T15:56:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    56,
                    29,
                    2,
                    353,
                    0
                ],
                "published": "2024-04-13T16:59:28Z",
                "published_parsed": [
                    2024,
                    4,
                    13,
                    16,
                    59,
                    28,
                    5,
                    104,
                    0
                ],
                "title": "Do LLMs Play Dice? Exploring Probability Distribution Sampling in Large\n  Language Models for Behavioral Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Play Dice? Exploring Probability Distribution Sampling in Large\n  Language Models for Behavioral Simulation"
                },
                "summary": "With the rapid advancement of large language models (LLMs) for handling\ncomplex language tasks, an increasing number of studies are employing LLMs as\nagents to emulate the sequential decision-making processes of humans often\nrepresented as Markov decision-making processes (MDPs). The actions in MDPs\nadhere to specific probability distributions and require iterative sampling.\nThis arouses curiosity regarding the capacity of LLM agents to comprehend\nprobability distributions, thereby guiding the agent's behavioral\ndecision-making through probabilistic sampling and generating behavioral\nsequences. To answer the above question, we divide the problem into two main\naspects: sequence simulation with known probability distribution and sequence\nsimulation with unknown probability distribution. Our analysis indicates that\nLLM agents can understand probabilities, but they struggle with probability\nsampling. Their ability to perform probabilistic sampling can be improved to\nsome extent by integrating coding tools, but this level of sampling precision\nstill makes it difficult to simulate human behavior as agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of large language models (LLMs) for handling\ncomplex language tasks, an increasing number of studies are employing LLMs as\nagents to emulate the sequential decision-making processes of humans often\nrepresented as Markov decision-making processes (MDPs). The actions in MDPs\nadhere to specific probability distributions and require iterative sampling.\nThis arouses curiosity regarding the capacity of LLM agents to comprehend\nprobability distributions, thereby guiding the agent's behavioral\ndecision-making through probabilistic sampling and generating behavioral\nsequences. To answer the above question, we divide the problem into two main\naspects: sequence simulation with known probability distribution and sequence\nsimulation with unknown probability distribution. Our analysis indicates that\nLLM agents can understand probabilities, but they struggle with probability\nsampling. Their ability to perform probabilistic sampling can be improved to\nsome extent by integrating coding tools, but this level of sampling precision\nstill makes it difficult to simulate human behavior as agents."
                },
                "authors": [
                    {
                        "name": "Jia Gu"
                    },
                    {
                        "name": "Liang Pang"
                    },
                    {
                        "name": "Huawei Shen"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "The 31st International Conference on Computational Linguistics\n  (COLING 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09632v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09632v2",
                "updated": "2024-12-18T15:55:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    55,
                    28,
                    2,
                    353,
                    0
                ],
                "published": "2024-11-27T19:53:05Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    19,
                    53,
                    5,
                    2,
                    332,
                    0
                ],
                "title": "Methods to Assess the UK Government's Current Role as a Data Provider\n  for AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Methods to Assess the UK Government's Current Role as a Data Provider\n  for AI"
                },
                "summary": "Governments typically collect and steward a vast amount of high-quality data\non their citizens and institutions, and the UK government is exploring how it\ncan better publish and provision this data to the benefit of the AI landscape.\nHowever, the compositions of generative AI training corpora remain closely\nguarded secrets, making the planning of data sharing initiatives difficult. To\naddress this, we devise two methods to assess UK government data usage for the\ntraining of Large Language Models (LLMs) and 'peek behind the curtain' in order\nto observe the UK government's current contributions as a data provider for AI.\nThe first method, an ablation study that utilises LLM 'unlearning', seeks to\nexamine the importance of the information held on UK government websites for\nLLMs and their performance in citizen query tasks. The second method, an\ninformation leakage study, seeks to ascertain whether LLMs are aware of the\ninformation held in the datasets published on the UK government's open data\ninitiative data.gov.uk. Our findings indicate that UK government websites are\nimportant data sources for AI (heterogenously across subject matters) while\ndata.gov.uk is not. This paper serves as a technical report, explaining\nin-depth the designs, mechanics, and limitations of the above experiments. It\nis accompanied by a complementary non-technical report on the ODI website in\nwhich we summarise the experiments and key findings, interpret them, and build\na set of actionable recommendations for the UK government to take forward as it\nseeks to design AI policy. While we focus on UK open government data, we\nbelieve that the methods introduced in this paper present a reproducible\napproach to tackle the opaqueness of AI training corpora and provide\norganisations a framework to evaluate and maximize their contributions to AI\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Governments typically collect and steward a vast amount of high-quality data\non their citizens and institutions, and the UK government is exploring how it\ncan better publish and provision this data to the benefit of the AI landscape.\nHowever, the compositions of generative AI training corpora remain closely\nguarded secrets, making the planning of data sharing initiatives difficult. To\naddress this, we devise two methods to assess UK government data usage for the\ntraining of Large Language Models (LLMs) and 'peek behind the curtain' in order\nto observe the UK government's current contributions as a data provider for AI.\nThe first method, an ablation study that utilises LLM 'unlearning', seeks to\nexamine the importance of the information held on UK government websites for\nLLMs and their performance in citizen query tasks. The second method, an\ninformation leakage study, seeks to ascertain whether LLMs are aware of the\ninformation held in the datasets published on the UK government's open data\ninitiative data.gov.uk. Our findings indicate that UK government websites are\nimportant data sources for AI (heterogenously across subject matters) while\ndata.gov.uk is not. This paper serves as a technical report, explaining\nin-depth the designs, mechanics, and limitations of the above experiments. It\nis accompanied by a complementary non-technical report on the ODI website in\nwhich we summarise the experiments and key findings, interpret them, and build\na set of actionable recommendations for the UK government to take forward as it\nseeks to design AI policy. While we focus on UK open government data, we\nbelieve that the methods introduced in this paper present a reproducible\napproach to tackle the opaqueness of AI training corpora and provide\norganisations a framework to evaluate and maximize their contributions to AI\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Neil Majithia"
                    },
                    {
                        "name": "Elena Simperl"
                    }
                ],
                "author_detail": {
                    "name": "Elena Simperl"
                },
                "author": "Elena Simperl",
                "arxiv_comment": "17 pages, 5 figures; v2 - incorporated editor feedback; for the\n  accompanying, non-technical ODI report see\n  https://theodi.org/insights/reports/the-uk-government-as-a-data-provider-for-ai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09632v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09632v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13952v1",
                "updated": "2024-12-18T15:32:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    32,
                    27,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T15:32:27Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    32,
                    27,
                    2,
                    353,
                    0
                ],
                "title": "Prompting Strategies for Enabling Large Language Models to Infer\n  Causation from Correlation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting Strategies for Enabling Large Language Models to Infer\n  Causation from Correlation"
                },
                "summary": "The reasoning abilities of Large Language Models (LLMs) are attracting\nincreasing attention. In this work, we focus on causal reasoning and address\nthe task of establishing causal relationships based on correlation information,\na highly challenging problem on which several LLMs have shown poor performance.\nWe introduce a prompting strategy for this problem that breaks the original\ntask into fixed subquestions, with each subquestion corresponding to one step\nof a formal causal discovery algorithm, the PC algorithm. The proposed\nprompting strategy, PC-SubQ, guides the LLM to follow these algorithmic steps,\nby sequentially prompting it with one subquestion at a time, augmenting the\nnext subquestion's prompt with the answer to the previous one(s). We evaluate\nour approach on an existing causal benchmark, Corr2Cause: our experiments\nindicate a performance improvement across five LLMs when comparing PC-SubQ to\nbaseline prompting strategies. Results are robust to causal query\nperturbations, when modifying the variable names or paraphrasing the\nexpressions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning abilities of Large Language Models (LLMs) are attracting\nincreasing attention. In this work, we focus on causal reasoning and address\nthe task of establishing causal relationships based on correlation information,\na highly challenging problem on which several LLMs have shown poor performance.\nWe introduce a prompting strategy for this problem that breaks the original\ntask into fixed subquestions, with each subquestion corresponding to one step\nof a formal causal discovery algorithm, the PC algorithm. The proposed\nprompting strategy, PC-SubQ, guides the LLM to follow these algorithmic steps,\nby sequentially prompting it with one subquestion at a time, augmenting the\nnext subquestion's prompt with the answer to the previous one(s). We evaluate\nour approach on an existing causal benchmark, Corr2Cause: our experiments\nindicate a performance improvement across five LLMs when comparing PC-SubQ to\nbaseline prompting strategies. Results are robust to causal query\nperturbations, when modifying the variable names or paraphrasing the\nexpressions."
                },
                "authors": [
                    {
                        "name": "Eleni Sgouritsa"
                    },
                    {
                        "name": "Virginia Aglietti"
                    },
                    {
                        "name": "Yee Whye Teh"
                    },
                    {
                        "name": "Arnaud Doucet"
                    },
                    {
                        "name": "Arthur Gretton"
                    },
                    {
                        "name": "Silvia Chiappa"
                    }
                ],
                "author_detail": {
                    "name": "Silvia Chiappa"
                },
                "author": "Silvia Chiappa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13949v1",
                "updated": "2024-12-18T15:29:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    29,
                    30,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T15:29:30Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    29,
                    30,
                    2,
                    353,
                    0
                ],
                "title": "Cracking the Code of Hallucination in LVLMs with Vision-aware Head\n  Divergence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cracking the Code of Hallucination in LVLMs with Vision-aware Head\n  Divergence"
                },
                "summary": "Large vision-language models (LVLMs) have made substantial progress in\nintegrating large language models (LLMs) with visual inputs, enabling advanced\nmultimodal reasoning. Despite their success, a persistent challenge is\nhallucination-where generated text fails to accurately reflect visual\ncontent-undermining both accuracy and reliability. Existing methods focus on\nalignment training or decoding refinements but primarily address symptoms at\nthe generation stage without probing the underlying causes. In this work, we\ninvestigate the internal mechanisms driving hallucination in LVLMs, with an\nemphasis on the multi-head attention module. Specifically, we introduce\nVision-aware Head Divergence (VHD), a metric that quantifies the sensitivity of\nattention head outputs to visual context. Based on this, our findings reveal\nthe presence of vision-aware attention heads that are more attuned to visual\ninformation; however, the model's overreliance on its prior language patterns\nis closely related to hallucinations. Building on these insights, we propose\nVision-aware Head Reinforcement (VHR), a training-free approach to mitigate\nhallucination by enhancing the role of vision-aware attention heads. Extensive\nexperiments demonstrate that our method achieves superior performance compared\nto state-of-the-art approaches in mitigating hallucinations, while maintaining\nhigh efficiency with negligible additional time overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (LVLMs) have made substantial progress in\nintegrating large language models (LLMs) with visual inputs, enabling advanced\nmultimodal reasoning. Despite their success, a persistent challenge is\nhallucination-where generated text fails to accurately reflect visual\ncontent-undermining both accuracy and reliability. Existing methods focus on\nalignment training or decoding refinements but primarily address symptoms at\nthe generation stage without probing the underlying causes. In this work, we\ninvestigate the internal mechanisms driving hallucination in LVLMs, with an\nemphasis on the multi-head attention module. Specifically, we introduce\nVision-aware Head Divergence (VHD), a metric that quantifies the sensitivity of\nattention head outputs to visual context. Based on this, our findings reveal\nthe presence of vision-aware attention heads that are more attuned to visual\ninformation; however, the model's overreliance on its prior language patterns\nis closely related to hallucinations. Building on these insights, we propose\nVision-aware Head Reinforcement (VHR), a training-free approach to mitigate\nhallucination by enhancing the role of vision-aware attention heads. Extensive\nexperiments demonstrate that our method achieves superior performance compared\nto state-of-the-art approaches in mitigating hallucinations, while maintaining\nhigh efficiency with negligible additional time overhead."
                },
                "authors": [
                    {
                        "name": "Jinghan He"
                    },
                    {
                        "name": "Kuan Zhu"
                    },
                    {
                        "name": "Haiyun Guo"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Zhenglin Hua"
                    },
                    {
                        "name": "Yuheng Jia"
                    },
                    {
                        "name": "Ming Tang"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    },
                    {
                        "name": "Jinqiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinqiao Wang"
                },
                "author": "Jinqiao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13942v1",
                "updated": "2024-12-18T15:24:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    24,
                    50,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T15:24:50Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    24,
                    50,
                    2,
                    353,
                    0
                ],
                "title": "A Rose by Any Other Name: LLM-Generated Explanations Are Good Proxies\n  for Human Explanations to Collect Label Distributions on NLI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Rose by Any Other Name: LLM-Generated Explanations Are Good Proxies\n  for Human Explanations to Collect Label Distributions on NLI"
                },
                "summary": "Disagreement in human labeling is ubiquitous, and can be captured in human\njudgment distributions (HJDs). Recent research has shown that explanations\nprovide valuable information for understanding human label variation (HLV) and\nlarge language models (LLMs) can approximate HJD from a few human-provided\nlabel-explanation pairs. However, collecting explanations for every label is\nstill time-consuming. This paper examines whether LLMs can be used to replace\nhumans in generating explanations for approximating HJD. Specifically, we use\nLLMs as annotators to generate model explanations for a few given human labels.\nWe test ways to obtain and combine these label-explanations with the goal to\napproximate human judgment distribution. We further compare the resulting human\nwith model-generated explanations, and test automatic and human explanation\nselection. Our experiments show that LLM explanations are promising for NLI: to\nestimate HJD, generated explanations yield comparable results to human's when\nprovided with human labels. Importantly, our results generalize from datasets\nwith human explanations to i) datasets where they are not available and ii)\nchallenging out-of-distribution test sets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disagreement in human labeling is ubiquitous, and can be captured in human\njudgment distributions (HJDs). Recent research has shown that explanations\nprovide valuable information for understanding human label variation (HLV) and\nlarge language models (LLMs) can approximate HJD from a few human-provided\nlabel-explanation pairs. However, collecting explanations for every label is\nstill time-consuming. This paper examines whether LLMs can be used to replace\nhumans in generating explanations for approximating HJD. Specifically, we use\nLLMs as annotators to generate model explanations for a few given human labels.\nWe test ways to obtain and combine these label-explanations with the goal to\napproximate human judgment distribution. We further compare the resulting human\nwith model-generated explanations, and test automatic and human explanation\nselection. Our experiments show that LLM explanations are promising for NLI: to\nestimate HJD, generated explanations yield comparable results to human's when\nprovided with human labels. Importantly, our results generalize from datasets\nwith human explanations to i) datasets where they are not available and ii)\nchallenging out-of-distribution test sets."
                },
                "authors": [
                    {
                        "name": "Beiduo Chen"
                    },
                    {
                        "name": "Siyao Peng"
                    },
                    {
                        "name": "Anna Korhonen"
                    },
                    {
                        "name": "Barbara Plank"
                    }
                ],
                "author_detail": {
                    "name": "Barbara Plank"
                },
                "author": "Barbara Plank",
                "arxiv_comment": "25 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13924v1",
                "updated": "2024-12-18T15:07:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    7,
                    23,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T15:07:23Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    7,
                    23,
                    2,
                    353,
                    0
                ],
                "title": "Language verY Rare for All",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language verY Rare for All"
                },
                "summary": "In the quest to overcome language barriers, encoder-decoder models like NLLB\nhave expanded machine translation to rare languages, with some models (e.g.,\nNLLB 1.3B) even trainable on a single GPU. While general-purpose LLMs perform\nwell in translation, open LLMs prove highly competitive when fine-tuned for\nspecific tasks involving unknown corpora. We introduce LYRA (Language verY Rare\nfor All), a novel approach that combines open LLM fine-tuning,\nretrieval-augmented generation (RAG), and transfer learning from related\nhigh-resource languages. This study is exclusively focused on single-GPU\ntraining to facilitate ease of adoption. Our study focuses on two-way\ntranslation between French and Mon\\'egasque, a rare language unsupported by\nexisting translation tools due to limited corpus availability. Our results\ndemonstrate LYRA's effectiveness, frequently surpassing and consistently\nmatching state-of-the-art encoder-decoder models in rare language translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the quest to overcome language barriers, encoder-decoder models like NLLB\nhave expanded machine translation to rare languages, with some models (e.g.,\nNLLB 1.3B) even trainable on a single GPU. While general-purpose LLMs perform\nwell in translation, open LLMs prove highly competitive when fine-tuned for\nspecific tasks involving unknown corpora. We introduce LYRA (Language verY Rare\nfor All), a novel approach that combines open LLM fine-tuning,\nretrieval-augmented generation (RAG), and transfer learning from related\nhigh-resource languages. This study is exclusively focused on single-GPU\ntraining to facilitate ease of adoption. Our study focuses on two-way\ntranslation between French and Mon\\'egasque, a rare language unsupported by\nexisting translation tools due to limited corpus availability. Our results\ndemonstrate LYRA's effectiveness, frequently surpassing and consistently\nmatching state-of-the-art encoder-decoder models in rare language translation."
                },
                "authors": [
                    {
                        "name": "Ibrahim Merad"
                    },
                    {
                        "name": "Amos Wolf"
                    },
                    {
                        "name": "Ziad Mazzawi"
                    },
                    {
                        "name": "Yannick Lo"
                    }
                ],
                "author_detail": {
                    "name": "Yannick Lo"
                },
                "author": "Yannick Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13922v1",
                "updated": "2024-12-18T15:05:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    5,
                    59,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T15:05:59Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    5,
                    59,
                    2,
                    353,
                    0
                ],
                "title": "Pipeline Analysis for Developing Instruct LLMs in Low-Resource\n  Languages: A Case Study on Basque",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pipeline Analysis for Developing Instruct LLMs in Low-Resource\n  Languages: A Case Study on Basque"
                },
                "summary": "Large language models (LLMs) are typically optimized for resource-rich\nlanguages like English, exacerbating the gap between high-resource and\nunderrepresented languages. This work presents a detailed analysis of\nstrategies for developing a model capable of following instructions in a\nlow-resource language, specifically Basque, by focusing on three key stages:\npre-training, instruction tuning, and alignment with human preferences. Our\nfindings demonstrate that continual pre-training with a high-quality Basque\ncorpus of around 600 million words improves natural language understanding\n(NLU) of the foundational model by over 12 points. Moreover, instruction tuning\nand human preference alignment using automatically translated datasets proved\nhighly effective, resulting in a 24-point improvement in instruction-following\nperformance. The resulting models, Llama-eus-8B and Llama-eus-8B-instruct,\nestablish a new state-of-the-art for Basque in the sub-10B parameter category.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are typically optimized for resource-rich\nlanguages like English, exacerbating the gap between high-resource and\nunderrepresented languages. This work presents a detailed analysis of\nstrategies for developing a model capable of following instructions in a\nlow-resource language, specifically Basque, by focusing on three key stages:\npre-training, instruction tuning, and alignment with human preferences. Our\nfindings demonstrate that continual pre-training with a high-quality Basque\ncorpus of around 600 million words improves natural language understanding\n(NLU) of the foundational model by over 12 points. Moreover, instruction tuning\nand human preference alignment using automatically translated datasets proved\nhighly effective, resulting in a 24-point improvement in instruction-following\nperformance. The resulting models, Llama-eus-8B and Llama-eus-8B-instruct,\nestablish a new state-of-the-art for Basque in the sub-10B parameter category."
                },
                "authors": [
                    {
                        "name": "Ander Corral"
                    },
                    {
                        "name": "Ixak Sarasua"
                    },
                    {
                        "name": "Xabier Saralegi"
                    }
                ],
                "author_detail": {
                    "name": "Xabier Saralegi"
                },
                "author": "Xabier Saralegi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01638v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01638v4",
                "updated": "2024-12-18T15:01:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    1,
                    32,
                    2,
                    353,
                    0
                ],
                "published": "2024-06-03T00:27:29Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    0,
                    27,
                    29,
                    0,
                    155,
                    0
                ],
                "title": "TimeCMA: Towards LLM-Empowered Multivariate Time Series Forecasting via\n  Cross-Modality Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TimeCMA: Towards LLM-Empowered Multivariate Time Series Forecasting via\n  Cross-Modality Alignment"
                },
                "summary": "Multivariate time series forecasting (MTSF) aims to learn temporal dynamics\namong variables to forecast future time series. Existing statistical and deep\nlearning-based methods suffer from limited learnable parameters and small-scale\ntraining data. Recently, large language models (LLMs) combining time series\nwith textual prompts have achieved promising performance in MTSF. However, we\ndiscovered that current LLM-based solutions fall short in learning disentangled\nembeddings. We introduce TimeCMA, an intuitive yet effective framework for MTSF\nvia cross-modality alignment. Specifically, we present a dual-modality encoding\nwith two branches: the time series encoding branch extracts disentangled yet\nweak time series embeddings, and the LLM-empowered encoding branch wraps the\nsame time series with text as prompts to obtain entangled yet robust prompt\nembeddings. As a result, such a cross-modality alignment retrieves both\ndisentangled and robust time series embeddings, ``the best of two worlds'',\nfrom the prompt embeddings based on time series and prompt modality\nsimilarities. As another key design, to reduce the computational costs from\ntime series with their length textual prompts, we design an effective prompt to\nencourage the most essential temporal information to be encapsulated in the\nlast token: only the last token is passed to downstream prediction. We further\nstore the last token embeddings to accelerate inference speed. Extensive\nexperiments on eight real datasets demonstrate that TimeCMA outperforms\nstate-of-the-arts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivariate time series forecasting (MTSF) aims to learn temporal dynamics\namong variables to forecast future time series. Existing statistical and deep\nlearning-based methods suffer from limited learnable parameters and small-scale\ntraining data. Recently, large language models (LLMs) combining time series\nwith textual prompts have achieved promising performance in MTSF. However, we\ndiscovered that current LLM-based solutions fall short in learning disentangled\nembeddings. We introduce TimeCMA, an intuitive yet effective framework for MTSF\nvia cross-modality alignment. Specifically, we present a dual-modality encoding\nwith two branches: the time series encoding branch extracts disentangled yet\nweak time series embeddings, and the LLM-empowered encoding branch wraps the\nsame time series with text as prompts to obtain entangled yet robust prompt\nembeddings. As a result, such a cross-modality alignment retrieves both\ndisentangled and robust time series embeddings, ``the best of two worlds'',\nfrom the prompt embeddings based on time series and prompt modality\nsimilarities. As another key design, to reduce the computational costs from\ntime series with their length textual prompts, we design an effective prompt to\nencourage the most essential temporal information to be encapsulated in the\nlast token: only the last token is passed to downstream prediction. We further\nstore the last token embeddings to accelerate inference speed. Extensive\nexperiments on eight real datasets demonstrate that TimeCMA outperforms\nstate-of-the-arts."
                },
                "authors": [
                    {
                        "name": "Chenxi Liu"
                    },
                    {
                        "name": "Qianxiong Xu"
                    },
                    {
                        "name": "Hao Miao"
                    },
                    {
                        "name": "Sun Yang"
                    },
                    {
                        "name": "Lingzheng Zhang"
                    },
                    {
                        "name": "Cheng Long"
                    },
                    {
                        "name": "Ziyue Li"
                    },
                    {
                        "name": "Rui Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhao"
                },
                "author": "Rui Zhao",
                "arxiv_comment": "Accepted by AAAI 2025 (Main Technical Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01638v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01638v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13911v1",
                "updated": "2024-12-18T14:52:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    52,
                    41,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T14:52:41Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    52,
                    41,
                    2,
                    353,
                    0
                ],
                "title": "Resilience of Networks to Spreading Computer Viruses: Optimal Anti-Virus\n  Deployment (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resilience of Networks to Spreading Computer Viruses: Optimal Anti-Virus\n  Deployment (Extended Version)"
                },
                "summary": "Deployment of anti-virus software is a common strategy for preventing and\ncontrolling the propagation of computer viruses and worms over a computer\nnetwork. As the deployment of such programs is often limited due to monetary or\noperational costs, devising optimal strategies for their allocation and\ndeployment can be of high value to the operation, performance, and resilience\nof the target networks.\n  We study the effects of anti-virus deployment (i.e., \"vaccination\")\nstrategies on the ability of a network to block the spread of a virus. Such\nability is obtained when the network reaches \"herd immunity\", achieved when a\nlarge fraction of the network entities is immune to the infection, which\nprovides protection even for entities which are not immune. We use a model that\nexplicitly accounts for the inherent heterogeneity of network nodes activity\nand derive optimal strategies for anti-virus deployment.\n  Numerical evaluations demonstrate that the system performance is very\nsensitive to the chosen strategy, and thus strategies which disregard the\nheterogeneous spread nature may perform significantly worse relatively to those\nderived in this work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deployment of anti-virus software is a common strategy for preventing and\ncontrolling the propagation of computer viruses and worms over a computer\nnetwork. As the deployment of such programs is often limited due to monetary or\noperational costs, devising optimal strategies for their allocation and\ndeployment can be of high value to the operation, performance, and resilience\nof the target networks.\n  We study the effects of anti-virus deployment (i.e., \"vaccination\")\nstrategies on the ability of a network to block the spread of a virus. Such\nability is obtained when the network reaches \"herd immunity\", achieved when a\nlarge fraction of the network entities is immune to the infection, which\nprovides protection even for entities which are not immune. We use a model that\nexplicitly accounts for the inherent heterogeneity of network nodes activity\nand derive optimal strategies for anti-virus deployment.\n  Numerical evaluations demonstrate that the system performance is very\nsensitive to the chosen strategy, and thus strategies which disregard the\nheterogeneous spread nature may perform significantly worse relatively to those\nderived in this work."
                },
                "authors": [
                    {
                        "name": "Jhonatan Tavori"
                    },
                    {
                        "name": "Hanoch Levy"
                    }
                ],
                "author_detail": {
                    "name": "Hanoch Levy"
                },
                "author": "Hanoch Levy",
                "arxiv_comment": "The extended version of the 2023 IEEE/IFIP NOMS conference paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08469v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08469v2",
                "updated": "2024-12-18T14:45:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    45,
                    15,
                    2,
                    353,
                    0
                ],
                "published": "2024-11-13T09:40:37Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    9,
                    40,
                    37,
                    2,
                    318,
                    0
                ],
                "title": "Building Trustworthy AI: Transparent AI Systems via Large Language\n  Models, Ontologies, and Logical Reasoning (TranspNet)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building Trustworthy AI: Transparent AI Systems via Large Language\n  Models, Ontologies, and Logical Reasoning (TranspNet)"
                },
                "summary": "Growing concerns over the lack of transparency in AI, particularly in\nhigh-stakes fields like healthcare and finance, drive the need for explainable\nand trustworthy systems. While Large Language Models (LLMs) perform\nexceptionally well in generating accurate outputs, their \"black box\" nature\nposes significant challenges to transparency and trust. To address this, the\npaper proposes the TranspNet pipeline, which integrates symbolic AI with LLMs.\nBy leveraging domain expert knowledge, retrieval-augmented generation (RAG),\nand formal reasoning frameworks like Answer Set Programming (ASP), TranspNet\nenhances LLM outputs with structured reasoning and verification.This approach\nstrives to help AI systems deliver results that are as accurate, explainable,\nand trustworthy as possible, aligning with regulatory expectations for\ntransparency and accountability. TranspNet provides a solution for developing\nAI systems that are reliable and interpretable, making it suitable for\nreal-world applications where trust is critical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Growing concerns over the lack of transparency in AI, particularly in\nhigh-stakes fields like healthcare and finance, drive the need for explainable\nand trustworthy systems. While Large Language Models (LLMs) perform\nexceptionally well in generating accurate outputs, their \"black box\" nature\nposes significant challenges to transparency and trust. To address this, the\npaper proposes the TranspNet pipeline, which integrates symbolic AI with LLMs.\nBy leveraging domain expert knowledge, retrieval-augmented generation (RAG),\nand formal reasoning frameworks like Answer Set Programming (ASP), TranspNet\nenhances LLM outputs with structured reasoning and verification.This approach\nstrives to help AI systems deliver results that are as accurate, explainable,\nand trustworthy as possible, aligning with regulatory expectations for\ntransparency and accountability. TranspNet provides a solution for developing\nAI systems that are reliable and interpretable, making it suitable for\nreal-world applications where trust is critical."
                },
                "authors": [
                    {
                        "name": "Fadi Al Machot"
                    },
                    {
                        "name": "Martin Thomas Horsch"
                    },
                    {
                        "name": "Habib Ullah"
                    }
                ],
                "author_detail": {
                    "name": "Habib Ullah"
                },
                "author": "Habib Ullah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08469v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08469v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2205.06740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2205.06740v2",
                "updated": "2024-12-18T14:41:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    41,
                    53,
                    2,
                    353,
                    0
                ],
                "published": "2022-05-13T16:19:21Z",
                "published_parsed": [
                    2022,
                    5,
                    13,
                    16,
                    19,
                    21,
                    4,
                    133,
                    0
                ],
                "title": "Towards Deployable OCR models for Indic languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Deployable OCR models for Indic languages"
                },
                "summary": "Recognition of text on word or line images, without the need for sub-word\nsegmentation has become the mainstream of research and development of text\nrecognition for Indian languages. Modelling unsegmented sequences using\nConnectionist Temporal Classification (CTC) is the most commonly used approach\nfor segmentation-free OCR. In this work we present a comprehensive empirical\nstudy of various neural network models that uses CTC for transcribing step-wise\npredictions in the neural network output to a Unicode sequence. The study is\nconducted for 13 Indian languages, using an internal dataset that has around\n1000 pages per language. We study the choice of line vs word as the recognition\nunit, and use of synthetic data to train the models. We compare our models with\npopular publicly available OCR tools for end-to-end document image recognition.\nOur end-to-end pipeline that employ our recognition models and existing text\nsegmentation tools outperform these public OCR tools for 8 out of the 13\nlanguages. We also introduce a new public dataset called Mozhi for word and\nline recognition in Indian language. The dataset contains more than 1.2 million\nannotated word images (120 thousand text lines) across 13 Indian languages. Our\ncode, trained models and the Mozhi dataset will be made available at\nhttp://cvit.iiit.ac.in/research/projects/cvit-projects/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recognition of text on word or line images, without the need for sub-word\nsegmentation has become the mainstream of research and development of text\nrecognition for Indian languages. Modelling unsegmented sequences using\nConnectionist Temporal Classification (CTC) is the most commonly used approach\nfor segmentation-free OCR. In this work we present a comprehensive empirical\nstudy of various neural network models that uses CTC for transcribing step-wise\npredictions in the neural network output to a Unicode sequence. The study is\nconducted for 13 Indian languages, using an internal dataset that has around\n1000 pages per language. We study the choice of line vs word as the recognition\nunit, and use of synthetic data to train the models. We compare our models with\npopular publicly available OCR tools for end-to-end document image recognition.\nOur end-to-end pipeline that employ our recognition models and existing text\nsegmentation tools outperform these public OCR tools for 8 out of the 13\nlanguages. We also introduce a new public dataset called Mozhi for word and\nline recognition in Indian language. The dataset contains more than 1.2 million\nannotated word images (120 thousand text lines) across 13 Indian languages. Our\ncode, trained models and the Mozhi dataset will be made available at\nhttp://cvit.iiit.ac.in/research/projects/cvit-projects/"
                },
                "authors": [
                    {
                        "name": "Minesh Mathew"
                    },
                    {
                        "name": "Ajoy Mondal"
                    },
                    {
                        "name": "CV Jawahar"
                    }
                ],
                "author_detail": {
                    "name": "CV Jawahar"
                },
                "author": "CV Jawahar",
                "arxiv_doi": "10.1007/978-3-031-78495-8_11",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-78495-8_11",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2205.06740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2205.06740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "presented at ICPR 2024;\n  https://link.springer.com/chapter/10.1007/978-3-031-78495-8_11",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15380v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15380v3",
                "updated": "2024-12-18T14:39:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    39,
                    2,
                    2,
                    353,
                    0
                ],
                "published": "2024-09-20T15:01:21Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    15,
                    1,
                    21,
                    4,
                    264,
                    0
                ],
                "title": "Kalahi: A handcrafted, grassroots cultural LLM evaluation suite for\n  Filipino",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kalahi: A handcrafted, grassroots cultural LLM evaluation suite for\n  Filipino"
                },
                "summary": "Multilingual large language models (LLMs) today may not necessarily provide\nculturally appropriate and relevant responses to its Filipino users. We\nintroduce Kalahi, a cultural LLM evaluation suite collaboratively created by\nnative Filipino speakers. It is composed of 150 high-quality, handcrafted and\nnuanced prompts that test LLMs for generations that are relevant to shared\nFilipino cultural knowledge and values. Strong LLM performance in Kalahi\nindicates a model's ability to generate responses similar to what an average\nFilipino would say or do in a given situation. We conducted experiments on LLMs\nwith multilingual and Filipino language support. Results show that Kalahi,\nwhile trivial for Filipinos, is challenging for LLMs, with the best model\nanswering only 46.0% of the questions correctly compared to native Filipino\nperformance of 89.10%. Thus, Kalahi can be used to accurately and reliably\nevaluate Filipino cultural representation in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual large language models (LLMs) today may not necessarily provide\nculturally appropriate and relevant responses to its Filipino users. We\nintroduce Kalahi, a cultural LLM evaluation suite collaboratively created by\nnative Filipino speakers. It is composed of 150 high-quality, handcrafted and\nnuanced prompts that test LLMs for generations that are relevant to shared\nFilipino cultural knowledge and values. Strong LLM performance in Kalahi\nindicates a model's ability to generate responses similar to what an average\nFilipino would say or do in a given situation. We conducted experiments on LLMs\nwith multilingual and Filipino language support. Results show that Kalahi,\nwhile trivial for Filipinos, is challenging for LLMs, with the best model\nanswering only 46.0% of the questions correctly compared to native Filipino\nperformance of 89.10%. Thus, Kalahi can be used to accurately and reliably\nevaluate Filipino cultural representation in LLMs."
                },
                "authors": [
                    {
                        "name": "Jann Railey Montalan"
                    },
                    {
                        "name": "Jian Gang Ngui"
                    },
                    {
                        "name": "Wei Qi Leong"
                    },
                    {
                        "name": "Yosephine Susanto"
                    },
                    {
                        "name": "Hamsawardhini Rengarajan"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    },
                    {
                        "name": "William Chandra Tjhi"
                    }
                ],
                "author_detail": {
                    "name": "William Chandra Tjhi"
                },
                "author": "William Chandra Tjhi",
                "arxiv_comment": "Accepted for presentation at Paclic 38, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15380v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15380v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18416v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18416v3",
                "updated": "2024-12-18T14:25:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    25,
                    8,
                    2,
                    353,
                    0
                ],
                "published": "2024-07-25T22:24:45Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    22,
                    24,
                    45,
                    3,
                    207,
                    0
                ],
                "title": "PersonaGym: Evaluating Persona Agents and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonaGym: Evaluating Persona Agents and LLMs"
                },
                "summary": "Persona agents, which are LLM agents that act according to an assigned\npersona, have demonstrated impressive contextual response capabilities across\nvarious applications. These persona agents offer significant enhancements\nacross diverse sectors, such as education, healthcare, and entertainment, where\nmodel developers can align agent responses to different user requirements\nthereby broadening the scope of agent applications. However, evaluating persona\nagent performance is incredibly challenging due to the complexity of assessing\npersona adherence in free-form interactions across various environments that\nare relevant to each persona agent. We introduce PersonaGym, the first dynamic\nevaluation framework for assessing persona agents, and PersonaScore, the first\nautomated human-aligned metric grounded in decision theory for comprehensive\nlarge-scale evaluation of persona agents. Our evaluation of 6 open and\nclosed-source LLMs, using a benchmark encompassing 200 personas and 10,000\nquestions, reveals significant opportunities for advancement in persona agent\ncapabilities across state-of-the-art models. For example, Claude 3.5 Sonnet\nonly has a 2.97% relative improvement in PersonaScore than GPT 3.5 despite\nbeing a much more advanced model. Importantly, we find that increased model\nsize and complexity do not necessarily imply enhanced persona agent\ncapabilities thereby highlighting the pressing need for algorithmic and\narchitectural invention towards faithful and performant persona agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persona agents, which are LLM agents that act according to an assigned\npersona, have demonstrated impressive contextual response capabilities across\nvarious applications. These persona agents offer significant enhancements\nacross diverse sectors, such as education, healthcare, and entertainment, where\nmodel developers can align agent responses to different user requirements\nthereby broadening the scope of agent applications. However, evaluating persona\nagent performance is incredibly challenging due to the complexity of assessing\npersona adherence in free-form interactions across various environments that\nare relevant to each persona agent. We introduce PersonaGym, the first dynamic\nevaluation framework for assessing persona agents, and PersonaScore, the first\nautomated human-aligned metric grounded in decision theory for comprehensive\nlarge-scale evaluation of persona agents. Our evaluation of 6 open and\nclosed-source LLMs, using a benchmark encompassing 200 personas and 10,000\nquestions, reveals significant opportunities for advancement in persona agent\ncapabilities across state-of-the-art models. For example, Claude 3.5 Sonnet\nonly has a 2.97% relative improvement in PersonaScore than GPT 3.5 despite\nbeing a much more advanced model. Importantly, we find that increased model\nsize and complexity do not necessarily imply enhanced persona agent\ncapabilities thereby highlighting the pressing need for algorithmic and\narchitectural invention towards faithful and performant persona agents."
                },
                "authors": [
                    {
                        "name": "Vinay Samuel"
                    },
                    {
                        "name": "Henry Peng Zou"
                    },
                    {
                        "name": "Yue Zhou"
                    },
                    {
                        "name": "Shreyas Chaudhari"
                    },
                    {
                        "name": "Ashwin Kalyan"
                    },
                    {
                        "name": "Tanmay Rajpurohit"
                    },
                    {
                        "name": "Ameet Deshpande"
                    },
                    {
                        "name": "Karthik Narasimhan"
                    },
                    {
                        "name": "Vishvak Murahari"
                    }
                ],
                "author_detail": {
                    "name": "Vishvak Murahari"
                },
                "author": "Vishvak Murahari",
                "arxiv_comment": "21 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18416v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18416v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13879v1",
                "updated": "2024-12-18T14:19:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    19,
                    23,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T14:19:23Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    19,
                    23,
                    2,
                    353,
                    0
                ],
                "title": "Crabs: Consuming Resrouce via Auto-generation for LLM-DoS Attack under\n  Black-box Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crabs: Consuming Resrouce via Auto-generation for LLM-DoS Attack under\n  Black-box Settings"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse tasks. LLMs continue to be vulnerable to external threats, particularly\nDenial-of-Service (DoS) attacks. Specifically, LLM-DoS attacks aim to exhaust\ncomputational resources and block services. However, prior works tend to focus\non performing white-box attacks, overlooking black-box settings. In this work,\nwe propose an automated algorithm designed for black-box LLMs, called\nAuto-Generation for LLM-DoS Attack (AutoDoS). AutoDoS introduces DoS Attack\nTree and optimizes the prompt node coverage to enhance effectiveness under\nblack-box conditions. Our method can bypass existing defense with enhanced\nstealthiness via semantic improvement of prompt nodes. Furthermore, we reveal\nthat implanting Length Trojan in Basic DoS Prompt aids in achieving higher\nattack efficacy. Experimental results show that AutoDoS amplifies service\nresponse latency by over 250 $\\times \\uparrow$, leading to severe resource\nconsumption in terms of GPU utilization and memory usage. Our code is available\nat \\url{https://github.com/shuita2333/AutoDoS}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse tasks. LLMs continue to be vulnerable to external threats, particularly\nDenial-of-Service (DoS) attacks. Specifically, LLM-DoS attacks aim to exhaust\ncomputational resources and block services. However, prior works tend to focus\non performing white-box attacks, overlooking black-box settings. In this work,\nwe propose an automated algorithm designed for black-box LLMs, called\nAuto-Generation for LLM-DoS Attack (AutoDoS). AutoDoS introduces DoS Attack\nTree and optimizes the prompt node coverage to enhance effectiveness under\nblack-box conditions. Our method can bypass existing defense with enhanced\nstealthiness via semantic improvement of prompt nodes. Furthermore, we reveal\nthat implanting Length Trojan in Basic DoS Prompt aids in achieving higher\nattack efficacy. Experimental results show that AutoDoS amplifies service\nresponse latency by over 250 $\\times \\uparrow$, leading to severe resource\nconsumption in terms of GPU utilization and memory usage. Our code is available\nat \\url{https://github.com/shuita2333/AutoDoS}."
                },
                "authors": [
                    {
                        "name": "Yuanhe Zhang"
                    },
                    {
                        "name": "Zhenhong Zhou"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Xinyue Wang"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Sen Su"
                    }
                ],
                "author_detail": {
                    "name": "Sen Su"
                },
                "author": "Sen Su",
                "arxiv_comment": "20 pages, 7 figures, 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23111v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23111v3",
                "updated": "2024-12-18T14:10:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    10,
                    10,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-30T15:23:44Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    23,
                    44,
                    2,
                    304,
                    0
                ],
                "title": "Exploring Gradient Subspaces: Addressing and Overcoming LoRA's\n  Limitations in Federated Fine-Tuning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Gradient Subspaces: Addressing and Overcoming LoRA's\n  Limitations in Federated Fine-Tuning of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, particularly in task generalization for both text and vision\ndata. While fine-tuning these models can significantly enhance their\nperformance on specific downstream tasks, it often requires high-quality data\nthat cannot be shared due to privacy concerns. Federated Learning (FL) offers a\npromising solution for collaborative training without direct data sharing.\nHowever, many parameter-efficient fine-tuning strategies for LLMs in FL,\nparticularly those based on Low-Rank Adaptation (LoRA), face limitations. In\nthis paper, we critically analyze the convergence and performance guarantees of\npopular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to\nconstrained subspace learning of low-rank matrices. This limitation hinders\neffective fine-tuning of LLMs in federated settings. Through rigorous\nanalytical and empirical evaluations, we demonstrate that direct weight\naveraging outperforms LoRA-based strategies, leading to superior performance\nfor fine-tuned models. Our comprehensive comparison unmasks inefficiencies in\nLoRA approaches and underscores the advantages of direct weight aggregation. We\nextend our analysis to low-rank gradient-based optimizers, such as GaLore, used\nduring local training steps. Our findings show that GaLore along with\ndirect-weight aggregation is a more effective approach, outperforming federated\nLoRA methods like FlexLoRA and FFA-LoRA across both text and image modalities.\nWhile privacy remains paramount in FL discourse, our focus is on assessing\nperformance outcomes of federated fine-tuned models and evaluating various FL\nframeworks from both theoretical and empirical perspectives. Our findings\nadvocate reassessing the reliance on LoRA within FL contexts, paving the way\nfor more efficient training methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, particularly in task generalization for both text and vision\ndata. While fine-tuning these models can significantly enhance their\nperformance on specific downstream tasks, it often requires high-quality data\nthat cannot be shared due to privacy concerns. Federated Learning (FL) offers a\npromising solution for collaborative training without direct data sharing.\nHowever, many parameter-efficient fine-tuning strategies for LLMs in FL,\nparticularly those based on Low-Rank Adaptation (LoRA), face limitations. In\nthis paper, we critically analyze the convergence and performance guarantees of\npopular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to\nconstrained subspace learning of low-rank matrices. This limitation hinders\neffective fine-tuning of LLMs in federated settings. Through rigorous\nanalytical and empirical evaluations, we demonstrate that direct weight\naveraging outperforms LoRA-based strategies, leading to superior performance\nfor fine-tuned models. Our comprehensive comparison unmasks inefficiencies in\nLoRA approaches and underscores the advantages of direct weight aggregation. We\nextend our analysis to low-rank gradient-based optimizers, such as GaLore, used\nduring local training steps. Our findings show that GaLore along with\ndirect-weight aggregation is a more effective approach, outperforming federated\nLoRA methods like FlexLoRA and FFA-LoRA across both text and image modalities.\nWhile privacy remains paramount in FL discourse, our focus is on assessing\nperformance outcomes of federated fine-tuned models and evaluating various FL\nframeworks from both theoretical and empirical perspectives. Our findings\nadvocate reassessing the reliance on LoRA within FL contexts, paving the way\nfor more efficient training methodologies."
                },
                "authors": [
                    {
                        "name": "Navyansh Mahla"
                    },
                    {
                        "name": "Ganesh Ramakrishnan"
                    }
                ],
                "author_detail": {
                    "name": "Ganesh Ramakrishnan"
                },
                "author": "Ganesh Ramakrishnan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23111v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23111v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06593v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06593v2",
                "updated": "2024-12-18T14:08:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    8,
                    45,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-09T15:45:03Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    15,
                    45,
                    3,
                    0,
                    344,
                    0
                ],
                "title": "Anchoring Bias in Large Language Models: An Experimental Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anchoring Bias in Large Language Models: An Experimental Study"
                },
                "summary": "Large Language Models (LLMs) like GPT-4 and Gemini have significantly\nadvanced artificial intelligence by enabling machines to generate and\ncomprehend human-like text. Despite their impressive capabilities, LLMs are not\nimmune to limitations, including various biases. While much research has\nexplored demographic biases, the cognitive biases in LLMs have not been equally\nscrutinized. This study delves into anchoring bias, a cognitive bias where\ninitial information disproportionately influences judgment. Utilizing an\nexperimental dataset, we examine how anchoring bias manifests in LLMs and\nverify the effectiveness of various mitigation strategies. Our findings\nhighlight the sensitivity of LLM responses to biased hints. At the same time,\nour experiments show that, to mitigate anchoring bias, one needs to collect\nhints from comprehensive angles to prevent the LLMs from being anchored to\nindividual pieces of information, while simple algorithms such as\nChain-of-Thought, Thoughts of Principles, Ignoring Anchor Hints, and Reflection\nare not sufficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) like GPT-4 and Gemini have significantly\nadvanced artificial intelligence by enabling machines to generate and\ncomprehend human-like text. Despite their impressive capabilities, LLMs are not\nimmune to limitations, including various biases. While much research has\nexplored demographic biases, the cognitive biases in LLMs have not been equally\nscrutinized. This study delves into anchoring bias, a cognitive bias where\ninitial information disproportionately influences judgment. Utilizing an\nexperimental dataset, we examine how anchoring bias manifests in LLMs and\nverify the effectiveness of various mitigation strategies. Our findings\nhighlight the sensitivity of LLM responses to biased hints. At the same time,\nour experiments show that, to mitigate anchoring bias, one needs to collect\nhints from comprehensive angles to prevent the LLMs from being anchored to\nindividual pieces of information, while simple algorithms such as\nChain-of-Thought, Thoughts of Principles, Ignoring Anchor Hints, and Reflection\nare not sufficient."
                },
                "authors": [
                    {
                        "name": "Jiaxu Lou"
                    },
                    {
                        "name": "Yifan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Sun"
                },
                "author": "Yifan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06593v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06593v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13178v2",
                "updated": "2024-12-18T14:00:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    0,
                    2,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-17T18:55:58Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    55,
                    58,
                    1,
                    352,
                    0
                ],
                "title": "SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM\n  Agents"
                },
                "summary": "With the integration of large language models (LLMs), embodied agents have\nstrong capabilities to execute complicated instructions in natural language,\npaving a way for the potential deployment of embodied robots. However, a\nforeseeable issue is that those embodied agents can also flawlessly execute\nsome hazardous tasks, potentially causing damages in real world. To study this\nissue, we present SafeAgentBench -- a new benchmark for safety-aware task\nplanning of embodied LLM agents. SafeAgentBench includes: (1) a new dataset\nwith 750 tasks, covering 10 potential hazards and 3 task types; (2)\nSafeAgentEnv, a universal embodied environment with a low-level controller,\nsupporting multi-agent execution with 17 high-level actions for 8\nstate-of-the-art baselines; and (3) reliable evaluation methods from both\nexecution and semantic perspectives. Experimental results show that the\nbest-performing baseline gets 69% success rate for safe tasks, but only 5%\nrejection rate for hazardous tasks, indicating significant safety risks. More\ndetails and codes are available at\nhttps://github.com/shengyin1224/SafeAgentBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the integration of large language models (LLMs), embodied agents have\nstrong capabilities to execute complicated instructions in natural language,\npaving a way for the potential deployment of embodied robots. However, a\nforeseeable issue is that those embodied agents can also flawlessly execute\nsome hazardous tasks, potentially causing damages in real world. To study this\nissue, we present SafeAgentBench -- a new benchmark for safety-aware task\nplanning of embodied LLM agents. SafeAgentBench includes: (1) a new dataset\nwith 750 tasks, covering 10 potential hazards and 3 task types; (2)\nSafeAgentEnv, a universal embodied environment with a low-level controller,\nsupporting multi-agent execution with 17 high-level actions for 8\nstate-of-the-art baselines; and (3) reliable evaluation methods from both\nexecution and semantic perspectives. Experimental results show that the\nbest-performing baseline gets 69% success rate for safe tasks, but only 5%\nrejection rate for hazardous tasks, indicating significant safety risks. More\ndetails and codes are available at\nhttps://github.com/shengyin1224/SafeAgentBench."
                },
                "authors": [
                    {
                        "name": "Sheng Yin"
                    },
                    {
                        "name": "Xianghe Pang"
                    },
                    {
                        "name": "Yuanzhuo Ding"
                    },
                    {
                        "name": "Menglan Chen"
                    },
                    {
                        "name": "Yutong Bi"
                    },
                    {
                        "name": "Yichen Xiong"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Zhen Xiang"
                    },
                    {
                        "name": "Jing Shao"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "arxiv_comment": "21 pages, 14 tables, 7 figures, submitted to ICRA 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13862v1",
                "updated": "2024-12-18T13:55:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    55,
                    42,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T13:55:42Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    55,
                    42,
                    2,
                    353,
                    0
                ],
                "title": "Energy-Based Preference Model Offers Better Offline Alignment than the\n  Bradley-Terry Preference Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Based Preference Model Offers Better Offline Alignment than the\n  Bradley-Terry Preference Model"
                },
                "summary": "Since the debut of DPO, it has been shown that aligning a target LLM with\nhuman preferences via the KL-constrained RLHF loss is mathematically equivalent\nto a special kind of reward modeling task. Concretely, the task requires: 1)\nusing the target LLM to parameterize the reward model, and 2) tuning the reward\nmodel so that it has a 1:1 linear relationship with the true reward. However,\nwe identify a significant issue: the DPO loss might have multiple minimizers,\nof which only one satisfies the required linearity condition. The problem\narises from a well-known issue of the underlying Bradley-Terry preference\nmodel: it does not always have a unique maximum likelihood estimator (MLE).\nConsequently,the minimizer of the RLHF loss might be unattainable because it is\nmerely one among many minimizers of the DPO loss. As a better alternative, we\npropose an energy-based model (EBM) that always has a unique MLE, inherently\nsatisfying the linearity requirement. To approximate the MLE in practice, we\npropose a contrastive loss named Energy Preference Alignment (EPA), wherein\neach positive sample is contrasted against one or more strong negatives as well\nas many free weak negatives. Theoretical properties of our EBM enable the\napproximation error of EPA to almost surely vanish when a sufficient number of\nnegatives are used. Empirically, we demonstrate that EPA consistently delivers\nbetter performance on open benchmarks compared to DPO, thereby showing the\nsuperiority of our EBM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the debut of DPO, it has been shown that aligning a target LLM with\nhuman preferences via the KL-constrained RLHF loss is mathematically equivalent\nto a special kind of reward modeling task. Concretely, the task requires: 1)\nusing the target LLM to parameterize the reward model, and 2) tuning the reward\nmodel so that it has a 1:1 linear relationship with the true reward. However,\nwe identify a significant issue: the DPO loss might have multiple minimizers,\nof which only one satisfies the required linearity condition. The problem\narises from a well-known issue of the underlying Bradley-Terry preference\nmodel: it does not always have a unique maximum likelihood estimator (MLE).\nConsequently,the minimizer of the RLHF loss might be unattainable because it is\nmerely one among many minimizers of the DPO loss. As a better alternative, we\npropose an energy-based model (EBM) that always has a unique MLE, inherently\nsatisfying the linearity requirement. To approximate the MLE in practice, we\npropose a contrastive loss named Energy Preference Alignment (EPA), wherein\neach positive sample is contrasted against one or more strong negatives as well\nas many free weak negatives. Theoretical properties of our EBM enable the\napproximation error of EPA to almost surely vanish when a sufficient number of\nnegatives are used. Empirically, we demonstrate that EPA consistently delivers\nbetter performance on open benchmarks compared to DPO, thereby showing the\nsuperiority of our EBM."
                },
                "authors": [
                    {
                        "name": "Yuzhong Hong"
                    },
                    {
                        "name": "Hanshan Zhang"
                    },
                    {
                        "name": "Junwei Bao"
                    },
                    {
                        "name": "Hongfei Jiang"
                    },
                    {
                        "name": "Yang Song"
                    }
                ],
                "author_detail": {
                    "name": "Yang Song"
                },
                "author": "Yang Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13860v1",
                "updated": "2024-12-18T13:53:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    53,
                    59,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T13:53:59Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    53,
                    59,
                    2,
                    353,
                    0
                ],
                "title": "Domain-adaptative Continual Learning for Low-resource Tasks: Evaluation\n  on Nepali",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain-adaptative Continual Learning for Low-resource Tasks: Evaluation\n  on Nepali"
                },
                "summary": "Continual learning has emerged as an important research direction due to the\ninfeasibility of retraining large language models (LLMs) from scratch in the\nevent of new data availability. Of great interest is the domain-adaptive\npre-training (DAPT) paradigm, which focuses on continually training a\npre-trained language model to adapt it to a domain it was not originally\ntrained on. In this work, we evaluate the feasibility of DAPT in a low-resource\nsetting, namely the Nepali language. We use synthetic data to continue training\nLlama 3 8B to adapt it to the Nepali language in a 4-bit QLoRA setting. We\nevaluate the adapted model on its performance, forgetting, and knowledge\nacquisition. We compare the base model and the final model on their Nepali\ngeneration abilities, their performance on popular benchmarks, and run\ncase-studies to probe their linguistic knowledge in Nepali. We see some\nunsurprising forgetting in the final model, but also surprisingly find that\nincreasing the number of shots during evaluation yields better percent\nincreases in the final model (as high as 19.29% increase) compared to the base\nmodel (4.98%), suggesting latent retention. We also explore layer-head\nself-attention heatmaps to establish dependency resolution abilities of the\nfinal model in Nepali.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning has emerged as an important research direction due to the\ninfeasibility of retraining large language models (LLMs) from scratch in the\nevent of new data availability. Of great interest is the domain-adaptive\npre-training (DAPT) paradigm, which focuses on continually training a\npre-trained language model to adapt it to a domain it was not originally\ntrained on. In this work, we evaluate the feasibility of DAPT in a low-resource\nsetting, namely the Nepali language. We use synthetic data to continue training\nLlama 3 8B to adapt it to the Nepali language in a 4-bit QLoRA setting. We\nevaluate the adapted model on its performance, forgetting, and knowledge\nacquisition. We compare the base model and the final model on their Nepali\ngeneration abilities, their performance on popular benchmarks, and run\ncase-studies to probe their linguistic knowledge in Nepali. We see some\nunsurprising forgetting in the final model, but also surprisingly find that\nincreasing the number of shots during evaluation yields better percent\nincreases in the final model (as high as 19.29% increase) compared to the base\nmodel (4.98%), suggesting latent retention. We also explore layer-head\nself-attention heatmaps to establish dependency resolution abilities of the\nfinal model in Nepali."
                },
                "authors": [
                    {
                        "name": "Sharad Duwal"
                    },
                    {
                        "name": "Suraj Prasai"
                    },
                    {
                        "name": "Suresh Manandhar"
                    }
                ],
                "author_detail": {
                    "name": "Suresh Manandhar"
                },
                "author": "Suresh Manandhar",
                "arxiv_comment": "10 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13859v1",
                "updated": "2024-12-18T13:53:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    53,
                    16,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T13:53:16Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    53,
                    16,
                    2,
                    353,
                    0
                ],
                "title": "Zero-Shot Prompting and Few-Shot Fine-Tuning: Revisiting Document Image\n  Classification Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Prompting and Few-Shot Fine-Tuning: Revisiting Document Image\n  Classification Using Large Language Models"
                },
                "summary": "Classifying scanned documents is a challenging problem that involves image,\nlayout, and text analysis for document understanding. Nevertheless, for certain\nbenchmark datasets, notably RVL-CDIP, the state of the art is closing in to\nnear-perfect performance when considering hundreds of thousands of training\nsamples. With the advent of large language models (LLMs), which are excellent\nfew-shot learners, the question arises to what extent the document\nclassification problem can be addressed with only a few training samples, or\neven none at all. In this paper, we investigate this question in the context of\nzero-shot prompting and few-shot model fine-tuning, with the aim of reducing\nthe need for human-annotated training samples as much as possible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classifying scanned documents is a challenging problem that involves image,\nlayout, and text analysis for document understanding. Nevertheless, for certain\nbenchmark datasets, notably RVL-CDIP, the state of the art is closing in to\nnear-perfect performance when considering hundreds of thousands of training\nsamples. With the advent of large language models (LLMs), which are excellent\nfew-shot learners, the question arises to what extent the document\nclassification problem can be addressed with only a few training samples, or\neven none at all. In this paper, we investigate this question in the context of\nzero-shot prompting and few-shot model fine-tuning, with the aim of reducing\nthe need for human-annotated training samples as much as possible."
                },
                "authors": [
                    {
                        "name": "Anna Scius-Bertrand"
                    },
                    {
                        "name": "Michael Jungo"
                    },
                    {
                        "name": "Lars Vgtlin"
                    },
                    {
                        "name": "Jean-Marc Spat"
                    },
                    {
                        "name": "Andreas Fischer"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Fischer"
                },
                "author": "Andreas Fischer",
                "arxiv_doi": "10.1007/978-3-031-78495-8_10",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-78495-8_10",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.13859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ICPR 2024",
                "arxiv_journal_ref": "International Conference on Pattern Recognition - ICPR 2024, pp\n  152-166. Cham: Springer Nature Switzerland",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07682v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07682v3",
                "updated": "2024-12-18T13:39:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    39,
                    47,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-10T17:13:35Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    13,
                    35,
                    1,
                    345,
                    0
                ],
                "title": "TRIM: Token Reduction and Inference Modeling for Cost-Effective Language\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRIM: Token Reduction and Inference Modeling for Cost-Effective Language\n  Generation"
                },
                "summary": "The inference cost of Large Language Models (LLMs) is a significant challenge\ndue to their computational demands, specially on tasks requiring long outputs.\nHowever, natural language often contains redundancy, which presents an\nopportunity for optimization. We have observed that LLMs can generate distilled\nlanguage-concise outputs that retain essential meaning, when prompted\nappropriately. We propose TRIM, a pipeline for saving computational cost in\nwhich a shorter distilled output from the LLM is reconstructed into a full\nnarrative by a smaller model with lower inference costs. Our experiments show\npromising results, particularly in general knowledge domains with 20.58% saved\ntokens on average with tiny decrease in evaluation metrics, hinting that this\napproach can effectively balance efficiency and accuracy in language processing\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference cost of Large Language Models (LLMs) is a significant challenge\ndue to their computational demands, specially on tasks requiring long outputs.\nHowever, natural language often contains redundancy, which presents an\nopportunity for optimization. We have observed that LLMs can generate distilled\nlanguage-concise outputs that retain essential meaning, when prompted\nappropriately. We propose TRIM, a pipeline for saving computational cost in\nwhich a shorter distilled output from the LLM is reconstructed into a full\nnarrative by a smaller model with lower inference costs. Our experiments show\npromising results, particularly in general knowledge domains with 20.58% saved\ntokens on average with tiny decrease in evaluation metrics, hinting that this\napproach can effectively balance efficiency and accuracy in language processing\ntasks."
                },
                "authors": [
                    {
                        "name": "Alfredo Garrachn Ruiz"
                    },
                    {
                        "name": "Toms de la Rosa"
                    },
                    {
                        "name": "Daniel Borrajo"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Borrajo"
                },
                "author": "Daniel Borrajo",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07682v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07682v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13845v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13845v1",
                "updated": "2024-12-18T13:38:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    38,
                    6,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T13:38:06Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    38,
                    6,
                    2,
                    353,
                    0
                ],
                "title": "Do Language Models Understand Time?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Language Models Understand Time?"
                },
                "summary": "Large language models (LLMs) have revolutionized video-based computer vision\napplications, including action recognition, anomaly detection, and video\nsummarization. Videos inherently pose unique challenges, combining spatial\ncomplexity with temporal dynamics that are absent in static images or textual\ndata. Current approaches to video understanding with LLMs often rely on\npretrained video encoders to extract spatiotemporal features and text encoders\nto capture semantic meaning. These representations are integrated within LLM\nframeworks, enabling multimodal reasoning across diverse video tasks. However,\nthe critical question persists: Can LLMs truly understand the concept of time,\nand how effectively can they reason about temporal relationships in videos?\nThis work critically examines the role of LLMs in video processing, with a\nspecific focus on their temporal reasoning capabilities. We identify key\nlimitations in the interaction between LLMs and pretrained encoders, revealing\ngaps in their ability to model long-term dependencies and abstract temporal\nconcepts such as causality and event progression. Furthermore, we analyze\nchallenges posed by existing video datasets, including biases, lack of temporal\nannotations, and domain-specific limitations that constrain the temporal\nunderstanding of LLMs. To address these gaps, we explore promising future\ndirections, including the co-evolution of LLMs and encoders, the development of\nenriched datasets with explicit temporal labels, and innovative architectures\nfor integrating spatial, temporal, and semantic reasoning. By addressing these\nchallenges, we aim to advance the temporal comprehension of LLMs, unlocking\ntheir full potential in video analysis and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized video-based computer vision\napplications, including action recognition, anomaly detection, and video\nsummarization. Videos inherently pose unique challenges, combining spatial\ncomplexity with temporal dynamics that are absent in static images or textual\ndata. Current approaches to video understanding with LLMs often rely on\npretrained video encoders to extract spatiotemporal features and text encoders\nto capture semantic meaning. These representations are integrated within LLM\nframeworks, enabling multimodal reasoning across diverse video tasks. However,\nthe critical question persists: Can LLMs truly understand the concept of time,\nand how effectively can they reason about temporal relationships in videos?\nThis work critically examines the role of LLMs in video processing, with a\nspecific focus on their temporal reasoning capabilities. We identify key\nlimitations in the interaction between LLMs and pretrained encoders, revealing\ngaps in their ability to model long-term dependencies and abstract temporal\nconcepts such as causality and event progression. Furthermore, we analyze\nchallenges posed by existing video datasets, including biases, lack of temporal\nannotations, and domain-specific limitations that constrain the temporal\nunderstanding of LLMs. To address these gaps, we explore promising future\ndirections, including the co-evolution of LLMs and encoders, the development of\nenriched datasets with explicit temporal labels, and innovative architectures\nfor integrating spatial, temporal, and semantic reasoning. By addressing these\nchallenges, we aim to advance the temporal comprehension of LLMs, unlocking\ntheir full potential in video analysis and beyond."
                },
                "authors": [
                    {
                        "name": "Xi Ding"
                    },
                    {
                        "name": "Lei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Wang"
                },
                "author": "Lei Wang",
                "arxiv_comment": "Research report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13845v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13845v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13844v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13844v1",
                "updated": "2024-12-18T13:37:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    37,
                    36,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T13:37:36Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    37,
                    36,
                    2,
                    353,
                    0
                ],
                "title": "CRM: Retrieval Model with Controllable Condition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRM: Retrieval Model with Controllable Condition"
                },
                "summary": "Recommendation systems (RecSys) are designed to connect users with relevant\nitems from a vast pool of candidates while aligning with the business goals of\nthe platform. A typical industrial RecSys is composed of two main stages,\nretrieval and ranking: (1) the retrieval stage aims at searching hundreds of\nitem candidates satisfied user interests; (2) based on the retrieved items, the\nranking stage aims at selecting the best dozen items by multiple targets\nestimation for each item candidate, including classification and regression\ntargets. Compared with ranking model, the retrieval model absence of item\ncandidate information during inference, therefore retrieval models are often\ntrained by classification target only (e.g., click-through rate), but failed to\nincorporate regression target (e.g., the expected watch-time), which limit the\neffectiveness of retrieval. In this paper, we propose the Controllable\nRetrieval Model (CRM), which integrates regression information as conditional\nfeatures into the two-tower retrieval paradigm. This modification enables the\nretrieval stage could fulfill the target gap with ranking model, enhancing the\nretrieval model ability to search item candidates satisfied the user interests\nand condition effectively. We validate the effectiveness of CRM through\nreal-world A/B testing and demonstrate its successful deployment in Kuaishou\nshort-video recommendation system, which serves over 400 million users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommendation systems (RecSys) are designed to connect users with relevant\nitems from a vast pool of candidates while aligning with the business goals of\nthe platform. A typical industrial RecSys is composed of two main stages,\nretrieval and ranking: (1) the retrieval stage aims at searching hundreds of\nitem candidates satisfied user interests; (2) based on the retrieved items, the\nranking stage aims at selecting the best dozen items by multiple targets\nestimation for each item candidate, including classification and regression\ntargets. Compared with ranking model, the retrieval model absence of item\ncandidate information during inference, therefore retrieval models are often\ntrained by classification target only (e.g., click-through rate), but failed to\nincorporate regression target (e.g., the expected watch-time), which limit the\neffectiveness of retrieval. In this paper, we propose the Controllable\nRetrieval Model (CRM), which integrates regression information as conditional\nfeatures into the two-tower retrieval paradigm. This modification enables the\nretrieval stage could fulfill the target gap with ranking model, enhancing the\nretrieval model ability to search item candidates satisfied the user interests\nand condition effectively. We validate the effectiveness of CRM through\nreal-world A/B testing and demonstrate its successful deployment in Kuaishou\nshort-video recommendation system, which serves over 400 million users."
                },
                "authors": [
                    {
                        "name": "Chi Liu"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Kuo Cai"
                    },
                    {
                        "name": "Weifeng Ding"
                    },
                    {
                        "name": "Qiang Luo"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13844v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13840v1",
                "updated": "2024-12-18T13:33:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    33,
                    28,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T13:33:28Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    33,
                    28,
                    2,
                    353,
                    0
                ],
                "title": "Unleashing the Power of Continual Learning on Non-Centralized Devices: A\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Power of Continual Learning on Non-Centralized Devices: A\n  Survey"
                },
                "summary": "Non-Centralized Continual Learning (NCCL) has become an emerging paradigm for\nenabling distributed devices such as vehicles and servers to handle streaming\ndata from a joint non-stationary environment. To achieve high reliability and\nscalability in deploying this paradigm in distributed systems, it is essential\nto conquer challenges stemming from both spatial and temporal dimensions,\nmanifesting as distribution shifts, catastrophic forgetting, heterogeneity, and\nprivacy issues. This survey focuses on a comprehensive examination of the\ndevelopment of the non-centralized continual learning algorithms and the\nreal-world deployment across distributed devices. We begin with an introduction\nto the background and fundamentals of non-centralized learning and continual\nlearning. Then, we review existing solutions from three levels to represent how\nexisting techniques alleviate the catastrophic forgetting and distribution\nshift. Additionally, we delve into the various types of heterogeneity issues,\nsecurity, and privacy attributes, as well as real-world applications across\nthree prevalent scenarios. Furthermore, we establish a large-scale benchmark to\nrevisit this problem and analyze the performance of the state-of-the-art NCCL\napproaches. Finally, we discuss the important challenges and future research\ndirections in NCCL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-Centralized Continual Learning (NCCL) has become an emerging paradigm for\nenabling distributed devices such as vehicles and servers to handle streaming\ndata from a joint non-stationary environment. To achieve high reliability and\nscalability in deploying this paradigm in distributed systems, it is essential\nto conquer challenges stemming from both spatial and temporal dimensions,\nmanifesting as distribution shifts, catastrophic forgetting, heterogeneity, and\nprivacy issues. This survey focuses on a comprehensive examination of the\ndevelopment of the non-centralized continual learning algorithms and the\nreal-world deployment across distributed devices. We begin with an introduction\nto the background and fundamentals of non-centralized learning and continual\nlearning. Then, we review existing solutions from three levels to represent how\nexisting techniques alleviate the catastrophic forgetting and distribution\nshift. Additionally, we delve into the various types of heterogeneity issues,\nsecurity, and privacy attributes, as well as real-world applications across\nthree prevalent scenarios. Furthermore, we establish a large-scale benchmark to\nrevisit this problem and analyze the performance of the state-of-the-art NCCL\napproaches. Finally, we discuss the important challenges and future research\ndirections in NCCL."
                },
                "authors": [
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Wenchao Xu"
                    },
                    {
                        "name": "Tianzhe Xiao"
                    },
                    {
                        "name": "Hong Liu"
                    },
                    {
                        "name": "Minzhu Tu"
                    },
                    {
                        "name": "Yuying Wang"
                    },
                    {
                        "name": "Xin Yang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Shui Yu"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13835v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13835v1",
                "updated": "2024-12-18T13:25:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    25,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T13:25:11Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    25,
                    11,
                    2,
                    353,
                    0
                ],
                "title": "RACQUET: Unveiling the Dangers of Overlooked Referential Ambiguity in\n  Visual LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RACQUET: Unveiling the Dangers of Overlooked Referential Ambiguity in\n  Visual LLMs"
                },
                "summary": "Ambiguity resolution is key to effective communication. While humans\neffortlessly address ambiguity through conversational grounding strategies, the\nextent to which current language models can emulate these strategies remains\nunclear. In this work, we examine referential ambiguity in image-based question\nanswering by introducing RACQUET, a carefully curated dataset targeting\ndistinct aspects of ambiguity. Through a series of evaluations, we reveal\nsignificant limitations and problems of overconfidence of state-of-the-art\nlarge multimodal language models in addressing ambiguity in their responses.\nThe overconfidence issue becomes particularly relevant for RACQUET-BIAS, a\nsubset designed to analyze a critical yet underexplored problem: failing to\naddress ambiguity leads to stereotypical, socially biased responses. Our\nresults underscore the urgency of equipping models with robust strategies to\ndeal with uncertainty without resorting to undesirable stereotypes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ambiguity resolution is key to effective communication. While humans\neffortlessly address ambiguity through conversational grounding strategies, the\nextent to which current language models can emulate these strategies remains\nunclear. In this work, we examine referential ambiguity in image-based question\nanswering by introducing RACQUET, a carefully curated dataset targeting\ndistinct aspects of ambiguity. Through a series of evaluations, we reveal\nsignificant limitations and problems of overconfidence of state-of-the-art\nlarge multimodal language models in addressing ambiguity in their responses.\nThe overconfidence issue becomes particularly relevant for RACQUET-BIAS, a\nsubset designed to analyze a critical yet underexplored problem: failing to\naddress ambiguity leads to stereotypical, socially biased responses. Our\nresults underscore the urgency of equipping models with robust strategies to\ndeal with uncertainty without resorting to undesirable stereotypes."
                },
                "authors": [
                    {
                        "name": "Alberto Testoni"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "Raquel Fernndez"
                    }
                ],
                "author_detail": {
                    "name": "Raquel Fernndez"
                },
                "author": "Raquel Fernndez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13835v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13835v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13834v1",
                "updated": "2024-12-18T13:24:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    24,
                    9,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T13:24:09Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    24,
                    9,
                    2,
                    353,
                    0
                ],
                "title": "Maybe you are looking for CroQS: Cross-modal Query Suggestion for\n  Text-to-Image Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maybe you are looking for CroQS: Cross-modal Query Suggestion for\n  Text-to-Image Retrieval"
                },
                "summary": "Query suggestion, a technique widely adopted in information retrieval,\nenhances system interactivity and the browsing experience of document\ncollections. In cross-modal retrieval, many works have focused on retrieving\nrelevant items from natural language queries, while few have explored query\nsuggestion solutions. In this work, we address query suggestion in cross-modal\nretrieval, introducing a novel task that focuses on suggesting minimal textual\nmodifications needed to explore visually consistent subsets of the collection,\nfollowing the premise of ''Maybe you are looking for''. To facilitate the\nevaluation and development of methods, we present a tailored benchmark named\nCroQS. This dataset comprises initial queries, grouped result sets, and\nhuman-defined suggested queries for each group. We establish dedicated metrics\nto rigorously evaluate the performance of various methods on this task,\nmeasuring representativeness, cluster specificity, and similarity of the\nsuggested queries to the original ones. Baseline methods from related fields,\nsuch as image captioning and content summarization, are adapted for this task\nto provide reference performance scores. Although relatively far from human\nperformance, our experiments reveal that both LLM-based and captioning-based\nmethods achieve competitive results on CroQS, improving the recall on cluster\nspecificity by more than 115% and representativeness mAP by more than 52% with\nrespect to the initial query. The dataset, the implementation of the baseline\nmethods and the notebooks containing our experiments are available here:\nhttps://paciosoft.com/CroQS-benchmark/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query suggestion, a technique widely adopted in information retrieval,\nenhances system interactivity and the browsing experience of document\ncollections. In cross-modal retrieval, many works have focused on retrieving\nrelevant items from natural language queries, while few have explored query\nsuggestion solutions. In this work, we address query suggestion in cross-modal\nretrieval, introducing a novel task that focuses on suggesting minimal textual\nmodifications needed to explore visually consistent subsets of the collection,\nfollowing the premise of ''Maybe you are looking for''. To facilitate the\nevaluation and development of methods, we present a tailored benchmark named\nCroQS. This dataset comprises initial queries, grouped result sets, and\nhuman-defined suggested queries for each group. We establish dedicated metrics\nto rigorously evaluate the performance of various methods on this task,\nmeasuring representativeness, cluster specificity, and similarity of the\nsuggested queries to the original ones. Baseline methods from related fields,\nsuch as image captioning and content summarization, are adapted for this task\nto provide reference performance scores. Although relatively far from human\nperformance, our experiments reveal that both LLM-based and captioning-based\nmethods achieve competitive results on CroQS, improving the recall on cluster\nspecificity by more than 115% and representativeness mAP by more than 52% with\nrespect to the initial query. The dataset, the implementation of the baseline\nmethods and the notebooks containing our experiments are available here:\nhttps://paciosoft.com/CroQS-benchmark/"
                },
                "authors": [
                    {
                        "name": "Giacomo Pacini"
                    },
                    {
                        "name": "Fabio Carrara"
                    },
                    {
                        "name": "Nicola Messina"
                    },
                    {
                        "name": "Nicola Tonellotto"
                    },
                    {
                        "name": "Giuseppe Amato"
                    },
                    {
                        "name": "Fabrizio Falchi"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Falchi"
                },
                "author": "Fabrizio Falchi",
                "arxiv_comment": "15 pages, 5 figures. To be published as full paper in the Proceedings\n  of the European Conference on Information Retrieval (ECIR) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13823v1",
                "updated": "2024-12-18T13:11:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    11,
                    58,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T13:11:58Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    11,
                    58,
                    2,
                    353,
                    0
                ],
                "title": "Prompt Categories Cluster for Weakly Supervised Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Categories Cluster for Weakly Supervised Semantic Segmentation"
                },
                "summary": "Weakly Supervised Semantic Segmentation (WSSS), which leverages image-level\nlabels, has garnered significant attention due to its cost-effectiveness. The\nprevious methods mainly strengthen the inter-class differences to avoid class\nsemantic ambiguity which may lead to erroneous activation. However, they\noverlook the positive function of some shared information between similar\nclasses. Categories within the same cluster share some similar features.\nAllowing the model to recognize these features can further relieve the semantic\nambiguity between these classes. To effectively identify and utilize this\nshared information, in this paper, we introduce a novel WSSS framework called\nPrompt Categories Clustering (PCC). Specifically, we explore the ability of\nLarge Language Models (LLMs) to derive category clusters through prompts. These\nclusters effectively represent the intrinsic relationships between categories.\nBy integrating this relational information into the training network, our model\nis able to better learn the hidden connections between categories. Experimental\nresults demonstrate the effectiveness of our approach, showing its ability to\nenhance performance on the PASCAL VOC 2012 dataset and surpass existing\nstate-of-the-art methods in WSSS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weakly Supervised Semantic Segmentation (WSSS), which leverages image-level\nlabels, has garnered significant attention due to its cost-effectiveness. The\nprevious methods mainly strengthen the inter-class differences to avoid class\nsemantic ambiguity which may lead to erroneous activation. However, they\noverlook the positive function of some shared information between similar\nclasses. Categories within the same cluster share some similar features.\nAllowing the model to recognize these features can further relieve the semantic\nambiguity between these classes. To effectively identify and utilize this\nshared information, in this paper, we introduce a novel WSSS framework called\nPrompt Categories Clustering (PCC). Specifically, we explore the ability of\nLarge Language Models (LLMs) to derive category clusters through prompts. These\nclusters effectively represent the intrinsic relationships between categories.\nBy integrating this relational information into the training network, our model\nis able to better learn the hidden connections between categories. Experimental\nresults demonstrate the effectiveness of our approach, showing its ability to\nenhance performance on the PASCAL VOC 2012 dataset and surpass existing\nstate-of-the-art methods in WSSS."
                },
                "authors": [
                    {
                        "name": "Wangyu Wu"
                    },
                    {
                        "name": "Xianglin Qiu"
                    },
                    {
                        "name": "Siqi Song"
                    },
                    {
                        "name": "Xiaowei Huang"
                    },
                    {
                        "name": "Fei Ma"
                    },
                    {
                        "name": "Jimin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Jimin Xiao"
                },
                "author": "Jimin Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15105v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15105v2",
                "updated": "2024-12-18T13:08:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    8,
                    47,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-19T13:37:24Z",
                "published_parsed": [
                    2024,
                    10,
                    19,
                    13,
                    37,
                    24,
                    5,
                    293,
                    0
                ],
                "title": "Standardizing Generative Face Video Compression using Supplemental\n  Enhancement Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standardizing Generative Face Video Compression using Supplemental\n  Enhancement Information"
                },
                "summary": "This paper proposes a Generative Face Video Compression (GFVC) approach using\nSupplemental Enhancement Information (SEI), where a series of compact spatial\nand temporal representations of a face video signal (i.e., 2D/3D key-points,\nfacial semantics and compact features) can be coded using SEI message and\ninserted into the coded video bitstream. At the time of writing, the proposed\nGFVC approach using SEI messages has been adopted into the official working\ndraft of Versatile Supplemental Enhancement Information (VSEI) standard by the\nJoint Video Experts Team (JVET) of ISO/IEC JTC 1/SC 29 and ITU-T SG16, which\nwill be standardized as a new version for \"ITU-T H.274 | ISO/IEC 23002-7\". To\nthe best of the authors' knowledge, the JVET work on the proposed SEI-based\nGFVC approach is the first standardization activity for generative video\ncompression. The proposed SEI approach has not only advanced the reconstruction\nquality of early-day Model-Based Coding (MBC) via the state-of-the-art\ngenerative technique, but also established a new SEI definition for future GFVC\napplications and deployment. Experimental results illustrate that the proposed\nSEI-based GFVC approach can achieve remarkable rate-distortion performance\ncompared with the latest Versatile Video Coding (VVC) standard, whilst also\npotentially enabling a wide variety of functionalities including user-specified\nanimation/filtering and metaverse-related applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a Generative Face Video Compression (GFVC) approach using\nSupplemental Enhancement Information (SEI), where a series of compact spatial\nand temporal representations of a face video signal (i.e., 2D/3D key-points,\nfacial semantics and compact features) can be coded using SEI message and\ninserted into the coded video bitstream. At the time of writing, the proposed\nGFVC approach using SEI messages has been adopted into the official working\ndraft of Versatile Supplemental Enhancement Information (VSEI) standard by the\nJoint Video Experts Team (JVET) of ISO/IEC JTC 1/SC 29 and ITU-T SG16, which\nwill be standardized as a new version for \"ITU-T H.274 | ISO/IEC 23002-7\". To\nthe best of the authors' knowledge, the JVET work on the proposed SEI-based\nGFVC approach is the first standardization activity for generative video\ncompression. The proposed SEI approach has not only advanced the reconstruction\nquality of early-day Model-Based Coding (MBC) via the state-of-the-art\ngenerative technique, but also established a new SEI definition for future GFVC\napplications and deployment. Experimental results illustrate that the proposed\nSEI-based GFVC approach can achieve remarkable rate-distortion performance\ncompared with the latest Versatile Video Coding (VVC) standard, whilst also\npotentially enabling a wide variety of functionalities including user-specified\nanimation/filtering and metaverse-related applications."
                },
                "authors": [
                    {
                        "name": "Bolin Chen"
                    },
                    {
                        "name": "Yan Ye"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Ru-Ling Liao"
                    },
                    {
                        "name": "Shanzhi Yin"
                    },
                    {
                        "name": "Shiqi Wang"
                    },
                    {
                        "name": "Kaifa Yang"
                    },
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Yiling Xu"
                    },
                    {
                        "name": "Ye-Kui Wang"
                    },
                    {
                        "name": "Shiv Gehlot"
                    },
                    {
                        "name": "Guan-Ming Su"
                    },
                    {
                        "name": "Peng Yin"
                    },
                    {
                        "name": "Sean McCarthy"
                    },
                    {
                        "name": "Gary J. Sullivan"
                    }
                ],
                "author_detail": {
                    "name": "Gary J. Sullivan"
                },
                "author": "Gary J. Sullivan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15105v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15105v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13147v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13147v2",
                "updated": "2024-12-18T13:05:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    5,
                    24,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-17T18:12:47Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    12,
                    47,
                    1,
                    352,
                    0
                ],
                "title": "Are Your LLMs Capable of Stable Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Your LLMs Capable of Stable Reasoning?"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has demonstrated\nremarkable progress in complex reasoning tasks. However, a significant\ndiscrepancy persists between benchmark performances and real-world\napplications. We identify this gap as primarily stemming from current\nevaluation protocols and metrics, which inadequately capture the full spectrum\nof LLM capabilities, particularly in complex reasoning tasks where both\naccuracy and consistency are crucial. This work makes two key contributions.\nFirst, we introduce G-Pass@k, a novel evaluation metric that provides a\ncontinuous assessment of model performance across multiple sampling attempts,\nquantifying both the model's peak performance potential and its stability.\nSecond, we present LiveMathBench, a dynamic benchmark comprising challenging,\ncontemporary mathematical problems designed to minimize data leakage risks\nduring evaluation. Through extensive experiments using G-Pass@k on\nstate-of-the-art LLMs with LiveMathBench, we provide comprehensive insights\ninto both their maximum capabilities and operational consistency. Our findings\nreveal substantial room for improvement in LLMs' \"realistic\" reasoning\ncapabilities, highlighting the need for more robust evaluation methods. The\nbenchmark and detailed results are available at:\nhttps://github.com/open-compass/GPassK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has demonstrated\nremarkable progress in complex reasoning tasks. However, a significant\ndiscrepancy persists between benchmark performances and real-world\napplications. We identify this gap as primarily stemming from current\nevaluation protocols and metrics, which inadequately capture the full spectrum\nof LLM capabilities, particularly in complex reasoning tasks where both\naccuracy and consistency are crucial. This work makes two key contributions.\nFirst, we introduce G-Pass@k, a novel evaluation metric that provides a\ncontinuous assessment of model performance across multiple sampling attempts,\nquantifying both the model's peak performance potential and its stability.\nSecond, we present LiveMathBench, a dynamic benchmark comprising challenging,\ncontemporary mathematical problems designed to minimize data leakage risks\nduring evaluation. Through extensive experiments using G-Pass@k on\nstate-of-the-art LLMs with LiveMathBench, we provide comprehensive insights\ninto both their maximum capabilities and operational consistency. Our findings\nreveal substantial room for improvement in LLMs' \"realistic\" reasoning\ncapabilities, highlighting the need for more robust evaluation methods. The\nbenchmark and detailed results are available at:\nhttps://github.com/open-compass/GPassK."
                },
                "authors": [
                    {
                        "name": "Junnan Liu"
                    },
                    {
                        "name": "Hongwei Liu"
                    },
                    {
                        "name": "Linchen Xiao"
                    },
                    {
                        "name": "Ziyi Wang"
                    },
                    {
                        "name": "Kuikun Liu"
                    },
                    {
                        "name": "Songyang Gao"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13147v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13147v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13817v1",
                "updated": "2024-12-18T13:04:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    4,
                    30,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T13:04:30Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    4,
                    30,
                    2,
                    353,
                    0
                ],
                "title": "Nullu: Mitigating Object Hallucinations in Large Vision-Language Models\n  via HalluSpace Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nullu: Mitigating Object Hallucinations in Large Vision-Language Models\n  via HalluSpace Projection"
                },
                "summary": "Recent studies have shown that large vision-language models (LVLMs) often\nsuffer from the issue of object hallucinations (OH). To mitigate this issue, we\nintroduce an efficient method that edits the model weights based on an unsafe\nsubspace, which we call HalluSpace in this paper. With truthful and\nhallucinated text prompts accompanying the visual content as inputs, the\nHalluSpace can be identified by extracting the hallucinated embedding features\nand removing the truthful representations in LVLMs. By orthogonalizing the\nmodel weights, input features will be projected into the Null space of the\nHalluSpace to reduce OH, based on which we name our method Nullu. We reveal\nthat HalluSpaces generally contain statistical bias and unimodal priors of the\nlarge language models (LLMs) applied to build LVLMs, which have been shown as\nessential causes of OH in previous studies. Therefore, null space projection\nsuppresses the LLMs' priors to filter out the hallucinated features, resulting\nin contextually accurate outputs. Experiments show that our method can\neffectively mitigate OH across different LVLM families without extra inference\ncosts and also show strong performance in general LVLM benchmarks. Code is\nreleased at \\url{https://github.com/Ziwei-Zheng/Nullu}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have shown that large vision-language models (LVLMs) often\nsuffer from the issue of object hallucinations (OH). To mitigate this issue, we\nintroduce an efficient method that edits the model weights based on an unsafe\nsubspace, which we call HalluSpace in this paper. With truthful and\nhallucinated text prompts accompanying the visual content as inputs, the\nHalluSpace can be identified by extracting the hallucinated embedding features\nand removing the truthful representations in LVLMs. By orthogonalizing the\nmodel weights, input features will be projected into the Null space of the\nHalluSpace to reduce OH, based on which we name our method Nullu. We reveal\nthat HalluSpaces generally contain statistical bias and unimodal priors of the\nlarge language models (LLMs) applied to build LVLMs, which have been shown as\nessential causes of OH in previous studies. Therefore, null space projection\nsuppresses the LLMs' priors to filter out the hallucinated features, resulting\nin contextually accurate outputs. Experiments show that our method can\neffectively mitigate OH across different LVLM families without extra inference\ncosts and also show strong performance in general LVLM benchmarks. Code is\nreleased at \\url{https://github.com/Ziwei-Zheng/Nullu}."
                },
                "authors": [
                    {
                        "name": "Le Yang"
                    },
                    {
                        "name": "Ziwei Zheng"
                    },
                    {
                        "name": "Boxu Chen"
                    },
                    {
                        "name": "Zhengyu Zhao"
                    },
                    {
                        "name": "Chenhao Lin"
                    },
                    {
                        "name": "Chao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chao Shen"
                },
                "author": "Chao Shen",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11156v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11156v4",
                "updated": "2024-12-18T12:48:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    48,
                    37,
                    2,
                    353,
                    0
                ],
                "published": "2024-06-17T02:47:09Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    2,
                    47,
                    9,
                    0,
                    169,
                    0
                ],
                "title": "DELRec: Distilling Sequential Pattern to Enhance LLMs-based Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DELRec: Distilling Sequential Pattern to Enhance LLMs-based Sequential\n  Recommendation"
                },
                "summary": "Sequential recommendation (SR) tasks aim to predict users' next interaction\nby learning their behavior sequence and capturing the connection between users'\npast interactions and their changing preferences. Conventional SR models often\nfocus solely on capturing sequential patterns within the training data,\nneglecting the broader context and semantic information embedded in item titles\nfrom external sources. This limits their predictive power and adaptability.\nLarge language models (LLMs) have recently shown promise in SR tasks due to\ntheir advanced understanding capabilities and strong generalization abilities.\nResearchers have attempted to enhance LLMs-based recommendation performance by\nincorporating information from conventional SR models. However, previous\napproaches have encountered problems such as 1) limited textual information\nleading to poor recommendation performance, 2) incomplete understanding and\nutilization of conventional SR model information by LLMs, and 3) excessive\ncomplexity and low interpretability of LLMs-based methods. To improve the\nperformance of LLMs-based SR, we propose a novel framework, Distilling\nSequential Pattern to Enhance LLMs-based Sequential Recommendation (DELRec),\nwhich aims to extract knowledge from conventional SR models and enable LLMs to\neasily comprehend and utilize the extracted knowledge for more effective SRs.\nDELRec consists of two main stages: 1) Distill Pattern from Conventional SR\nModels, focusing on extracting behavioral patterns exhibited by conventional SR\nmodels using soft prompts through two well-designed strategies; 2) LLMs-based\nSequential Recommendation, aiming to fine-tune LLMs to effectively use the\ndistilled auxiliary information to perform SR tasks. Extensive experimental\nresults conducted on four real datasets validate the effectiveness of the\nDELRec framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommendation (SR) tasks aim to predict users' next interaction\nby learning their behavior sequence and capturing the connection between users'\npast interactions and their changing preferences. Conventional SR models often\nfocus solely on capturing sequential patterns within the training data,\nneglecting the broader context and semantic information embedded in item titles\nfrom external sources. This limits their predictive power and adaptability.\nLarge language models (LLMs) have recently shown promise in SR tasks due to\ntheir advanced understanding capabilities and strong generalization abilities.\nResearchers have attempted to enhance LLMs-based recommendation performance by\nincorporating information from conventional SR models. However, previous\napproaches have encountered problems such as 1) limited textual information\nleading to poor recommendation performance, 2) incomplete understanding and\nutilization of conventional SR model information by LLMs, and 3) excessive\ncomplexity and low interpretability of LLMs-based methods. To improve the\nperformance of LLMs-based SR, we propose a novel framework, Distilling\nSequential Pattern to Enhance LLMs-based Sequential Recommendation (DELRec),\nwhich aims to extract knowledge from conventional SR models and enable LLMs to\neasily comprehend and utilize the extracted knowledge for more effective SRs.\nDELRec consists of two main stages: 1) Distill Pattern from Conventional SR\nModels, focusing on extracting behavioral patterns exhibited by conventional SR\nmodels using soft prompts through two well-designed strategies; 2) LLMs-based\nSequential Recommendation, aiming to fine-tune LLMs to effectively use the\ndistilled auxiliary information to perform SR tasks. Extensive experimental\nresults conducted on four real datasets validate the effectiveness of the\nDELRec framework."
                },
                "authors": [
                    {
                        "name": "Haoyi Zhang"
                    },
                    {
                        "name": "Guohao Sun"
                    },
                    {
                        "name": "Jinhu Lu"
                    },
                    {
                        "name": "Guanfeng Liu"
                    },
                    {
                        "name": "Xiu Susie Fang"
                    }
                ],
                "author_detail": {
                    "name": "Xiu Susie Fang"
                },
                "author": "Xiu Susie Fang",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11156v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11156v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13801v1",
                "updated": "2024-12-18T12:48:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    48,
                    36,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:48:36Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    48,
                    36,
                    2,
                    353,
                    0
                ],
                "title": "A Comprehensive Evaluation of Parameter-Efficient Fine-Tuning on\n  Method-Level Code Smell Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Evaluation of Parameter-Efficient Fine-Tuning on\n  Method-Level Code Smell Detection"
                },
                "summary": "Code smells are suboptimal coding practices that negatively impact the\nquality of software systems. Existing detection methods, relying on heuristics\nor Machine Learning (ML) and Deep Learning (DL) techniques, often face\nlimitations such as unsatisfactory performance. Parameter-Efficient Fine-Tuning\n(PEFT) methods have emerged as a resource-efficient approach for adapting LLMs\nto specific tasks, but their effectiveness for method-level code smell\ndetection remains underexplored. In this regard, this study evaluates\nstate-of-the-art PEFT methods on both small and large Language Models (LMs) for\ndetecting two types of method-level code smells: Complex Conditional and\nComplex Method. Using high-quality datasets sourced from GitHub, we fine-tuned\nfour small LMs and six LLMs with PEFT techniques, including prompt tuning,\nprefix tuning, LoRA, and (IA)3. Results show that PEFT methods achieve\ncomparable or better performance than full fine-tuning while consuming less GPU\nmemory. Notably, LLMs did not outperform small LMs, suggesting smaller models'\nsuitability for this task. Additionally, increasing training dataset size\nsignificantly boosted performance, while increasing trainable parameters did\nnot. Our findings highlight PEFT methods as effective and scalable solutions,\noutperforming existing heuristic-based and DL-based detectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code smells are suboptimal coding practices that negatively impact the\nquality of software systems. Existing detection methods, relying on heuristics\nor Machine Learning (ML) and Deep Learning (DL) techniques, often face\nlimitations such as unsatisfactory performance. Parameter-Efficient Fine-Tuning\n(PEFT) methods have emerged as a resource-efficient approach for adapting LLMs\nto specific tasks, but their effectiveness for method-level code smell\ndetection remains underexplored. In this regard, this study evaluates\nstate-of-the-art PEFT methods on both small and large Language Models (LMs) for\ndetecting two types of method-level code smells: Complex Conditional and\nComplex Method. Using high-quality datasets sourced from GitHub, we fine-tuned\nfour small LMs and six LLMs with PEFT techniques, including prompt tuning,\nprefix tuning, LoRA, and (IA)3. Results show that PEFT methods achieve\ncomparable or better performance than full fine-tuning while consuming less GPU\nmemory. Notably, LLMs did not outperform small LMs, suggesting smaller models'\nsuitability for this task. Additionally, increasing training dataset size\nsignificantly boosted performance, while increasing trainable parameters did\nnot. Our findings highlight PEFT methods as effective and scalable solutions,\noutperforming existing heuristic-based and DL-based detectors."
                },
                "authors": [
                    {
                        "name": "Beiqi Zhang"
                    },
                    {
                        "name": "Peng Liang"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Xiyu Zhou"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Qiong Feng"
                    },
                    {
                        "name": "Zengyang Li"
                    },
                    {
                        "name": "Lin Li"
                    }
                ],
                "author_detail": {
                    "name": "Lin Li"
                },
                "author": "Lin Li",
                "arxiv_comment": "22 pages, 7 images, 8 tables, Manuscript submitted to a journal\n  (2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13795v1",
                "updated": "2024-12-18T12:39:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    39,
                    53,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:39:53Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    39,
                    53,
                    2,
                    353,
                    0
                ],
                "title": "Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and\n  Post-LN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and\n  Post-LN"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success, yet recent\nfindings reveal that their deeper layers often contribute minimally and can be\npruned without affecting overall performance. While some view this as an\nopportunity for model compression, we identify it as a training shortfall\nrooted in the widespread use of Pre-Layer Normalization (Pre-LN). We\ndemonstrate that Pre-LN, commonly employed in models like GPT and LLaMA, leads\nto diminished gradient norms in its deeper layers, reducing their\neffectiveness. In contrast, Post-Layer Normalization (Post-LN) preserves larger\ngradient norms in deeper layers but suffers from vanishing gradients in earlier\nlayers. To address this, we introduce Mix-LN, a novel normalization technique\nthat combines the strengths of Pre-LN and Post-LN within the same model. Mix-LN\napplies Post-LN to the earlier layers and Pre-LN to the deeper layers, ensuring\nmore uniform gradients across layers. This allows all parts of the\nnetwork--both shallow and deep layers--to contribute effectively to training.\nExtensive experiments with various model sizes from 70M to 7B demonstrate that\nMix-LN consistently outperforms both Pre-LN and Post-LN, promoting more\nbalanced, healthier gradient norms throughout the network, and enhancing the\noverall quality of LLM pre-training. Furthermore, we demonstrate that models\npre-trained with Mix-LN learn better compared to those using Pre-LN or Post-LN\nduring supervised fine-tuning (SFT) and reinforcement learning from human\nfeedback (RLHF), highlighting the critical importance of high-quality deep\nlayers. By effectively addressing the inefficiencies of deep layers in current\nLLMs, Mix-LN unlocks their potential, enhancing model capacity without\nincreasing model size. Our code is available at\nhttps://github.com/pixeli99/MixLN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success, yet recent\nfindings reveal that their deeper layers often contribute minimally and can be\npruned without affecting overall performance. While some view this as an\nopportunity for model compression, we identify it as a training shortfall\nrooted in the widespread use of Pre-Layer Normalization (Pre-LN). We\ndemonstrate that Pre-LN, commonly employed in models like GPT and LLaMA, leads\nto diminished gradient norms in its deeper layers, reducing their\neffectiveness. In contrast, Post-Layer Normalization (Post-LN) preserves larger\ngradient norms in deeper layers but suffers from vanishing gradients in earlier\nlayers. To address this, we introduce Mix-LN, a novel normalization technique\nthat combines the strengths of Pre-LN and Post-LN within the same model. Mix-LN\napplies Post-LN to the earlier layers and Pre-LN to the deeper layers, ensuring\nmore uniform gradients across layers. This allows all parts of the\nnetwork--both shallow and deep layers--to contribute effectively to training.\nExtensive experiments with various model sizes from 70M to 7B demonstrate that\nMix-LN consistently outperforms both Pre-LN and Post-LN, promoting more\nbalanced, healthier gradient norms throughout the network, and enhancing the\noverall quality of LLM pre-training. Furthermore, we demonstrate that models\npre-trained with Mix-LN learn better compared to those using Pre-LN or Post-LN\nduring supervised fine-tuning (SFT) and reinforcement learning from human\nfeedback (RLHF), highlighting the critical importance of high-quality deep\nlayers. By effectively addressing the inefficiencies of deep layers in current\nLLMs, Mix-LN unlocks their potential, enhancing model capacity without\nincreasing model size. Our code is available at\nhttps://github.com/pixeli99/MixLN."
                },
                "authors": [
                    {
                        "name": "Pengxiang Li"
                    },
                    {
                        "name": "Lu Yin"
                    },
                    {
                        "name": "Shiwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Shiwei Liu"
                },
                "author": "Shiwei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13791v1",
                "updated": "2024-12-18T12:33:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    33,
                    50,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:33:50Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    33,
                    50,
                    2,
                    353,
                    0
                ],
                "title": "Physics Reasoner: Knowledge-Augmented Reasoning for Solving Physics\n  Problems with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics Reasoner: Knowledge-Augmented Reasoning for Solving Physics\n  Problems with Large Language Models"
                },
                "summary": "Physics problems constitute a significant aspect of reasoning, necessitating\ncomplicated reasoning ability and abundant physics knowledge. However, existing\nlarge language models (LLMs) frequently fail due to a lack of knowledge or\nincorrect knowledge application. To mitigate these issues, we propose Physics\nReasoner, a knowledge-augmented framework to solve physics problems with LLMs.\nSpecifically, the proposed framework constructs a comprehensive formula set to\nprovide explicit physics knowledge and utilizes checklists containing detailed\ninstructions to guide effective knowledge application. Namely, given a physics\nproblem, Physics Reasoner solves it through three stages: problem analysis,\nformula retrieval, and guided reasoning. During the process, checklists are\nemployed to enhance LLMs' self-improvement in the analysis and reasoning\nstages. Empirically, Physics Reasoner mitigates the issues of insufficient\nknowledge and incorrect application, achieving state-of-the-art performance on\nSciBench with an average accuracy improvement of 5.8%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics problems constitute a significant aspect of reasoning, necessitating\ncomplicated reasoning ability and abundant physics knowledge. However, existing\nlarge language models (LLMs) frequently fail due to a lack of knowledge or\nincorrect knowledge application. To mitigate these issues, we propose Physics\nReasoner, a knowledge-augmented framework to solve physics problems with LLMs.\nSpecifically, the proposed framework constructs a comprehensive formula set to\nprovide explicit physics knowledge and utilizes checklists containing detailed\ninstructions to guide effective knowledge application. Namely, given a physics\nproblem, Physics Reasoner solves it through three stages: problem analysis,\nformula retrieval, and guided reasoning. During the process, checklists are\nemployed to enhance LLMs' self-improvement in the analysis and reasoning\nstages. Empirically, Physics Reasoner mitigates the issues of insufficient\nknowledge and incorrect application, achieving state-of-the-art performance on\nSciBench with an average accuracy improvement of 5.8%."
                },
                "authors": [
                    {
                        "name": "Xinyu Pang"
                    },
                    {
                        "name": "Ruixin Hong"
                    },
                    {
                        "name": "Zhanke Zhou"
                    },
                    {
                        "name": "Fangrui Lv"
                    },
                    {
                        "name": "Xinwei Yang"
                    },
                    {
                        "name": "Zhilong Liang"
                    },
                    {
                        "name": "Bo Han"
                    },
                    {
                        "name": "Changshui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Changshui Zhang"
                },
                "author": "Changshui Zhang",
                "arxiv_comment": "COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13782v1",
                "updated": "2024-12-18T12:21:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    21,
                    46,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:21:46Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    21,
                    46,
                    2,
                    353,
                    0
                ],
                "title": "Knowledge Editing with Dynamic Knowledge Graphs for Multi-hop Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Editing with Dynamic Knowledge Graphs for Multi-hop Question\n  Answering"
                },
                "summary": "Multi-hop question answering (MHQA) poses a significant challenge for large\nlanguage models (LLMs) due to the extensive knowledge demands involved.\nKnowledge editing, which aims to precisely modify the LLMs to incorporate\nspecific knowledge without negatively impacting other unrelated knowledge,\noffers a potential solution for addressing MHQA challenges with LLMs. However,\ncurrent solutions struggle to effectively resolve issues of knowledge\nconflicts. Most parameter-preserving editing methods are hindered by inaccurate\nretrieval and overlook secondary editing issues, which can introduce noise into\nthe reasoning process of LLMs. In this paper, we introduce KEDKG, a novel\nknowledge editing method that leverages a dynamic knowledge graph for MHQA,\ndesigned to ensure the reliability of answers. KEDKG involves two primary\nsteps: dynamic knowledge graph construction and knowledge graph augmented\ngeneration. Initially, KEDKG autonomously constructs a dynamic knowledge graph\nto store revised information while resolving potential knowledge conflicts.\nSubsequently, it employs a fine-grained retrieval strategy coupled with an\nentity and relation detector to enhance the accuracy of graph retrieval for LLM\ngeneration. Experimental results on benchmarks show that KEDKG surpasses\nprevious state-of-the-art models, delivering more accurate and reliable answers\nin environments with dynamic information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-hop question answering (MHQA) poses a significant challenge for large\nlanguage models (LLMs) due to the extensive knowledge demands involved.\nKnowledge editing, which aims to precisely modify the LLMs to incorporate\nspecific knowledge without negatively impacting other unrelated knowledge,\noffers a potential solution for addressing MHQA challenges with LLMs. However,\ncurrent solutions struggle to effectively resolve issues of knowledge\nconflicts. Most parameter-preserving editing methods are hindered by inaccurate\nretrieval and overlook secondary editing issues, which can introduce noise into\nthe reasoning process of LLMs. In this paper, we introduce KEDKG, a novel\nknowledge editing method that leverages a dynamic knowledge graph for MHQA,\ndesigned to ensure the reliability of answers. KEDKG involves two primary\nsteps: dynamic knowledge graph construction and knowledge graph augmented\ngeneration. Initially, KEDKG autonomously constructs a dynamic knowledge graph\nto store revised information while resolving potential knowledge conflicts.\nSubsequently, it employs a fine-grained retrieval strategy coupled with an\nentity and relation detector to enhance the accuracy of graph retrieval for LLM\ngeneration. Experimental results on benchmarks show that KEDKG surpasses\nprevious state-of-the-art models, delivering more accurate and reliable answers\nin environments with dynamic information."
                },
                "authors": [
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Yigeng Zhou"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Yequan Wang"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Daojing He"
                    },
                    {
                        "name": "Fangming Liu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13781v1",
                "updated": "2024-12-18T12:20:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    20,
                    4,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:20:04Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    20,
                    4,
                    2,
                    353,
                    0
                ],
                "title": "Meta-Reflection: A Feedback-Free Reflection Learning Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-Reflection: A Feedback-Free Reflection Learning Framework"
                },
                "summary": "Despite the remarkable capabilities of large language models (LLMs) in\nnatural language understanding and reasoning, they often display undesirable\nbehaviors, such as generating hallucinations and unfaithful reasoning. A\nprevalent strategy to mitigate these issues is the use of reflection, which\nrefines responses through an iterative process. However, while promising,\nreflection heavily relies on high-quality external feedback and requires\niterative multi-agent inference processes, thus hindering its practical\napplication. In this paper, we propose Meta-Reflection, a novel feedback-free\nreflection mechanism that necessitates only a single inference pass without\nexternal feedback. Motivated by the human ability to remember and retrieve\nreflections from past experiences when encountering similar problems,\nMeta-Reflection integrates reflective insights into a codebook, allowing the\nhistorical insights to be stored, retrieved, and used to guide LLMs in\nproblem-solving. To thoroughly investigate and evaluate the practicality of\nMeta-Reflection in real-world scenarios, we introduce an industrial e-commerce\nbenchmark named E-commerce Customer Intent Detection (ECID). Extensive\nexperiments conducted on both public datasets and the ECID benchmark highlight\nthe effectiveness and efficiency of our proposed approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable capabilities of large language models (LLMs) in\nnatural language understanding and reasoning, they often display undesirable\nbehaviors, such as generating hallucinations and unfaithful reasoning. A\nprevalent strategy to mitigate these issues is the use of reflection, which\nrefines responses through an iterative process. However, while promising,\nreflection heavily relies on high-quality external feedback and requires\niterative multi-agent inference processes, thus hindering its practical\napplication. In this paper, we propose Meta-Reflection, a novel feedback-free\nreflection mechanism that necessitates only a single inference pass without\nexternal feedback. Motivated by the human ability to remember and retrieve\nreflections from past experiences when encountering similar problems,\nMeta-Reflection integrates reflective insights into a codebook, allowing the\nhistorical insights to be stored, retrieved, and used to guide LLMs in\nproblem-solving. To thoroughly investigate and evaluate the practicality of\nMeta-Reflection in real-world scenarios, we introduce an industrial e-commerce\nbenchmark named E-commerce Customer Intent Detection (ECID). Extensive\nexperiments conducted on both public datasets and the ECID benchmark highlight\nthe effectiveness and efficiency of our proposed approach."
                },
                "authors": [
                    {
                        "name": "Yaoke Wang"
                    },
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Xintong Bao"
                    },
                    {
                        "name": "Wenqiao Zhang"
                    },
                    {
                        "name": "Suyang Dai"
                    },
                    {
                        "name": "Kehan Chen"
                    },
                    {
                        "name": "Wenqiang Li"
                    },
                    {
                        "name": "Gang Huang"
                    },
                    {
                        "name": "Siliang Tang"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13774v1",
                "updated": "2024-12-18T12:11:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    11,
                    39,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:11:39Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    11,
                    39,
                    2,
                    353,
                    0
                ],
                "title": "Designing an LLM-Based Copilot for Manufacturing Equipment Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing an LLM-Based Copilot for Manufacturing Equipment Selection"
                },
                "summary": "Effective decision-making in automation equipment selection is critical for\nreducing ramp-up time and maintaining production quality, especially in the\nface of increasing product variation and market demands. However, limited\nexpertise and resource constraints often result in inefficiencies during the\nramp-up phase when new products are integrated into production lines. Existing\nmethods often lack structured and tailored solutions to support automation\nengineers in reducing ramp-up time, leading to compromises in quality. This\nresearch investigates whether large-language models (LLMs), combined with\nRetrieval-Augmented Generation (RAG), can assist in streamlining equipment\nselection in ramp-up planning. We propose a factual-driven copilot integrating\nLLMs with structured and semi-structured knowledge retrieval for three\ncomponent types (robots, feeders and vision systems), providing a guided and\ntraceable state-machine process for decision-making in automation equipment\nselection. The system was demonstrated to an industrial partner, who tested it\non three internal use-cases. Their feedback affirmed its capability to provide\nlogical and actionable recommendations for automation equipment. More\nspecifically, among 22 equipment prompts analyzed, 19 involved selecting the\ncorrect equipment while considering most requirements, and in 6 cases, all\nrequirements were fully met.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective decision-making in automation equipment selection is critical for\nreducing ramp-up time and maintaining production quality, especially in the\nface of increasing product variation and market demands. However, limited\nexpertise and resource constraints often result in inefficiencies during the\nramp-up phase when new products are integrated into production lines. Existing\nmethods often lack structured and tailored solutions to support automation\nengineers in reducing ramp-up time, leading to compromises in quality. This\nresearch investigates whether large-language models (LLMs), combined with\nRetrieval-Augmented Generation (RAG), can assist in streamlining equipment\nselection in ramp-up planning. We propose a factual-driven copilot integrating\nLLMs with structured and semi-structured knowledge retrieval for three\ncomponent types (robots, feeders and vision systems), providing a guided and\ntraceable state-machine process for decision-making in automation equipment\nselection. The system was demonstrated to an industrial partner, who tested it\non three internal use-cases. Their feedback affirmed its capability to provide\nlogical and actionable recommendations for automation equipment. More\nspecifically, among 22 equipment prompts analyzed, 19 involved selecting the\ncorrect equipment while considering most requirements, and in 6 cases, all\nrequirements were fully met."
                },
                "authors": [
                    {
                        "name": "Jonas Werheid"
                    },
                    {
                        "name": "Oleksandr Melnychuk"
                    },
                    {
                        "name": "Hans Zhou"
                    },
                    {
                        "name": "Meike Huber"
                    },
                    {
                        "name": "Christoph Rippe"
                    },
                    {
                        "name": "Dominik Joosten"
                    },
                    {
                        "name": "Zozan Keskin"
                    },
                    {
                        "name": "Max Wittstamm"
                    },
                    {
                        "name": "Sathya Subramani"
                    },
                    {
                        "name": "Benny Drescher"
                    },
                    {
                        "name": "Amon Gppert"
                    },
                    {
                        "name": "Anas Abdelrazeq"
                    },
                    {
                        "name": "Robert H. Schmitt"
                    }
                ],
                "author_detail": {
                    "name": "Robert H. Schmitt"
                },
                "author": "Robert H. Schmitt",
                "arxiv_comment": "Preprint submitted to Manufacturing Letters (MFGLET)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13771v1",
                "updated": "2024-12-18T12:07:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:07:58Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "title": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization"
                },
                "summary": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems."
                },
                "authors": [
                    {
                        "name": "Guanghan Li"
                    },
                    {
                        "name": "Xun Zhang"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Yifan Yin"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "arxiv_comment": "7 pages, 3 figures, AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05200v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05200v3",
                "updated": "2024-12-18T12:07:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    27,
                    2,
                    353,
                    0
                ],
                "published": "2024-08-09T17:44:45Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    17,
                    44,
                    45,
                    4,
                    222,
                    0
                ],
                "title": "KlF: Knowledge Localization and Fusion for Language Model Continual\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KlF: Knowledge Localization and Fusion for Language Model Continual\n  Learning"
                },
                "summary": "Language model continual learning (CL) has recently attracted significant\ninterest for its ability to adapt large language models (LLMs) to dynamic\nreal-world scenarios without retraining. A major challenge in this domain is\ncatastrophic forgetting, where models lose previously acquired knowledge upon\nlearning new tasks. Existing approaches commonly utilize multiple\nparameter-efficient fine-tuning (PEFT) blocks to acquire task-specific\nknowledge, yet these methods are inefficient and fail to leverage potential\nknowledge transfer across tasks. In this paper, we introduce a novel CL\nframework for language models, named Knowledge Localization and Fusion (KlF),\nwhich boosts knowledge transfer without depending on memory replay. KlF\ninitially segregates the model into 'skill units' based on parameter\ndependencies, allowing for more precise control. Subsequently, it employs a\nnovel group-wise knowledge localization technique to ascertain the importance\ndistribution of skill units for a new task. By comparing this importance\ndistribution with those from previous tasks, we implement a fine-grained\nknowledge fusion strategy that retains task-specific knowledge, thereby\npreventing forgetting, and updates task-shared knowledge, which facilitates\nbi-directional knowledge transfer. As a result, KlF achieves an optimal balance\nbetween retaining prior knowledge and excelling in new tasks. KlF also\ndemonstrates strong generalizability, making it suitable for various base\nmodels and adaptable to PEFT methods like LoRA. Furthermore, it offers notable\nextensibility, supporting enhancements through integration with memory replay\ntechniques. Comprehensive experiments conducted on two CL benchmarks, involving\nmodels ranging from 220M to 7B parameters, affirm the effectiveness of KlF and\nits variants across different settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language model continual learning (CL) has recently attracted significant\ninterest for its ability to adapt large language models (LLMs) to dynamic\nreal-world scenarios without retraining. A major challenge in this domain is\ncatastrophic forgetting, where models lose previously acquired knowledge upon\nlearning new tasks. Existing approaches commonly utilize multiple\nparameter-efficient fine-tuning (PEFT) blocks to acquire task-specific\nknowledge, yet these methods are inefficient and fail to leverage potential\nknowledge transfer across tasks. In this paper, we introduce a novel CL\nframework for language models, named Knowledge Localization and Fusion (KlF),\nwhich boosts knowledge transfer without depending on memory replay. KlF\ninitially segregates the model into 'skill units' based on parameter\ndependencies, allowing for more precise control. Subsequently, it employs a\nnovel group-wise knowledge localization technique to ascertain the importance\ndistribution of skill units for a new task. By comparing this importance\ndistribution with those from previous tasks, we implement a fine-grained\nknowledge fusion strategy that retains task-specific knowledge, thereby\npreventing forgetting, and updates task-shared knowledge, which facilitates\nbi-directional knowledge transfer. As a result, KlF achieves an optimal balance\nbetween retaining prior knowledge and excelling in new tasks. KlF also\ndemonstrates strong generalizability, making it suitable for various base\nmodels and adaptable to PEFT methods like LoRA. Furthermore, it offers notable\nextensibility, supporting enhancements through integration with memory replay\ntechniques. Comprehensive experiments conducted on two CL benchmarks, involving\nmodels ranging from 220M to 7B parameters, affirm the effectiveness of KlF and\nits variants across different settings."
                },
                "authors": [
                    {
                        "name": "Yujie Feng"
                    },
                    {
                        "name": "Xu Chu"
                    },
                    {
                        "name": "Yongxin Xu"
                    },
                    {
                        "name": "Zexin Lu"
                    },
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ming Wu"
                },
                "author": "Xiao-Ming Wu",
                "arxiv_comment": "This version updates the model name from Task Skill Localization and\n  Consolidation (TaSL) to Knowledge Localization and Fusion (KlF). It is an\n  extension of the ACL 2024 paper titled Continual Dialog State Tracking via\n  Task Skill Localization and Consolidation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05200v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05200v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13765v1",
                "updated": "2024-12-18T12:01:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    1,
                    53,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:01:53Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    1,
                    53,
                    2,
                    353,
                    0
                ],
                "title": "LLM-SEM: A Sentiment-Based Student Engagement Metric Using LLMS for\n  E-Learning Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-SEM: A Sentiment-Based Student Engagement Metric Using LLMS for\n  E-Learning Platforms"
                },
                "summary": "Current methods for analyzing student engagement in e-learning platforms,\nincluding automated systems, often struggle with challenges such as handling\nfuzzy sentiment in text comments and relying on limited metadata. Traditional\napproaches, such as surveys and questionnaires, also face issues like small\nsample sizes and scalability. In this paper, we introduce LLM-SEM (Language\nModel-Based Student Engagement Metric), a novel approach that leverages video\nmetadata and sentiment analysis of student comments to measure engagement. By\nutilizing recent Large Language Models (LLMs), we generate high-quality\nsentiment predictions to mitigate text fuzziness and normalize key features\nsuch as views and likes. Our holistic method combines comprehensive metadata\nwith sentiment polarity scores to gauge engagement at both the course and\nlesson levels. Extensive experiments were conducted to evaluate various LLM\nmodels, demonstrating the effectiveness of LLM-SEM in providing a scalable and\naccurate measure of student engagement. We fine-tuned LLMs, including AraBERT,\nTXLM-RoBERTa, LLama 3B and Gemma 9B from Ollama, using human-annotated\nsentiment datasets to enhance prediction accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current methods for analyzing student engagement in e-learning platforms,\nincluding automated systems, often struggle with challenges such as handling\nfuzzy sentiment in text comments and relying on limited metadata. Traditional\napproaches, such as surveys and questionnaires, also face issues like small\nsample sizes and scalability. In this paper, we introduce LLM-SEM (Language\nModel-Based Student Engagement Metric), a novel approach that leverages video\nmetadata and sentiment analysis of student comments to measure engagement. By\nutilizing recent Large Language Models (LLMs), we generate high-quality\nsentiment predictions to mitigate text fuzziness and normalize key features\nsuch as views and likes. Our holistic method combines comprehensive metadata\nwith sentiment polarity scores to gauge engagement at both the course and\nlesson levels. Extensive experiments were conducted to evaluate various LLM\nmodels, demonstrating the effectiveness of LLM-SEM in providing a scalable and\naccurate measure of student engagement. We fine-tuned LLMs, including AraBERT,\nTXLM-RoBERTa, LLama 3B and Gemma 9B from Ollama, using human-annotated\nsentiment datasets to enhance prediction accuracy."
                },
                "authors": [
                    {
                        "name": "Ali Hamdi"
                    },
                    {
                        "name": "Ahmed Abdelmoneim Mazrou"
                    },
                    {
                        "name": "Mohamed Shaltout"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Shaltout"
                },
                "author": "Mohamed Shaltout",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13746v1",
                "updated": "2024-12-18T11:28:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    11,
                    28,
                    5,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T11:28:05Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    11,
                    28,
                    5,
                    2,
                    353,
                    0
                ],
                "title": "RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented\n  Generation for Preference Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented\n  Generation for Preference Alignment"
                },
                "summary": "Despite the significant progress made by existing retrieval augmented\nlanguage models (RALMs) in providing trustworthy responses and grounding in\nreliable sources, they often overlook effective alignment with human\npreferences. In the alignment process, reward models (RMs) act as a crucial\nproxy for human values to guide optimization. However, it remains unclear how\nto evaluate and select a reliable RM for preference alignment in RALMs. To this\nend, we propose RAG-RewardBench, the first benchmark for evaluating RMs in RAG\nsettings. First, we design four crucial and challenging RAG-specific scenarios\nto assess RMs, including multi-hop reasoning, fine-grained citation,\nappropriate abstain, and conflict robustness. Then, we incorporate 18 RAG\nsubsets, six retrievers, and 24 RALMs to increase the diversity of data\nsources. Finally, we adopt an LLM-as-a-judge approach to improve preference\nannotation efficiency and effectiveness, exhibiting a strong correlation with\nhuman annotations. Based on the RAG-RewardBench, we conduct a comprehensive\nevaluation of 45 RMs and uncover their limitations in RAG scenarios.\nAdditionally, we also reveal that existing trained RALMs show almost no\nimprovement in preference alignment, highlighting the need for a shift towards\npreference-aligned training.We release our benchmark and code publicly at\nhttps://huggingface.co/datasets/jinzhuoran/RAG-RewardBench/ for future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the significant progress made by existing retrieval augmented\nlanguage models (RALMs) in providing trustworthy responses and grounding in\nreliable sources, they often overlook effective alignment with human\npreferences. In the alignment process, reward models (RMs) act as a crucial\nproxy for human values to guide optimization. However, it remains unclear how\nto evaluate and select a reliable RM for preference alignment in RALMs. To this\nend, we propose RAG-RewardBench, the first benchmark for evaluating RMs in RAG\nsettings. First, we design four crucial and challenging RAG-specific scenarios\nto assess RMs, including multi-hop reasoning, fine-grained citation,\nappropriate abstain, and conflict robustness. Then, we incorporate 18 RAG\nsubsets, six retrievers, and 24 RALMs to increase the diversity of data\nsources. Finally, we adopt an LLM-as-a-judge approach to improve preference\nannotation efficiency and effectiveness, exhibiting a strong correlation with\nhuman annotations. Based on the RAG-RewardBench, we conduct a comprehensive\nevaluation of 45 RMs and uncover their limitations in RAG scenarios.\nAdditionally, we also reveal that existing trained RALMs show almost no\nimprovement in preference alignment, highlighting the need for a shift towards\npreference-aligned training.We release our benchmark and code publicly at\nhttps://huggingface.co/datasets/jinzhuoran/RAG-RewardBench/ for future work."
                },
                "authors": [
                    {
                        "name": "Zhuoran Jin"
                    },
                    {
                        "name": "Hongbang Yuan"
                    },
                    {
                        "name": "Tianyi Men"
                    },
                    {
                        "name": "Pengfei Cao"
                    },
                    {
                        "name": "Yubo Chen"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "arxiv_comment": "26 pages, 12 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01850v2",
                "updated": "2024-12-18T11:25:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    11,
                    25,
                    55,
                    2,
                    353,
                    0
                ],
                "published": "2024-11-04T07:05:02Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    7,
                    5,
                    2,
                    0,
                    309,
                    0
                ],
                "title": "ManiBox: Enhancing Spatial Grasping Generalization via Scalable\n  Simulation Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ManiBox: Enhancing Spatial Grasping Generalization via Scalable\n  Simulation Data Generation"
                },
                "summary": "Learning a precise robotic grasping policy is crucial for embodied agents\noperating in complex real-world manipulation tasks. Despite significant\nadvancements, most models still struggle with accurate spatial positioning of\nobjects to be grasped. We first show that this spatial generalization challenge\nstems primarily from the extensive data requirements for adequate spatial\nunderstanding. However, collecting such data with real robots is prohibitively\nexpensive, and relying on simulation data often leads to visual generalization\ngaps upon deployment. To overcome these challenges, we then focus on\nstate-based policy generalization and present \\textbf{ManiBox}, a novel\nbounding-box-guided manipulation method built on a simulation-based\nteacher-student framework. The teacher policy efficiently generates scalable\nsimulation data using bounding boxes, which are proven to uniquely determine\nthe objects' spatial positions. The student policy then utilizes these\nlow-dimensional spatial states to enable zero-shot transfer to real robots.\nThrough comprehensive evaluations in simulated and real-world environments,\nManiBox demonstrates a marked improvement in spatial grasping generalization\nand adaptability to diverse objects and backgrounds. Further, our empirical\nstudy into scaling laws for policy performance indicates that spatial volume\ngeneralization scales with data volume in a power law. For a certain level of\nspatial volume, the success rate of grasping empirically follows\nMichaelis-Menten kinetics relative to data volume, showing a saturation effect\nas data increases. Our videos and code are available in\nhttps://thkkk.github.io/manibox.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning a precise robotic grasping policy is crucial for embodied agents\noperating in complex real-world manipulation tasks. Despite significant\nadvancements, most models still struggle with accurate spatial positioning of\nobjects to be grasped. We first show that this spatial generalization challenge\nstems primarily from the extensive data requirements for adequate spatial\nunderstanding. However, collecting such data with real robots is prohibitively\nexpensive, and relying on simulation data often leads to visual generalization\ngaps upon deployment. To overcome these challenges, we then focus on\nstate-based policy generalization and present \\textbf{ManiBox}, a novel\nbounding-box-guided manipulation method built on a simulation-based\nteacher-student framework. The teacher policy efficiently generates scalable\nsimulation data using bounding boxes, which are proven to uniquely determine\nthe objects' spatial positions. The student policy then utilizes these\nlow-dimensional spatial states to enable zero-shot transfer to real robots.\nThrough comprehensive evaluations in simulated and real-world environments,\nManiBox demonstrates a marked improvement in spatial grasping generalization\nand adaptability to diverse objects and backgrounds. Further, our empirical\nstudy into scaling laws for policy performance indicates that spatial volume\ngeneralization scales with data volume in a power law. For a certain level of\nspatial volume, the success rate of grasping empirically follows\nMichaelis-Menten kinetics relative to data volume, showing a saturation effect\nas data increases. Our videos and code are available in\nhttps://thkkk.github.io/manibox."
                },
                "authors": [
                    {
                        "name": "Hengkai Tan"
                    },
                    {
                        "name": "Xuezhou Xu"
                    },
                    {
                        "name": "Chengyang Ying"
                    },
                    {
                        "name": "Xinyi Mao"
                    },
                    {
                        "name": "Songming Liu"
                    },
                    {
                        "name": "Xingxing Zhang"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13724v1",
                "updated": "2024-12-18T11:04:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    11,
                    4,
                    58,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T11:04:58Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    11,
                    4,
                    58,
                    2,
                    353,
                    0
                ],
                "title": "USEFUSE: Utile Stride for Enhanced Performance in Fused Layer\n  Architecture of Deep Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "USEFUSE: Utile Stride for Enhanced Performance in Fused Layer\n  Architecture of Deep Neural Networks"
                },
                "summary": "Convolutional Neural Networks (CNNs) are crucial in various applications, but\ntheir deployment on resource-constrained edge devices poses challenges. This\nstudy presents the Sum-of-Products (SOP) units for convolution, which utilize\nlow-latency left-to-right bit-serial arithmetic to minimize response time and\nenhance overall performance. The study proposes a methodology for fusing\nmultiple convolution layers to reduce off-chip memory communication and\nincrease overall performance. An effective mechanism detects and skips\ninefficient convolutions after ReLU layers, minimizing power consumption\nwithout compromising accuracy. Furthermore, efficient tile movement guarantees\nuniform access to the fusion pyramid. An analysis demonstrates the utile stride\nstrategy improves operational intensity. Two designs cater to varied demands:\none focuses on minimal response time for mission-critical applications, and\nanother focuses on resource-constrained devices with comparable latency. This\napproach notably reduced redundant computations, improving the efficiency of\nCNN deployment on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convolutional Neural Networks (CNNs) are crucial in various applications, but\ntheir deployment on resource-constrained edge devices poses challenges. This\nstudy presents the Sum-of-Products (SOP) units for convolution, which utilize\nlow-latency left-to-right bit-serial arithmetic to minimize response time and\nenhance overall performance. The study proposes a methodology for fusing\nmultiple convolution layers to reduce off-chip memory communication and\nincrease overall performance. An effective mechanism detects and skips\ninefficient convolutions after ReLU layers, minimizing power consumption\nwithout compromising accuracy. Furthermore, efficient tile movement guarantees\nuniform access to the fusion pyramid. An analysis demonstrates the utile stride\nstrategy improves operational intensity. Two designs cater to varied demands:\none focuses on minimal response time for mission-critical applications, and\nanother focuses on resource-constrained devices with comparable latency. This\napproach notably reduced redundant computations, improving the efficiency of\nCNN deployment on edge devices."
                },
                "authors": [
                    {
                        "name": "Muhammad Sohail Ibrahim"
                    },
                    {
                        "name": "Muhammad Usman"
                    },
                    {
                        "name": "Jeong-A Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jeong-A Lee"
                },
                "author": "Jeong-A Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04503v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04503v2",
                "updated": "2024-12-18T11:02:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    11,
                    2,
                    14,
                    2,
                    353,
                    0
                ],
                "published": "2024-07-05T13:44:09Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    13,
                    44,
                    9,
                    4,
                    187,
                    0
                ],
                "title": "When LLMs Play the Telephone Game: Cumulative Changes and Attractors in\n  Iterated Cultural Transmissions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When LLMs Play the Telephone Game: Cumulative Changes and Attractors in\n  Iterated Cultural Transmissions"
                },
                "summary": "As large language models (LLMs) start interacting with each other and\ngenerating an increasing amount of text online, it becomes crucial to better\nunderstand how information is transformed as it passes from one LLM to the\nnext. While significant research has examined individual LLM behaviors,\nexisting studies have largely overlooked the collective behaviors and\ninformation distortions arising from iterated LLM interactions. Small biases,\nnegligible at the single output level, risk being amplified in iterated\ninteractions, potentially leading the content to evolve towards attractor\nstates. In a series of telephone game experiments, we apply a transmission\nchain design borrowed from the human cultural evolution literature: LLM agents\niteratively receive, produce, and transmit texts from the previous to the next\nagent in the chain. By tracking the evolution of text toxicity, positivity,\ndifficulty, and length across transmission chains, we uncover the existence of\nbiases and attractors, and study their dependence on the initial text, the\ninstructions, language model, and model size. For instance, we find that more\nopen-ended instructions lead to stronger attraction effects compared to more\nconstrained tasks. We also find that different text properties display\ndifferent sensitivity to attraction effects, with toxicity leading to stronger\nattractors than length. These findings highlight the importance of accounting\nfor multi-step transmission dynamics and represent a first step towards a more\ncomprehensive understanding of LLM cultural dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) start interacting with each other and\ngenerating an increasing amount of text online, it becomes crucial to better\nunderstand how information is transformed as it passes from one LLM to the\nnext. While significant research has examined individual LLM behaviors,\nexisting studies have largely overlooked the collective behaviors and\ninformation distortions arising from iterated LLM interactions. Small biases,\nnegligible at the single output level, risk being amplified in iterated\ninteractions, potentially leading the content to evolve towards attractor\nstates. In a series of telephone game experiments, we apply a transmission\nchain design borrowed from the human cultural evolution literature: LLM agents\niteratively receive, produce, and transmit texts from the previous to the next\nagent in the chain. By tracking the evolution of text toxicity, positivity,\ndifficulty, and length across transmission chains, we uncover the existence of\nbiases and attractors, and study their dependence on the initial text, the\ninstructions, language model, and model size. For instance, we find that more\nopen-ended instructions lead to stronger attraction effects compared to more\nconstrained tasks. We also find that different text properties display\ndifferent sensitivity to attraction effects, with toxicity leading to stronger\nattractors than length. These findings highlight the importance of accounting\nfor multi-step transmission dynamics and represent a first step towards a more\ncomprehensive understanding of LLM cultural dynamics."
                },
                "authors": [
                    {
                        "name": "Jrmy Perez"
                    },
                    {
                        "name": "Grgur Kova"
                    },
                    {
                        "name": "Corentin Lger"
                    },
                    {
                        "name": "Cdric Colas"
                    },
                    {
                        "name": "Gaia Molinaro"
                    },
                    {
                        "name": "Maxime Derex"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    },
                    {
                        "name": "Clment Moulin-Frier"
                    }
                ],
                "author_detail": {
                    "name": "Clment Moulin-Frier"
                },
                "author": "Clment Moulin-Frier",
                "arxiv_comment": "Code available at https://github.com/jeremyperez2/TelephoneGameLLM.\n  Companion website with a Data Explorer tool at\n  https://sites.google.com/view/telephone-game-llm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04503v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04503v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13720v1",
                "updated": "2024-12-18T11:00:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    11,
                    0,
                    58,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T11:00:58Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    11,
                    0,
                    58,
                    2,
                    353,
                    0
                ],
                "title": "Federated Learning and RAG Integration: A Scalable Approach for Medical\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning and RAG Integration: A Scalable Approach for Medical\n  Large Language Models"
                },
                "summary": "This study analyzes the performance of domain-specific Large Language Models\n(LLMs) for the medical field by integrating Retrieval-Augmented Generation\n(RAG) systems within a federated learning framework. Leveraging the inherent\nadvantages of federated learning, such as preserving data privacy and enabling\ndistributed computation, this research explores the integration of RAG systems\nwith models trained under varying client configurations to optimize\nperformance. Experimental results demonstrate that the federated learning-based\nmodels integrated with RAG systems consistently outperform their non-integrated\ncounterparts across all evaluation metrics. This study highlights the potential\nof combining federated learning and RAG systems for developing domain-specific\nLLMs in the medical field, providing a scalable and privacy-preserving solution\nfor enhancing text generation capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study analyzes the performance of domain-specific Large Language Models\n(LLMs) for the medical field by integrating Retrieval-Augmented Generation\n(RAG) systems within a federated learning framework. Leveraging the inherent\nadvantages of federated learning, such as preserving data privacy and enabling\ndistributed computation, this research explores the integration of RAG systems\nwith models trained under varying client configurations to optimize\nperformance. Experimental results demonstrate that the federated learning-based\nmodels integrated with RAG systems consistently outperform their non-integrated\ncounterparts across all evaluation metrics. This study highlights the potential\nof combining federated learning and RAG systems for developing domain-specific\nLLMs in the medical field, providing a scalable and privacy-preserving solution\nfor enhancing text generation capabilities."
                },
                "authors": [
                    {
                        "name": "Jincheol Jung"
                    },
                    {
                        "name": "Hongju Jeong"
                    },
                    {
                        "name": "Eui-Nam Huh"
                    }
                ],
                "author_detail": {
                    "name": "Eui-Nam Huh"
                },
                "author": "Eui-Nam Huh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13705v1",
                "updated": "2024-12-18T10:49:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    10,
                    49,
                    41,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T10:49:41Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    10,
                    49,
                    41,
                    2,
                    353,
                    0
                ],
                "title": "Mitigating Adversarial Attacks in LLMs through Defensive Suffix\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Adversarial Attacks in LLMs through Defensive Suffix\n  Generation"
                },
                "summary": "Large language models (LLMs) have exhibited outstanding performance in\nnatural language processing tasks. However, these models remain susceptible to\nadversarial attacks in which slight input perturbations can lead to harmful or\nmisleading outputs. A gradient-based defensive suffix generation algorithm is\ndesigned to bolster the robustness of LLMs. By appending carefully optimized\ndefensive suffixes to input prompts, the algorithm mitigates adversarial\ninfluences while preserving the models' utility. To enhance adversarial\nunderstanding, a novel total loss function ($L_{\\text{total}}$) combining\ndefensive loss ($L_{\\text{def}}$) and adversarial loss ($L_{\\text{adv}}$)\ngenerates defensive suffixes more effectively. Experimental evaluations\nconducted on open-source LLMs such as Gemma-7B, mistral-7B, Llama2-7B, and\nLlama2-13B show that the proposed method reduces attack success rates (ASR) by\nan average of 11\\% compared to models without defensive suffixes. Additionally,\nthe perplexity score of Gemma-7B decreased from 6.57 to 3.93 when applying the\ndefensive suffix generated by openELM-270M. Furthermore, TruthfulQA evaluations\ndemonstrate consistent improvements with Truthfulness scores increasing by up\nto 10\\% across tested configurations. This approach significantly enhances the\nsecurity of LLMs in critical applications without requiring extensive\nretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited outstanding performance in\nnatural language processing tasks. However, these models remain susceptible to\nadversarial attacks in which slight input perturbations can lead to harmful or\nmisleading outputs. A gradient-based defensive suffix generation algorithm is\ndesigned to bolster the robustness of LLMs. By appending carefully optimized\ndefensive suffixes to input prompts, the algorithm mitigates adversarial\ninfluences while preserving the models' utility. To enhance adversarial\nunderstanding, a novel total loss function ($L_{\\text{total}}$) combining\ndefensive loss ($L_{\\text{def}}$) and adversarial loss ($L_{\\text{adv}}$)\ngenerates defensive suffixes more effectively. Experimental evaluations\nconducted on open-source LLMs such as Gemma-7B, mistral-7B, Llama2-7B, and\nLlama2-13B show that the proposed method reduces attack success rates (ASR) by\nan average of 11\\% compared to models without defensive suffixes. Additionally,\nthe perplexity score of Gemma-7B decreased from 6.57 to 3.93 when applying the\ndefensive suffix generated by openELM-270M. Furthermore, TruthfulQA evaluations\ndemonstrate consistent improvements with Truthfulness scores increasing by up\nto 10\\% across tested configurations. This approach significantly enhances the\nsecurity of LLMs in critical applications without requiring extensive\nretraining."
                },
                "authors": [
                    {
                        "name": "Minkyoung Kim"
                    },
                    {
                        "name": "Yunha Kim"
                    },
                    {
                        "name": "Hyeram Seo"
                    },
                    {
                        "name": "Heejung Choi"
                    },
                    {
                        "name": "Jiye Han"
                    },
                    {
                        "name": "Gaeun Kee"
                    },
                    {
                        "name": "Soyoung Ko"
                    },
                    {
                        "name": "HyoJe Jung"
                    },
                    {
                        "name": "Byeolhee Kim"
                    },
                    {
                        "name": "Young-Hak Kim"
                    },
                    {
                        "name": "Sanghyun Park"
                    },
                    {
                        "name": "Tae Joon Jun"
                    }
                ],
                "author_detail": {
                    "name": "Tae Joon Jun"
                },
                "author": "Tae Joon Jun",
                "arxiv_comment": "9 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13698v1",
                "updated": "2024-12-18T10:42:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    10,
                    42,
                    53,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T10:42:53Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    10,
                    42,
                    53,
                    2,
                    353,
                    0
                ],
                "title": "Towards Efficient and Explainable Hate Speech Detection via Model\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient and Explainable Hate Speech Detection via Model\n  Distillation"
                },
                "summary": "Automatic detection of hate and abusive language is essential to combat its\nonline spread. Moreover, recognising and explaining hate speech serves to\neducate people about its negative effects. However, most current detection\nmodels operate as black boxes, lacking interpretability and explainability. In\nthis context, Large Language Models (LLMs) have proven effective for hate\nspeech detection and to promote interpretability. Nevertheless, they are\ncomputationally costly to run. In this work, we propose distilling big language\nmodels by using Chain-of-Thought to extract explanations that support the hate\nspeech classification task. Having small language models for these tasks will\ncontribute to their use in operational settings. In this paper, we demonstrate\nthat distilled models deliver explanations of the same quality as larger models\nwhile surpassing them in classification performance. This dual capability,\nclassifying and explaining, advances hate speech detection making it more\naffordable, understandable and actionable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic detection of hate and abusive language is essential to combat its\nonline spread. Moreover, recognising and explaining hate speech serves to\neducate people about its negative effects. However, most current detection\nmodels operate as black boxes, lacking interpretability and explainability. In\nthis context, Large Language Models (LLMs) have proven effective for hate\nspeech detection and to promote interpretability. Nevertheless, they are\ncomputationally costly to run. In this work, we propose distilling big language\nmodels by using Chain-of-Thought to extract explanations that support the hate\nspeech classification task. Having small language models for these tasks will\ncontribute to their use in operational settings. In this paper, we demonstrate\nthat distilled models deliver explanations of the same quality as larger models\nwhile surpassing them in classification performance. This dual capability,\nclassifying and explaining, advances hate speech detection making it more\naffordable, understandable and actionable."
                },
                "authors": [
                    {
                        "name": "Paloma Piot"
                    },
                    {
                        "name": "Javier Parapar"
                    }
                ],
                "author_detail": {
                    "name": "Javier Parapar"
                },
                "author": "Javier Parapar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13693v1",
                "updated": "2024-12-18T10:33:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    10,
                    33,
                    55,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T10:33:55Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    10,
                    33,
                    55,
                    2,
                    353,
                    0
                ],
                "title": "A2H: A UI Converter from Android to HarmonyOS Platform",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A2H: A UI Converter from Android to HarmonyOS Platform"
                },
                "summary": "With the growing importance of smartphones, developers face the challenge of\ncreating separate applications for multiple platforms (e.g., Android, iOS, and\nHarmonyOS), leading to increased development costs and longer iteration cycles.\nOne potential solution is to develop an app on one platform and then\nautomatically convert it to other platforms, reducing the need for separate\ndevelopment efforts. However, migrating user interfaces (UIs) between platforms\nis particularly challenging due to significant differences in layout structures\nand development paradigms, such as the disparity between XML layout files in\nAndroid and ArkUI framework in HarmonyOS. Manual conversion of UIs is\ntime-consuming, error-prone, and inefficient, necessitating an automated\nsolution to streamline the process and enable seamless migration from Android\nto HarmonyOS. To address this challenge, we propose the A2H Converter, an\nautomated tool for migrating Android UIs to HarmonyOS. The tool employs an\nlarge language model (LLM)-driven multi-agent framework to convert Android XML\nlayouts into HarmonyOS ArkUI layouts. Using the RAG combing with decision\nrules, the system maps Android UI components to ArkUI equivalents, while a\nreflective mechanism continuously improves conversion accuracy. A2H Converter\nhandles project-level layouts, ensuring consistency across multiple files and\naddressing complex UI logic. Experiments on six Android applications collected\nfrom GitHub demonstrate that our A2H Converter achieves a migration success\nrate of over 90.1\\%, 89.3\\%, and 89.2\\% at the component, page, and project\nlevels, respectively. The demo video is available at. The tool is available at\nhttp://124.70.54.129:37860/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing importance of smartphones, developers face the challenge of\ncreating separate applications for multiple platforms (e.g., Android, iOS, and\nHarmonyOS), leading to increased development costs and longer iteration cycles.\nOne potential solution is to develop an app on one platform and then\nautomatically convert it to other platforms, reducing the need for separate\ndevelopment efforts. However, migrating user interfaces (UIs) between platforms\nis particularly challenging due to significant differences in layout structures\nand development paradigms, such as the disparity between XML layout files in\nAndroid and ArkUI framework in HarmonyOS. Manual conversion of UIs is\ntime-consuming, error-prone, and inefficient, necessitating an automated\nsolution to streamline the process and enable seamless migration from Android\nto HarmonyOS. To address this challenge, we propose the A2H Converter, an\nautomated tool for migrating Android UIs to HarmonyOS. The tool employs an\nlarge language model (LLM)-driven multi-agent framework to convert Android XML\nlayouts into HarmonyOS ArkUI layouts. Using the RAG combing with decision\nrules, the system maps Android UI components to ArkUI equivalents, while a\nreflective mechanism continuously improves conversion accuracy. A2H Converter\nhandles project-level layouts, ensuring consistency across multiple files and\naddressing complex UI logic. Experiments on six Android applications collected\nfrom GitHub demonstrate that our A2H Converter achieves a migration success\nrate of over 90.1\\%, 89.3\\%, and 89.2\\% at the component, page, and project\nlevels, respectively. The demo video is available at. The tool is available at\nhttp://124.70.54.129:37860/."
                },
                "authors": [
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Lina Gong"
                    },
                    {
                        "name": "Yujun Huang"
                    },
                    {
                        "name": "Di Cui"
                    },
                    {
                        "name": "Mingqiang Wei"
                    }
                ],
                "author_detail": {
                    "name": "Mingqiang Wei"
                },
                "author": "Mingqiang Wei",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16203v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16203v2",
                "updated": "2024-12-18T10:25:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    10,
                    25,
                    12,
                    2,
                    353,
                    0
                ],
                "published": "2024-05-25T12:27:21Z",
                "published_parsed": [
                    2024,
                    5,
                    25,
                    12,
                    27,
                    21,
                    5,
                    146,
                    0
                ],
                "title": "Evolutionary Large Language Model for Automated Feature Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary Large Language Model for Automated Feature Transformation"
                },
                "summary": "Feature transformation aims to reconstruct the feature space of raw features\nto enhance the performance of downstream models. However, the exponential\ngrowth in the combinations of features and operations poses a challenge, making\nit difficult for existing methods to efficiently explore a wide space.\nAdditionally, their optimization is solely driven by the accuracy of downstream\nmodels in specific domains, neglecting the acquisition of general feature\nknowledge. To fill this research gap, we propose an evolutionary LLM framework\nfor automated feature transformation. This framework consists of two parts: 1)\nconstructing a multi-population database through an RL data collector while\nutilizing evolutionary algorithm strategies for database maintenance, and 2)\nutilizing the ability of Large Language Model (LLM) in sequence understanding,\nwe employ few-shot prompts to guide LLM in generating superior samples based on\nfeature transformation sequence distinction. Leveraging the multi-population\ndatabase initially provides a wide search scope to discover excellent\npopulations. Through culling and evolution, the high-quality populations are\nafforded greater opportunities, thereby furthering the pursuit of optimal\nindividuals. Through the integration of LLMs with evolutionary algorithms, we\nachieve efficient exploration within a vast space, while harnessing feature\nknowledge to propel optimization, thus realizing a more adaptable search\nparadigm. Finally, we empirically demonstrate the effectiveness and generality\nof our proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature transformation aims to reconstruct the feature space of raw features\nto enhance the performance of downstream models. However, the exponential\ngrowth in the combinations of features and operations poses a challenge, making\nit difficult for existing methods to efficiently explore a wide space.\nAdditionally, their optimization is solely driven by the accuracy of downstream\nmodels in specific domains, neglecting the acquisition of general feature\nknowledge. To fill this research gap, we propose an evolutionary LLM framework\nfor automated feature transformation. This framework consists of two parts: 1)\nconstructing a multi-population database through an RL data collector while\nutilizing evolutionary algorithm strategies for database maintenance, and 2)\nutilizing the ability of Large Language Model (LLM) in sequence understanding,\nwe employ few-shot prompts to guide LLM in generating superior samples based on\nfeature transformation sequence distinction. Leveraging the multi-population\ndatabase initially provides a wide search scope to discover excellent\npopulations. Through culling and evolution, the high-quality populations are\nafforded greater opportunities, thereby furthering the pursuit of optimal\nindividuals. Through the integration of LLMs with evolutionary algorithms, we\nachieve efficient exploration within a vast space, while harnessing feature\nknowledge to propel optimization, thus realizing a more adaptable search\nparadigm. Finally, we empirically demonstrate the effectiveness and generality\nof our proposed method."
                },
                "authors": [
                    {
                        "name": "Nanxu Gong"
                    },
                    {
                        "name": "Chandan K. Reddy"
                    },
                    {
                        "name": "Wangyang Ying"
                    },
                    {
                        "name": "Haifeng Chen"
                    },
                    {
                        "name": "Yanjie Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yanjie Fu"
                },
                "author": "Yanjie Fu",
                "arxiv_comment": "Accepted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16203v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16203v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13682v1",
                "updated": "2024-12-18T10:10:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    10,
                    10,
                    12,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T10:10:12Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    10,
                    10,
                    12,
                    2,
                    353,
                    0
                ],
                "title": "ChinaTravel: A Real-World Benchmark for Language Agents in Chinese\n  Travel Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChinaTravel: A Real-World Benchmark for Language Agents in Chinese\n  Travel Planning"
                },
                "summary": "Recent advances in LLMs, particularly in language reasoning and tool\nintegration, have rapidly sparked the real-world development of Language\nAgents. Among these, travel planning represents a prominent domain, combining\nacademic challenges with practical value due to its complexity and market\ndemand. However, existing benchmarks fail to reflect the diverse, real-world\nrequirements crucial for deployment. To address this gap, we introduce\nChinaTravel, a benchmark specifically designed for authentic Chinese travel\nplanning scenarios. We collect the travel requirements from questionnaires and\npropose a compositionally generalizable domain-specific language that enables a\nscalable evaluation process, covering feasibility, constraint satisfaction, and\npreference comparison. Empirical studies reveal the potential of neuro-symbolic\nagents in travel planning, achieving a constraint satisfaction rate of 27.9%,\nsignificantly surpassing purely neural models at 2.6%. Moreover, we identify\nkey challenges in real-world travel planning deployments, including open\nlanguage reasoning and unseen concept composition. These findings highlight the\nsignificance of ChinaTravel as a pivotal milestone for advancing language\nagents in complex, real-world planning scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in LLMs, particularly in language reasoning and tool\nintegration, have rapidly sparked the real-world development of Language\nAgents. Among these, travel planning represents a prominent domain, combining\nacademic challenges with practical value due to its complexity and market\ndemand. However, existing benchmarks fail to reflect the diverse, real-world\nrequirements crucial for deployment. To address this gap, we introduce\nChinaTravel, a benchmark specifically designed for authentic Chinese travel\nplanning scenarios. We collect the travel requirements from questionnaires and\npropose a compositionally generalizable domain-specific language that enables a\nscalable evaluation process, covering feasibility, constraint satisfaction, and\npreference comparison. Empirical studies reveal the potential of neuro-symbolic\nagents in travel planning, achieving a constraint satisfaction rate of 27.9%,\nsignificantly surpassing purely neural models at 2.6%. Moreover, we identify\nkey challenges in real-world travel planning deployments, including open\nlanguage reasoning and unseen concept composition. These findings highlight the\nsignificance of ChinaTravel as a pivotal milestone for advancing language\nagents in complex, real-world planning scenarios."
                },
                "authors": [
                    {
                        "name": "Jie-Jing Shao"
                    },
                    {
                        "name": "Xiao-Wen Yang"
                    },
                    {
                        "name": "Bo-Wen Zhang"
                    },
                    {
                        "name": "Baizhi Chen"
                    },
                    {
                        "name": "Wen-Da Wei"
                    },
                    {
                        "name": "Lan-Zhe Guo"
                    },
                    {
                        "name": "Yu-feng Li"
                    }
                ],
                "author_detail": {
                    "name": "Yu-feng Li"
                },
                "author": "Yu-feng Li",
                "arxiv_comment": "Webpage: https://www.lamda.nju.edu.cn/shaojj/chinatravel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13679v1",
                "updated": "2024-12-18T10:07:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    10,
                    7,
                    54,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T10:07:54Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    10,
                    7,
                    54,
                    2,
                    353,
                    0
                ],
                "title": "On Enhancing Root Cause Analysis with SQL Summaries for Failures in\n  Database Workload Replays at SAP HANA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Enhancing Root Cause Analysis with SQL Summaries for Failures in\n  Database Workload Replays at SAP HANA"
                },
                "summary": "Capturing the workload of a database and replaying this workload for a new\nversion of the database can be an effective approach for regression testing.\nHowever, false positive errors caused by many factors such as data privacy\nlimitations, time dependency or non-determinism in multi-threaded environment\ncan negatively impact the effectiveness. Therefore, we employ a machine\nlearning based framework to automate the root cause analysis of failures found\nduring replays. However, handling unseen novel issues not found in the training\ndata is one general challenge of machine learning approaches with respect to\ngeneralizability of the learned model. We describe how we continue to address\nthis challenge for more robust long-term solutions. From our experience,\nretraining with new failures is inadequate due to features overlapping across\ndistinct root causes. Hence, we leverage a large language model (LLM) to\nanalyze failed SQL statements and extract concise failure summaries as an\nadditional feature to enhance the classification process. Our experiments show\nthe F1-Macro score improved by 4.77% for our data. We consider our approach\nbeneficial for providing end users with additional information to gain more\ninsights into the found issues and to improve the assessment of the replay\nresults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capturing the workload of a database and replaying this workload for a new\nversion of the database can be an effective approach for regression testing.\nHowever, false positive errors caused by many factors such as data privacy\nlimitations, time dependency or non-determinism in multi-threaded environment\ncan negatively impact the effectiveness. Therefore, we employ a machine\nlearning based framework to automate the root cause analysis of failures found\nduring replays. However, handling unseen novel issues not found in the training\ndata is one general challenge of machine learning approaches with respect to\ngeneralizability of the learned model. We describe how we continue to address\nthis challenge for more robust long-term solutions. From our experience,\nretraining with new failures is inadequate due to features overlapping across\ndistinct root causes. Hence, we leverage a large language model (LLM) to\nanalyze failed SQL statements and extract concise failure summaries as an\nadditional feature to enhance the classification process. Our experiments show\nthe F1-Macro score improved by 4.77% for our data. We consider our approach\nbeneficial for providing end users with additional information to gain more\ninsights into the found issues and to improve the assessment of the replay\nresults."
                },
                "authors": [
                    {
                        "name": "Neetha Jambigi"
                    },
                    {
                        "name": "Joshua Hammesfahr"
                    },
                    {
                        "name": "Moritz Mueller"
                    },
                    {
                        "name": "Thomas Bach"
                    },
                    {
                        "name": "Michael Felderer"
                    }
                ],
                "author_detail": {
                    "name": "Michael Felderer"
                },
                "author": "Michael Felderer",
                "arxiv_comment": "The 35th IEEE International Symposium on Software Reliability\n  Engineering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13670v1",
                "updated": "2024-12-18T09:53:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    53,
                    12,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T09:53:12Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    53,
                    12,
                    2,
                    353,
                    0
                ],
                "title": "AntiLeak-Bench: Preventing Data Contamination by Automatically\n  Constructing Benchmarks with Updated Real-World Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AntiLeak-Bench: Preventing Data Contamination by Automatically\n  Constructing Benchmarks with Updated Real-World Knowledge"
                },
                "summary": "Data contamination hinders fair LLM evaluation by introducing test data into\nnewer models' training sets. Existing studies solve this challenge by updating\nbenchmarks with newly collected data. However, they fail to guarantee\ncontamination-free evaluation as the newly collected data may contain\npre-existing knowledge, and their benchmark updates rely on intensive human\nlabor. To address these issues, we in this paper propose AntiLeak-Bench, an\nautomated anti-leakage benchmarking framework. Instead of simply using newly\ncollected data, we construct samples with explicitly new knowledge absent from\nLLMs' training sets, which thus ensures strictly contamination-free evaluation.\nWe further design a fully automated workflow to build and update our benchmark\nwithout human labor. This significantly reduces the cost of benchmark\nmaintenance to accommodate emerging LLMs. Through extensive experiments, we\nhighlight that data contamination likely exists before LLMs' cutoff time and\ndemonstrate AntiLeak-Bench effectively overcomes this challenge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data contamination hinders fair LLM evaluation by introducing test data into\nnewer models' training sets. Existing studies solve this challenge by updating\nbenchmarks with newly collected data. However, they fail to guarantee\ncontamination-free evaluation as the newly collected data may contain\npre-existing knowledge, and their benchmark updates rely on intensive human\nlabor. To address these issues, we in this paper propose AntiLeak-Bench, an\nautomated anti-leakage benchmarking framework. Instead of simply using newly\ncollected data, we construct samples with explicitly new knowledge absent from\nLLMs' training sets, which thus ensures strictly contamination-free evaluation.\nWe further design a fully automated workflow to build and update our benchmark\nwithout human labor. This significantly reduces the cost of benchmark\nmaintenance to accommodate emerging LLMs. Through extensive experiments, we\nhighlight that data contamination likely exists before LLMs' cutoff time and\ndemonstrate AntiLeak-Bench effectively overcomes this challenge."
                },
                "authors": [
                    {
                        "name": "Xiaobao Wu"
                    },
                    {
                        "name": "Liangming Pan"
                    },
                    {
                        "name": "Yuxi Xie"
                    },
                    {
                        "name": "Ruiwen Zhou"
                    },
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Yubo Ma"
                    },
                    {
                        "name": "Mingzhe Du"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Anh Tuan Luu"
                    },
                    {
                        "name": "William Yang Wang"
                    }
                ],
                "author_detail": {
                    "name": "William Yang Wang"
                },
                "author": "William Yang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13667v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13667v1",
                "updated": "2024-12-18T09:50:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    50,
                    0,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T09:50:00Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    50,
                    0,
                    2,
                    353,
                    0
                ],
                "title": "Exploring Multi-Modal Integration with Tool-Augmented LLM Agents for\n  Precise Causal Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Multi-Modal Integration with Tool-Augmented LLM Agents for\n  Precise Causal Discovery"
                },
                "summary": "Causal inference is an imperative foundation for decision-making across\ndomains, such as smart health, AI for drug discovery and AIOps. Traditional\nstatistical causal discovery methods, while well-established, predominantly\nrely on observational data and often overlook the semantic cues inherent in\ncause-and-effect relationships. The advent of Large Language Models (LLMs) has\nushered in an affordable way of leveraging the semantic cues for\nknowledge-driven causal discovery, but the development of LLMs for causal\ndiscovery lags behind other areas, particularly in the exploration of\nmulti-modality data. To bridge the gap, we introduce MATMCD, a multi-agent\nsystem powered by tool-augmented LLMs. MATMCD has two key agents: a Data\nAugmentation agent that retrieves and processes modality-augmented data, and a\nCausal Constraint agent that integrates multi-modal data for knowledge-driven\ninference. Delicate design of the inner-workings ensures successful cooperation\nof the agents. Our empirical study across seven datasets suggests the\nsignificant potential of multi-modality enhanced causal discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal inference is an imperative foundation for decision-making across\ndomains, such as smart health, AI for drug discovery and AIOps. Traditional\nstatistical causal discovery methods, while well-established, predominantly\nrely on observational data and often overlook the semantic cues inherent in\ncause-and-effect relationships. The advent of Large Language Models (LLMs) has\nushered in an affordable way of leveraging the semantic cues for\nknowledge-driven causal discovery, but the development of LLMs for causal\ndiscovery lags behind other areas, particularly in the exploration of\nmulti-modality data. To bridge the gap, we introduce MATMCD, a multi-agent\nsystem powered by tool-augmented LLMs. MATMCD has two key agents: a Data\nAugmentation agent that retrieves and processes modality-augmented data, and a\nCausal Constraint agent that integrates multi-modal data for knowledge-driven\ninference. Delicate design of the inner-workings ensures successful cooperation\nof the agents. Our empirical study across seven datasets suggests the\nsignificant potential of multi-modality enhanced causal discovery."
                },
                "authors": [
                    {
                        "name": "ChengAo Shen"
                    },
                    {
                        "name": "Zhengzhang Chen"
                    },
                    {
                        "name": "Dongsheng Luo"
                    },
                    {
                        "name": "Dongkuan Xu"
                    },
                    {
                        "name": "Haifeng Chen"
                    },
                    {
                        "name": "Jingchao Ni"
                    }
                ],
                "author_detail": {
                    "name": "Jingchao Ni"
                },
                "author": "Jingchao Ni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13667v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13667v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13666v1",
                "updated": "2024-12-18T09:48:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    48,
                    53,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T09:48:53Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    48,
                    53,
                    2,
                    353,
                    0
                ],
                "title": "Evaluation of LLM Vulnerabilities to Being Misused for Personalized\n  Disinformation Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation of LLM Vulnerabilities to Being Misused for Personalized\n  Disinformation Generation"
                },
                "summary": "The capabilities of recent large language models (LLMs) to generate\nhigh-quality content indistinguishable by humans from human-written texts rises\nmany concerns regarding their misuse. Previous research has shown that LLMs can\nbe effectively misused for generating disinformation news articles following\npredefined narratives. Their capabilities to generate personalized (in various\naspects) content have also been evaluated and mostly found usable. However, a\ncombination of personalization and disinformation abilities of LLMs has not\nbeen comprehensively studied yet. Such a dangerous combination should trigger\nintegrated safety filters of the LLMs, if there are some. This study fills this\ngap by evaluation of vulnerabilities of recent open and closed LLMs, and their\nwillingness to generate personalized disinformation news articles in English.\nWe further explore whether the LLMs can reliably meta-evaluate the\npersonalization quality and whether the personalization affects the\ngenerated-texts detectability. Our results demonstrate the need for stronger\nsafety-filters and disclaimers, as those are not properly functioning in most\nof the evaluated LLMs. Additionally, our study revealed that the\npersonalization actually reduces the safety-filter activations; thus\neffectively functioning as a jailbreak. Such behavior must be urgently\naddressed by LLM developers and service providers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capabilities of recent large language models (LLMs) to generate\nhigh-quality content indistinguishable by humans from human-written texts rises\nmany concerns regarding their misuse. Previous research has shown that LLMs can\nbe effectively misused for generating disinformation news articles following\npredefined narratives. Their capabilities to generate personalized (in various\naspects) content have also been evaluated and mostly found usable. However, a\ncombination of personalization and disinformation abilities of LLMs has not\nbeen comprehensively studied yet. Such a dangerous combination should trigger\nintegrated safety filters of the LLMs, if there are some. This study fills this\ngap by evaluation of vulnerabilities of recent open and closed LLMs, and their\nwillingness to generate personalized disinformation news articles in English.\nWe further explore whether the LLMs can reliably meta-evaluate the\npersonalization quality and whether the personalization affects the\ngenerated-texts detectability. Our results demonstrate the need for stronger\nsafety-filters and disclaimers, as those are not properly functioning in most\nof the evaluated LLMs. Additionally, our study revealed that the\npersonalization actually reduces the safety-filter activations; thus\neffectively functioning as a jailbreak. Such behavior must be urgently\naddressed by LLM developers and service providers."
                },
                "authors": [
                    {
                        "name": "Aneta Zugecova"
                    },
                    {
                        "name": "Dominik Macko"
                    },
                    {
                        "name": "Ivan Srba"
                    },
                    {
                        "name": "Robert Moro"
                    },
                    {
                        "name": "Jakub Kopal"
                    },
                    {
                        "name": "Katarina Marcincinova"
                    },
                    {
                        "name": "Matus Mesarcik"
                    }
                ],
                "author_detail": {
                    "name": "Matus Mesarcik"
                },
                "author": "Matus Mesarcik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13035v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13035v3",
                "updated": "2024-12-18T09:48:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    48,
                    32,
                    2,
                    353,
                    0
                ],
                "published": "2024-09-19T18:11:59Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    18,
                    11,
                    59,
                    3,
                    263,
                    0
                ],
                "title": "TACO-RL: Task Aware Prompt Compression Optimization with Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TACO-RL: Task Aware Prompt Compression Optimization with Reinforcement\n  Learning"
                },
                "summary": "The increasing prevalence of large language models (LLMs) such as GPT-4 in\nvarious applications has led to a surge in the size of prompts required for\noptimal performance, leading to challenges in computational efficiency. Prompt\ncompression aims to reduce the inference cost by minimizing input tokens\nwithout compromising on the task performance. However, existing prompt\ncompression techniques either rely on sub-optimal metrics such as information\nentropy or model it as a task-agnostic token classification problem that fails\nto capture task-specific information. To address these issues, we propose a\nnovel and efficient reinforcement learning (RL) based task-aware prompt\ncompression method. To ensure low latency requirements, we leverage existing\nTransformer encoder-based token classification model while guiding the learning\nprocess with task-specific reward signals using lightweight REINFORCE\nalgorithm. We evaluate the performance of our method on three diverse and\nchallenging tasks including text summarization, question answering and code\nsummarization. We demonstrate that our RL-guided compression method improves\nthe task performance by 8% - 189% across these three scenarios over\nstate-of-the-art compression techniques while satisfying the same compression\nrate and latency requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing prevalence of large language models (LLMs) such as GPT-4 in\nvarious applications has led to a surge in the size of prompts required for\noptimal performance, leading to challenges in computational efficiency. Prompt\ncompression aims to reduce the inference cost by minimizing input tokens\nwithout compromising on the task performance. However, existing prompt\ncompression techniques either rely on sub-optimal metrics such as information\nentropy or model it as a task-agnostic token classification problem that fails\nto capture task-specific information. To address these issues, we propose a\nnovel and efficient reinforcement learning (RL) based task-aware prompt\ncompression method. To ensure low latency requirements, we leverage existing\nTransformer encoder-based token classification model while guiding the learning\nprocess with task-specific reward signals using lightweight REINFORCE\nalgorithm. We evaluate the performance of our method on three diverse and\nchallenging tasks including text summarization, question answering and code\nsummarization. We demonstrate that our RL-guided compression method improves\nthe task performance by 8% - 189% across these three scenarios over\nstate-of-the-art compression techniques while satisfying the same compression\nrate and latency requirements."
                },
                "authors": [
                    {
                        "name": "Shivam Shandilya"
                    },
                    {
                        "name": "Menglin Xia"
                    },
                    {
                        "name": "Supriyo Ghosh"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Jue Zhang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Victor Rhle"
                    }
                ],
                "author_detail": {
                    "name": "Victor Rhle"
                },
                "author": "Victor Rhle",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13035v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13035v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05342v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05342v2",
                "updated": "2024-12-18T09:47:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    47,
                    53,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-06T09:33:47Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    9,
                    33,
                    47,
                    4,
                    341,
                    0
                ],
                "title": "Multi-Party Supervised Fine-tuning of Language Models for Multi-Party\n  Dialogue Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Party Supervised Fine-tuning of Language Models for Multi-Party\n  Dialogue Generation"
                },
                "summary": "Large Language Models (LLM) are usually fine-tuned to participate in dyadic\nor two-party dialogues, which can not adapt well to multi-party dialogues\n(MPD), which hinders their applications in such scenarios including\nmulti-personal meetings, discussions and daily communication. Previous\nLLM-based researches mainly focus on the multi-agent framework, while their\nbase LLMs are still pairwisely fine-tuned. In this work, we design a\nmulti-party fine-tuning framework (MuPaS) for LLMs on the multi-party dialogue\ndatasets, and prove such a straightforward framework can let the LLM align with\nthe multi-party conversation style efficiently and effectively. We also design\ntwo training strategies which can convert MuPaS into the MPD simulator.\nSubstantial experiments show that MuPaS can achieve state-of-the-art\nmulti-party response, higher accuracy of the-next-speaker prediction, higher\nhuman and automatic evaluated utterance qualities, and can even generate\nreasonably with out-of-distribution scene, topic and role descriptions. The\nMuPaS framework bridges the LLM training with more complicated multi-party\napplications, such as conversation generation, virtual rehearsal or\nmeta-universe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) are usually fine-tuned to participate in dyadic\nor two-party dialogues, which can not adapt well to multi-party dialogues\n(MPD), which hinders their applications in such scenarios including\nmulti-personal meetings, discussions and daily communication. Previous\nLLM-based researches mainly focus on the multi-agent framework, while their\nbase LLMs are still pairwisely fine-tuned. In this work, we design a\nmulti-party fine-tuning framework (MuPaS) for LLMs on the multi-party dialogue\ndatasets, and prove such a straightforward framework can let the LLM align with\nthe multi-party conversation style efficiently and effectively. We also design\ntwo training strategies which can convert MuPaS into the MPD simulator.\nSubstantial experiments show that MuPaS can achieve state-of-the-art\nmulti-party response, higher accuracy of the-next-speaker prediction, higher\nhuman and automatic evaluated utterance qualities, and can even generate\nreasonably with out-of-distribution scene, topic and role descriptions. The\nMuPaS framework bridges the LLM training with more complicated multi-party\napplications, such as conversation generation, virtual rehearsal or\nmeta-universe."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Ningyuan Xi"
                    },
                    {
                        "name": "Teng Chen"
                    },
                    {
                        "name": "Qingqing Gu"
                    },
                    {
                        "name": "Yue Zhao"
                    },
                    {
                        "name": "Xiaokai Chen"
                    },
                    {
                        "name": "Zhonglin Jiang"
                    },
                    {
                        "name": "Yong Chen"
                    },
                    {
                        "name": "Luo Ji"
                    }
                ],
                "author_detail": {
                    "name": "Luo Ji"
                },
                "author": "Luo Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05342v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05342v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13660v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13660v1",
                "updated": "2024-12-18T09:38:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    38,
                    43,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T09:38:43Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    38,
                    43,
                    2,
                    353,
                    0
                ],
                "title": "PsyDT: Using LLMs to Construct the Digital Twin of Psychological\n  Counselor with Personalized Counseling Style for Psychological Counseling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PsyDT: Using LLMs to Construct the Digital Twin of Psychological\n  Counselor with Personalized Counseling Style for Psychological Counseling"
                },
                "summary": "Currently, large language models (LLMs) have made significant progress in the\nfield of psychological counseling. However, existing mental health LLMs\noverlook a critical issue where they do not consider the fact that different\npsychological counselors exhibit different personal styles, including\nlinguistic style and therapy techniques, etc. As a result, these LLMs fail to\nsatisfy the individual needs of clients who seek different counseling styles.\nTo help bridge this gap, we propose PsyDT, a novel framework using LLMs to\nconstruct the Digital Twin of Psychological counselor with personalized\ncounseling style. Compared to the time-consuming and costly approach of\ncollecting a large number of real-world counseling cases to create a specific\ncounselor's digital twin, our framework offers a faster and more cost-effective\nsolution. To construct PsyDT, we utilize dynamic one-shot learning by using\nGPT-4 to capture counselor's unique counseling style, mainly focusing on\nlinguistic style and therapy techniques. Subsequently, using existing\nsingle-turn long-text dialogues with client's questions, GPT-4 is guided to\nsynthesize multi-turn dialogues of specific counselor. Finally, we fine-tune\nthe LLMs on the synthetic dataset, PsyDTCorpus, to achieve the digital twin of\npsychological counselor with personalized counseling style. Experimental\nresults indicate that our proposed PsyDT framework can synthesize multi-turn\ndialogues that closely resemble real-world counseling cases and demonstrate\nbetter performance compared to other baselines, thereby show that our framework\ncan effectively construct the digital twin of psychological counselor with a\nspecific counseling style.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Currently, large language models (LLMs) have made significant progress in the\nfield of psychological counseling. However, existing mental health LLMs\noverlook a critical issue where they do not consider the fact that different\npsychological counselors exhibit different personal styles, including\nlinguistic style and therapy techniques, etc. As a result, these LLMs fail to\nsatisfy the individual needs of clients who seek different counseling styles.\nTo help bridge this gap, we propose PsyDT, a novel framework using LLMs to\nconstruct the Digital Twin of Psychological counselor with personalized\ncounseling style. Compared to the time-consuming and costly approach of\ncollecting a large number of real-world counseling cases to create a specific\ncounselor's digital twin, our framework offers a faster and more cost-effective\nsolution. To construct PsyDT, we utilize dynamic one-shot learning by using\nGPT-4 to capture counselor's unique counseling style, mainly focusing on\nlinguistic style and therapy techniques. Subsequently, using existing\nsingle-turn long-text dialogues with client's questions, GPT-4 is guided to\nsynthesize multi-turn dialogues of specific counselor. Finally, we fine-tune\nthe LLMs on the synthetic dataset, PsyDTCorpus, to achieve the digital twin of\npsychological counselor with personalized counseling style. Experimental\nresults indicate that our proposed PsyDT framework can synthesize multi-turn\ndialogues that closely resemble real-world counseling cases and demonstrate\nbetter performance compared to other baselines, thereby show that our framework\ncan effectively construct the digital twin of psychological counselor with a\nspecific counseling style."
                },
                "authors": [
                    {
                        "name": "Haojie Xie"
                    },
                    {
                        "name": "Yirong Chen"
                    },
                    {
                        "name": "Xiaofen Xing"
                    },
                    {
                        "name": "Jingkai Lin"
                    },
                    {
                        "name": "Xiangmin Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangmin Xu"
                },
                "author": "Xiangmin Xu",
                "arxiv_comment": "9 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13660v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05166v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05166v4",
                "updated": "2024-12-18T09:37:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    37,
                    8,
                    2,
                    353,
                    0
                ],
                "published": "2023-12-08T16:47:09Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    16,
                    47,
                    9,
                    4,
                    342,
                    0
                ],
                "title": "Multi-Agent Reinforcement Learning via Distributed MPC as a Function\n  Approximator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Reinforcement Learning via Distributed MPC as a Function\n  Approximator"
                },
                "summary": "This paper presents a novel approach to multi-agent reinforcement learning\n(RL) for linear systems with convex polytopic constraints. Existing work on RL\nhas demonstrated the use of model predictive control (MPC) as a function\napproximator for the policy and value functions. The current paper is the first\nwork to extend this idea to the multi-agent setting. We propose the use of a\ndistributed MPC scheme as a function approximator, with a structure allowing\nfor distributed learning and deployment. We then show that Q-learning updates\ncan be performed distributively without introducing nonstationarity, by\nreconstructing a centralized learning update. The effectiveness of the approach\nis demonstrated on two numerical examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach to multi-agent reinforcement learning\n(RL) for linear systems with convex polytopic constraints. Existing work on RL\nhas demonstrated the use of model predictive control (MPC) as a function\napproximator for the policy and value functions. The current paper is the first\nwork to extend this idea to the multi-agent setting. We propose the use of a\ndistributed MPC scheme as a function approximator, with a structure allowing\nfor distributed learning and deployment. We then show that Q-learning updates\ncan be performed distributively without introducing nonstationarity, by\nreconstructing a centralized learning update. The effectiveness of the approach\nis demonstrated on two numerical examples."
                },
                "authors": [
                    {
                        "name": "Samuel Mallick"
                    },
                    {
                        "name": "Filippo Airaldi"
                    },
                    {
                        "name": "Azita Dabiri"
                    },
                    {
                        "name": "Bart De Schutter"
                    }
                ],
                "author_detail": {
                    "name": "Bart De Schutter"
                },
                "author": "Bart De Schutter",
                "arxiv_comment": "12 pages, 8 figures, accepted for publication in Automatica, code can\n  be found at https://github.com/SamuelMallick/dmpcrl-concept/tree/paper-2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05166v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05166v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13652v1",
                "updated": "2024-12-18T09:31:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    31,
                    6,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T09:31:06Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    31,
                    6,
                    2,
                    353,
                    0
                ],
                "title": "RelationField: Relate Anything in Radiance Fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RelationField: Relate Anything in Radiance Fields"
                },
                "summary": "Neural radiance fields are an emerging 3D scene representation and recently\neven been extended to learn features for scene understanding by distilling\nopen-vocabulary features from vision-language models. However, current method\nprimarily focus on object-centric representations, supporting object\nsegmentation or detection, while understanding semantic relationships between\nobjects remains largely unexplored. To address this gap, we propose\nRelationField, the first method to extract inter-object relationships directly\nfrom neural radiance fields. RelationField represents relationships between\nobjects as pairs of rays within a neural radiance field, effectively extending\nits formulation to include implicit relationship queries. To teach\nRelationField complex, open-vocabulary relationships, relationship knowledge is\ndistilled from multi-modal LLMs. To evaluate RelationField, we solve\nopen-vocabulary 3D scene graph generation tasks and relationship-guided\ninstance segmentation, achieving state-of-the-art performance in both tasks.\nSee the project website at https://relationfield.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural radiance fields are an emerging 3D scene representation and recently\neven been extended to learn features for scene understanding by distilling\nopen-vocabulary features from vision-language models. However, current method\nprimarily focus on object-centric representations, supporting object\nsegmentation or detection, while understanding semantic relationships between\nobjects remains largely unexplored. To address this gap, we propose\nRelationField, the first method to extract inter-object relationships directly\nfrom neural radiance fields. RelationField represents relationships between\nobjects as pairs of rays within a neural radiance field, effectively extending\nits formulation to include implicit relationship queries. To teach\nRelationField complex, open-vocabulary relationships, relationship knowledge is\ndistilled from multi-modal LLMs. To evaluate RelationField, we solve\nopen-vocabulary 3D scene graph generation tasks and relationship-guided\ninstance segmentation, achieving state-of-the-art performance in both tasks.\nSee the project website at https://relationfield.github.io."
                },
                "authors": [
                    {
                        "name": "Sebastian Koch"
                    },
                    {
                        "name": "Johanna Wald"
                    },
                    {
                        "name": "Mirco Colosi"
                    },
                    {
                        "name": "Narunas Vaskevicius"
                    },
                    {
                        "name": "Pedro Hermosilla"
                    },
                    {
                        "name": "Federico Tombari"
                    },
                    {
                        "name": "Timo Ropinski"
                    }
                ],
                "author_detail": {
                    "name": "Timo Ropinski"
                },
                "author": "Timo Ropinski",
                "arxiv_comment": "Project page: https://relationfield.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13649v1",
                "updated": "2024-12-18T09:27:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T09:27:33Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation"
                },
                "summary": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods."
                },
                "authors": [
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Zhenglin Wang"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Yilong Lai"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13645v1",
                "updated": "2024-12-18T09:22:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    22,
                    8,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T09:22:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    22,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "On the Role of Model Prior in Real-World Inductive Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Role of Model Prior in Real-World Inductive Reasoning"
                },
                "summary": "Large Language Models (LLMs) show impressive inductive reasoning\ncapabilities, enabling them to generate hypotheses that could generalize\neffectively to new instances when guided by in-context demonstrations. However,\nin real-world applications, LLMs' hypothesis generation is not solely\ndetermined by these demonstrations but is significantly shaped by task-specific\nmodel priors. Despite their critical influence, the distinct contributions of\nmodel priors versus demonstrations to hypothesis generation have been\nunderexplored. This study bridges this gap by systematically evaluating three\ninductive reasoning strategies across five real-world tasks with three LLMs.\nOur empirical findings reveal that, hypothesis generation is primarily driven\nby the model's inherent priors; removing demonstrations results in minimal loss\nof hypothesis quality and downstream usage. Further analysis shows the result\nis consistent across various label formats with different label configurations,\nand prior is hard to override, even under flipped labeling. These insights\nadvance our understanding of the dynamics of hypothesis generation in LLMs and\nhighlight the potential for better utilizing model priors in real-world\ninductive reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show impressive inductive reasoning\ncapabilities, enabling them to generate hypotheses that could generalize\neffectively to new instances when guided by in-context demonstrations. However,\nin real-world applications, LLMs' hypothesis generation is not solely\ndetermined by these demonstrations but is significantly shaped by task-specific\nmodel priors. Despite their critical influence, the distinct contributions of\nmodel priors versus demonstrations to hypothesis generation have been\nunderexplored. This study bridges this gap by systematically evaluating three\ninductive reasoning strategies across five real-world tasks with three LLMs.\nOur empirical findings reveal that, hypothesis generation is primarily driven\nby the model's inherent priors; removing demonstrations results in minimal loss\nof hypothesis quality and downstream usage. Further analysis shows the result\nis consistent across various label formats with different label configurations,\nand prior is hard to override, even under flipped labeling. These insights\nadvance our understanding of the dynamics of hypothesis generation in LLMs and\nhighlight the potential for better utilizing model priors in real-world\ninductive reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Zhuo Liu"
                    },
                    {
                        "name": "Ding Yu"
                    },
                    {
                        "name": "Hangfeng He"
                    }
                ],
                "author_detail": {
                    "name": "Hangfeng He"
                },
                "author": "Hangfeng He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13631v1",
                "updated": "2024-12-18T09:06:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    6,
                    48,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T09:06:48Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    6,
                    48,
                    2,
                    353,
                    0
                ],
                "title": "Mind Your Theory: Theory of Mind Goes Deeper Than Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind Your Theory: Theory of Mind Goes Deeper Than Reasoning"
                },
                "summary": "Theory of Mind (ToM) capabilities in LLMs have recently become a central\nobject of investigation. Cognitive science distinguishes between two steps\nrequired for ToM tasks: 1) determine whether to invoke ToM, which includes the\nappropriate Depth of Mentalizing (DoM), or level of recursion required to\ncomplete a task; and 2) applying the correct inference given the DoM. In this\nposition paper, we first identify several lines of work in different\ncommunities in AI, including LLM benchmarking, ToM add-ons, ToM probing, and\nformal models for ToM. We argue that recent work in AI tends to focus\nexclusively on the second step which are typically framed as static logic\nproblems. We conclude with suggestions for improved evaluation of ToM\ncapabilities inspired by dynamic environments used in cognitive tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theory of Mind (ToM) capabilities in LLMs have recently become a central\nobject of investigation. Cognitive science distinguishes between two steps\nrequired for ToM tasks: 1) determine whether to invoke ToM, which includes the\nappropriate Depth of Mentalizing (DoM), or level of recursion required to\ncomplete a task; and 2) applying the correct inference given the DoM. In this\nposition paper, we first identify several lines of work in different\ncommunities in AI, including LLM benchmarking, ToM add-ons, ToM probing, and\nformal models for ToM. We argue that recent work in AI tends to focus\nexclusively on the second step which are typically framed as static logic\nproblems. We conclude with suggestions for improved evaluation of ToM\ncapabilities inspired by dynamic environments used in cognitive tasks."
                },
                "authors": [
                    {
                        "name": "Eitan Wagner"
                    },
                    {
                        "name": "Nitay Alon"
                    },
                    {
                        "name": "Joseph M. Barnby"
                    },
                    {
                        "name": "Omri Abend"
                    }
                ],
                "author_detail": {
                    "name": "Omri Abend"
                },
                "author": "Omri Abend",
                "arxiv_comment": "4 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13626v1",
                "updated": "2024-12-18T09:04:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    4,
                    55,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T09:04:55Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    4,
                    55,
                    2,
                    353,
                    0
                ],
                "title": "LIFT: Improving Long Context Understanding Through Long Input\n  Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LIFT: Improving Long Context Understanding Through Long Input\n  Fine-Tuning"
                },
                "summary": "Long context understanding remains challenging for large language models due\nto their limited context windows. This paper introduces Long Input Fine-Tuning\n(LIFT) for long context modeling, a novel framework that enhances LLM\nperformance on long-context tasks by adapting model parameters to the context\nat test time. LIFT enables efficient processing of lengthy inputs without the\ncomputational burden of offline long-context adaptation, and can improve the\nlong-context capabilities of arbitrary short-context models. The framework is\nfurther enhanced by integrating in-context learning and pre-LIFT supervised\nfine-tuning. The combination of in-context learning and LIFT enables\nshort-context models like Llama 3 to handle arbitrarily long contexts and\nconsistently improves their performance on popular long-context benchmarks like\nLooGLE and LongBench. We also provide a comprehensive analysis of the strengths\nand limitations of LIFT on long context understanding, offering valuable\ndirections for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context understanding remains challenging for large language models due\nto their limited context windows. This paper introduces Long Input Fine-Tuning\n(LIFT) for long context modeling, a novel framework that enhances LLM\nperformance on long-context tasks by adapting model parameters to the context\nat test time. LIFT enables efficient processing of lengthy inputs without the\ncomputational burden of offline long-context adaptation, and can improve the\nlong-context capabilities of arbitrary short-context models. The framework is\nfurther enhanced by integrating in-context learning and pre-LIFT supervised\nfine-tuning. The combination of in-context learning and LIFT enables\nshort-context models like Llama 3 to handle arbitrarily long contexts and\nconsistently improves their performance on popular long-context benchmarks like\nLooGLE and LongBench. We also provide a comprehensive analysis of the strengths\nand limitations of LIFT on long context understanding, offering valuable\ndirections for future research."
                },
                "authors": [
                    {
                        "name": "Yansheng Mao"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Zilong Zheng"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10843v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10843v3",
                "updated": "2024-12-18T08:54:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    8,
                    54,
                    3,
                    2,
                    353,
                    0
                ],
                "published": "2024-08-19T11:42:54Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    11,
                    42,
                    54,
                    0,
                    232,
                    0
                ],
                "title": "Detecting Wildfires on UAVs with Real-time Segmentation Trained by\n  Larger Teacher Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Wildfires on UAVs with Real-time Segmentation Trained by\n  Larger Teacher Models"
                },
                "summary": "Early detection of wildfires is essential to prevent large-scale fires\nresulting in extensive environmental, structural, and societal damage. Uncrewed\naerial vehicles (UAVs) can cover large remote areas effectively with quick\ndeployment requiring minimal infrastructure and equipping them with small\ncameras and computers enables autonomous real-time detection. In remote areas,\nhowever, detection methods are limited to onboard computation due to the lack\nof high-bandwidth mobile networks. For accurate camera-based localisation,\nsegmentation of the detected smoke is essential but training data for deep\nlearning-based wildfire smoke segmentation is limited. This study shows how\nsmall specialised segmentation models can be trained using only bounding box\nlabels, leveraging zero-shot foundation model supervision. The method offers\nthe advantages of needing only fairly easily obtainable bounding box labels and\nrequiring training solely for the smaller student network. The proposed method\nachieved 63.3% mIoU on a manually annotated and diverse wildfire dataset. The\nused model can perform in real-time at ~25 fps with a UAV-carried NVIDIA Jetson\nOrin NX computer while reliably recognising smoke, as demonstrated at\nreal-world forest burning events. Code is available at:\nhttps://gitlab.com/fgi_nls/public/wildfire-real-time-segmentation",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early detection of wildfires is essential to prevent large-scale fires\nresulting in extensive environmental, structural, and societal damage. Uncrewed\naerial vehicles (UAVs) can cover large remote areas effectively with quick\ndeployment requiring minimal infrastructure and equipping them with small\ncameras and computers enables autonomous real-time detection. In remote areas,\nhowever, detection methods are limited to onboard computation due to the lack\nof high-bandwidth mobile networks. For accurate camera-based localisation,\nsegmentation of the detected smoke is essential but training data for deep\nlearning-based wildfire smoke segmentation is limited. This study shows how\nsmall specialised segmentation models can be trained using only bounding box\nlabels, leveraging zero-shot foundation model supervision. The method offers\nthe advantages of needing only fairly easily obtainable bounding box labels and\nrequiring training solely for the smaller student network. The proposed method\nachieved 63.3% mIoU on a manually annotated and diverse wildfire dataset. The\nused model can perform in real-time at ~25 fps with a UAV-carried NVIDIA Jetson\nOrin NX computer while reliably recognising smoke, as demonstrated at\nreal-world forest burning events. Code is available at:\nhttps://gitlab.com/fgi_nls/public/wildfire-real-time-segmentation"
                },
                "authors": [
                    {
                        "name": "Julius Pesonen"
                    },
                    {
                        "name": "Teemu Hakala"
                    },
                    {
                        "name": "Vin Karjalainen"
                    },
                    {
                        "name": "Niko Koivumki"
                    },
                    {
                        "name": "Lauri Markelin"
                    },
                    {
                        "name": "Anna-Maria Raita-Hakola"
                    },
                    {
                        "name": "Juha Suomalainen"
                    },
                    {
                        "name": "Ilkka Plnen"
                    },
                    {
                        "name": "Eija Honkavaara"
                    }
                ],
                "author_detail": {
                    "name": "Eija Honkavaara"
                },
                "author": "Eija Honkavaara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10843v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10843v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11223v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11223v2",
                "updated": "2024-12-18T08:49:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    8,
                    49,
                    16,
                    2,
                    353,
                    0
                ],
                "published": "2024-11-18T01:25:58Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    1,
                    25,
                    58,
                    0,
                    323,
                    0
                ],
                "title": "Efficient Transfer Learning for Video-language Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Transfer Learning for Video-language Foundation Models"
                },
                "summary": "Pre-trained vision-language models provide a robust foundation for efficient\ntransfer learning across various downstream tasks. In the field of video action\nrecognition, mainstream approaches often introduce additional parameter modules\nto capture temporal information. While the increased model capacity brought by\nthese additional parameters helps better fit the video-specific inductive\nbiases, existing methods require learning a large number of parameters and are\nprone to catastrophic forgetting of the original generalizable knowledge. In\nthis paper, we propose a simple yet effective Multi-modal Spatio-Temporal\nAdapter (MSTA) to improve the alignment between representations in the text and\nvision branches, achieving a balance between general knowledge and\ntask-specific knowledge. Furthermore, to mitigate over-fitting and enhance\ngeneralizability, we introduce a spatio-temporal description-guided consistency\nconstraint. This constraint involves feeding template inputs (i.e., ``a video\nof $\\{\\textbf{cls}\\}$'') into the trainable language branch, while\nLLM-generated spatio-temporal descriptions are input into the pre-trained\nlanguage branch, enforcing consistency between the outputs of the two branches.\nThis mechanism prevents over-fitting to downstream tasks and improves the\ndistinguishability of the trainable branch within the spatio-temporal semantic\nspace. We evaluate the effectiveness of our approach across four tasks:\nzero-shot transfer, few-shot learning, base-to-novel generalization, and\nfully-supervised learning. Compared to many state-of-the-art methods, our MSTA\nachieves outstanding performance across all evaluations, while using only 2-7\\%\nof the trainable parameters in the original model. Code will be avaliable at\nhttps://github.com/chenhaoxing/ETL4Video.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained vision-language models provide a robust foundation for efficient\ntransfer learning across various downstream tasks. In the field of video action\nrecognition, mainstream approaches often introduce additional parameter modules\nto capture temporal information. While the increased model capacity brought by\nthese additional parameters helps better fit the video-specific inductive\nbiases, existing methods require learning a large number of parameters and are\nprone to catastrophic forgetting of the original generalizable knowledge. In\nthis paper, we propose a simple yet effective Multi-modal Spatio-Temporal\nAdapter (MSTA) to improve the alignment between representations in the text and\nvision branches, achieving a balance between general knowledge and\ntask-specific knowledge. Furthermore, to mitigate over-fitting and enhance\ngeneralizability, we introduce a spatio-temporal description-guided consistency\nconstraint. This constraint involves feeding template inputs (i.e., ``a video\nof $\\{\\textbf{cls}\\}$'') into the trainable language branch, while\nLLM-generated spatio-temporal descriptions are input into the pre-trained\nlanguage branch, enforcing consistency between the outputs of the two branches.\nThis mechanism prevents over-fitting to downstream tasks and improves the\ndistinguishability of the trainable branch within the spatio-temporal semantic\nspace. We evaluate the effectiveness of our approach across four tasks:\nzero-shot transfer, few-shot learning, base-to-novel generalization, and\nfully-supervised learning. Compared to many state-of-the-art methods, our MSTA\nachieves outstanding performance across all evaluations, while using only 2-7\\%\nof the trainable parameters in the original model. Code will be avaliable at\nhttps://github.com/chenhaoxing/ETL4Video."
                },
                "authors": [
                    {
                        "name": "Haoxing Chen"
                    },
                    {
                        "name": "Zizheng Huang"
                    },
                    {
                        "name": "Yan Hong"
                    },
                    {
                        "name": "Yanshuo Wang"
                    },
                    {
                        "name": "Zhongcai Lyu"
                    },
                    {
                        "name": "Zhuoer Xu"
                    },
                    {
                        "name": "Jun Lan"
                    },
                    {
                        "name": "Zhangxuan Gu"
                    }
                ],
                "author_detail": {
                    "name": "Zhangxuan Gu"
                },
                "author": "Zhangxuan Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11223v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11223v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13612v1",
                "updated": "2024-12-18T08:42:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    8,
                    42,
                    25,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T08:42:25Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    8,
                    42,
                    25,
                    2,
                    353,
                    0
                ],
                "title": "Are LLMs Good Literature Review Writers? Evaluating the Literature\n  Review Writing Ability of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLMs Good Literature Review Writers? Evaluating the Literature\n  Review Writing Ability of Large Language Models"
                },
                "summary": "The literature review is a crucial form of academic writing that involves\ncomplex processes of literature collection, organization, and summarization.\nThe emergence of large language models (LLMs) has introduced promising tools to\nautomate these processes. However, their actual capabilities in writing\ncomprehensive literature reviews remain underexplored, such as whether they can\ngenerate accurate and reliable references. To address this gap, we propose a\nframework to assess the literature review writing ability of LLMs\nautomatically. We evaluate the performance of LLMs across three tasks:\ngenerating references, writing abstracts, and writing literature reviews. We\nemploy external tools for a multidimensional evaluation, which includes\nassessing hallucination rates in references, semantic coverage, and factual\nconsistency with human-written context. By analyzing the experimental results,\nwe find that, despite advancements, even the most sophisticated models still\ncannot avoid generating hallucinated references. Additionally, different models\nexhibit varying performance in literature review writing across different\ndisciplines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The literature review is a crucial form of academic writing that involves\ncomplex processes of literature collection, organization, and summarization.\nThe emergence of large language models (LLMs) has introduced promising tools to\nautomate these processes. However, their actual capabilities in writing\ncomprehensive literature reviews remain underexplored, such as whether they can\ngenerate accurate and reliable references. To address this gap, we propose a\nframework to assess the literature review writing ability of LLMs\nautomatically. We evaluate the performance of LLMs across three tasks:\ngenerating references, writing abstracts, and writing literature reviews. We\nemploy external tools for a multidimensional evaluation, which includes\nassessing hallucination rates in references, semantic coverage, and factual\nconsistency with human-written context. By analyzing the experimental results,\nwe find that, despite advancements, even the most sophisticated models still\ncannot avoid generating hallucinated references. Additionally, different models\nexhibit varying performance in literature review writing across different\ndisciplines."
                },
                "authors": [
                    {
                        "name": "Xuemei Tang"
                    },
                    {
                        "name": "Xufeng Duan"
                    },
                    {
                        "name": "Zhenguang G. Cai"
                    }
                ],
                "author_detail": {
                    "name": "Zhenguang G. Cai"
                },
                "author": "Zhenguang G. Cai",
                "arxiv_comment": "12 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13602v1",
                "updated": "2024-12-18T08:32:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    8,
                    32,
                    53,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T08:32:53Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    8,
                    32,
                    53,
                    2,
                    353,
                    0
                ],
                "title": "Beyond Outcomes: Transparent Assessment of LLM Reasoning in Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Outcomes: Transparent Assessment of LLM Reasoning in Games"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in real-world\napplications that demand complex reasoning. To track progress, robust\nbenchmarks are required to evaluate their capabilities beyond superficial\npattern recognition. However, current LLM reasoning benchmarks often face\nchallenges such as insufficient interpretability, performance saturation or\ndata contamination. To address these challenges, we introduce GAMEBoT, a gaming\narena designed for rigorous and transparent assessment of LLM reasoning\ncapabilities. GAMEBoT decomposes complex reasoning in games into predefined\nmodular subproblems. This decomposition allows us to design a suite of\nChain-of-Thought (CoT) prompts that leverage domain knowledge to guide LLMs in\naddressing these subproblems before action selection. Furthermore, we develop a\nsuite of rule-based algorithms to generate ground truth for these subproblems,\nenabling rigorous validation of the LLMs' intermediate reasoning steps. This\napproach facilitates evaluation of both the quality of final actions and the\naccuracy of the underlying reasoning process. GAMEBoT also naturally alleviates\nthe risk of data contamination through dynamic games and head-to-head LLM\ncompetitions. We benchmark 17 prominent LLMs across eight games, encompassing\nvarious strategic abilities and game characteristics. Our results suggest that\nGAMEBoT presents a significant challenge, even when LLMs are provided with\ndetailed CoT prompts. Project page: \\url{https://visual-ai.github.io/gamebot}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in real-world\napplications that demand complex reasoning. To track progress, robust\nbenchmarks are required to evaluate their capabilities beyond superficial\npattern recognition. However, current LLM reasoning benchmarks often face\nchallenges such as insufficient interpretability, performance saturation or\ndata contamination. To address these challenges, we introduce GAMEBoT, a gaming\narena designed for rigorous and transparent assessment of LLM reasoning\ncapabilities. GAMEBoT decomposes complex reasoning in games into predefined\nmodular subproblems. This decomposition allows us to design a suite of\nChain-of-Thought (CoT) prompts that leverage domain knowledge to guide LLMs in\naddressing these subproblems before action selection. Furthermore, we develop a\nsuite of rule-based algorithms to generate ground truth for these subproblems,\nenabling rigorous validation of the LLMs' intermediate reasoning steps. This\napproach facilitates evaluation of both the quality of final actions and the\naccuracy of the underlying reasoning process. GAMEBoT also naturally alleviates\nthe risk of data contamination through dynamic games and head-to-head LLM\ncompetitions. We benchmark 17 prominent LLMs across eight games, encompassing\nvarious strategic abilities and game characteristics. Our results suggest that\nGAMEBoT presents a significant challenge, even when LLMs are provided with\ndetailed CoT prompts. Project page: \\url{https://visual-ai.github.io/gamebot}"
                },
                "authors": [
                    {
                        "name": "Wenye Lin"
                    },
                    {
                        "name": "Jonathan Roberts"
                    },
                    {
                        "name": "Yunhan Yang"
                    },
                    {
                        "name": "Samuel Albanie"
                    },
                    {
                        "name": "Zongqing Lu"
                    },
                    {
                        "name": "Kai Han"
                    }
                ],
                "author_detail": {
                    "name": "Kai Han"
                },
                "author": "Kai Han",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]