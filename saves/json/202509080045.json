[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.04377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04377v1",
                "updated": "2025-09-04T16:40:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    40,
                    1,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T16:40:01Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    40,
                    1,
                    3,
                    247,
                    0
                ],
                "title": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient\n  Large Language Model Inference"
                },
                "summary": "KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks."
                },
                "authors": [
                    {
                        "name": "Krishna Teja Chitty-Venkata"
                    },
                    {
                        "name": "Jie Ye"
                    },
                    {
                        "name": "Xian-He Sun"
                    },
                    {
                        "name": "Anthony Kougkas"
                    },
                    {
                        "name": "Murali Emani"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    },
                    {
                        "name": "Bogdan Nicolae"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Nicolae"
                },
                "author": "Bogdan Nicolae",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12084v2",
                "updated": "2025-09-04T15:21:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    21,
                    11,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-21T12:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "title": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis"
                },
                "summary": "This study presents a comprehensive multi-level analysis of the NVIDIA Hopper\nGPU architecture, focusing on its performance characteristics and novel\nfeatures. We benchmark Hopper's memory subsystem, highlighting improvements in\nthe L2 partitioned cache and global memory access compared to Ampere and Ada\nLovelace. The evaluation of Hopper's fourth-generation tensor cores reveals the\nbenefits of FP8 precision and asynchronous wgmma instructions for matrix\noperations. Additionally, we investigate the performance of DPX instructions\nfor dynamic programming, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. Through multi-level evaluation, we discover that the Hopper\narchitecture demonstrates significant acceleration potential in real-world\napplications. For instance, the asynchronous programming model supported by TMA\nachieves a 1.5x speedup in matrix multiplication, FP8 delivers nearly double\nthe performance of FP16, and DPX instructions accelerate a computational\nbiology algorithm by at least 4.75x. Our findings provide actionable insights\nfor optimizing compute-intensive workloads, from AI training to bioinformatics,\non Hopper GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a comprehensive multi-level analysis of the NVIDIA Hopper\nGPU architecture, focusing on its performance characteristics and novel\nfeatures. We benchmark Hopper's memory subsystem, highlighting improvements in\nthe L2 partitioned cache and global memory access compared to Ampere and Ada\nLovelace. The evaluation of Hopper's fourth-generation tensor cores reveals the\nbenefits of FP8 precision and asynchronous wgmma instructions for matrix\noperations. Additionally, we investigate the performance of DPX instructions\nfor dynamic programming, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. Through multi-level evaluation, we discover that the Hopper\narchitecture demonstrates significant acceleration potential in real-world\napplications. For instance, the asynchronous programming model supported by TMA\nachieves a 1.5x speedup in matrix multiplication, FP8 delivers nearly double\nthe performance of FP16, and DPX instructions accelerate a computational\nbiology algorithm by at least 4.75x. Our findings provide actionable insights\nfor optimizing compute-intensive workloads, from AI training to bioinformatics,\non Hopper GPUs."
                },
                "authors": [
                    {
                        "name": "Weile Luo"
                    },
                    {
                        "name": "Ruibo Fan"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Hongyuan Liu"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2402.13499",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v2",
                "updated": "2025-09-04T13:14:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    14,
                    33,
                    3,
                    247,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04185v1",
                "updated": "2025-09-04T13:02:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    2,
                    39,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T13:02:39Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    2,
                    39,
                    3,
                    247,
                    0
                ],
                "title": "Set Block Decoding is a Language Model Inference Accelerator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Set Block Decoding is a Language Model Inference Accelerator"
                },
                "summary": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training."
                },
                "authors": [
                    {
                        "name": "Itai Gat"
                    },
                    {
                        "name": "Heli Ben-Hamu"
                    },
                    {
                        "name": "Marton Havasi"
                    },
                    {
                        "name": "Daniel Haziza"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Gabriel Synnaeve"
                    },
                    {
                        "name": "David Lopez-Paz"
                    },
                    {
                        "name": "Brian Karrer"
                    },
                    {
                        "name": "Yaron Lipman"
                    }
                ],
                "author_detail": {
                    "name": "Yaron Lipman"
                },
                "author": "Yaron Lipman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04180v1",
                "updated": "2025-09-04T12:54:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    54,
                    32,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T12:54:32Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    54,
                    32,
                    3,
                    247,
                    0
                ],
                "title": "VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer\n  Vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer\n  Vision"
                },
                "summary": "AI models rely on annotated data to learn pattern and perform prediction.\nAnnotation is usually a labor-intensive step that require associating labels\nranging from a simple classification label to more complex tasks such as object\ndetection, oriented bounding box estimation, and instance segmentation.\nTraditional tools often require extensive manual input, limiting scalability\nfor large datasets. To address this, we introduce VisioFirm, an open-source web\napplication designed to streamline image labeling through AI-assisted\nautomation. VisioFirm integrates state-of-the-art foundation models into an\ninterface with a filtering pipeline to reduce human-in-the-loop efforts. This\nhybrid approach employs CLIP combined with pre-trained detectors like\nUltralytics models for common classes and zero-shot models such as Grounding\nDINO for custom labels, generating initial annotations with low-confidence\nthresholding to maximize recall. Through this framework, when tested on\nCOCO-type of classes, initial prediction have been proven to be mostly correct\nthough the users can refine these via interactive tools supporting bounding\nboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has\non-the-fly segmentation powered by Segment Anything accelerated through WebGPU\nfor browser-side efficiency. The tool supports multiple export formats (YOLO,\nCOCO, Pascal VOC, CSV) and operates offline after model caching, enhancing\naccessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort\nthrough benchmarks on diverse datasets, while maintaining high annotation\naccuracy via clustering of connected CLIP-based disambiguate components and\nIoU-graph for redundant detection suppression. VisioFirm can be accessed from\n\\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI models rely on annotated data to learn pattern and perform prediction.\nAnnotation is usually a labor-intensive step that require associating labels\nranging from a simple classification label to more complex tasks such as object\ndetection, oriented bounding box estimation, and instance segmentation.\nTraditional tools often require extensive manual input, limiting scalability\nfor large datasets. To address this, we introduce VisioFirm, an open-source web\napplication designed to streamline image labeling through AI-assisted\nautomation. VisioFirm integrates state-of-the-art foundation models into an\ninterface with a filtering pipeline to reduce human-in-the-loop efforts. This\nhybrid approach employs CLIP combined with pre-trained detectors like\nUltralytics models for common classes and zero-shot models such as Grounding\nDINO for custom labels, generating initial annotations with low-confidence\nthresholding to maximize recall. Through this framework, when tested on\nCOCO-type of classes, initial prediction have been proven to be mostly correct\nthough the users can refine these via interactive tools supporting bounding\nboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has\non-the-fly segmentation powered by Segment Anything accelerated through WebGPU\nfor browser-side efficiency. The tool supports multiple export formats (YOLO,\nCOCO, Pascal VOC, CSV) and operates offline after model caching, enhancing\naccessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort\nthrough benchmarks on diverse datasets, while maintaining high annotation\naccuracy via clustering of connected CLIP-based disambiguate components and\nIoU-graph for redundant detection suppression. VisioFirm can be accessed from\n\\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}."
                },
                "authors": [
                    {
                        "name": "Safouane El Ghazouali"
                    },
                    {
                        "name": "Umberto Michelucci"
                    }
                ],
                "author_detail": {
                    "name": "Umberto Michelucci"
                },
                "author": "Umberto Michelucci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19740v2",
                "updated": "2025-09-04T09:08:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    9,
                    8,
                    29,
                    3,
                    247,
                    0
                ],
                "published": "2025-08-27T10:11:27Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    11,
                    27,
                    2,
                    239,
                    0
                ],
                "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval"
                },
                "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04010v1",
                "updated": "2025-09-04T08:41:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    41,
                    6,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T08:41:06Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    41,
                    6,
                    3,
                    247,
                    0
                ],
                "title": "Systematic Timing Leakage Analysis of NIST PQDSS Candidates: Tooling and\n  Lessons Learned",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Timing Leakage Analysis of NIST PQDSS Candidates: Tooling and\n  Lessons Learned"
                },
                "summary": "The PQDSS standardization process requires cryptographic primitives to be\nfree from vulnerabilities, including timing and cache side-channels. Resistance\nto timing leakage is therefore an essential property, and achieving this\ntypically relies on software implementations that follow constant-time\nprinciples. Moreover, ensuring that all implementations are constant-time is\ncrucial for fair performance comparisons, as secure implementations often incur\nadditional overhead. Such analysis also helps identify scheme proposals that\nare inherently difficult to implement in constant time. Because constant-time\nproperties can be broken during compilation, it is often necessary to analyze\nthe compiled binary directly. Since manual binary analysis is extremely\nchallenging, automated analysis becomes highly important. Although several\ntools exist to assist with such analysis, they often have usability limitations\nand are difficult to set up correctly. To support the developers besides the\nNIST committee in verifying candidates, we developed a toolchain that automates\nconfiguration, execution, and result analysis for several widely used\nconstant-time analysis tools. We selected TIMECOP and Binsec/Rel2 to verify\nconstant-time policy compliance at the binary level, and dudect and RTLF to\ndetect side-channel vulnerabilities through statistical analysis of execution\ntime behavior. We demonstrate its effectiveness and practicability by\nevaluating the NIST PQDSS round 1 and round 2 implementations. We reported 26\nissues in total to the respective developers, and 5 of them have already been\nfixed. We also discuss our different findings, as well as the benefits of\nshortcomings of the different tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The PQDSS standardization process requires cryptographic primitives to be\nfree from vulnerabilities, including timing and cache side-channels. Resistance\nto timing leakage is therefore an essential property, and achieving this\ntypically relies on software implementations that follow constant-time\nprinciples. Moreover, ensuring that all implementations are constant-time is\ncrucial for fair performance comparisons, as secure implementations often incur\nadditional overhead. Such analysis also helps identify scheme proposals that\nare inherently difficult to implement in constant time. Because constant-time\nproperties can be broken during compilation, it is often necessary to analyze\nthe compiled binary directly. Since manual binary analysis is extremely\nchallenging, automated analysis becomes highly important. Although several\ntools exist to assist with such analysis, they often have usability limitations\nand are difficult to set up correctly. To support the developers besides the\nNIST committee in verifying candidates, we developed a toolchain that automates\nconfiguration, execution, and result analysis for several widely used\nconstant-time analysis tools. We selected TIMECOP and Binsec/Rel2 to verify\nconstant-time policy compliance at the binary level, and dudect and RTLF to\ndetect side-channel vulnerabilities through statistical analysis of execution\ntime behavior. We demonstrate its effectiveness and practicability by\nevaluating the NIST PQDSS round 1 and round 2 implementations. We reported 26\nissues in total to the respective developers, and 5 of them have already been\nfixed. We also discuss our different findings, as well as the benefits of\nshortcomings of the different tools."
                },
                "authors": [
                    {
                        "name": "Olivier Adjonyo"
                    },
                    {
                        "name": "Sebastien Bardin"
                    },
                    {
                        "name": "Emanuele Bellini"
                    },
                    {
                        "name": "Gilbert Ndollane Dione"
                    },
                    {
                        "name": "Mahmudul Faisal Al Ameen"
                    },
                    {
                        "name": "Robert Merget"
                    },
                    {
                        "name": "Frederic Recoules"
                    },
                    {
                        "name": "Yanis Sellami"
                    }
                ],
                "author_detail": {
                    "name": "Yanis Sellami"
                },
                "author": "Yanis Sellami",
                "arxiv_comment": "20 pages, 1 figure, to be published and presented at Sixth PQC\n  Standardization Conference by NIST, partially supported by the \"France 2030\"\n  government investment plan managed by the French National Research Agency,\n  under the reference ANR-22-PECY-0005",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12689v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12689v3",
                "updated": "2025-09-04T06:20:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    6,
                    20,
                    55,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-22T07:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "IC-Cache: Efficient Large Language Model Serving via In-context Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IC-Cache: Efficient Large Language Model Serving via In-context Caching"
                },
                "summary": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 70% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge transfer among requests. However, naively caching and reusing past\nresponses leads to a big quality drop. In this paper, we introduce IC-Cache, a\ncaching system that enables live LLM capability augmentation to improve serving\nefficiency: by leveraging historical request-response pairs from larger models\nas in-context examples, IC-Cache empowers small LLMs to imitate and even exceed\nthe compositional abilities (e.g., reasoning) of their larger counterparts,\nenabling selective offloading of requests to reduce cost and latency. Achieving\nthis live augmentation at scale introduces intricate trade-offs between\nresponse quality, latency, and system throughput. For a new request, IC-Cache\nefficiently selects similar, high-utility examples to prepend them to the new\nrequest's input. At scale, it adaptively routes requests across LLMs of varying\ncapabilities, accounting for response quality and serving loads. IC-Cache\nemploys a cost-aware cache replay mechanism that refines example quality\noffline to maximize online cache utility and efficiency. Evaluations on\nmillions of realistic requests demonstrate that IC-Cache improves LLM serving\nthroughput by 1.4-5.9x and reduces latency by 28-71% without hurting response\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 70% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge transfer among requests. However, naively caching and reusing past\nresponses leads to a big quality drop. In this paper, we introduce IC-Cache, a\ncaching system that enables live LLM capability augmentation to improve serving\nefficiency: by leveraging historical request-response pairs from larger models\nas in-context examples, IC-Cache empowers small LLMs to imitate and even exceed\nthe compositional abilities (e.g., reasoning) of their larger counterparts,\nenabling selective offloading of requests to reduce cost and latency. Achieving\nthis live augmentation at scale introduces intricate trade-offs between\nresponse quality, latency, and system throughput. For a new request, IC-Cache\nefficiently selects similar, high-utility examples to prepend them to the new\nrequest's input. At scale, it adaptively routes requests across LLMs of varying\ncapabilities, accounting for response quality and serving loads. IC-Cache\nemploys a cost-aware cache replay mechanism that refines example quality\noffline to maximize online cache utility and efficiency. Evaluations on\nmillions of realistic requests demonstrate that IC-Cache improves LLM serving\nthroughput by 1.4-5.9x and reduces latency by 28-71% without hurting response\nquality."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Nikhil Sarda"
                    },
                    {
                        "name": "Lillian Tsai"
                    },
                    {
                        "name": "Jiaming Shen"
                    },
                    {
                        "name": "Yanqi Zhou"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Henry M. Levy"
                    },
                    {
                        "name": "David Culler"
                    }
                ],
                "author_detail": {
                    "name": "David Culler"
                },
                "author": "David Culler",
                "arxiv_doi": "10.1145/3731569.3764829",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731569.3764829",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.12689v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12689v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01228v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01228v2",
                "updated": "2025-09-03T20:54:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    20,
                    54,
                    57,
                    2,
                    246,
                    0
                ],
                "published": "2024-10-02T04:12:13Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    4,
                    12,
                    13,
                    2,
                    276,
                    0
                ],
                "title": "ConServe: Fine-Grained GPU Harvesting for LLM Online and Offline\n  Co-Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConServe: Fine-Grained GPU Harvesting for LLM Online and Offline\n  Co-Serving"
                },
                "summary": "Large language model (LLM) serving demands low latency and high throughput,\nbut high load variability makes it challenging to achieve high GPU utilization.\nIn this paper, we identify a synergetic but overlooked opportunity to co-serve\nlatency-critical online requests alongside latency-tolerant offline tasks such\nas model benchmarking. While promising, existing serving systems fail to\nco-serve them efficiently, as their coarse-grained resource management at the\nrequest or iteration level cannot harvest millisecond-level GPU idle cycles\nwithout introducing interference that violates online latency objectives.\nConServe is a new LLM co-serving system that achieves high throughput and\nstrong online latency guarantees by managing resources at finer granularities.\nConServe introduces three techniques: (1) a latency-aware token-level scheduler\nthat precisely sizes offline batches and tokens to fit within online latency\nobjectives; (2) sub-iteration, layer-wise preemption that allows offline tasks\nto yield to online load spikes; and (3) incremental KV cache management that\nenables preempting and resuming offline requests at near-zero cost. Evaluations\nwith Llama-3.1 and Qwen-2.5 models on real-world workloads show that ConServe\ndelivers an average of 2.2$\\times$ higher throughput and reduces online serving\ntail latency by 2.9$\\times$ on average compared to state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving demands low latency and high throughput,\nbut high load variability makes it challenging to achieve high GPU utilization.\nIn this paper, we identify a synergetic but overlooked opportunity to co-serve\nlatency-critical online requests alongside latency-tolerant offline tasks such\nas model benchmarking. While promising, existing serving systems fail to\nco-serve them efficiently, as their coarse-grained resource management at the\nrequest or iteration level cannot harvest millisecond-level GPU idle cycles\nwithout introducing interference that violates online latency objectives.\nConServe is a new LLM co-serving system that achieves high throughput and\nstrong online latency guarantees by managing resources at finer granularities.\nConServe introduces three techniques: (1) a latency-aware token-level scheduler\nthat precisely sizes offline batches and tokens to fit within online latency\nobjectives; (2) sub-iteration, layer-wise preemption that allows offline tasks\nto yield to online load spikes; and (3) incremental KV cache management that\nenables preempting and resuming offline requests at near-zero cost. Evaluations\nwith Llama-3.1 and Qwen-2.5 models on real-world workloads show that ConServe\ndelivers an average of 2.2$\\times$ higher throughput and reduces online serving\ntail latency by 2.9$\\times$ on average compared to state-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Yifan Qiao"
                    },
                    {
                        "name": "Shu Anzai"
                    },
                    {
                        "name": "Shan Yu"
                    },
                    {
                        "name": "Haoran Ma"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Miryung Kim"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jiarong Xing"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Harry Xu"
                    }
                ],
                "author_detail": {
                    "name": "Harry Xu"
                },
                "author": "Harry Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01228v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01228v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03394v1",
                "updated": "2025-09-03T15:15:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    15,
                    44,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T15:15:44Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    15,
                    44,
                    2,
                    246,
                    0
                ],
                "title": "CloudFormer: An Attention-based Performance Prediction for Public Clouds\n  with Unknown Workload",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CloudFormer: An Attention-based Performance Prediction for Public Clouds\n  with Unknown Workload"
                },
                "summary": "Cloud platforms are increasingly relied upon to host diverse,\nresource-intensive workloads due to their scalability, flexibility, and\ncost-efficiency. In multi-tenant cloud environments, virtual machines are\nconsolidated on shared physical servers to improve resource utilization. While\nvirtualization guarantees resource partitioning for CPU, memory, and storage,\nit cannot ensure performance isolation. Competition for shared resources such\nas last-level cache, memory bandwidth, and network interfaces often leads to\nsevere performance degradation. Existing management techniques, including VM\nscheduling and resource provisioning, require accurate performance prediction\nto mitigate interference. However, this remains challenging in public clouds\ndue to the black-box nature of VMs and the highly dynamic nature of workloads.\nTo address these limitations, we propose CloudFormer, a dual-branch\nTransformer-based model designed to predict VM performance degradation in\nblack-box environments. CloudFormer jointly models temporal dynamics and\nsystem-level interactions, leveraging 206 system metrics at one-second\nresolution across both static and dynamic scenarios. This design enables the\nmodel to capture transient interference effects and adapt to varying workload\nconditions without scenario-specific tuning. Complementing the methodology, we\nprovide a fine-grained dataset that significantly expands the temporal\nresolution and metric diversity compared to existing benchmarks. Experimental\nresults demonstrate that CloudFormer consistently outperforms state-of-the-art\nbaselines across multiple evaluation metrics, achieving robust generalization\nacross diverse and previously unseen workloads. Notably, CloudFormer attains a\nmean absolute error (MAE) of just 7.8%, representing a substantial improvement\nin predictive accuracy and outperforming existing methods at least by 28%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud platforms are increasingly relied upon to host diverse,\nresource-intensive workloads due to their scalability, flexibility, and\ncost-efficiency. In multi-tenant cloud environments, virtual machines are\nconsolidated on shared physical servers to improve resource utilization. While\nvirtualization guarantees resource partitioning for CPU, memory, and storage,\nit cannot ensure performance isolation. Competition for shared resources such\nas last-level cache, memory bandwidth, and network interfaces often leads to\nsevere performance degradation. Existing management techniques, including VM\nscheduling and resource provisioning, require accurate performance prediction\nto mitigate interference. However, this remains challenging in public clouds\ndue to the black-box nature of VMs and the highly dynamic nature of workloads.\nTo address these limitations, we propose CloudFormer, a dual-branch\nTransformer-based model designed to predict VM performance degradation in\nblack-box environments. CloudFormer jointly models temporal dynamics and\nsystem-level interactions, leveraging 206 system metrics at one-second\nresolution across both static and dynamic scenarios. This design enables the\nmodel to capture transient interference effects and adapt to varying workload\nconditions without scenario-specific tuning. Complementing the methodology, we\nprovide a fine-grained dataset that significantly expands the temporal\nresolution and metric diversity compared to existing benchmarks. Experimental\nresults demonstrate that CloudFormer consistently outperforms state-of-the-art\nbaselines across multiple evaluation metrics, achieving robust generalization\nacross diverse and previously unseen workloads. Notably, CloudFormer attains a\nmean absolute error (MAE) of just 7.8%, representing a substantial improvement\nin predictive accuracy and outperforming existing methods at least by 28%."
                },
                "authors": [
                    {
                        "name": "Amirhossein Shahbazinia"
                    },
                    {
                        "name": "Darong Huang"
                    },
                    {
                        "name": "Luis Costero"
                    },
                    {
                        "name": "David Atienza"
                    }
                ],
                "author_detail": {
                    "name": "David Atienza"
                },
                "author": "David Atienza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00079v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00079v4",
                "updated": "2025-09-03T14:56:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    56,
                    29,
                    2,
                    246,
                    0
                ],
                "published": "2024-06-24T02:05:32Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    2,
                    5,
                    32,
                    0,
                    176,
                    0
                ],
                "title": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving"
                },
                "summary": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests."
                },
                "authors": [
                    {
                        "name": "Ruoyu Qin"
                    },
                    {
                        "name": "Zheming Li"
                    },
                    {
                        "name": "Weiran He"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yongwei Wu"
                    },
                    {
                        "name": "Weimin Zheng"
                    },
                    {
                        "name": "Xinran Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xinran Xu"
                },
                "author": "Xinran Xu",
                "arxiv_comment": "23 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00079v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00079v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03377v1",
                "updated": "2025-09-03T14:53:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    53,
                    45,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T14:53:45Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    53,
                    45,
                    2,
                    246,
                    0
                ],
                "title": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing"
                },
                "summary": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04416v2",
                "updated": "2025-09-03T14:28:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    28,
                    23,
                    2,
                    246,
                    0
                ],
                "published": "2025-07-06T15:08:49Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "title": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based\n  Sequence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based\n  Sequence Modeling"
                },
                "summary": "Transformers have become the cornerstone of modern large-scale language\nmodels, but their reliance on softmax attention poses a computational\nbottleneck at both training and inference. Recurrent models offer high\nefficiency, but compressing the full sequence into a fixed-size and holistic\nrepresentation suffers from memory degradation in long contexts and limits\nfine-grained retrieval. To address this, we propose RAT, an intermediate design\nthat bridges the efficiency of RNNs and capacity of attention. RAT partitions\nthe input into chunks, applies recurrence within each chunk for local\ndependencies, and softmax-based attention across chunks for long-range\ninteractions. This design mitigates memory degradation and enables direct\naccess to distant tokens, while retaining computational efficiency.\nEmpirically, with a chunk size of 16, the RAT block achieves a 7x improvement\nin training speed with 100K token sequences and 9x in generation at the 4K\nposition, while maintaining similar performance compared to standard attention.\nWe demonstrate this by training 1.3B parameter models from scratch and\nperforming large-scale evaluations, including short- and long-context\nbenchmarks, as well as supervised fine-tuning~(SFT). We further propose a\nhybrid architecture that interleaves RAT with local attention. By combining\nefficient long-range modeling with strong local interactions, this hybrid\ndesign not only improves inference speed and reduces cache memory usage, but\nalso consistently enhances performance and shows the overall best results. Code\nis available at https://github.com/CLAIRE-Labo/RAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the cornerstone of modern large-scale language\nmodels, but their reliance on softmax attention poses a computational\nbottleneck at both training and inference. Recurrent models offer high\nefficiency, but compressing the full sequence into a fixed-size and holistic\nrepresentation suffers from memory degradation in long contexts and limits\nfine-grained retrieval. To address this, we propose RAT, an intermediate design\nthat bridges the efficiency of RNNs and capacity of attention. RAT partitions\nthe input into chunks, applies recurrence within each chunk for local\ndependencies, and softmax-based attention across chunks for long-range\ninteractions. This design mitigates memory degradation and enables direct\naccess to distant tokens, while retaining computational efficiency.\nEmpirically, with a chunk size of 16, the RAT block achieves a 7x improvement\nin training speed with 100K token sequences and 9x in generation at the 4K\nposition, while maintaining similar performance compared to standard attention.\nWe demonstrate this by training 1.3B parameter models from scratch and\nperforming large-scale evaluations, including short- and long-context\nbenchmarks, as well as supervised fine-tuning~(SFT). We further propose a\nhybrid architecture that interleaves RAT with local attention. By combining\nefficient long-range modeling with strong local interactions, this hybrid\ndesign not only improves inference speed and reduces cache memory usage, but\nalso consistently enhances performance and shows the overall best results. Code\nis available at https://github.com/CLAIRE-Labo/RAT."
                },
                "authors": [
                    {
                        "name": "Xiuying Wei"
                    },
                    {
                        "name": "Anunay Yadav"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03560v1",
                "updated": "2025-09-03T11:23:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    23,
                    35,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T11:23:35Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    23,
                    35,
                    2,
                    246,
                    0
                ],
                "title": "A Cegar-centric Bounded Reachability Analysis for Compositional Affine\n  Hybrid Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Cegar-centric Bounded Reachability Analysis for Compositional Affine\n  Hybrid Systems"
                },
                "summary": "Reachability analysis of compositional hybrid systems, where individual\ncomponents are modeled as hybrid automata, poses unique challenges. In addition\nto preserving the compositional semantics while computing system behaviors,\nalgorithms have to cater to the explosion in the number of locations in the\nparallel product automaton. In this paper, we propose a bounded reachability\nanalysis algorithm for compositional hybrid systems with piecewise affine\ndynamics, based on the principle of counterexample guided abstraction\nrefinement (CEGAR). In particular, the algorithm searches for a counterexample\nin the discrete abstraction of the composition model, without explicitly\ncomputing a product automaton. When a counterexample is discovered in the\nabstraction, its validity is verified by a refinement of the state-space guided\nby the abstract counterexample. The state-space refinement is through a\nsymbolic reachability analysis, particularly using a state-of-the-art algorithm\nwith support functions as the continuous state representation. In addition, the\nalgorithm mixes different semantics of composition with the objective of\nimproved efficiency. Step compositional semantics is followed while exploring\nthe abstract (discrete) state-space, while shallow compositional semantics is\nfollowed during state-space refinement with symbolic reachability analysis.\nOptimizations such as caching the results of the symbolic reachability\nanalysis, which can be later reused, have been proposed. We implement this\nalgorithm in the tool SAT-Reach and demonstrate the scalability benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reachability analysis of compositional hybrid systems, where individual\ncomponents are modeled as hybrid automata, poses unique challenges. In addition\nto preserving the compositional semantics while computing system behaviors,\nalgorithms have to cater to the explosion in the number of locations in the\nparallel product automaton. In this paper, we propose a bounded reachability\nanalysis algorithm for compositional hybrid systems with piecewise affine\ndynamics, based on the principle of counterexample guided abstraction\nrefinement (CEGAR). In particular, the algorithm searches for a counterexample\nin the discrete abstraction of the composition model, without explicitly\ncomputing a product automaton. When a counterexample is discovered in the\nabstraction, its validity is verified by a refinement of the state-space guided\nby the abstract counterexample. The state-space refinement is through a\nsymbolic reachability analysis, particularly using a state-of-the-art algorithm\nwith support functions as the continuous state representation. In addition, the\nalgorithm mixes different semantics of composition with the objective of\nimproved efficiency. Step compositional semantics is followed while exploring\nthe abstract (discrete) state-space, while shallow compositional semantics is\nfollowed during state-space refinement with symbolic reachability analysis.\nOptimizations such as caching the results of the symbolic reachability\nanalysis, which can be later reused, have been proposed. We implement this\nalgorithm in the tool SAT-Reach and demonstrate the scalability benefits."
                },
                "authors": [
                    {
                        "name": "Atanu Kundu"
                    },
                    {
                        "name": "Pratyay Sarkar"
                    },
                    {
                        "name": "Rajarshi Ray"
                    }
                ],
                "author_detail": {
                    "name": "Rajarshi Ray"
                },
                "author": "Rajarshi Ray",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03136v1",
                "updated": "2025-09-03T08:38:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    40,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T08:38:40Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    40,
                    2,
                    246,
                    0
                ],
                "title": "Adaptive KV-Cache Compression without Manually Setting Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive KV-Cache Compression without Manually Setting Budget"
                },
                "summary": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable."
                },
                "authors": [
                    {
                        "name": "Chenxia Tang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20353v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20353v2",
                "updated": "2025-09-03T06:56:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    6,
                    56,
                    21,
                    2,
                    246,
                    0
                ],
                "published": "2025-05-26T05:58:49Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    58,
                    49,
                    0,
                    146,
                    0
                ],
                "title": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation"
                },
                "summary": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Yanxuan Yu"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Ben Lengerich"
                    },
                    {
                        "name": "Ying Nian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ying Nian Wu"
                },
                "author": "Ying Nian Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20353v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20353v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18002v2",
                "updated": "2025-09-02T18:10:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    18,
                    10,
                    0,
                    1,
                    245,
                    0
                ],
                "published": "2024-10-23T16:25:22Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "title": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges"
                },
                "summary": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks."
                },
                "authors": [
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Hanzhi Yu"
                    },
                    {
                        "name": "Mingzhe Chen"
                    },
                    {
                        "name": "Yuchen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Liu"
                },
                "author": "Yuchen Liu",
                "arxiv_comment": "Under Major Revision in IEEE Network",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02532v1",
                "updated": "2025-09-02T17:35:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    17,
                    35,
                    42,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T17:35:42Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    17,
                    35,
                    42,
                    1,
                    245,
                    0
                ],
                "title": "A Novel Coded Caching Scheme for Partially Cooperative Device-to-Device\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Coded Caching Scheme for Partially Cooperative Device-to-Device\n  Networks"
                },
                "summary": "Device-to-device (D2D) communication is one of the most promising techniques\nfor future wireless cellular communication systems. This paper considers coded\ncaching in a partially cooperative wireless D2D network, where only a subset of\nusers transmit during delivery, while all users request files. The\nnon-transmitting users are referred to as selfish users. All existing schemes\nthat do not require knowledge of the identity of selfish users before content\nplacement are limited to the high-memory regime, particularly when the number\nof selfish users is large. We propose a novel coded caching scheme for a\npartially cooperative D2D network that operates in all feasible memory regimes,\nregardless of the number of selfish users. We also derive a lower bound on the\ntransmission load of a partially cooperative D2D coded caching scheme. Using\nthis bound, the proposed scheme is shown to be optimal in the high-memory\nregime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Device-to-device (D2D) communication is one of the most promising techniques\nfor future wireless cellular communication systems. This paper considers coded\ncaching in a partially cooperative wireless D2D network, where only a subset of\nusers transmit during delivery, while all users request files. The\nnon-transmitting users are referred to as selfish users. All existing schemes\nthat do not require knowledge of the identity of selfish users before content\nplacement are limited to the high-memory regime, particularly when the number\nof selfish users is large. We propose a novel coded caching scheme for a\npartially cooperative D2D network that operates in all feasible memory regimes,\nregardless of the number of selfish users. We also derive a lower bound on the\ntransmission load of a partially cooperative D2D coded caching scheme. Using\nthis bound, the proposed scheme is shown to be optimal in the high-memory\nregime."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "7 pages and 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v5",
                "updated": "2025-09-02T16:39:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    39,
                    56,
                    1,
                    245,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation"
                },
                "summary": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. However, existing Ethernet-based solutions, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilization due to both increasing traffic demands and\nthe expanding scale of datacenter topologies, which also exacerbate network\nfailures. To address these limitations, we propose REPS, a lightweight\ndecentralized per-packet adaptive load balancing algorithm designed to optimize\nnetwork utilization while ensuring rapid recovery from link failures. REPS\nadapts to network conditions by caching good-performing paths. In case of a\nnetwork failure, REPS re-routes traffic away from it in less than 100\nmicroseconds. REPS is designed to be deployed with next-generation out-of-order\ntransports, such as Ultra Ethernet, and uses less than 25 bytes of\nper-connection state regardless of the topology size. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. However, existing Ethernet-based solutions, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilization due to both increasing traffic demands and\nthe expanding scale of datacenter topologies, which also exacerbate network\nfailures. To address these limitations, we propose REPS, a lightweight\ndecentralized per-packet adaptive load balancing algorithm designed to optimize\nnetwork utilization while ensuring rapid recovery from link failures. REPS\nadapts to network conditions by caching good-performing paths. In case of a\nnetwork failure, REPS re-routes traffic away from it in less than 100\nmicroseconds. REPS is designed to be deployed with next-generation out-of-order\ntransports, such as Ultra Ethernet, and uses less than 25 bytes of\nper-connection state regardless of the topology size. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Lukas Gianinazzi"
                    },
                    {
                        "name": "Mikhail Khalilov"
                    },
                    {
                        "name": "Elias Achermann"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02480v1",
                "updated": "2025-09-02T16:30:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    30,
                    49,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T16:30:49Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    30,
                    49,
                    1,
                    245,
                    0
                ],
                "title": "MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to\n  Break the GPU Memory Wall",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to\n  Break the GPU Memory Wall"
                },
                "summary": "Training LLMs larger than the aggregated memory of multiple GPUs is\nincreasingly necessary due to the faster growth of LLM sizes compared to GPU\nmemory. To this end, multi-tier host memory or disk offloading techniques are\nproposed by state of art. Despite advanced asynchronous multi-tier read/write\nstrategies, such offloading strategies result in significant I/O overheads in\nthe critical path of training, resulting in slower iterations. To this end, we\npropose MLP-Offload, a novel multi-level, multi-path offloading engine\nspecifically designed for optimizing LLM training on resource-constrained\nsetups by mitigating I/O bottlenecks. We make several key observations that\ndrive the design of MLP-Offload, such as I/O overheads during the update\ndominate the iteration time; I/O bandwidth of the third-level remote storage\ntier remains unutilized; and, contention due to concurrent offloading amplifies\nI/O bottlenecks. Driven by these insights, we design and implement MLP-Offload\nto offload the optimizer states across multiple tiers in a cache-efficient and\nconcurrency-controlled fashion to mitigate I/O bottlenecks during the backward\nand update phases. Evaluations on models up to 280B parameters shows that\nMLP-Offload achieves 2.5$\\times$ faster iterations compared to the\nstate-of-the-art LLM training runtimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training LLMs larger than the aggregated memory of multiple GPUs is\nincreasingly necessary due to the faster growth of LLM sizes compared to GPU\nmemory. To this end, multi-tier host memory or disk offloading techniques are\nproposed by state of art. Despite advanced asynchronous multi-tier read/write\nstrategies, such offloading strategies result in significant I/O overheads in\nthe critical path of training, resulting in slower iterations. To this end, we\npropose MLP-Offload, a novel multi-level, multi-path offloading engine\nspecifically designed for optimizing LLM training on resource-constrained\nsetups by mitigating I/O bottlenecks. We make several key observations that\ndrive the design of MLP-Offload, such as I/O overheads during the update\ndominate the iteration time; I/O bandwidth of the third-level remote storage\ntier remains unutilized; and, contention due to concurrent offloading amplifies\nI/O bottlenecks. Driven by these insights, we design and implement MLP-Offload\nto offload the optimizer states across multiple tiers in a cache-efficient and\nconcurrency-controlled fashion to mitigate I/O bottlenecks during the backward\nand update phases. Evaluations on models up to 280B parameters shows that\nMLP-Offload achieves 2.5$\\times$ faster iterations compared to the\nstate-of-the-art LLM training runtimes."
                },
                "authors": [
                    {
                        "name": "Avinash Maurya"
                    },
                    {
                        "name": "M. Mustafa Rafique"
                    },
                    {
                        "name": "Franck Cappello"
                    },
                    {
                        "name": "Bogdan Nicolae"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Nicolae"
                },
                "author": "Bogdan Nicolae",
                "arxiv_doi": "10.1145/3712285.3759864",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3712285.3759864",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SC'25: The International Conference for High Performance Computing,\n  Networking, Storage and Analysis",
                "arxiv_journal_ref": "SC'25: The International Conference for High Performance\n  Computing, Networking, Storage and Analysis, 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.0; E.2; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02408v1",
                "updated": "2025-09-02T15:19:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    15,
                    19,
                    6,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T15:19:06Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    15,
                    19,
                    6,
                    1,
                    245,
                    0
                ],
                "title": "Cache Management for Mixture-of-Experts LLMs -- extended version",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Management for Mixture-of-Experts LLMs -- extended version"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks. One of the main challenges towards the successful\ndeployment of LLMs is memory management, since they typically involve billions\nof parameters. To this end, architectures based on Mixture-of-Experts have been\nproposed, which aim to reduce the size of the parameters that are activated\nwhen producing a token. This raises the equally critical issue of efficiently\nmanaging the limited cache of the system, in that frequently used experts\nshould be stored in the fast cache rather than in the slower secondary memory.\n  In this work, we introduce and study a new paging problem that models expert\nmanagement optimization. Our formulation captures both the layered architecture\nof LLMs and the requirement that experts are cached efficiently. We first\npresent lower bounds on the competitive ratio of both deterministic and\nrandomized algorithms, which show that under mild assumptions, LRU-like\npolicies have good theoretical competitive performance. We then propose a\nlayer-based extension of LRU that is tailored to the problem at hand.\n  Extensive simulations on both synthetic datasets and actual traces of MoE\nusage show that our algorithm outperforms policies for the classic paging\nproblem, such as the standard LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks. One of the main challenges towards the successful\ndeployment of LLMs is memory management, since they typically involve billions\nof parameters. To this end, architectures based on Mixture-of-Experts have been\nproposed, which aim to reduce the size of the parameters that are activated\nwhen producing a token. This raises the equally critical issue of efficiently\nmanaging the limited cache of the system, in that frequently used experts\nshould be stored in the fast cache rather than in the slower secondary memory.\n  In this work, we introduce and study a new paging problem that models expert\nmanagement optimization. Our formulation captures both the layered architecture\nof LLMs and the requirement that experts are cached efficiently. We first\npresent lower bounds on the competitive ratio of both deterministic and\nrandomized algorithms, which show that under mild assumptions, LRU-like\npolicies have good theoretical competitive performance. We then propose a\nlayer-based extension of LRU that is tailored to the problem at hand.\n  Extensive simulations on both synthetic datasets and actual traces of MoE\nusage show that our algorithm outperforms policies for the classic paging\nproblem, such as the standard LRU."
                },
                "authors": [
                    {
                        "name": "Spyros Angelopoulos"
                    },
                    {
                        "name": "Loris Marchal"
                    },
                    {
                        "name": "Adrien Obrecht"
                    },
                    {
                        "name": "Bertrand Simon"
                    }
                ],
                "author_detail": {
                    "name": "Bertrand Simon"
                },
                "author": "Bertrand Simon",
                "arxiv_doi": "10.1007/978-3-031-99872-0_2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-99872-0_2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v2",
                "updated": "2025-09-02T13:09:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    13,
                    9,
                    37,
                    1,
                    245,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing reliance on expensive vector\ndatabase lookups. To scale efficiently, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically skewed MedRAG workload reduces database calls by 78.9% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our work highlights that approximate caching\nis a viable and effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing reliance on expensive vector\ndatabase lookups. To scale efficiently, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically skewed MedRAG workload reduces database calls by 78.9% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our work highlights that approximate caching\nis a viable and effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Zhang Ji"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    }
                ],
                "author_detail": {
                    "name": "Martijn de Vos"
                },
                "author": "Martijn de Vos",
                "arxiv_doi": "10.1145/3721146.3721941",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721146.3721941",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02232v1",
                "updated": "2025-09-02T11:58:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    58,
                    6,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T11:58:06Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    58,
                    6,
                    1,
                    245,
                    0
                ],
                "title": "Efficient Geometry Compression and Communication for 3D Gaussian\n  Splatting Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Geometry Compression and Communication for 3D Gaussian\n  Splatting Point Clouds"
                },
                "summary": "Storage and transmission challenges in dynamic 3D scene representation based\non the i3DV platform, With increasing scene complexity, the explosive growth of\n3D Gaussian data volume causes excessive storage space occupancy. To address\nthis issue, we propose adopting the AVS PCRM reference software for efficient\ncompression of Gaussian point cloud geometry data. The strategy deeply\nintegrates the advanced encoding capabilities of AVS PCRM into the i3DV\nplatform, forming technical complementarity with the original rate-distortion\noptimization mechanism based on binary hash tables. On one hand, the hash table\nefficiently caches inter-frame Gaussian point transformation relationships,\nwhich allows for high-fidelity transmission within a 40 Mbps bandwidth\nconstraint. On the other hand, AVS PCRM performs precise compression on\ngeometry data. Experimental results demonstrate that the joint framework\nmaintains the advantages of fast rendering and high-quality synthesis in 3D\nGaussian technology while achieving significant 10\\%-25\\% bitrate savings on\nuniversal test sets. It provides a superior rate-distortion tradeoff solution\nfor the storage, transmission, and interaction of 3D volumetric video.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Storage and transmission challenges in dynamic 3D scene representation based\non the i3DV platform, With increasing scene complexity, the explosive growth of\n3D Gaussian data volume causes excessive storage space occupancy. To address\nthis issue, we propose adopting the AVS PCRM reference software for efficient\ncompression of Gaussian point cloud geometry data. The strategy deeply\nintegrates the advanced encoding capabilities of AVS PCRM into the i3DV\nplatform, forming technical complementarity with the original rate-distortion\noptimization mechanism based on binary hash tables. On one hand, the hash table\nefficiently caches inter-frame Gaussian point transformation relationships,\nwhich allows for high-fidelity transmission within a 40 Mbps bandwidth\nconstraint. On the other hand, AVS PCRM performs precise compression on\ngeometry data. Experimental results demonstrate that the joint framework\nmaintains the advantages of fast rendering and high-quality synthesis in 3D\nGaussian technology while achieving significant 10\\%-25\\% bitrate savings on\nuniversal test sets. It provides a superior rate-distortion tradeoff solution\nfor the storage, transmission, and interaction of 3D volumetric video."
                },
                "authors": [
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Yanting Li"
                    },
                    {
                        "name": "Luyang Tang"
                    },
                    {
                        "name": "Wei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wei Gao"
                },
                "author": "Wei Gao",
                "arxiv_doi": "10.1145/3680207.3765659",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3680207.3765659",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages,5 figures",
                "arxiv_journal_ref": "ACM MOBICOM 2025",
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15212v2",
                "updated": "2025-09-02T11:29:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    29,
                    34,
                    1,
                    245,
                    0
                ],
                "published": "2025-08-21T03:48:28Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    48,
                    28,
                    3,
                    233,
                    0
                ],
                "title": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning"
                },
                "summary": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Guanchen Li"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Emad Barsoum"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02121v1",
                "updated": "2025-09-02T09:17:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    9,
                    17,
                    40,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T09:17:40Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    9,
                    17,
                    40,
                    1,
                    245,
                    0
                ],
                "title": "Batch Query Processing and Optimization for Agentic Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch Query Processing and Optimization for Agentic Workflows"
                },
                "summary": "Large Language Models (LLMs) in agentic workflows combine multi-step\nreasoning, tool use, and collaboration across multiple specialized agents.\nExisting LLM serving engines optimize individual calls in isolation, while\nmulti-agent frameworks focus on orchestration without system-level performance\nplanning. As a result, repeated prompts, overlapping contexts, and concurrent\nexecutions create substantial redundancy and poor GPU utilization, especially\nin batch analytics scenarios. We introduce Halo, a system that brings batch\nquery processing and optimization into agentic LLM workflows. Halo represents\neach workflow as a structured query plan DAG and constructs a consolidated\ngraph for batched queries that exposes shared computation. Guided by a cost\nmodel that jointly considers prefill and decode costs, cache reuse, and GPU\nplacement, Halo performs plan-level optimization to minimize redundant\nexecution. Its runtime integrates adaptive batching, KV-cache sharing and\nmigration, along with compute-communication overlap to maximize hardware\nefficiency. Evaluation across six benchmarks shows that Halo achieves up to\n18.6x speedup for batch inference and 4.7x throughput improvement under online\nserving, scaling to workloads of tens of thousands of queries and complex\ngraphs. These gains are achieved without compromising output quality. By\nunifying query optimization with LLM serving, Halo enables efficient agentic\nworkflows in data analytics and decision-making applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) in agentic workflows combine multi-step\nreasoning, tool use, and collaboration across multiple specialized agents.\nExisting LLM serving engines optimize individual calls in isolation, while\nmulti-agent frameworks focus on orchestration without system-level performance\nplanning. As a result, repeated prompts, overlapping contexts, and concurrent\nexecutions create substantial redundancy and poor GPU utilization, especially\nin batch analytics scenarios. We introduce Halo, a system that brings batch\nquery processing and optimization into agentic LLM workflows. Halo represents\neach workflow as a structured query plan DAG and constructs a consolidated\ngraph for batched queries that exposes shared computation. Guided by a cost\nmodel that jointly considers prefill and decode costs, cache reuse, and GPU\nplacement, Halo performs plan-level optimization to minimize redundant\nexecution. Its runtime integrates adaptive batching, KV-cache sharing and\nmigration, along with compute-communication overlap to maximize hardware\nefficiency. Evaluation across six benchmarks shows that Halo achieves up to\n18.6x speedup for batch inference and 4.7x throughput improvement under online\nserving, scaling to workloads of tens of thousands of queries and complex\ngraphs. These gains are achieved without compromising output quality. By\nunifying query optimization with LLM serving, Halo enables efficient agentic\nworkflows in data analytics and decision-making applications."
                },
                "authors": [
                    {
                        "name": "Junyi Shen"
                    },
                    {
                        "name": "Noppanat Wadlom"
                    },
                    {
                        "name": "Yao Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Lu"
                },
                "author": "Yao Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02004v1",
                "updated": "2025-09-02T06:40:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    6,
                    40,
                    45,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T06:40:45Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    6,
                    40,
                    45,
                    1,
                    245,
                    0
                ],
                "title": "Augmented Shuffle Differential Privacy Protocols for Large-Domain\n  Categorical and Key-Value Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Shuffle Differential Privacy Protocols for Large-Domain\n  Categorical and Key-Value Data"
                },
                "summary": "Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy\nby introducing a shuffler who randomly shuffles data in a distributed system.\nHowever, most shuffle DP protocols are vulnerable to two attacks: collusion\nattacks by the data collector and users and data poisoning attacks. A recent\nstudy addresses this issue by introducing an augmented shuffle DP protocol,\nwhere users do not add noise and the shuffler performs random sampling and\ndummy data addition. However, it focuses on frequency estimation over\ncategorical data with a small domain and cannot be applied to a large domain\ndue to prohibitively high communication and computational costs.\n  In this paper, we fill this gap by introducing a novel augmented shuffle DP\nprotocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME\nprotocol uses a hash function to filter out unpopular items and then accurately\ncalculates frequencies for popular items. To perform this within one round of\ninteraction between users and the shuffler, our protocol carefully communicates\nwithin a system using multiple encryption. We also apply our FME protocol to\nmore advanced KV (Key-Value) statistics estimation with an additional technique\nto reduce bias. For both categorical and KV data, we prove that our protocol\nprovides computational DP, high robustness to the above two attacks, accuracy,\nand efficiency. We show the effectiveness of our proposals through comparisons\nwith twelve existing protocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy\nby introducing a shuffler who randomly shuffles data in a distributed system.\nHowever, most shuffle DP protocols are vulnerable to two attacks: collusion\nattacks by the data collector and users and data poisoning attacks. A recent\nstudy addresses this issue by introducing an augmented shuffle DP protocol,\nwhere users do not add noise and the shuffler performs random sampling and\ndummy data addition. However, it focuses on frequency estimation over\ncategorical data with a small domain and cannot be applied to a large domain\ndue to prohibitively high communication and computational costs.\n  In this paper, we fill this gap by introducing a novel augmented shuffle DP\nprotocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME\nprotocol uses a hash function to filter out unpopular items and then accurately\ncalculates frequencies for popular items. To perform this within one round of\ninteraction between users and the shuffler, our protocol carefully communicates\nwithin a system using multiple encryption. We also apply our FME protocol to\nmore advanced KV (Key-Value) statistics estimation with an additional technique\nto reduce bias. For both categorical and KV data, we prove that our protocol\nprovides computational DP, high robustness to the above two attacks, accuracy,\nand efficiency. We show the effectiveness of our proposals through comparisons\nwith twelve existing protocols."
                },
                "authors": [
                    {
                        "name": "Takao Murakami"
                    },
                    {
                        "name": "Yuichi Sei"
                    },
                    {
                        "name": "Reo Eriguchi"
                    }
                ],
                "author_detail": {
                    "name": "Reo Eriguchi"
                },
                "author": "Reo Eriguchi",
                "arxiv_comment": "Full version of the paper accepted at NDSS 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01742v1",
                "updated": "2025-09-01T19:49:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    19,
                    49,
                    21,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T19:49:21Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    19,
                    49,
                    21,
                    0,
                    244,
                    0
                ],
                "title": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure\n  HBM Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure\n  HBM Accelerators"
                },
                "summary": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO((log log N)^2) bandwidth overhead. BOLT introduces three key innovations: (i)\na new OMAP algorithm that leverages isolated HBM as an unobservable cache to\naccelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO((log log N)^2) bandwidth overhead. BOLT introduces three key innovations: (i)\na new OMAP algorithm that leverages isolated HBM as an unobservable cache to\naccelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook."
                },
                "authors": [
                    {
                        "name": "Yitong Guo"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Haobin Hiroki Chen"
                    },
                    {
                        "name": "Yukui Luo"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Chenghong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chenghong Wang"
                },
                "author": "Chenghong Wang",
                "arxiv_comment": "Accepted by CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01395v1",
                "updated": "2025-09-01T11:41:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    11,
                    41,
                    10,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T11:41:10Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    11,
                    41,
                    10,
                    0,
                    244,
                    0
                ],
                "title": "LLMs cannot spot math errors, even when allowed to peek into the\n  solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs cannot spot math errors, even when allowed to peek into the\n  solution"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable performance on math word\nproblems, yet they have been shown to struggle with meta-reasoning tasks such\nas identifying errors in student solutions. In this work, we investigate the\nchallenge of locating the first error step in stepwise solutions using two\nerror reasoning datasets: VtG and PRM800K. Our experiments show that\nstate-of-the-art LLMs struggle to locate the first error step in student\nsolutions even when given access to the reference solution. To that end, we\npropose an approach that generates an intermediate corrected student solution,\naligning more closely with the original student's solution, which helps improve\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable performance on math word\nproblems, yet they have been shown to struggle with meta-reasoning tasks such\nas identifying errors in student solutions. In this work, we investigate the\nchallenge of locating the first error step in stepwise solutions using two\nerror reasoning datasets: VtG and PRM800K. Our experiments show that\nstate-of-the-art LLMs struggle to locate the first error step in student\nsolutions even when given access to the reference solution. To that end, we\npropose an approach that generates an intermediate corrected student solution,\naligning more closely with the original student's solution, which helps improve\nperformance."
                },
                "authors": [
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "Accepted to EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15779v2",
                "updated": "2025-09-01T07:26:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    7,
                    26,
                    57,
                    0,
                    244,
                    0
                ],
                "published": "2025-02-17T08:12:34Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    12,
                    34,
                    0,
                    48,
                    0
                ],
                "title": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer"
                },
                "summary": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP."
                },
                "authors": [
                    {
                        "name": "Euntae Choi"
                    },
                    {
                        "name": "Sumin Song"
                    },
                    {
                        "name": "Woosang Lim"
                    },
                    {
                        "name": "Sungjoo Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Sungjoo Yoo"
                },
                "author": "Sungjoo Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v3",
                "updated": "2025-09-01T03:51:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    51,
                    9,
                    0,
                    244,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01092v1",
                "updated": "2025-09-01T03:31:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T03:31:44Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "title": "REFRAG: Rethinking RAG based Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REFRAG: Rethinking RAG based Decoding"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes."
                },
                "authors": [
                    {
                        "name": "Xiaoqiang Lin"
                    },
                    {
                        "name": "Aritra Ghosh"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    },
                    {
                        "name": "Anshumali Shrivastava"
                    },
                    {
                        "name": "Vijai Mohan"
                    }
                ],
                "author_detail": {
                    "name": "Vijai Mohan"
                },
                "author": "Vijai Mohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01085v1",
                "updated": "2025-09-01T03:16:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    16,
                    52,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T03:16:52Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    16,
                    52,
                    0,
                    244,
                    0
                ],
                "title": "Bidirectional Sparse Attention for Faster Video Diffusion Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional Sparse Attention for Faster Video Diffusion Training"
                },
                "summary": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention."
                },
                "authors": [
                    {
                        "name": "Chenlu Zhan"
                    },
                    {
                        "name": "Wen Li"
                    },
                    {
                        "name": "Chuyu Shen"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Suhui Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06133v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06133v2",
                "updated": "2025-08-31T15:09:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    15,
                    9,
                    36,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-08T08:54:21Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "title": "LLM Serving Optimization with Variable Prefill and Decode Lengths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Serving Optimization with Variable Prefill and Decode Lengths"
                },
                "summary": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency."
                },
                "authors": [
                    {
                        "name": "Meixuan Wang"
                    },
                    {
                        "name": "Yinyu Ye"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06133v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06133v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00883v1",
                "updated": "2025-08-31T14:51:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    51,
                    19,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-31T14:51:19Z",
                "published_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    51,
                    19,
                    6,
                    243,
                    0
                ],
                "title": "Accelerating Latency-Critical Applications with AI-Powered\n  Semi-Automatic Fine-Grained Parallelization on SMT Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Latency-Critical Applications with AI-Powered\n  Semi-Automatic Fine-Grained Parallelization on SMT Processors"
                },
                "summary": "Latency-critical applications tend to show low utilization of functional\nunits due to frequent cache misses and mispredictions during speculative\nexecution in high-performance superscalar processors. However, due to\nsignificant impact on single-thread performance, Simultaneous Multithreading\n(SMT) technology is rarely used with heavy threads of latency-critical\napplications. In this paper, we explore utilization of SMT technology to\nsupport fine-grained parallelization of latency-critical applications.\nFollowing the advancements in the development of Large Language Models (LLMs),\nwe introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we\nextend AI Coding Agent in Cursor IDE with additional tools connected through\nModel Context Protocol, enabling end-to-end AI Agent for parallelization.\nAdditional connected tools enable LLM-guided hotspot detection, collection of\ndynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance\nsimulation to estimate performance gains. We apply Aira with Relic parallel\nframework for fine-grained task parallelism on SMT cores to parallelize\nlatency-critical benchmarks representing real-world applications used in\nindustry. We show 17% geomean performance gain from parallelization of\nlatency-critical benchmarks using Aira with Relic framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency-critical applications tend to show low utilization of functional\nunits due to frequent cache misses and mispredictions during speculative\nexecution in high-performance superscalar processors. However, due to\nsignificant impact on single-thread performance, Simultaneous Multithreading\n(SMT) technology is rarely used with heavy threads of latency-critical\napplications. In this paper, we explore utilization of SMT technology to\nsupport fine-grained parallelization of latency-critical applications.\nFollowing the advancements in the development of Large Language Models (LLMs),\nwe introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we\nextend AI Coding Agent in Cursor IDE with additional tools connected through\nModel Context Protocol, enabling end-to-end AI Agent for parallelization.\nAdditional connected tools enable LLM-guided hotspot detection, collection of\ndynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance\nsimulation to estimate performance gains. We apply Aira with Relic parallel\nframework for fine-grained task parallelism on SMT cores to parallelize\nlatency-critical benchmarks representing real-world applications used in\nindustry. We show 17% geomean performance gain from parallelization of\nlatency-critical benchmarks using Aira with Relic framework."
                },
                "authors": [
                    {
                        "name": "Denis Los"
                    },
                    {
                        "name": "Igor Petushkov"
                    }
                ],
                "author_detail": {
                    "name": "Igor Petushkov"
                },
                "author": "Igor Petushkov",
                "arxiv_journal_ref": "International Journal of Open Information Technologies, vol. 13,\n  no. 9, pp. 129-134, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10431v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10431v3",
                "updated": "2025-08-31T05:43:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    5,
                    43,
                    55,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-14T08:04:15Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    4,
                    15,
                    3,
                    226,
                    0
                ],
                "title": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches"
                },
                "summary": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE."
                },
                "authors": [
                    {
                        "name": "Chris Cao"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "arxiv_comment": "This version includes updated analysis of RCO Bugs (one additional\n  bug identified). Appendix added with code snippets for bug fixes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10431v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10431v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00625v1",
                "updated": "2025-08-30T22:47:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    22,
                    47,
                    15,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T22:47:15Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    22,
                    47,
                    15,
                    5,
                    242,
                    0
                ],
                "title": "NetGent: Agent-Based Automation of Network Application Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NetGent: Agent-Based Automation of Network Application Workflows"
                },
                "summary": "We present NetGent, an AI-agent framework for automating complex application\nworkflows to generate realistic network traffic datasets. Developing\ngeneralizable ML models for networking requires data collection from network\nenvironments with traffic that results from a diverse set of real-world web\napplications. However, using existing browser automation tools that are\ndiverse, repeatable, realistic, and efficient remains fragile and costly.\nNetGent addresses this challenge by allowing users to specify workflows as\nnatural-language rules that define state-dependent actions. These abstract\nspecifications are compiled into nondeterministic finite automata (NFAs), which\na state synthesis component translates into reusable, executable code. This\ndesign enables deterministic replay, reduces redundant LLM calls through state\ncaching, and adapts quickly when application interfaces change. In experiments,\nNetGent automated more than 50+ workflows spanning video-on-demand streaming,\nlive video streaming, video conferencing, social media, and web scraping,\nproducing realistic traffic traces while remaining robust to UI variability. By\ncombining the flexibility of language-based agents with the reliability of\ncompiled execution, NetGent provides a scalable foundation for generating the\ndiverse, repeatable datasets needed to advance ML in networking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present NetGent, an AI-agent framework for automating complex application\nworkflows to generate realistic network traffic datasets. Developing\ngeneralizable ML models for networking requires data collection from network\nenvironments with traffic that results from a diverse set of real-world web\napplications. However, using existing browser automation tools that are\ndiverse, repeatable, realistic, and efficient remains fragile and costly.\nNetGent addresses this challenge by allowing users to specify workflows as\nnatural-language rules that define state-dependent actions. These abstract\nspecifications are compiled into nondeterministic finite automata (NFAs), which\na state synthesis component translates into reusable, executable code. This\ndesign enables deterministic replay, reduces redundant LLM calls through state\ncaching, and adapts quickly when application interfaces change. In experiments,\nNetGent automated more than 50+ workflows spanning video-on-demand streaming,\nlive video streaming, video conferencing, social media, and web scraping,\nproducing realistic traffic traces while remaining robust to UI variability. By\ncombining the flexibility of language-based agents with the reliability of\ncompiled execution, NetGent provides a scalable foundation for generating the\ndiverse, repeatable datasets needed to advance ML in networking."
                },
                "authors": [
                    {
                        "name": "Jaber Daneshamooz"
                    },
                    {
                        "name": "Eugene Vuong"
                    },
                    {
                        "name": "Laasya Koduru"
                    },
                    {
                        "name": "Sanjay Chandrasekaran"
                    },
                    {
                        "name": "Arpit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Arpit Gupta"
                },
                "author": "Arpit Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00579v1",
                "updated": "2025-08-30T18:25:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    18,
                    25,
                    19,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T18:25:19Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    18,
                    25,
                    19,
                    5,
                    242,
                    0
                ],
                "title": "KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for\n  KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for\n  KV Cache"
                },
                "summary": "Transformer-based large language models (LLMs) demonstrate impressive\npotential in various practical applications. However, long context inference\nposes a significant challenge due to the enormous memory requirements of the\nkey-value (KV) cache, which can scale to multiple gigabytes as sequence length\nand batch size increase. In this paper, we present KVComp, a generic and\nefficient KV cache management framework optimized for long-text generation that\nsynergistically works with both latency-critical and throughput-critical\ninference systems. KVComp employs novel lossy compression techniques\nspecifically designed for KV cache data characteristics, featuring careful\nco-design of compression algorithms and system architecture. Our approach\nmaintains compatibility with the growing nature of KV cache while preserving\nhigh computational efficiency. Experimental results show that KVComp achieves\non average 47\\% and up to 83\\% higher memory reduction rate compared to\nexisting methods with little/no model accuracy degradation. Furthermore, KVComp\nachieves extremely high execution throughput, effectively reducing\ndecompression overhead and, in some cases, even accelerating the matrix-vector\nmultiplication operation and outperform cuBLAS-based attention kernels with\nless data movement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) demonstrate impressive\npotential in various practical applications. However, long context inference\nposes a significant challenge due to the enormous memory requirements of the\nkey-value (KV) cache, which can scale to multiple gigabytes as sequence length\nand batch size increase. In this paper, we present KVComp, a generic and\nefficient KV cache management framework optimized for long-text generation that\nsynergistically works with both latency-critical and throughput-critical\ninference systems. KVComp employs novel lossy compression techniques\nspecifically designed for KV cache data characteristics, featuring careful\nco-design of compression algorithms and system architecture. Our approach\nmaintains compatibility with the growing nature of KV cache while preserving\nhigh computational efficiency. Experimental results show that KVComp achieves\non average 47\\% and up to 83\\% higher memory reduction rate compared to\nexisting methods with little/no model accuracy degradation. Furthermore, KVComp\nachieves extremely high execution throughput, effectively reducing\ndecompression overhead and, in some cases, even accelerating the matrix-vector\nmultiplication operation and outperform cuBLAS-based attention kernels with\nless data movement."
                },
                "authors": [
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Taolue Yang"
                    },
                    {
                        "name": "Youyuan Liu"
                    },
                    {
                        "name": "Chengming Zhang"
                    },
                    {
                        "name": "Xubin He"
                    },
                    {
                        "name": "Sian Jin"
                    }
                ],
                "author_detail": {
                    "name": "Sian Jin"
                },
                "author": "Sian Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.13777v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.13777v2",
                "updated": "2025-08-30T14:49:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    14,
                    49,
                    34,
                    5,
                    242,
                    0
                ],
                "published": "2023-10-20T19:22:58Z",
                "published_parsed": [
                    2023,
                    10,
                    20,
                    19,
                    22,
                    58,
                    4,
                    293,
                    0
                ],
                "title": "Discrete and Continuous Caching Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete and Continuous Caching Games"
                },
                "summary": "We investigate a discrete search game called the Multiple Caching Game where\nthe searcher's aim is to find all of a set of $d$ treasures hidden in $n$\nlocations. Allowed queries are sets of locations of size $k$, and the searcher\nwins if in all $d$ queries, at least one treasure is hidden in one of the $k$\npicked locations. P\\'alv\\\"olgyi showed that the value of the game is at most\n$\\frac{k^d}{\\binom{n+d-1}{d}}$, with equality for large enough $n$. We\nconjecture the exact cases of equality. We also investigate variants of the\ngame and show an example where their values are different, answering a question\nof P\\'alv\\\"olgyi.\n  This game is closely related to a continuous variant, Alpern's Caching Game,\nbased on which we define other continous variants of the multiple caching game\nand examine their values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate a discrete search game called the Multiple Caching Game where\nthe searcher's aim is to find all of a set of $d$ treasures hidden in $n$\nlocations. Allowed queries are sets of locations of size $k$, and the searcher\nwins if in all $d$ queries, at least one treasure is hidden in one of the $k$\npicked locations. P\\'alv\\\"olgyi showed that the value of the game is at most\n$\\frac{k^d}{\\binom{n+d-1}{d}}$, with equality for large enough $n$. We\nconjecture the exact cases of equality. We also investigate variants of the\ngame and show an example where their values are different, answering a question\nof P\\'alv\\\"olgyi.\n  This game is closely related to a continuous variant, Alpern's Caching Game,\nbased on which we define other continous variants of the multiple caching game\nand examine their values."
                },
                "authors": [
                    {
                        "name": "ron Jnosik"
                    },
                    {
                        "name": "Csenge Mikls"
                    },
                    {
                        "name": "Dniel G. Simon"
                    },
                    {
                        "name": "Kristf Zlomy"
                    }
                ],
                "author_detail": {
                    "name": "Kristf Zlomy"
                },
                "author": "Kristf Zlomy",
                "arxiv_doi": "10.1142/S0219198925500057",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1142/S0219198925500057",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.13777v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.13777v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "International Game Theory Review 27 (3), 2025",
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91A05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v3",
                "updated": "2025-08-30T09:35:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    9,
                    35,
                    22,
                    5,
                    242,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "DiffKV: Differentiated Memory Management for Large Language Models with\n  Parallel KV Compaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffKV: Differentiated Memory Management for Large Language Models with\n  Parallel KV Compaction"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable capabilities but face\nsubstantial serving costs due to their high memory demands, with the key-value\n(KV) cache being a primary bottleneck. State-of-the-art KV cache compression\ntechniques, such as quantization and pruning, apply uniform treatment to both\nkeys and values, and discard unimportant tokens entirely, overlooking the\nfine-grained distinctions in the significance of individual KV cache\ncomponents. To address such limitations, we introduce \\textit{DiffKV}, a novel\nframework for efficient KV cache compression that exploits three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. These levels of\ndifferentiation introduce irregular memory usage patterns across different\nrequests and attention heads, posing significant scalability challenges for\nmemory management. To address these challenges, DiffKV proposes an on-GPU\nmemory manager that compacts fragmented free memory list into contiguous\nregions in parallel, effectively translating sparsity in the KV cache into\nperformance gains. We evaluate DiffKV on several mainstream LLMs, including the\nemerging thinking models that generate extended chains of thought. DiffKV is\nable to compress the KV cache by $2.7\\times$ to $5.7\\times$ with near-lossless\naccuracy on complex workloads requiring sophisticated reasoning and\nlong-generation capabilities, and enhances throughput by $1.9\\times$ to\n$5.4\\times$. Source codes of DiffKV are available at\nhttps://github.com/zyqCSL/DiffKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable capabilities but face\nsubstantial serving costs due to their high memory demands, with the key-value\n(KV) cache being a primary bottleneck. State-of-the-art KV cache compression\ntechniques, such as quantization and pruning, apply uniform treatment to both\nkeys and values, and discard unimportant tokens entirely, overlooking the\nfine-grained distinctions in the significance of individual KV cache\ncomponents. To address such limitations, we introduce \\textit{DiffKV}, a novel\nframework for efficient KV cache compression that exploits three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. These levels of\ndifferentiation introduce irregular memory usage patterns across different\nrequests and attention heads, posing significant scalability challenges for\nmemory management. To address these challenges, DiffKV proposes an on-GPU\nmemory manager that compacts fragmented free memory list into contiguous\nregions in parallel, effectively translating sparsity in the KV cache into\nperformance gains. We evaluate DiffKV on several mainstream LLMs, including the\nemerging thinking models that generate extended chains of thought. DiffKV is\nable to compress the KV cache by $2.7\\times$ to $5.7\\times$ with near-lossless\naccuracy on complex workloads requiring sophisticated reasoning and\nlong-generation capabilities, and enhances throughput by $1.9\\times$ to\n$5.4\\times$. Source codes of DiffKV are available at\nhttps://github.com/zyqCSL/DiffKV."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "SOSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00419v1",
                "updated": "2025-08-30T08:57:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    8,
                    57,
                    53,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T08:57:53Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    8,
                    57,
                    53,
                    5,
                    242,
                    0
                ],
                "title": "LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging\n  and KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging\n  and KV Cache Compression"
                },
                "summary": "In this paper, we introduce LightVLM, a simple but effective method that can\nbe seamlessly deployed upon existing Vision-Language Models (VLMs) to greatly\naccelerate the inference process in a training-free manner. We divide the\ninference procedure of VLMs into two stages, i.e., encoding and decoding, and\npropose to simultaneously accelerate VLMs in both stages to largely improve\nmodel efficiency. During encoding, we propose pyramid token merging to reduce\ntokens of different LLM layers in a hierarchical manner by finally only keeping\na few dominant tokens to achieve high efficiency. During decoding, aimed at\nreducing the high latency of outputting long sequences, we propose KV Cache\ncompression to remove unnecessary caches to increase the network throughput.\nExperimental results show that LightVLM successfully retains 100% performance\nwhen only preserving 35% image tokens, and maintains around 98% performance\nwhen keeping only 3% image tokens. LightVLM could 2.02$\\times$ the network\nthroughput and reduce the prefilling time by 3.65$\\times$. LightVLM also makes\nlarge VLMs faster again by enabling a heavy model (e.g., InternVL2.5 26B) to\ninfer faster than significantly smaller models (e.g., InternVL2.5 8B),\nhopefully facilitating the real-world deployment. When generating long text\nsequences (e.g., 4096 tokens), LightVLM could reduce the inference time by\n3.21$\\times$, largely outperforming existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce LightVLM, a simple but effective method that can\nbe seamlessly deployed upon existing Vision-Language Models (VLMs) to greatly\naccelerate the inference process in a training-free manner. We divide the\ninference procedure of VLMs into two stages, i.e., encoding and decoding, and\npropose to simultaneously accelerate VLMs in both stages to largely improve\nmodel efficiency. During encoding, we propose pyramid token merging to reduce\ntokens of different LLM layers in a hierarchical manner by finally only keeping\na few dominant tokens to achieve high efficiency. During decoding, aimed at\nreducing the high latency of outputting long sequences, we propose KV Cache\ncompression to remove unnecessary caches to increase the network throughput.\nExperimental results show that LightVLM successfully retains 100% performance\nwhen only preserving 35% image tokens, and maintains around 98% performance\nwhen keeping only 3% image tokens. LightVLM could 2.02$\\times$ the network\nthroughput and reduce the prefilling time by 3.65$\\times$. LightVLM also makes\nlarge VLMs faster again by enabling a heavy model (e.g., InternVL2.5 26B) to\ninfer faster than significantly smaller models (e.g., InternVL2.5 8B),\nhopefully facilitating the real-world deployment. When generating long text\nsequences (e.g., 4096 tokens), LightVLM could reduce the inference time by\n3.21$\\times$, largely outperforming existing methods."
                },
                "authors": [
                    {
                        "name": "Lianyu Hu"
                    },
                    {
                        "name": "Fanhua Shang"
                    },
                    {
                        "name": "Wei Feng"
                    },
                    {
                        "name": "Liang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wan"
                },
                "author": "Liang Wan",
                "arxiv_comment": "EMNLP2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00388v1",
                "updated": "2025-08-30T06:56:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    6,
                    56,
                    28,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T06:56:28Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    6,
                    56,
                    28,
                    5,
                    242,
                    0
                ],
                "title": "GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV\n  Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV\n  Cache Eviction"
                },
                "summary": "Efficient Key-Value (KV) cache management is essential for processing long\ntext sequences in large language models (LLMs), where memory constraints often\nlimit performance. Conventional KV eviction strategies, such as top-k selection\nbased on attention scores, depend on static heuristics that fail to capture the\nevolving implicit dependencies among tokens during inference. To overcome this,\nwe propose GraphKV, a graph-based framework that redefines token selection for\nKV cache compression. In GraphKV, tokens are modeled as nodes with importance\nscores, and edges represent their similarity relationships. Through a\ndecay-signal-propagation mechanism, token importance is dynamically updated by\npropagating information across the graph, enabling adaptive retention of the\nmost contextually significant tokens. GraphKV can be seamlessly utilized in\nexisting KV cache eviction methods such as SnapKV and PyramidKV in a\nplug-and-play manner. Codes will be released on Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Key-Value (KV) cache management is essential for processing long\ntext sequences in large language models (LLMs), where memory constraints often\nlimit performance. Conventional KV eviction strategies, such as top-k selection\nbased on attention scores, depend on static heuristics that fail to capture the\nevolving implicit dependencies among tokens during inference. To overcome this,\nwe propose GraphKV, a graph-based framework that redefines token selection for\nKV cache compression. In GraphKV, tokens are modeled as nodes with importance\nscores, and edges represent their similarity relationships. Through a\ndecay-signal-propagation mechanism, token importance is dynamically updated by\npropagating information across the graph, enabling adaptive retention of the\nmost contextually significant tokens. GraphKV can be seamlessly utilized in\nexisting KV cache eviction methods such as SnapKV and PyramidKV in a\nplug-and-play manner. Codes will be released on Github."
                },
                "authors": [
                    {
                        "name": "Xuelin Li"
                    },
                    {
                        "name": "Xiangqi Jin"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11435v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11435v2",
                "updated": "2025-08-29T20:39:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    20,
                    39,
                    21,
                    4,
                    241,
                    0
                ],
                "published": "2025-04-15T17:51:39Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    51,
                    39,
                    1,
                    105,
                    0
                ],
                "title": "Robust Containment Queries over Collections of Trimmed NURBS Surfaces\n  via Generalized Winding Numbers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Containment Queries over Collections of Trimmed NURBS Surfaces\n  via Generalized Winding Numbers"
                },
                "summary": "We propose a containment query that is robust to the watertightness of\nregions bound by trimmed NURBS surfaces, as this property is difficult to\nguarantee for in-the-wild CAD models. Containment is determined through the\ngeneralized winding number (GWN), a mathematical construction that is\nindifferent to the arrangement of surfaces in the shape. Applying contemporary\ntechniques for the 3D GWN to trimmed NURBS surfaces requires some form of\ngeometric discretization, introducing computational inefficiency to the\nalgorithm and even risking containment misclassifications near the surface. In\ncontrast, our proposed method uses a novel reformulation of the relevant\nsurface integral based on Stokes' theorem, which operates on the boundary and\ntrimming curves as provided through rapidly converging adaptive quadrature.\nBatches of queries are further accelerated by memoizing (i.e.\\ caching and\nreusing) quadrature node positions and tangents as they are evaluated. We\ndemonstrate that our GWN method is robust to complex trimming geometry in a CAD\nmodel, and is accurate up to arbitrary precision at arbitrary distances from\nthe surface. The derived containment query is therefore robust to model\nnon-watertightness while respecting all curved features of the input shape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a containment query that is robust to the watertightness of\nregions bound by trimmed NURBS surfaces, as this property is difficult to\nguarantee for in-the-wild CAD models. Containment is determined through the\ngeneralized winding number (GWN), a mathematical construction that is\nindifferent to the arrangement of surfaces in the shape. Applying contemporary\ntechniques for the 3D GWN to trimmed NURBS surfaces requires some form of\ngeometric discretization, introducing computational inefficiency to the\nalgorithm and even risking containment misclassifications near the surface. In\ncontrast, our proposed method uses a novel reformulation of the relevant\nsurface integral based on Stokes' theorem, which operates on the boundary and\ntrimming curves as provided through rapidly converging adaptive quadrature.\nBatches of queries are further accelerated by memoizing (i.e.\\ caching and\nreusing) quadrature node positions and tangents as they are evaluated. We\ndemonstrate that our GWN method is robust to complex trimming geometry in a CAD\nmodel, and is accurate up to arbitrary precision at arbitrary distances from\nthe surface. The derived containment query is therefore robust to model\nnon-watertightness while respecting all curved features of the input shape."
                },
                "authors": [
                    {
                        "name": "Jacob Spainhour"
                    },
                    {
                        "name": "Kenneth Weiss"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth Weiss"
                },
                "author": "Kenneth Weiss",
                "arxiv_comment": "18 Pages, 16 Figures, 1 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11435v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11435v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68U05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.3.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00202v1",
                "updated": "2025-08-29T19:23:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    23,
                    35,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T19:23:35Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    23,
                    35,
                    4,
                    241,
                    0
                ],
                "title": "From TLinFormer to TConstFormer: The Leap to Constant-Time Transformer\n  Attention: Achieving O(1) Computation and O(1) KV Cache during Autoregressive\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From TLinFormer to TConstFormer: The Leap to Constant-Time Transformer\n  Attention: Achieving O(1) Computation and O(1) KV Cache during Autoregressive\n  Inference"
                },
                "summary": "Although the Transformer has become the cornerstone of modern AI, its\nautoregressive inference suffers from a linearly growing KV Cache and a\ncomputational complexity of O(N^2 d), severely hindering its ability to process\nultra-long sequences. To overcome this limitation, this paper introduces the\nTConstFormer architecture, building upon our previous work, TLinFormer.\nTConstFormer employs an innovative periodic state update mechanism to achieve a\ntruly constant-size O(1) KV Cache. The computational complexity of this\nmechanism is also O(1) in an amortized sense: it performs purely constant-time\ncomputations for $k-1$ consecutive steps (e.g., $k=256$) and executes a single\nlinear-time global information synchronization only on the $k$-th step.\nTheoretical calculations and experimental results demonstrate that TConstFormer\nexhibits an overwhelming advantage over baseline models in terms of speed,\nmemory efficiency, and overall performance on long-text inference tasks. This\nbreakthrough paves the way for efficient and robust streaming language model\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although the Transformer has become the cornerstone of modern AI, its\nautoregressive inference suffers from a linearly growing KV Cache and a\ncomputational complexity of O(N^2 d), severely hindering its ability to process\nultra-long sequences. To overcome this limitation, this paper introduces the\nTConstFormer architecture, building upon our previous work, TLinFormer.\nTConstFormer employs an innovative periodic state update mechanism to achieve a\ntruly constant-size O(1) KV Cache. The computational complexity of this\nmechanism is also O(1) in an amortized sense: it performs purely constant-time\ncomputations for $k-1$ consecutive steps (e.g., $k=256$) and executes a single\nlinear-time global information synchronization only on the $k$-th step.\nTheoretical calculations and experimental results demonstrate that TConstFormer\nexhibits an overwhelming advantage over baseline models in terms of speed,\nmemory efficiency, and overall performance on long-text inference tasks. This\nbreakthrough paves the way for efficient and robust streaming language model\napplications."
                },
                "authors": [
                    {
                        "name": "Zhongpan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongpan Tang"
                },
                "author": "Zhongpan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00195v1",
                "updated": "2025-08-29T19:12:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    12,
                    4,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T19:12:04Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    12,
                    4,
                    4,
                    241,
                    0
                ],
                "title": "Democratizing Agentic AI with Fast Test-Time Scaling on the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Democratizing Agentic AI with Fast Test-Time Scaling on the Edge"
                },
                "summary": "Deploying agentic AI on edge devices is crucial for privacy and\nresponsiveness, but memory constraints typically relegate these systems to\nsmaller Large Language Models (LLMs) with inferior reasoning capabilities.\nTest-Time Scaling (TTS) can bridge this reasoning gap by dedicating more\ncompute during inference, but existing methods incur prohibitive overhead on\nedge hardware. To overcome this, we introduce FlashTTS, a serving system that\nmakes TTS practical for memory-constrained LLM reasoning. FlashTTS introduces\nthree synergistic optimizations: (i) Speculative Beam Extension to mitigate\nsystem stragglers from irregular reasoning paths; (ii) Asymmetric Multi-Model\nMemory Allocation to dynamically balance memory between generation and\nverification; and (iii) Dynamic Prefix-Aware Scheduling to maximize KV-cache\nreuse. Built as a plug-and-play library for vLLM, FlashTTS enables edge LLMs on\na single consumer GPU (24 GB) to match the accuracy and latency of large cloud\nmodels. Our evaluation demonstrates that FlashTTS achieves an average 2.2x\nhigher goodput and reduces latency by 38%-68% compared to a vLLM baseline,\npaving the way for democratized, high-performance agentic AI on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying agentic AI on edge devices is crucial for privacy and\nresponsiveness, but memory constraints typically relegate these systems to\nsmaller Large Language Models (LLMs) with inferior reasoning capabilities.\nTest-Time Scaling (TTS) can bridge this reasoning gap by dedicating more\ncompute during inference, but existing methods incur prohibitive overhead on\nedge hardware. To overcome this, we introduce FlashTTS, a serving system that\nmakes TTS practical for memory-constrained LLM reasoning. FlashTTS introduces\nthree synergistic optimizations: (i) Speculative Beam Extension to mitigate\nsystem stragglers from irregular reasoning paths; (ii) Asymmetric Multi-Model\nMemory Allocation to dynamically balance memory between generation and\nverification; and (iii) Dynamic Prefix-Aware Scheduling to maximize KV-cache\nreuse. Built as a plug-and-play library for vLLM, FlashTTS enables edge LLMs on\na single consumer GPU (24 GB) to match the accuracy and latency of large cloud\nmodels. Our evaluation demonstrates that FlashTTS achieves an average 2.2x\nhigher goodput and reduces latency by 38%-68% compared to a vLLM baseline,\npaving the way for democratized, high-performance agentic AI on edge devices."
                },
                "authors": [
                    {
                        "name": "Hao Mark Chen"
                    },
                    {
                        "name": "Zhiwen Mo"
                    },
                    {
                        "name": "Guanxi Lu"
                    },
                    {
                        "name": "Shuang Liang"
                    },
                    {
                        "name": "Lingxiao Ma"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Hongxiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Fan"
                },
                "author": "Hongxiang Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16217v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16217v2",
                "updated": "2025-08-29T18:45:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    18,
                    45,
                    22,
                    4,
                    241,
                    0
                ],
                "published": "2025-07-22T04:21:03Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    4,
                    21,
                    3,
                    1,
                    203,
                    0
                ],
                "title": "Towards Compute-Optimal Many-Shot In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Compute-Optimal Many-Shot In-Context Learning"
                },
                "summary": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL."
                },
                "authors": [
                    {
                        "name": "Shahriar Golchin"
                    },
                    {
                        "name": "Yanfei Chen"
                    },
                    {
                        "name": "Rujun Han"
                    },
                    {
                        "name": "Manan Gandhi"
                    },
                    {
                        "name": "Tianli Yu"
                    },
                    {
                        "name": "Swaroop Mishra"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Chen-Yu Lee"
                    },
                    {
                        "name": "Tomas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Pfister"
                },
                "author": "Tomas Pfister",
                "arxiv_comment": "Final version; accepted at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16217v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16217v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05930v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05930v2",
                "updated": "2025-08-29T09:58:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    58,
                    17,
                    4,
                    241,
                    0
                ],
                "published": "2025-06-06T09:55:59Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    9,
                    55,
                    59,
                    4,
                    157,
                    0
                ],
                "title": "Neural Visibility Cache for Real-Time Light Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Visibility Cache for Real-Time Light Sampling"
                },
                "summary": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR)."
                },
                "authors": [
                    {
                        "name": "Jakub Bokansk"
                    },
                    {
                        "name": "Daniel Meister"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Meister"
                },
                "author": "Daniel Meister",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05930v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05930v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15683v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15683v2",
                "updated": "2025-08-29T07:40:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    7,
                    40,
                    34,
                    4,
                    241,
                    0
                ],
                "published": "2025-05-21T15:58:08Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "title": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting\n  Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting\n  Framework for Large Language Models"
                },
                "summary": "Private data holds promise for improving LLMs due to its high quality, but\nits scattered distribution across data silos and the high computational demands\nof LLMs limit their deployment in federated environments. To address this, the\ntransformer-based federated split models are proposed, which offload most model\nparameters to the server (or distributed clients) while retaining only a small\nportion on the client to ensure data privacy. Despite this design, they still\nface three challenges: 1) Peer-to-peer key encryption struggles to secure\ntransmitted vectors effectively; 2) The auto-regressive nature of LLMs means\nthat federated split learning can only train and infer sequentially, causing\nhigh communication overhead; 3) Fixed partition points lack adaptability to\ndownstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure,\nEfficient, and Adaptive Federated splitting framework based on LLaMA2. First,\nwe inject Gaussian noise into forward-pass hidden states to enable secure\nend-to-end vector transmission. Second, we employ attention-mask compression\nand KV cache collaboration to reduce communication costs, accelerating training\nand inference. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements. Experiments on\nnatural language understanding, summarization, and conversational QA tasks show\nthat FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and\nachieves up to 8x speedups in training and inference. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FedSEA-LLaMA in security and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private data holds promise for improving LLMs due to its high quality, but\nits scattered distribution across data silos and the high computational demands\nof LLMs limit their deployment in federated environments. To address this, the\ntransformer-based federated split models are proposed, which offload most model\nparameters to the server (or distributed clients) while retaining only a small\nportion on the client to ensure data privacy. Despite this design, they still\nface three challenges: 1) Peer-to-peer key encryption struggles to secure\ntransmitted vectors effectively; 2) The auto-regressive nature of LLMs means\nthat federated split learning can only train and infer sequentially, causing\nhigh communication overhead; 3) Fixed partition points lack adaptability to\ndownstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure,\nEfficient, and Adaptive Federated splitting framework based on LLaMA2. First,\nwe inject Gaussian noise into forward-pass hidden states to enable secure\nend-to-end vector transmission. Second, we employ attention-mask compression\nand KV cache collaboration to reduce communication costs, accelerating training\nand inference. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements. Experiments on\nnatural language understanding, summarization, and conversational QA tasks show\nthat FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and\nachieves up to 8x speedups in training and inference. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FedSEA-LLaMA in security and adaptability."
                },
                "authors": [
                    {
                        "name": "Zishuai Zhang"
                    },
                    {
                        "name": "Hainan zhang"
                    },
                    {
                        "name": "Weihua Li"
                    },
                    {
                        "name": "Qinnan zhang"
                    },
                    {
                        "name": "jin Dong"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15683v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15683v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20865v1",
                "updated": "2025-08-28T14:58:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    58,
                    47,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T14:58:47Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    58,
                    47,
                    3,
                    240,
                    0
                ],
                "title": "Deep Multiple Quantization Network on Long Behavior Sequence for\n  Click-Through Rate Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Multiple Quantization Network on Long Behavior Sequence for\n  Click-Through Rate Prediction"
                },
                "summary": "In Click-Through Rate (CTR) prediction, the long behavior sequence,\ncomprising the user's long period of historical interactions with items has a\nvital influence on assessing the user's interest in the candidate item.\nExisting approaches strike efficiency and effectiveness through a two-stage\nparadigm: first retrieving hundreds of candidate-related items and then\nextracting interest intensity vector through target attention. However, we\nargue that the discrepancy in target attention's relevance distribution between\nthe retrieved items and the full long behavior sequence inevitably leads to a\nperformance decline. To alleviate the discrepancy, we propose the Deep Multiple\nQuantization Network (DMQN) to process long behavior sequence end-to-end\nthrough compressing the long behavior sequence. Firstly, the entire spectrum of\nlong behavior sequence will be quantized into multiple codeword sequences based\non multiple independent codebooks. Hierarchical Sequential Transduction Unit is\nincorporated to facilitate the interaction of reduced codeword sequences. Then,\nattention between the candidate and multiple codeword sequences will output the\ninterest vector. To enable online serving, intermediate representations of the\ncodeword sequences are cached, significantly reducing latency. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMQN. The A/B test in our advertising system shows that DMQN\nimproves CTR by 3.5% and RPM by 2.0%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Click-Through Rate (CTR) prediction, the long behavior sequence,\ncomprising the user's long period of historical interactions with items has a\nvital influence on assessing the user's interest in the candidate item.\nExisting approaches strike efficiency and effectiveness through a two-stage\nparadigm: first retrieving hundreds of candidate-related items and then\nextracting interest intensity vector through target attention. However, we\nargue that the discrepancy in target attention's relevance distribution between\nthe retrieved items and the full long behavior sequence inevitably leads to a\nperformance decline. To alleviate the discrepancy, we propose the Deep Multiple\nQuantization Network (DMQN) to process long behavior sequence end-to-end\nthrough compressing the long behavior sequence. Firstly, the entire spectrum of\nlong behavior sequence will be quantized into multiple codeword sequences based\non multiple independent codebooks. Hierarchical Sequential Transduction Unit is\nincorporated to facilitate the interaction of reduced codeword sequences. Then,\nattention between the candidate and multiple codeword sequences will output the\ninterest vector. To enable online serving, intermediate representations of the\ncodeword sequences are cached, significantly reducing latency. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMQN. The A/B test in our advertising system shows that DMQN\nimproves CTR by 3.5% and RPM by 2.0%."
                },
                "authors": [
                    {
                        "name": "Zhuoxing Wei"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Qingchen Xie"
                    }
                ],
                "author_detail": {
                    "name": "Qingchen Xie"
                },
                "author": "Qingchen Xie",
                "arxiv_doi": "10.1145/3726302.3730177",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730177",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.20865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, 1 figures, SIGIR 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18250v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18250v2",
                "updated": "2025-08-28T08:49:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    49,
                    24,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-25T17:41:13Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    41,
                    13,
                    0,
                    237,
                    0
                ],
                "title": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study"
                },
                "summary": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM."
                },
                "authors": [
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Fernando Garca-Redondo"
                    },
                    {
                        "name": "Arvind Sharma"
                    },
                    {
                        "name": "Van Dai Nguyen"
                    },
                    {
                        "name": "Andrea Fantini"
                    },
                    {
                        "name": "Philippe Matagne"
                    },
                    {
                        "name": "Siddharth Rao"
                    },
                    {
                        "name": "Subhali Subhechha"
                    },
                    {
                        "name": "Lynn Verschueren"
                    },
                    {
                        "name": "Mohammed Aftab Baig"
                    },
                    {
                        "name": "Marie Garcia Bardon"
                    },
                    {
                        "name": "Geert Hellings"
                    }
                ],
                "author_detail": {
                    "name": "Geert Hellings"
                },
                "author": "Geert Hellings",
                "arxiv_comment": "Manuscript submitted to IEEE Trans. Elec. Dev. Work enabled in part\n  by NanoIC pilot line; acquisition and operation jointly funded by Chips Joint\n  Undertaking, through EU's Digital Europe (101183266) and Horizon Europe\n  programs (101183277), as well as by the participating states\n  (Belgium-Flanders, France, Germany, Finland, Ireland, Romania)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18250v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18250v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20524v1",
                "updated": "2025-08-28T08:05:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    5,
                    42,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T08:05:42Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    5,
                    42,
                    3,
                    240,
                    0
                ],
                "title": "Matrixed-Spectrum Decomposition Accelerated Linear Boltzmann Transport\n  Equation Solver for Fast Scatter Correction in Multi-Spectral CT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrixed-Spectrum Decomposition Accelerated Linear Boltzmann Transport\n  Equation Solver for Fast Scatter Correction in Multi-Spectral CT"
                },
                "summary": "X-ray scatter has been a serious concern in computed tomography (CT), leading\nto image artifacts and distortion of CT values. The linear Boltzmann transport\nequation (LBTE) is recognized as a fast and accurate approach for scatter\nestimation. However, for multi-spectral CT, it is cumbersome to compute\nmultiple scattering components for different spectra separately when applying\nLBTE-based scatter correction. In this work, we propose a Matrixed-Spectrum\nDecomposition accelerated LBTE solver (MSD-LBTE) that can be used to compute\nX-ray scatter distributions from CT acquisitions at two or more different\nspectra simultaneously, in a unified framework with no sacrifice in accuracy\nand nearly no increase in computation in theory. First, a matrixed-spectrum\nsolver of LBTE is obtained by introducing an additional label dimension to\nexpand the phase space. Then, we propose a ``spectrum basis'' for LBTE and a\nprinciple of selection of basis using the QR decomposition, along with the\nabove solver to construct the MSD-LBTE. Based on MSD-LBTE, a unified scatter\ncorrection method can be established for multi-spectral CT. We validate the\neffectiveness and accuracy of our method by comparing it with the Monte Carlo\nmethod, including the computational time. We also evaluate the scatter\ncorrection performance using two different phantoms for fast-kV switching based\ndual-energy CT, and using an elliptical phantom in a numerical simulation for\nkV-modulation enabled CT scans, validating that our proposed method can\nsignificantly reduce the computational cost at multiple spectra and effectively\nreduce scatter artifact in reconstructed CT images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-ray scatter has been a serious concern in computed tomography (CT), leading\nto image artifacts and distortion of CT values. The linear Boltzmann transport\nequation (LBTE) is recognized as a fast and accurate approach for scatter\nestimation. However, for multi-spectral CT, it is cumbersome to compute\nmultiple scattering components for different spectra separately when applying\nLBTE-based scatter correction. In this work, we propose a Matrixed-Spectrum\nDecomposition accelerated LBTE solver (MSD-LBTE) that can be used to compute\nX-ray scatter distributions from CT acquisitions at two or more different\nspectra simultaneously, in a unified framework with no sacrifice in accuracy\nand nearly no increase in computation in theory. First, a matrixed-spectrum\nsolver of LBTE is obtained by introducing an additional label dimension to\nexpand the phase space. Then, we propose a ``spectrum basis'' for LBTE and a\nprinciple of selection of basis using the QR decomposition, along with the\nabove solver to construct the MSD-LBTE. Based on MSD-LBTE, a unified scatter\ncorrection method can be established for multi-spectral CT. We validate the\neffectiveness and accuracy of our method by comparing it with the Monte Carlo\nmethod, including the computational time. We also evaluate the scatter\ncorrection performance using two different phantoms for fast-kV switching based\ndual-energy CT, and using an elliptical phantom in a numerical simulation for\nkV-modulation enabled CT scans, validating that our proposed method can\nsignificantly reduce the computational cost at multiple spectra and effectively\nreduce scatter artifact in reconstructed CT images."
                },
                "authors": [
                    {
                        "name": "Guoxi Zhu"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Zhiqiang Chen"
                    },
                    {
                        "name": "Hewei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Hewei Gao"
                },
                "author": "Hewei Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20433v1",
                "updated": "2025-08-28T05:22:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    5,
                    22,
                    25,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T05:22:25Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    5,
                    22,
                    25,
                    3,
                    240,
                    0
                ],
                "title": "MegaCacheX: Towards Cost-Effective Hierarchical Collaborative Content\n  Caching in Emerging Mega-Constellations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaCacheX: Towards Cost-Effective Hierarchical Collaborative Content\n  Caching in Emerging Mega-Constellations"
                },
                "summary": "Significant latency in global content delivery primarily arises from\ninsufficient terrestrial infrastructure. Deploying space-based content delivery\nnetworks within emerging mega-constellations provides an effective means to\nbridge the digital divide. However, space-based caching faces constraints from\nphysical-layer dynamics, including dynamic topologies, time-varying\ninter-satellite link conditions, and limited onboard energy. In addition,\nexisting mechanisms often lack fine-grained content categorization and global\noptimization. This paper proposes MegaCacheX, a cost-effective hierarchical\nframework for collaborative content distribution that achieves\n\"Earth-independence\" by providing cloud services directly from space.\nSpecifically, data centers in Sun-synchronous orbit act as primary content\nsources, while caching nodes in mega-constellations and ground stations\ncollaboratively form a distributed edge layer. MegaCacheX optimizes caching\nstrategies by integrating content popularity, regional user distribution, and\nsatellite trajectory predictions. Multi-tier caching nodes serve as service\nanchors, enabling seamless content delivery with low latency. A prototype\nimplemented on a microservices-based, containerized testbed demonstrates that\nMegaCacheX reduces global content access latency by about 36% compared to\nbaseline approaches, while maintaining cost efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant latency in global content delivery primarily arises from\ninsufficient terrestrial infrastructure. Deploying space-based content delivery\nnetworks within emerging mega-constellations provides an effective means to\nbridge the digital divide. However, space-based caching faces constraints from\nphysical-layer dynamics, including dynamic topologies, time-varying\ninter-satellite link conditions, and limited onboard energy. In addition,\nexisting mechanisms often lack fine-grained content categorization and global\noptimization. This paper proposes MegaCacheX, a cost-effective hierarchical\nframework for collaborative content distribution that achieves\n\"Earth-independence\" by providing cloud services directly from space.\nSpecifically, data centers in Sun-synchronous orbit act as primary content\nsources, while caching nodes in mega-constellations and ground stations\ncollaboratively form a distributed edge layer. MegaCacheX optimizes caching\nstrategies by integrating content popularity, regional user distribution, and\nsatellite trajectory predictions. Multi-tier caching nodes serve as service\nanchors, enabling seamless content delivery with low latency. A prototype\nimplemented on a microservices-based, containerized testbed demonstrates that\nMegaCacheX reduces global content access latency by about 36% compared to\nbaseline approaches, while maintaining cost efficiency."
                },
                "authors": [
                    {
                        "name": "Haoyang Shi"
                    },
                    {
                        "name": "Xing Zhang"
                    },
                    {
                        "name": "Sitong Li"
                    },
                    {
                        "name": "Minghang Li"
                    },
                    {
                        "name": "Xinming Lu"
                    },
                    {
                        "name": "Shaoxiang Xu"
                    },
                    {
                        "name": "Guoquan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guoquan Wang"
                },
                "author": "Guoquan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20424v1",
                "updated": "2025-08-28T04:46:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    46,
                    44,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T04:46:44Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    46,
                    44,
                    3,
                    240,
                    0
                ],
                "title": "Breaking Diffusion with Cache: Exploiting Approximate Caches in\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Diffusion with Cache: Exploiting Approximate Caches in\n  Diffusion Models"
                },
                "summary": "Diffusion models are a powerful class of generative models that produce\ncontent, such as images, from user prompts, but they are computationally\nintensive. To mitigate this cost, recent academic and industry work has adopted\napproximate caching, which reuses intermediate states from similar prompts in a\ncache. While efficient, this optimization introduces new security risks by\nbreaking isolation among users. This work aims to comprehensively assess new\nsecurity vulnerabilities arising from approximate caching. First, we\ndemonstrate a remote covert channel established with the cache, where a sender\ninjects prompts with special keywords into the cache and a receiver can recover\nthat even after days, to exchange information. Second, we introduce a prompt\nstealing attack using the cache, where an attacker can recover existing cached\nprompts based on cache hit prompts. Finally, we introduce a poisoning attack\nthat embeds the attacker's logos into the previously stolen prompt, to render\nthem in future user prompts that hit the cache. These attacks are all performed\nremotely through the serving system, which indicates severe security\nvulnerabilities in approximate caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models are a powerful class of generative models that produce\ncontent, such as images, from user prompts, but they are computationally\nintensive. To mitigate this cost, recent academic and industry work has adopted\napproximate caching, which reuses intermediate states from similar prompts in a\ncache. While efficient, this optimization introduces new security risks by\nbreaking isolation among users. This work aims to comprehensively assess new\nsecurity vulnerabilities arising from approximate caching. First, we\ndemonstrate a remote covert channel established with the cache, where a sender\ninjects prompts with special keywords into the cache and a receiver can recover\nthat even after days, to exchange information. Second, we introduce a prompt\nstealing attack using the cache, where an attacker can recover existing cached\nprompts based on cache hit prompts. Finally, we introduce a poisoning attack\nthat embeds the attacker's logos into the previously stolen prompt, to render\nthem in future user prompts that hit the cache. These attacks are all performed\nremotely through the serving system, which indicates severe security\nvulnerabilities in approximate caching."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Shuncheng Jie"
                    },
                    {
                        "name": "Sihang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sihang Liu"
                },
                "author": "Sihang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20407v1",
                "updated": "2025-08-28T04:10:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    10,
                    19,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T04:10:19Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    10,
                    19,
                    3,
                    240,
                    0
                ],
                "title": "Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full\n  Context-Aware Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full\n  Context-Aware Linear Attention"
                },
                "summary": "The Transformer architecture has become a cornerstone of modern artificial\nintelligence, but its core self-attention mechanism suffers from a complexity\nbottleneck that scales quadratically with sequence length, severely limiting\nits application in long-sequence tasks. To address this challenge, existing\nlinear attention methods typically sacrifice model performance by relying on\ndata-agnostic kernel approximations or restrictive context selection. This\npaper returns to the first principles of connectionism, starting from the\ntopological structure of information flow, to introduce a novel linear\nattention architecture-\\textbf{TLinFormer}. By reconfiguring neuron connection\npatterns, TLinFormer achieves strict linear complexity while computing exact\nattention scores and ensuring information flow remains aware of the full\nhistorical context. This design aims to bridge the performance gap prevalent\nbetween existing efficient attention methods and standard attention. Through a\nseries of experiments, we systematically evaluate the performance of TLinFormer\nagainst a standard Transformer baseline on long-sequence inference tasks. The\nresults demonstrate that TLinFormer exhibits overwhelming advantages in key\nmetrics such as \\textbf{inference latency}, \\textbf{KV cache efficiency},\n\\textbf{memory footprint}, and \\textbf{overall speedup}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Transformer architecture has become a cornerstone of modern artificial\nintelligence, but its core self-attention mechanism suffers from a complexity\nbottleneck that scales quadratically with sequence length, severely limiting\nits application in long-sequence tasks. To address this challenge, existing\nlinear attention methods typically sacrifice model performance by relying on\ndata-agnostic kernel approximations or restrictive context selection. This\npaper returns to the first principles of connectionism, starting from the\ntopological structure of information flow, to introduce a novel linear\nattention architecture-\\textbf{TLinFormer}. By reconfiguring neuron connection\npatterns, TLinFormer achieves strict linear complexity while computing exact\nattention scores and ensuring information flow remains aware of the full\nhistorical context. This design aims to bridge the performance gap prevalent\nbetween existing efficient attention methods and standard attention. Through a\nseries of experiments, we systematically evaluate the performance of TLinFormer\nagainst a standard Transformer baseline on long-sequence inference tasks. The\nresults demonstrate that TLinFormer exhibits overwhelming advantages in key\nmetrics such as \\textbf{inference latency}, \\textbf{KV cache efficiency},\n\\textbf{memory footprint}, and \\textbf{overall speedup}."
                },
                "authors": [
                    {
                        "name": "Zhongpan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongpan Tang"
                },
                "author": "Zhongpan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v5",
                "updated": "2025-08-28T03:57:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    3,
                    57,
                    52,
                    3,
                    240,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from (1) the distribution variance in the LLM activations and (2) the\nsensitivity difference among various kinds of layers. To address these issues,\nwe propose a training-free approach called Activation-aware Singular Value\nDecomposition (ASVD). Specifically, ASVD manages activation outliers by\ntransforming the weight matrix based on the activation distribution. This\ntransformation allows the outliers in the activation matrix to be absorbed into\nthe transformed weight matrix, thereby enhancing decomposition accuracy.\nAdditionally, we propose an efficient iterative calibration process to optimize\nlayer-specific decomposition by addressing the varying sensitivity of different\nLLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the\nsuccess of the low-rank decomposition of projection matrices in the\nself-attention module, we further introduce ASVD to compress the KV cache. By\nreducing the channel dimension of KV activations, memory requirements for KV\ncache can be largely reduced. ASVD can further achieve 50% KV cache reductions\nwithout performance drop in a training-free manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from (1) the distribution variance in the LLM activations and (2) the\nsensitivity difference among various kinds of layers. To address these issues,\nwe propose a training-free approach called Activation-aware Singular Value\nDecomposition (ASVD). Specifically, ASVD manages activation outliers by\ntransforming the weight matrix based on the activation distribution. This\ntransformation allows the outliers in the activation matrix to be absorbed into\nthe transformed weight matrix, thereby enhancing decomposition accuracy.\nAdditionally, we propose an efficient iterative calibration process to optimize\nlayer-specific decomposition by addressing the varying sensitivity of different\nLLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the\nsuccess of the low-rank decomposition of projection matrices in the\nself-attention module, we further introduce ASVD to compress the KV cache. By\nreducing the channel dimension of KV activations, memory requirements for KV\ncache can be largely reduced. ASVD can further achieve 50% KV cache reductions\nwithout performance drop in a training-free manner."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09888v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09888v2",
                "updated": "2025-08-28T01:40:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    1,
                    40,
                    30,
                    3,
                    240,
                    0
                ],
                "published": "2025-02-14T03:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "title": "Climber: Toward Efficient Scaling Laws for Large Recommendation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Climber: Toward Efficient Scaling Laws for Large Recommendation Models"
                },
                "summary": "Transformer-based generative models have achieved remarkable success across\ndomains with various scaling law manifestations. However, our extensive\nexperiments reveal persistent challenges when applying Transformer to\nrecommendation systems: (1) Transformer scaling is not ideal with increased\ncomputational resources, due to structural incompatibilities with\nrecommendation-specific features such as multi-source data heterogeneity; (2)\ncritical online inference latency constraints (tens of milliseconds) that\nintensify with longer user behavior sequences and growing computational\ndemands. We propose Climber, an efficient recommendation framework comprising\ntwo synergistic components: the model architecture for efficient scaling and\nthe co-designed acceleration techniques. Our proposed model adopts two core\ninnovations: (1) multi-scale sequence extraction that achieves a time\ncomplexity reduction by a constant factor, enabling more efficient scaling with\nsequence length; (2) dynamic temperature modulation adapting attention\ndistributions to the multi-scenario and multi-behavior patterns. Complemented\nby acceleration techniques, Climber achieves a 5.15$\\times$ throughput gain\nwithout performance degradation by adopting a \"single user, multiple item\"\nbatched processing and memory-efficient Key-Value caching. Comprehensive\noffline experiments on multiple datasets validate that Climber exhibits a more\nideal scaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19\\% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based generative models have achieved remarkable success across\ndomains with various scaling law manifestations. However, our extensive\nexperiments reveal persistent challenges when applying Transformer to\nrecommendation systems: (1) Transformer scaling is not ideal with increased\ncomputational resources, due to structural incompatibilities with\nrecommendation-specific features such as multi-source data heterogeneity; (2)\ncritical online inference latency constraints (tens of milliseconds) that\nintensify with longer user behavior sequences and growing computational\ndemands. We propose Climber, an efficient recommendation framework comprising\ntwo synergistic components: the model architecture for efficient scaling and\nthe co-designed acceleration techniques. Our proposed model adopts two core\ninnovations: (1) multi-scale sequence extraction that achieves a time\ncomplexity reduction by a constant factor, enabling more efficient scaling with\nsequence length; (2) dynamic temperature modulation adapting attention\ndistributions to the multi-scenario and multi-behavior patterns. Complemented\nby acceleration techniques, Climber achieves a 5.15$\\times$ throughput gain\nwithout performance degradation by adopting a \"single user, multiple item\"\nbatched processing and memory-efficient Key-Value caching. Comprehensive\noffline experiments on multiple datasets validate that Climber exhibits a more\nideal scaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19\\% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily."
                },
                "authors": [
                    {
                        "name": "Songpei Xu"
                    },
                    {
                        "name": "Shijia Wang"
                    },
                    {
                        "name": "Da Guo"
                    },
                    {
                        "name": "Xianwen Guo"
                    },
                    {
                        "name": "Qiang Xiao"
                    },
                    {
                        "name": "Bin Huang"
                    },
                    {
                        "name": "Guanlin Wu"
                    },
                    {
                        "name": "Chuanjiang Luo"
                    }
                ],
                "author_detail": {
                    "name": "Chuanjiang Luo"
                },
                "author": "Chuanjiang Luo",
                "arxiv_doi": "10.1145/3746252.3761561",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3761561",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09888v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09888v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00105v1",
                "updated": "2025-08-28T00:46:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    0,
                    46,
                    51,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T00:46:51Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    0,
                    46,
                    51,
                    3,
                    240,
                    0
                ],
                "title": "AdaptCache: KV Cache Native Storage Hierarchy for Low-Delay and\n  High-Quality Language Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaptCache: KV Cache Native Storage Hierarchy for Low-Delay and\n  High-Quality Language Model Serving"
                },
                "summary": "Large language model (LLM) applications often reuse previously processed\ncontext, such as chat history and documents, which introduces significant\nredundant computation. Existing LLM serving systems address such redundant\ncomputation by storing the KV caches of processed context and loading the\ncorresponding KV cache when a new request reuses the context. Further, as these\nLLM applications scale, the total size of KV caches becomes excessively large\nand requires both DRAM and SSD for full storage.\n  However, prior work that stores KV caches in DRAM and SSD suffers from high\nloading delays, as most KV cache hits come from SSD, which is slow to load. To\nincrease the KV cache hit rate on DRAM, we identify lossy KV cache compression\nas a promising approach. We design a lossy compression system that decides the\ncompression algorithm, compression rate and device placement for each KV cache\nentry to maximise DRAM hits and minimise loading delay without significantly\ndegrading generation quality. Compared to various static compression baselines\nacross three tasks, our system AdaptCache achieves 1.43--2.4 x delay savings at\nthe same quality and 6--55% quality improvements at the same delay.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) applications often reuse previously processed\ncontext, such as chat history and documents, which introduces significant\nredundant computation. Existing LLM serving systems address such redundant\ncomputation by storing the KV caches of processed context and loading the\ncorresponding KV cache when a new request reuses the context. Further, as these\nLLM applications scale, the total size of KV caches becomes excessively large\nand requires both DRAM and SSD for full storage.\n  However, prior work that stores KV caches in DRAM and SSD suffers from high\nloading delays, as most KV cache hits come from SSD, which is slow to load. To\nincrease the KV cache hit rate on DRAM, we identify lossy KV cache compression\nas a promising approach. We design a lossy compression system that decides the\ncompression algorithm, compression rate and device placement for each KV cache\nentry to maximise DRAM hits and minimise loading delay without significantly\ndegrading generation quality. Compared to various static compression baselines\nacross three tasks, our system AdaptCache achieves 1.43--2.4 x delay savings at\nthe same quality and 6--55% quality improvements at the same delay."
                },
                "authors": [
                    {
                        "name": "Shaoting Feng"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Siddhant Ray"
                    },
                    {
                        "name": "Samuel Shen"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Ganesh Ananthanarayanan"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20272v1",
                "updated": "2025-08-27T21:05:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    21,
                    5,
                    5,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T21:05:05Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    21,
                    5,
                    5,
                    2,
                    239,
                    0
                ],
                "title": "DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource\n  Allocation and Markov Decision Process in Named Data Networking (NDN)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource\n  Allocation and Markov Decision Process in Named Data Networking (NDN)"
                },
                "summary": "Named Data Networking (NDN) represents a transformative shift in network\narchitecture, prioritizing content names over host addresses to enhance data\ndissemination. Efficient queue and resource management are critical to NDN\nperformance, especially under dynamic and high-traffic conditions. This paper\nintroduces DRR-MDPF, a novel hybrid strategy that integrates the Markov\nDecision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR)\nalgorithm. MDPF enables routers to intelligently predict optimal forwarding\ndecisions based on key metrics such as bandwidth, delay, and the number of\nunsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation\namong competing data flows. The proposed method models each router as a\nlearning agent capable of adjusting its strategies through continuous feedback\nand probabilistic updates. Simulation results using ndnSIM demonstrate that\nDRR-MDPF significantly outperforms state-of-the-art strategies including SAF,\nRFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest\nSatisfaction Rate (ISR), packet drop rate, content retrieval time, and load\nbalancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and\nheavy traffic, offering enhanced adaptability and lower computational\ncomplexity due to its single-path routing design. Furthermore, its multi-metric\ndecision-making capability enables more accurate interface selection, leading\nto optimized network performance. Overall, DRR-MDPF serves as an intelligent,\nadaptive, and scalable queue management solution for NDN, effectively\naddressing core challenges such as resource allocation, congestion control, and\nroute optimization in dynamic networking environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Data Networking (NDN) represents a transformative shift in network\narchitecture, prioritizing content names over host addresses to enhance data\ndissemination. Efficient queue and resource management are critical to NDN\nperformance, especially under dynamic and high-traffic conditions. This paper\nintroduces DRR-MDPF, a novel hybrid strategy that integrates the Markov\nDecision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR)\nalgorithm. MDPF enables routers to intelligently predict optimal forwarding\ndecisions based on key metrics such as bandwidth, delay, and the number of\nunsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation\namong competing data flows. The proposed method models each router as a\nlearning agent capable of adjusting its strategies through continuous feedback\nand probabilistic updates. Simulation results using ndnSIM demonstrate that\nDRR-MDPF significantly outperforms state-of-the-art strategies including SAF,\nRFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest\nSatisfaction Rate (ISR), packet drop rate, content retrieval time, and load\nbalancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and\nheavy traffic, offering enhanced adaptability and lower computational\ncomplexity due to its single-path routing design. Furthermore, its multi-metric\ndecision-making capability enables more accurate interface selection, leading\nto optimized network performance. Overall, DRR-MDPF serves as an intelligent,\nadaptive, and scalable queue management solution for NDN, effectively\naddressing core challenges such as resource allocation, congestion control, and\nroute optimization in dynamic networking environments."
                },
                "authors": [
                    {
                        "name": "Fatemeh Roshanzadeh"
                    },
                    {
                        "name": "Hamid Barati"
                    },
                    {
                        "name": "Ali Barati"
                    }
                ],
                "author_detail": {
                    "name": "Ali Barati"
                },
                "author": "Ali Barati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20253v1",
                "updated": "2025-08-27T20:18:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    20,
                    18,
                    37,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T20:18:37Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    20,
                    18,
                    37,
                    2,
                    239,
                    0
                ],
                "title": "SpeedMalloc: Improving Multi-threaded Applications via a Lightweight\n  Core for Memory Allocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeedMalloc: Improving Multi-threaded Applications via a Lightweight\n  Core for Memory Allocation"
                },
                "summary": "Memory allocation, though constituting only a small portion of the executed\ncode, can have a \"butterfly effect\" on overall program performance, leading to\nsignificant and far-reaching impacts. Despite accounting for just approximately\n5% of total instructions, memory allocation can result in up to a 2.7x\nperformance variation depending on the allocator used. This effect arises from\nthe complexity of memory allocation in modern multi-threaded multi-core\nsystems, where allocator metadata becomes intertwined with user data, leading\nto cache pollution or increased cross-thread synchronization overhead.\nOffloading memory allocators to accelerators, e.g., Mallacc and Memento, is a\npotential direction to improve the allocator performance and mitigate cache\npollution. However, these accelerators currently have limited support for\nmulti-threaded applications, and synchronization between cores and accelerators\nremains a significant challenge.\n  We present SpeedMalloc, using a lightweight support-core to process memory\nallocation tasks in multi-threaded applications. The support-core is a\nlightweight programmable processor with efficient cross-core data\nsynchronization and houses all allocator metadata in its own caches. This\ndesign minimizes cache conflicts with user data and eliminates the need for\ncross-core metadata synchronization. In addition, using a general-purpose core\ninstead of domain-specific accelerators makes SpeedMalloc capable of adopting\nnew allocator designs. We compare SpeedMalloc with state-of-the-art software\nand hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and\nMemento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on\nmultithreaded workloads over these five allocators, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory allocation, though constituting only a small portion of the executed\ncode, can have a \"butterfly effect\" on overall program performance, leading to\nsignificant and far-reaching impacts. Despite accounting for just approximately\n5% of total instructions, memory allocation can result in up to a 2.7x\nperformance variation depending on the allocator used. This effect arises from\nthe complexity of memory allocation in modern multi-threaded multi-core\nsystems, where allocator metadata becomes intertwined with user data, leading\nto cache pollution or increased cross-thread synchronization overhead.\nOffloading memory allocators to accelerators, e.g., Mallacc and Memento, is a\npotential direction to improve the allocator performance and mitigate cache\npollution. However, these accelerators currently have limited support for\nmulti-threaded applications, and synchronization between cores and accelerators\nremains a significant challenge.\n  We present SpeedMalloc, using a lightweight support-core to process memory\nallocation tasks in multi-threaded applications. The support-core is a\nlightweight programmable processor with efficient cross-core data\nsynchronization and houses all allocator metadata in its own caches. This\ndesign minimizes cache conflicts with user data and eliminates the need for\ncross-core metadata synchronization. In addition, using a general-purpose core\ninstead of domain-specific accelerators makes SpeedMalloc capable of adopting\nnew allocator designs. We compare SpeedMalloc with state-of-the-art software\nand hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and\nMemento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on\nmultithreaded workloads over these five allocators, respectively."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Qinzhe Wu"
                    },
                    {
                        "name": "Krishna Kavi"
                    },
                    {
                        "name": "Gayatri Mehta"
                    },
                    {
                        "name": "Jonathan C. Beard"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    },
                    {
                        "name": "Lizy K. John"
                    }
                ],
                "author_detail": {
                    "name": "Lizy K. John"
                },
                "author": "Lizy K. John",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00100v1",
                "updated": "2025-08-27T17:45:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    17,
                    45,
                    16,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T17:45:16Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    17,
                    45,
                    16,
                    2,
                    239,
                    0
                ],
                "title": "MODE: Mixture of Document Experts for RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MODE: Mixture of Document Experts for RAG"
                },
                "summary": "Retrieval-Augmented Generation (RAG) often relies on large vector databases\nand cross-encoders tuned for large-scale corpora, which can be excessive for\nsmall, domain-specific collections. We present MODE (Mixture of Document\nExperts), a lightweight alternative that replaces fine-grained nearest-neighbor\nsearch with cluster-and-route retrieval. Documents are embedded, grouped into\nsemantically coherent clusters, and represented by cached centroids. At query\ntime, we route to the top centroid(s) and retrieve context only within those\nclusters, eliminating external vector-database infrastructure and reranking\nwhile keeping latency low. On HotpotQA and SQuAD corpora with 100-500 chunks,\nMODE matches or exceeds a dense-retrieval baseline in answer quality while\nreducing end-to-end retrieval time. Ablations show that cluster granularity and\nmulti-cluster routing control the recall/precision trade-off, and that tighter\nclusters improve downstream accuracy. MODE offers a practical recipe for small\nand medium corpora where simplicity, speed, and topical focus matter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) often relies on large vector databases\nand cross-encoders tuned for large-scale corpora, which can be excessive for\nsmall, domain-specific collections. We present MODE (Mixture of Document\nExperts), a lightweight alternative that replaces fine-grained nearest-neighbor\nsearch with cluster-and-route retrieval. Documents are embedded, grouped into\nsemantically coherent clusters, and represented by cached centroids. At query\ntime, we route to the top centroid(s) and retrieve context only within those\nclusters, eliminating external vector-database infrastructure and reranking\nwhile keeping latency low. On HotpotQA and SQuAD corpora with 100-500 chunks,\nMODE matches or exceeds a dense-retrieval baseline in answer quality while\nreducing end-to-end retrieval time. Ablations show that cluster granularity and\nmulti-cluster routing control the recall/precision trade-off, and that tighter\nclusters improve downstream accuracy. MODE offers a practical recipe for small\nand medium corpora where simplicity, speed, and topical focus matter."
                },
                "authors": [
                    {
                        "name": "Rahul Anand"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Anand"
                },
                "author": "Rahul Anand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13575v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13575v3",
                "updated": "2025-08-27T16:34:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    16,
                    34,
                    47,
                    2,
                    239,
                    0
                ],
                "published": "2025-07-17T23:37:19Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    23,
                    37,
                    19,
                    3,
                    198,
                    0
                ],
                "title": "Apple Intelligence Foundation Language Models: Tech Report 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apple Intelligence Foundation Language Models: Tech Report 2025"
                },
                "summary": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute."
                },
                "authors": [
                    {
                        "name": "Ethan Li"
                    },
                    {
                        "name": "Anders Boesen Lindbo Larsen"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Xiyou Zhou"
                    },
                    {
                        "name": "Jun Qin"
                    },
                    {
                        "name": "Dian Ang Yap"
                    },
                    {
                        "name": "Narendran Raghavan"
                    },
                    {
                        "name": "Xuankai Chang"
                    },
                    {
                        "name": "Margit Bowler"
                    },
                    {
                        "name": "Eray Yildiz"
                    },
                    {
                        "name": "John Peebles"
                    },
                    {
                        "name": "Hannah Gillis Coleman"
                    },
                    {
                        "name": "Matteo Ronchi"
                    },
                    {
                        "name": "Peter Gray"
                    },
                    {
                        "name": "Keen You"
                    },
                    {
                        "name": "Anthony Spalvieri-Kruse"
                    },
                    {
                        "name": "Ruoming Pang"
                    },
                    {
                        "name": "Reed Li"
                    },
                    {
                        "name": "Yuli Yang"
                    },
                    {
                        "name": "Emad Soroush"
                    },
                    {
                        "name": "Zhiyun Lu"
                    },
                    {
                        "name": "Crystal Xiao"
                    },
                    {
                        "name": "Rong Situ"
                    },
                    {
                        "name": "Jordan Huffaker"
                    },
                    {
                        "name": "David Griffiths"
                    },
                    {
                        "name": "Zaid Ahmed"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Daniel Parilla"
                    },
                    {
                        "name": "Asaf Liberman"
                    },
                    {
                        "name": "Jennifer Mallalieu"
                    },
                    {
                        "name": "Parsa Mazaheri"
                    },
                    {
                        "name": "Qibin Chen"
                    },
                    {
                        "name": "Manjot Bilkhu"
                    },
                    {
                        "name": "Aonan Zhang"
                    },
                    {
                        "name": "Eric Wang"
                    },
                    {
                        "name": "Dave Nelson"
                    },
                    {
                        "name": "Michael FitzMaurice"
                    },
                    {
                        "name": "Thomas Voice"
                    },
                    {
                        "name": "Jeremy Liu"
                    },
                    {
                        "name": "Josh Shaffer"
                    },
                    {
                        "name": "Shiwen Zhao"
                    },
                    {
                        "name": "Prasanth Yadla"
                    },
                    {
                        "name": "Farzin Rasteh"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Arsalan Farooq"
                    },
                    {
                        "name": "Jeremy Snow"
                    },
                    {
                        "name": "Stephen Murphy"
                    },
                    {
                        "name": "Tao Lei"
                    },
                    {
                        "name": "Minsik Cho"
                    },
                    {
                        "name": "George Horrell"
                    },
                    {
                        "name": "Sam Dodge"
                    },
                    {
                        "name": "Lindsay Hislop"
                    },
                    {
                        "name": "Sumeet Singh"
                    },
                    {
                        "name": "Alex Dombrowski"
                    },
                    {
                        "name": "Aiswarya Raghavan"
                    },
                    {
                        "name": "Sasha Sirovica"
                    },
                    {
                        "name": "Mandana Saebi"
                    },
                    {
                        "name": "Faye Lao"
                    },
                    {
                        "name": "Max Lam"
                    },
                    {
                        "name": "TJ Lu"
                    },
                    {
                        "name": "Zhaoyang Xu"
                    },
                    {
                        "name": "Karanjeet Singh"
                    },
                    {
                        "name": "Marc Kirchner"
                    },
                    {
                        "name": "David Mizrahi"
                    },
                    {
                        "name": "Rajat Arora"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Henry Mason"
                    },
                    {
                        "name": "Lawrence Zhou"
                    },
                    {
                        "name": "Yi Hua"
                    },
                    {
                        "name": "Ankur Jain"
                    },
                    {
                        "name": "Felix Bai"
                    },
                    {
                        "name": "Joseph Astrauskas"
                    },
                    {
                        "name": "Floris Weers"
                    },
                    {
                        "name": "Josh Gardner"
                    },
                    {
                        "name": "Mira Chiang"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Tony Sun"
                    },
                    {
                        "name": "Quentin Keunebroek"
                    },
                    {
                        "name": "Matthew Hopkins"
                    },
                    {
                        "name": "Bugu Wu"
                    },
                    {
                        "name": "Tao Jia"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Xingyu Zhou"
                    },
                    {
                        "name": "Nanzhu Wang"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Ruixuan Hou"
                    },
                    {
                        "name": "Rene Rauch"
                    },
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Afshin Dehghan"
                    },
                    {
                        "name": "Jonathan Janke"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Cha Chen"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Feng Nan"
                    },
                    {
                        "name": "Josh Elman"
                    },
                    {
                        "name": "Dong Yin"
                    },
                    {
                        "name": "Yusuf Goren"
                    },
                    {
                        "name": "Jeff Lai"
                    },
                    {
                        "name": "Yiran Fei"
                    },
                    {
                        "name": "Syd Evans"
                    },
                    {
                        "name": "Muyang Yu"
                    },
                    {
                        "name": "Guoli Yin"
                    },
                    {
                        "name": "Yi Qin"
                    },
                    {
                        "name": "Erin Feldman"
                    },
                    {
                        "name": "Isha Garg"
                    },
                    {
                        "name": "Aparna Rajamani"
                    },
                    {
                        "name": "Karla Vega"
                    },
                    {
                        "name": "Walker Cheng"
                    },
                    {
                        "name": "TJ Collins"
                    },
                    {
                        "name": "Hans Han"
                    },
                    {
                        "name": "Raul Rea Menacho"
                    },
                    {
                        "name": "Simon Yeung"
                    },
                    {
                        "name": "Sophy Lee"
                    },
                    {
                        "name": "Phani Mutyala"
                    },
                    {
                        "name": "Ying-Chang Cheng"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Sprite Chu"
                    },
                    {
                        "name": "Justin Lazarow"
                    },
                    {
                        "name": "Alessandro Pappalardo"
                    },
                    {
                        "name": "Federico Scozzafava"
                    },
                    {
                        "name": "Jing Lu"
                    },
                    {
                        "name": "Erik Daxberger"
                    },
                    {
                        "name": "Laurent Duchesne"
                    },
                    {
                        "name": "Jen Liu"
                    },
                    {
                        "name": "David Gera"
                    },
                    {
                        "name": "Stefano Ligas"
                    },
                    {
                        "name": "Mary Beth Kery"
                    },
                    {
                        "name": "Brent Ramerth"
                    },
                    {
                        "name": "Ciro Sannino"
                    },
                    {
                        "name": "Marcin Eichner"
                    },
                    {
                        "name": "Haoshuo Huang"
                    },
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Moritz Schwarzer-Becker"
                    },
                    {
                        "name": "David Riazati"
                    },
                    {
                        "name": "Mingfei Gao"
                    },
                    {
                        "name": "Bailin Wang"
                    },
                    {
                        "name": "Jack Cackler"
                    },
                    {
                        "name": "Yang Lu"
                    },
                    {
                        "name": "Ransen Niu"
                    },
                    {
                        "name": "John Dennison"
                    },
                    {
                        "name": "Guillaume Klein"
                    },
                    {
                        "name": "Jeffrey Bigham"
                    },
                    {
                        "name": "Deepak Gopinath"
                    },
                    {
                        "name": "Navid Shiee"
                    },
                    {
                        "name": "Darren Botten"
                    },
                    {
                        "name": "Guillaume Tartavel"
                    },
                    {
                        "name": "Alex Guillen Garcia"
                    },
                    {
                        "name": "Sam Xu"
                    },
                    {
                        "name": "Victoria MnchJuan Haladjian"
                    },
                    {
                        "name": "Zi-Yi Dou"
                    },
                    {
                        "name": "Matthias Paulik"
                    },
                    {
                        "name": "Adolfo Lopez Mendez"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Hong-You Chen"
                    },
                    {
                        "name": "Chao Jia"
                    },
                    {
                        "name": "Dhaval Doshi"
                    },
                    {
                        "name": "Zhengdong Zhang"
                    },
                    {
                        "name": "Raunak Manjani"
                    },
                    {
                        "name": "Aaron Franklin"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "David Chen"
                    },
                    {
                        "name": "Artsiom Peshko"
                    },
                    {
                        "name": "Nandhitha Raghuram"
                    },
                    {
                        "name": "Hans Hao"
                    },
                    {
                        "name": "Jiulong Shan"
                    },
                    {
                        "name": "Kavya Nerella"
                    },
                    {
                        "name": "Ramsey Tantawi"
                    },
                    {
                        "name": "Vivek Kumar"
                    },
                    {
                        "name": "Saiwen Wang"
                    },
                    {
                        "name": "Brycen Wershing"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    },
                    {
                        "name": "Dhruti Shah"
                    },
                    {
                        "name": "Ob Adaranijo"
                    },
                    {
                        "name": "Xin Zheng"
                    },
                    {
                        "name": "Tait Madsen"
                    },
                    {
                        "name": "Hadas Kotek"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Yin Xia"
                    },
                    {
                        "name": "Hanli Li"
                    },
                    {
                        "name": "Suma Jayaram"
                    },
                    {
                        "name": "Yanchao Sun"
                    },
                    {
                        "name": "Ahmed Fakhry"
                    },
                    {
                        "name": "Vasileios Saveris"
                    },
                    {
                        "name": "Dustin Withers"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Alp Aygar"
                    },
                    {
                        "name": "Andres Romero Mier Y Teran"
                    },
                    {
                        "name": "Kaiwei Huang"
                    },
                    {
                        "name": "Mark Lee"
                    },
                    {
                        "name": "Xiujun Li"
                    },
                    {
                        "name": "Yuhong Li"
                    },
                    {
                        "name": "Tyler Johnson"
                    },
                    {
                        "name": "Jay Tang"
                    },
                    {
                        "name": "Joseph Yitan Cheng"
                    },
                    {
                        "name": "Futang Peng"
                    },
                    {
                        "name": "Andrew Walkingshaw"
                    },
                    {
                        "name": "Lucas Guibert"
                    },
                    {
                        "name": "Abhishek Sharma"
                    },
                    {
                        "name": "Cheng Shen"
                    },
                    {
                        "name": "Piotr Maj"
                    },
                    {
                        "name": "Yasutaka Tanaka"
                    },
                    {
                        "name": "You-Cyuan Jhang"
                    },
                    {
                        "name": "Vivian Ma"
                    },
                    {
                        "name": "Tommi Vehvilainen"
                    },
                    {
                        "name": "Kelvin Zou"
                    },
                    {
                        "name": "Jeff Nichols"
                    },
                    {
                        "name": "Matthew Lei"
                    },
                    {
                        "name": "David Qiu"
                    },
                    {
                        "name": "Yihao Qian"
                    },
                    {
                        "name": "Gokul Santhanam"
                    },
                    {
                        "name": "Wentao Wu"
                    },
                    {
                        "name": "Yena Han"
                    },
                    {
                        "name": "Dominik Moritz"
                    },
                    {
                        "name": "Haijing Fu"
                    },
                    {
                        "name": "Mingze Xu"
                    },
                    {
                        "name": "Vivek Rathod"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Louis D'hauwe"
                    },
                    {
                        "name": "Qin Ba"
                    },
                    {
                        "name": "Haitian Sun"
                    },
                    {
                        "name": "Haoran Yan"
                    },
                    {
                        "name": "Philipp Dufter"
                    },
                    {
                        "name": "Anh Nguyen"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Emma Wang"
                    },
                    {
                        "name": "Keyu He"
                    },
                    {
                        "name": "Rahul Nair"
                    },
                    {
                        "name": "Sanskruti Shah"
                    },
                    {
                        "name": "Jiarui Lu"
                    },
                    {
                        "name": "Patrick Sonnenberg"
                    },
                    {
                        "name": "Jeremy Warner"
                    },
                    {
                        "name": "Yuanzhi Li"
                    },
                    {
                        "name": "Bowen Pan"
                    },
                    {
                        "name": "Ziyi Zhong"
                    },
                    {
                        "name": "Joe Zhou"
                    },
                    {
                        "name": "Sam Davarnia"
                    },
                    {
                        "name": "Olli Saarikivi"
                    },
                    {
                        "name": "Irina Belousova"
                    },
                    {
                        "name": "Rachel Burger"
                    },
                    {
                        "name": "Shang-Chen Wu"
                    },
                    {
                        "name": "Di Feng"
                    },
                    {
                        "name": "Bas Straathof"
                    },
                    {
                        "name": "James Chou"
                    },
                    {
                        "name": "Yuanyang Zhang"
                    },
                    {
                        "name": "Marco Zuliani"
                    },
                    {
                        "name": "Eduardo Jimenez"
                    },
                    {
                        "name": "Abhishek Sundararajan"
                    },
                    {
                        "name": "Xianzhi Du"
                    },
                    {
                        "name": "Chang Lan"
                    },
                    {
                        "name": "Nilesh Shahdadpuri"
                    },
                    {
                        "name": "Peter Grasch"
                    },
                    {
                        "name": "Sergiu Sima"
                    },
                    {
                        "name": "Josh Newnham"
                    },
                    {
                        "name": "Varsha Paidi"
                    },
                    {
                        "name": "Jianyu Wang"
                    },
                    {
                        "name": "Kaelen Haag"
                    },
                    {
                        "name": "Alex Braunstein"
                    },
                    {
                        "name": "Daniele Molinari"
                    },
                    {
                        "name": "Richard Wei"
                    },
                    {
                        "name": "Brenda Yang"
                    },
                    {
                        "name": "Nicholas Lusskin"
                    },
                    {
                        "name": "Joanna Arreaza-Taylor"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Nicholas Seidl"
                    },
                    {
                        "name": "Simon Wang"
                    },
                    {
                        "name": "Jiaming Hu"
                    },
                    {
                        "name": "Yiping Ma"
                    },
                    {
                        "name": "Mengyu Li"
                    },
                    {
                        "name": "Kieran Liu"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Sachin Ravi"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Kevin Smith"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Binazir Karimzadeh"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Jinhao Lei"
                    },
                    {
                        "name": "Wei Fang"
                    },
                    {
                        "name": "Alec Doane"
                    },
                    {
                        "name": "Sam Wiseman"
                    },
                    {
                        "name": "Ismael Fernandez"
                    },
                    {
                        "name": "Jane Li"
                    },
                    {
                        "name": "Andrew Hansen"
                    },
                    {
                        "name": "Javier Movellan"
                    },
                    {
                        "name": "Christopher Neubauer"
                    },
                    {
                        "name": "Hanzhi Zhou"
                    },
                    {
                        "name": "Chris Chaney"
                    },
                    {
                        "name": "Nazir Kamaldin"
                    },
                    {
                        "name": "Valentin Wolf"
                    },
                    {
                        "name": "Fernando Bermdez-Medina"
                    },
                    {
                        "name": "Joris Pelemans"
                    },
                    {
                        "name": "Peter Fu"
                    },
                    {
                        "name": "Howard Xing"
                    },
                    {
                        "name": "Xiang Kong"
                    },
                    {
                        "name": "Wayne Shan"
                    },
                    {
                        "name": "Gabriel Jacoby-Cooper"
                    },
                    {
                        "name": "Dongcai Shen"
                    },
                    {
                        "name": "Tom Gunter"
                    },
                    {
                        "name": "Guillaume Seguin"
                    },
                    {
                        "name": "Fangping Shi"
                    },
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Areeba Kamal"
                    },
                    {
                        "name": "Dan Masi"
                    },
                    {
                        "name": "Saptarshi Guha"
                    },
                    {
                        "name": "Qi Zhu"
                    },
                    {
                        "name": "Jenna Thibodeau"
                    },
                    {
                        "name": "Changyuan Zhang"
                    },
                    {
                        "name": "Rebecca Callahan"
                    },
                    {
                        "name": "Charles Maalouf"
                    },
                    {
                        "name": "Wilson Tsao"
                    },
                    {
                        "name": "Boyue Li"
                    },
                    {
                        "name": "Qingqing Cao"
                    },
                    {
                        "name": "Naomy Sabo"
                    },
                    {
                        "name": "Cheng Leong"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Anupama Mann Anupama"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Kenneth Jung"
                    },
                    {
                        "name": "Zhifeng Chen"
                    },
                    {
                        "name": "Mohana Prasad Sathya Moorthy"
                    },
                    {
                        "name": "Yifei He"
                    },
                    {
                        "name": "Erik Hornberger"
                    },
                    {
                        "name": "Devi Krishna"
                    },
                    {
                        "name": "Senyu Tong"
                    },
                    {
                        "name": "Michael"
                    },
                    {
                        "name": "Lee"
                    },
                    {
                        "name": "David Haldimann"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Chris Bartels"
                    },
                    {
                        "name": "Sushma Rao"
                    },
                    {
                        "name": "Nathalie Tran"
                    },
                    {
                        "name": "Simon Lehnerer"
                    },
                    {
                        "name": "Co Giang"
                    },
                    {
                        "name": "Patrick Dong"
                    },
                    {
                        "name": "Junting Pan"
                    },
                    {
                        "name": "Biyao Wang"
                    },
                    {
                        "name": "Dongxu Li"
                    },
                    {
                        "name": "Mehrdad Farajtabar"
                    },
                    {
                        "name": "Dongseong Hwang"
                    },
                    {
                        "name": "Grace Duanmu"
                    },
                    {
                        "name": "Eshan Verma"
                    },
                    {
                        "name": "Sujeeth Reddy"
                    },
                    {
                        "name": "Qi Shan"
                    },
                    {
                        "name": "Hongbin Gao"
                    },
                    {
                        "name": "Nan Du"
                    },
                    {
                        "name": "Pragnya Sridhar"
                    },
                    {
                        "name": "Forrest Huang"
                    },
                    {
                        "name": "Yingbo Wang"
                    },
                    {
                        "name": "Nikhil Bhendawade"
                    },
                    {
                        "name": "Diane Zhu"
                    },
                    {
                        "name": "Sai Aitharaju"
                    },
                    {
                        "name": "Fred Hohman"
                    },
                    {
                        "name": "Lauren Gardiner"
                    },
                    {
                        "name": "Chung-Cheng Chiu"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Alper Kokmen"
                    },
                    {
                        "name": "Frank Chu"
                    },
                    {
                        "name": "Ke Ye"
                    },
                    {
                        "name": "Kaan Elgin"
                    },
                    {
                        "name": "Oron Levy"
                    },
                    {
                        "name": "John Park"
                    },
                    {
                        "name": "Donald Zhang"
                    },
                    {
                        "name": "Eldon Schoop"
                    },
                    {
                        "name": "Nina Wenzel"
                    },
                    {
                        "name": "Michael Booker"
                    },
                    {
                        "name": "Hyunjik Kim"
                    },
                    {
                        "name": "Chinguun Erdenebileg"
                    },
                    {
                        "name": "Nan Dun"
                    },
                    {
                        "name": "Eric Liang Yang"
                    },
                    {
                        "name": "Priyal Chhatrapati"
                    },
                    {
                        "name": "Vishaal Mahtani"
                    },
                    {
                        "name": "Haiming Gang"
                    },
                    {
                        "name": "Kohen Chia"
                    },
                    {
                        "name": "Deepa Seshadri"
                    },
                    {
                        "name": "Donghan Yu"
                    },
                    {
                        "name": "Yan Meng"
                    },
                    {
                        "name": "Kelsey Peterson"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Yongqiang Wang"
                    },
                    {
                        "name": "Carina Peng"
                    },
                    {
                        "name": "Doug Kang"
                    },
                    {
                        "name": "Anuva Agarwal"
                    },
                    {
                        "name": "Albert Antony"
                    },
                    {
                        "name": "Juan Lao Tebar"
                    },
                    {
                        "name": "Albin Madappally Jose"
                    },
                    {
                        "name": "Regan Poston"
                    },
                    {
                        "name": "Andy De Wang"
                    },
                    {
                        "name": "Gerard Casamayor"
                    },
                    {
                        "name": "Elmira Amirloo"
                    },
                    {
                        "name": "Violet Yao"
                    },
                    {
                        "name": "Wojciech Kryscinski"
                    },
                    {
                        "name": "Kun Duan"
                    },
                    {
                        "name": "Lezhi L"
                    }
                ],
                "author_detail": {
                    "name": "Lezhi L"
                },
                "arxiv_affiliation": "Taoyi",
                "author": "Lezhi L",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13575v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13575v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09570v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09570v2",
                "updated": "2025-08-27T12:13:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    12,
                    13,
                    45,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-13T07:40:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    7,
                    40,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "Re-thinking Memory-Bound Limitations in CGRAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-thinking Memory-Bound Limitations in CGRAs"
                },
                "summary": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns."
                },
                "authors": [
                    {
                        "name": "Xiangfeng Liu"
                    },
                    {
                        "name": "Zhe Jiang"
                    },
                    {
                        "name": "Anzhen Zhu"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Mingsong Lyu"
                    },
                    {
                        "name": "Qingxu Deng"
                    },
                    {
                        "name": "Nan Guan"
                    }
                ],
                "author_detail": {
                    "name": "Nan Guan"
                },
                "author": "Nan Guan",
                "arxiv_doi": "10.1145/3760386",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3760386",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.09570v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09570v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "25 pages, 18 figures, CODES+ISSS 2025",
                "arxiv_journal_ref": "ACM Transactions on Embedded Computing Systems 2025",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.0; B.6.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21091v1",
                "updated": "2025-08-27T10:37:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    37,
                    24,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T10:37:24Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    37,
                    24,
                    2,
                    239,
                    0
                ],
                "title": "ERTACache: Error Rectification and Timesteps Adjustment for Efficient\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ERTACache: Error Rectification and Timesteps Adjustment for Efficient\n  Diffusion"
                },
                "summary": "Diffusion models suffer from substantial computational overhead due to their\ninherently iterative inference process. While feature caching offers a\npromising acceleration strategy by reusing intermediate outputs across\ntimesteps, naive reuse often incurs noticeable quality degradation. In this\nwork, we formally analyze the cumulative error introduced by caching and\ndecompose it into two principal components: feature shift error, caused by\ninaccuracies in cached outputs, and step amplification error, which arises from\nerror propagation under fixed timestep schedules. To address these issues, we\npropose ERTACache, a principled caching framework that jointly rectifies both\nerror types. Our method employs an offline residual profiling stage to identify\nreusable steps, dynamically adjusts integration intervals via a\ntrajectory-aware correction coefficient, and analytically approximates\ncache-induced errors through a closed-form residual linearization model.\nTogether, these components enable accurate and efficient sampling under\naggressive cache reuse. Extensive experiments across standard image and video\ngeneration benchmarks show that ERTACache achieves up to 2x inference speedup\nwhile consistently preserving or even improving visual quality. Notably, on the\nstate-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x\nacceleration with minimal VBench degradation, effectively maintaining baseline\nfidelity while significantly improving efficiency. The code is available at\nhttps://github.com/bytedance/ERTACache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models suffer from substantial computational overhead due to their\ninherently iterative inference process. While feature caching offers a\npromising acceleration strategy by reusing intermediate outputs across\ntimesteps, naive reuse often incurs noticeable quality degradation. In this\nwork, we formally analyze the cumulative error introduced by caching and\ndecompose it into two principal components: feature shift error, caused by\ninaccuracies in cached outputs, and step amplification error, which arises from\nerror propagation under fixed timestep schedules. To address these issues, we\npropose ERTACache, a principled caching framework that jointly rectifies both\nerror types. Our method employs an offline residual profiling stage to identify\nreusable steps, dynamically adjusts integration intervals via a\ntrajectory-aware correction coefficient, and analytically approximates\ncache-induced errors through a closed-form residual linearization model.\nTogether, these components enable accurate and efficient sampling under\naggressive cache reuse. Extensive experiments across standard image and video\ngeneration benchmarks show that ERTACache achieves up to 2x inference speedup\nwhile consistently preserving or even improving visual quality. Notably, on the\nstate-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x\nacceleration with minimal VBench degradation, effectively maintaining baseline\nfidelity while significantly improving efficiency. The code is available at\nhttps://github.com/bytedance/ERTACache."
                },
                "authors": [
                    {
                        "name": "Xurui Peng"
                    },
                    {
                        "name": "Hong Liu"
                    },
                    {
                        "name": "Chenqian Yan"
                    },
                    {
                        "name": "Rui Ma"
                    },
                    {
                        "name": "Fangmin Chen"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Zhihua Wu"
                    },
                    {
                        "name": "Songwei Liu"
                    },
                    {
                        "name": "Mingbao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Mingbao Lin"
                },
                "author": "Mingbao Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19670v1",
                "updated": "2025-08-27T08:30:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    8,
                    30,
                    33,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T08:30:33Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    8,
                    30,
                    33,
                    2,
                    239,
                    0
                ],
                "title": "Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed\n  Criticality Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed\n  Criticality Systems"
                },
                "summary": "As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate\nheterogeneous computing platforms, combining general-purpose processors with\nspecialized accelerators such as AI engines, GPUs, and high-speed networking\ninterfaces. This heterogeneity introduces challenges, as these accelerators and\nDMA-capable devices act as independent bus masters, directly accessing memory.\nConsequently, ensuring both security and timing predictability in such\nenvironments becomes critical. To address these concerns, the Input-Output\nMemory Management Unit (IOMMU) plays a key role in mediating and regulating\nmemory access, preventing unauthorized transactions while enforcing isolation\nand access control policies. While prior work has explored IOMMU-related\nside-channel vulnerabilities from a security standpoint, its role in\nperformance interference remains largely unexplored. Moreover, many of the same\narchitectural properties that enable side-channel leakage, such as shared TLBs,\ncaching effects, and translation overheads, can also introduce timing\nunpredictability. In this work, we analyze the contention effects within IOMMU\nstructures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how\ntheir shared nature introduce unpredictable delays. Our findings reveal that\nIOMMU-induced interference primarily affects small memory transactions, where\ntranslation overheads significantly impact execution time. Additionally, we\nhypothesize that contention effects arising from IOTLBs exhibit similar\nbehavior across architectures due to shared caching principles, such as\nprefetching and hierarchical TLB structures. Notably, our experiments show that\nIOMMU interference can delay DMA transactions by up to 1.79x for lower-size\ntransfers on the Arm SMMUv2 implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate\nheterogeneous computing platforms, combining general-purpose processors with\nspecialized accelerators such as AI engines, GPUs, and high-speed networking\ninterfaces. This heterogeneity introduces challenges, as these accelerators and\nDMA-capable devices act as independent bus masters, directly accessing memory.\nConsequently, ensuring both security and timing predictability in such\nenvironments becomes critical. To address these concerns, the Input-Output\nMemory Management Unit (IOMMU) plays a key role in mediating and regulating\nmemory access, preventing unauthorized transactions while enforcing isolation\nand access control policies. While prior work has explored IOMMU-related\nside-channel vulnerabilities from a security standpoint, its role in\nperformance interference remains largely unexplored. Moreover, many of the same\narchitectural properties that enable side-channel leakage, such as shared TLBs,\ncaching effects, and translation overheads, can also introduce timing\nunpredictability. In this work, we analyze the contention effects within IOMMU\nstructures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how\ntheir shared nature introduce unpredictable delays. Our findings reveal that\nIOMMU-induced interference primarily affects small memory transactions, where\ntranslation overheads significantly impact execution time. Additionally, we\nhypothesize that contention effects arising from IOTLBs exhibit similar\nbehavior across architectures due to shared caching principles, such as\nprefetching and hierarchical TLB structures. Notably, our experiments show that\nIOMMU interference can delay DMA transactions by up to 1.79x for lower-size\ntransfers on the Arm SMMUv2 implementation."
                },
                "authors": [
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Jose Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v4",
                "updated": "2025-08-27T04:58:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    4,
                    58,
                    58,
                    2,
                    239,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "arxiv_comment": "Accepted to EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19247v1",
                "updated": "2025-08-26T17:59:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    59,
                    47,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T17:59:47Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    59,
                    47,
                    1,
                    238,
                    0
                ],
                "title": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D\n  Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D\n  Space"
                },
                "summary": "3D local editing of specified regions is crucial for game industry and robot\ninteraction. Recent methods typically edit rendered multi-view images and then\nreconstruct 3D models, but they face challenges in precisely preserving\nunedited regions and overall coherence. Inspired by structured 3D generative\nmodels, we propose VoxHammer, a novel training-free approach that performs\nprecise and coherent editing in 3D latent space. Given a 3D model, VoxHammer\nfirst predicts its inversion trajectory and obtains its inverted latents and\nkey-value tokens at each timestep. Subsequently, in the denoising and editing\nphase, we replace the denoising features of preserved regions with the\ncorresponding inverted latents and cached key-value tokens. By retaining these\ncontextual features, this approach ensures consistent reconstruction of\npreserved areas and coherent integration of edited parts. To evaluate the\nconsistency of preserved regions, we constructed Edit3D-Bench, a\nhuman-annotated dataset comprising hundreds of samples, each with carefully\nlabeled 3D editing regions. Experiments demonstrate that VoxHammer\nsignificantly outperforms existing methods in terms of both 3D consistency of\npreserved regions and overall quality. Our method holds promise for\nsynthesizing high-quality edited paired data, thereby laying the data\nfoundation for in-context 3D generation. See our project page at\nhttps://huanngzh.github.io/VoxHammer-Page/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D local editing of specified regions is crucial for game industry and robot\ninteraction. Recent methods typically edit rendered multi-view images and then\nreconstruct 3D models, but they face challenges in precisely preserving\nunedited regions and overall coherence. Inspired by structured 3D generative\nmodels, we propose VoxHammer, a novel training-free approach that performs\nprecise and coherent editing in 3D latent space. Given a 3D model, VoxHammer\nfirst predicts its inversion trajectory and obtains its inverted latents and\nkey-value tokens at each timestep. Subsequently, in the denoising and editing\nphase, we replace the denoising features of preserved regions with the\ncorresponding inverted latents and cached key-value tokens. By retaining these\ncontextual features, this approach ensures consistent reconstruction of\npreserved areas and coherent integration of edited parts. To evaluate the\nconsistency of preserved regions, we constructed Edit3D-Bench, a\nhuman-annotated dataset comprising hundreds of samples, each with carefully\nlabeled 3D editing regions. Experiments demonstrate that VoxHammer\nsignificantly outperforms existing methods in terms of both 3D consistency of\npreserved regions and overall quality. Our method holds promise for\nsynthesizing high-quality edited paired data, thereby laying the data\nfoundation for in-context 3D generation. See our project page at\nhttps://huanngzh.github.io/VoxHammer-Page/."
                },
                "authors": [
                    {
                        "name": "Lin Li"
                    },
                    {
                        "name": "Zehuan Huang"
                    },
                    {
                        "name": "Haoran Feng"
                    },
                    {
                        "name": "Gengxiong Zhuang"
                    },
                    {
                        "name": "Rui Chen"
                    },
                    {
                        "name": "Chunchao Guo"
                    },
                    {
                        "name": "Lu Sheng"
                    }
                ],
                "author_detail": {
                    "name": "Lu Sheng"
                },
                "author": "Lu Sheng",
                "arxiv_comment": "Project page: https://huanngzh.github.io/VoxHammer-Page/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18983v1",
                "updated": "2025-08-26T12:32:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    32,
                    9,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T12:32:09Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    32,
                    9,
                    1,
                    238,
                    0
                ],
                "title": "Enabling MoE on the Edge via Importance-Driven Expert Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling MoE on the Edge via Importance-Driven Expert Scheduling"
                },
                "summary": "The Mixture of Experts (MoE) architecture has emerged as a key technique for\nscaling Large Language Models by activating only a subset of experts per query.\nDeploying MoE on consumer-grade edge hardware, however, is constrained by\nlimited device memory, making dynamic expert offloading essential. Unlike prior\nwork that treats offloading purely as a scheduling problem, we leverage expert\nimportance to guide decisions, substituting low-importance activated experts\nwith functionally similar ones already cached in GPU memory, thereby preserving\naccuracy. As a result, this design reduces memory usage and data transfer,\nwhile largely eliminating PCIe overhead. In addition, we introduce a scheduling\npolicy that maximizes the reuse ratio of GPU-cached experts, further boosting\nefficiency. Extensive evaluations show that our approach delivers 48% lower\ndecoding latency with over 60% expert cache hit rate, while maintaining nearly\nlossless accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) architecture has emerged as a key technique for\nscaling Large Language Models by activating only a subset of experts per query.\nDeploying MoE on consumer-grade edge hardware, however, is constrained by\nlimited device memory, making dynamic expert offloading essential. Unlike prior\nwork that treats offloading purely as a scheduling problem, we leverage expert\nimportance to guide decisions, substituting low-importance activated experts\nwith functionally similar ones already cached in GPU memory, thereby preserving\naccuracy. As a result, this design reduces memory usage and data transfer,\nwhile largely eliminating PCIe overhead. In addition, we introduce a scheduling\npolicy that maximizes the reuse ratio of GPU-cached experts, further boosting\nefficiency. Extensive evaluations show that our approach delivers 48% lower\ndecoding latency with over 60% expert cache hit rate, while maintaining nearly\nlossless accuracy."
                },
                "authors": [
                    {
                        "name": "Guoying Zhu"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Haipeng Dai"
                    },
                    {
                        "name": "Xuechen Liu"
                    },
                    {
                        "name": "Weijun Wang"
                    },
                    {
                        "name": "Keran Li"
                    },
                    {
                        "name": "Jun xiao"
                    },
                    {
                        "name": "Ligeng Chen"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18736v1",
                "updated": "2025-08-26T07:09:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    7,
                    9,
                    9,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T07:09:09Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    7,
                    9,
                    9,
                    1,
                    238,
                    0
                ],
                "title": "Rethinking Caching for LLM Serving Systems: Beyond Traditional\n  Heuristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Caching for LLM Serving Systems: Beyond Traditional\n  Heuristics"
                },
                "summary": "Serving Large Language Models (LLMs) at scale requires meeting strict Service\nLevel Objectives (SLOs) under severe computational and memory constraints.\nNevertheless, traditional caching strategies fall short: exact-matching and\nprefix caches neglect query semantics, while state-of-the-art semantic caches\nremain confined to traditional intuitions, offering little conceptual\ndeparture. Building on this, we present SISO, a semantic caching system that\nredefines efficiency for LLM serving. SISO introduces centroid-based caching to\nmaximize coverage with minimal memory, locality-aware replacement to preserve\nhigh-value entries, and dynamic thresholding to balance accuracy and latency\nunder varying workloads. Across diverse datasets, SISO delivers up to\n1.71$\\times$ higher hit ratios and consistently stronger SLO attainment\ncompared to state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) at scale requires meeting strict Service\nLevel Objectives (SLOs) under severe computational and memory constraints.\nNevertheless, traditional caching strategies fall short: exact-matching and\nprefix caches neglect query semantics, while state-of-the-art semantic caches\nremain confined to traditional intuitions, offering little conceptual\ndeparture. Building on this, we present SISO, a semantic caching system that\nredefines efficiency for LLM serving. SISO introduces centroid-based caching to\nmaximize coverage with minimal memory, locality-aware replacement to preserve\nhigh-value entries, and dynamic thresholding to balance accuracy and latency\nunder varying workloads. Across diverse datasets, SISO delivers up to\n1.71$\\times$ higher hit ratios and consistently stronger SLO attainment\ncompared to state-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Jungwoo Kim"
                    },
                    {
                        "name": "Minsang Kim"
                    },
                    {
                        "name": "Jaeheon Lee"
                    },
                    {
                        "name": "Chanwoo Moon"
                    },
                    {
                        "name": "Heejin Kim"
                    },
                    {
                        "name": "Taeho Hwang"
                    },
                    {
                        "name": "Woosuk Chung"
                    },
                    {
                        "name": "Yeseong Kim"
                    },
                    {
                        "name": "Sungjin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sungjin Lee"
                },
                "author": "Sungjin Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09822v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09822v3",
                "updated": "2025-08-26T03:23:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    3,
                    23,
                    53,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-13T13:54:51Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    51,
                    2,
                    225,
                    0
                ],
                "title": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining"
                },
                "summary": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/"
                },
                "authors": [
                    {
                        "name": "Zijian Song"
                    },
                    {
                        "name": "Sihan Qin"
                    },
                    {
                        "name": "Tianshui Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Guangrun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guangrun Wang"
                },
                "author": "Guangrun Wang",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09822v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09822v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08045v2",
                "updated": "2025-08-26T01:55:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    1,
                    55,
                    27,
                    1,
                    238,
                    0
                ],
                "published": "2025-07-10T01:51:17Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    1,
                    51,
                    17,
                    3,
                    191,
                    0
                ],
                "title": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing"
                },
                "summary": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Junyi Wen"
                    },
                    {
                        "name": "Junyuan Liang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Ting Cai"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19365v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19365v3",
                "updated": "2025-08-26T01:45:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    1,
                    45,
                    34,
                    1,
                    238,
                    0
                ],
                "published": "2025-04-27T22:05:14Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "title": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration"
                },
                "summary": "GPUs are critical for compute-intensive applications, yet emerging workloads\nsuch as recommender systems, graph analytics, and data analytics often exceed\nGPU memory capacity. Existing solutions allow GPUs to use CPU DRAM or SSDs as\nexternal memory, and the GPU-centric approach enables GPU threads to directly\nissue NVMe requests, further avoiding CPU intervention. However, current\nGPU-centric approaches adopt synchronous I/O, forcing threads to stall during\nlong communication delays.\n  We propose AGILE, a lightweight asynchronous GPU-centric I/O library that\neliminates deadlock risks and integrates a flexible HBM-based software cache.\nAGILE overlaps computation and I/O, improving performance by up to 1.88$\\times$\nacross workloads with diverse computation-to-communication ratios. Compared to\nBaM on DLRM, AGILE achieves up to 1.75$\\times$ speedup through efficient design\nand overlapping; on graph applications, AGILE reduces software cache overhead\nby up to 3.12$\\times$ and NVMe I/O overhead by up to 2.85$\\times$; AGILE also\nlowers per-thread register usage by up to 1.32$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPUs are critical for compute-intensive applications, yet emerging workloads\nsuch as recommender systems, graph analytics, and data analytics often exceed\nGPU memory capacity. Existing solutions allow GPUs to use CPU DRAM or SSDs as\nexternal memory, and the GPU-centric approach enables GPU threads to directly\nissue NVMe requests, further avoiding CPU intervention. However, current\nGPU-centric approaches adopt synchronous I/O, forcing threads to stall during\nlong communication delays.\n  We propose AGILE, a lightweight asynchronous GPU-centric I/O library that\neliminates deadlock risks and integrates a flexible HBM-based software cache.\nAGILE overlaps computation and I/O, improving performance by up to 1.88$\\times$\nacross workloads with diverse computation-to-communication ratios. Compared to\nBaM on DLRM, AGILE achieves up to 1.75$\\times$ speedup through efficient design\nand overlapping; on graph applications, AGILE reduces software cache overhead\nby up to 3.12$\\times$ and NVMe I/O overhead by up to 2.85$\\times$; AGILE also\nlowers per-thread register usage by up to 1.32$\\times$."
                },
                "authors": [
                    {
                        "name": "Zhuoping Yang"
                    },
                    {
                        "name": "Jinming Zhuang"
                    },
                    {
                        "name": "Xingzhen Chen"
                    },
                    {
                        "name": "Alex K. Jones"
                    },
                    {
                        "name": "Peipei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Peipei Zhou"
                },
                "author": "Peipei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19365v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19365v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18572v1",
                "updated": "2025-08-26T00:09:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    0,
                    9,
                    3,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T00:09:03Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    0,
                    9,
                    3,
                    1,
                    238,
                    0
                ],
                "title": "Strata: Hierarchical Context Caching for Long Context Language Model\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strata: Hierarchical Context Caching for Long Context Language Model\n  Serving"
                },
                "summary": "Large Language Models (LLMs) with expanding context windows face significant\nperformance hurdles. While caching key-value (KV) states is critical for\navoiding redundant computation, the storage footprint of long-context caches\nquickly exceeds GPU memory capacity, forcing production systems to adopt\nhierarchical caching across memory hierarchies. However, transferring large\ncached contexts back to the GPU introduces severe performance bottlenecks:\nfragmented I/O from paged layouts prevents full bandwidth utilization, and\nexisting schedulers fail to account for cache-loading delays, leaving systems\nloading-bound rather than compute-bound. We present Strata, a hierarchical\ncontext caching framework designed for efficient long context LLM serving.\nStrata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling\nGPU and CPU memory layouts and employs cache-aware request scheduling to\nbalance compute with I/O latency and overlapping unavoidable stalls with\ncomplementary tasks. Built on SGLang and deployed in production, Strata\nachieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache\nand 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without\ndegrading short-context performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with expanding context windows face significant\nperformance hurdles. While caching key-value (KV) states is critical for\navoiding redundant computation, the storage footprint of long-context caches\nquickly exceeds GPU memory capacity, forcing production systems to adopt\nhierarchical caching across memory hierarchies. However, transferring large\ncached contexts back to the GPU introduces severe performance bottlenecks:\nfragmented I/O from paged layouts prevents full bandwidth utilization, and\nexisting schedulers fail to account for cache-loading delays, leaving systems\nloading-bound rather than compute-bound. We present Strata, a hierarchical\ncontext caching framework designed for efficient long context LLM serving.\nStrata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling\nGPU and CPU memory layouts and employs cache-aware request scheduling to\nbalance compute with I/O latency and overlapping unavoidable stalls with\ncomplementary tasks. Built on SGLang and deployed in production, Strata\nachieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache\nand 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without\ndegrading short-context performance."
                },
                "authors": [
                    {
                        "name": "Zhiqiang Xie"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Yuwei An"
                    },
                    {
                        "name": "Vikram Sharma Mailthody"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Michael Garland"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "13 pages, 14 figures, under peer review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18540v1",
                "updated": "2025-08-25T22:21:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    22,
                    21,
                    4,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T22:21:04Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    22,
                    21,
                    4,
                    0,
                    237,
                    0
                ],
                "title": "Real-time 3D Visualization of Radiance Fields on Light Field Displays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time 3D Visualization of Radiance Fields on Light Field Displays"
                },
                "summary": "Radiance fields have revolutionized photo-realistic 3D scene visualization by\nenabling high-fidelity reconstruction of complex environments, making them an\nideal match for light field displays. However, integrating these technologies\npresents significant computational challenges, as light field displays require\nmultiple high-resolution renderings from slightly shifted viewpoints, while\nradiance fields rely on computationally intensive volume rendering. In this\npaper, we propose a unified and efficient framework for real-time radiance\nfield rendering on light field displays. Our method supports a wide range of\nradiance field representations, including NeRFs, 3D Gaussian Splatting, and\nSparse Voxels, within a shared architecture based on a single-pass plane\nsweeping strategy and caching of shared, non-directional components. The\nframework generalizes across different scene formats without retraining, and\navoids redundant computation across views. We further demonstrate a real-time\ninteractive application on a Looking Glass display, achieving 200+ FPS at 512p\nacross 45 views, enabling seamless, immersive 3D interaction. On standard\nbenchmarks, our method achieves up to 22x speedup compared to independently\nrendering each view, while preserving image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radiance fields have revolutionized photo-realistic 3D scene visualization by\nenabling high-fidelity reconstruction of complex environments, making them an\nideal match for light field displays. However, integrating these technologies\npresents significant computational challenges, as light field displays require\nmultiple high-resolution renderings from slightly shifted viewpoints, while\nradiance fields rely on computationally intensive volume rendering. In this\npaper, we propose a unified and efficient framework for real-time radiance\nfield rendering on light field displays. Our method supports a wide range of\nradiance field representations, including NeRFs, 3D Gaussian Splatting, and\nSparse Voxels, within a shared architecture based on a single-pass plane\nsweeping strategy and caching of shared, non-directional components. The\nframework generalizes across different scene formats without retraining, and\navoids redundant computation across views. We further demonstrate a real-time\ninteractive application on a Looking Glass display, achieving 200+ FPS at 512p\nacross 45 views, enabling seamless, immersive 3D interaction. On standard\nbenchmarks, our method achieves up to 22x speedup compared to independently\nrendering each view, while preserving image quality."
                },
                "authors": [
                    {
                        "name": "Jonghyun Kim"
                    },
                    {
                        "name": "Cheng Sun"
                    },
                    {
                        "name": "Michael Stengel"
                    },
                    {
                        "name": "Matthew Chan"
                    },
                    {
                        "name": "Andrew Russell"
                    },
                    {
                        "name": "Jaehyun Jung"
                    },
                    {
                        "name": "Wil Braithwaite"
                    },
                    {
                        "name": "Shalini De Mello"
                    },
                    {
                        "name": "David Luebke"
                    }
                ],
                "author_detail": {
                    "name": "David Luebke"
                },
                "author": "David Luebke",
                "arxiv_comment": "10 pages, 14 figures. J. Kim, C. Sun, and M. Stengel contributed\n  equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18494v1",
                "updated": "2025-08-25T21:07:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    21,
                    7,
                    52,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T21:07:52Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    21,
                    7,
                    52,
                    0,
                    237,
                    0
                ],
                "title": "DiskJoin: Large-scale Vector Similarity Join with SSD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiskJoin: Large-scale Vector Similarity Join with SSD"
                },
                "summary": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x."
                },
                "authors": [
                    {
                        "name": "Yanqi Chen"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Alexandra Meliou"
                    },
                    {
                        "name": "Eric Lo"
                    }
                ],
                "author_detail": {
                    "name": "Eric Lo"
                },
                "author": "Eric Lo",
                "arxiv_comment": "Accepted at SIGMOD 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v3",
                "updated": "2025-08-25T15:48:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    48,
                    28,
                    0,
                    237,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "CIKM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17892v1",
                "updated": "2025-08-25T10:59:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T10:59:02Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "title": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance\ncomparable to or better than the full context in the long context scenarios.\nWithout additional post training or operator development, ILRe can process a\nsingle $1M$ tokens request in less than half a minute (speedup $\\approx\n180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with model\nLlama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance\ncomparable to or better than the full context in the long context scenarios.\nWithout additional post training or operator development, ILRe can process a\nsingle $1M$ tokens request in less than half a minute (speedup $\\approx\n180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with model\nLlama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "Mandi Liu"
                    },
                    {
                        "name": "Jiangzhou Ji"
                    },
                    {
                        "name": "Huaijun Li"
                    },
                    {
                        "name": "Haobo Yang"
                    },
                    {
                        "name": "Yaohan He"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17756v1",
                "updated": "2025-08-25T07:49:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    7,
                    49,
                    17,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T07:49:17Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    7,
                    49,
                    17,
                    0,
                    237,
                    0
                ],
                "title": "SuperGen: An Efficient Ultra-high-resolution Video Generation System\n  with Sketching and Tiling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperGen: An Efficient Ultra-high-resolution Video Generation System\n  with Sketching and Tiling"
                },
                "summary": "Diffusion models have recently achieved remarkable success in generative\ntasks (e.g., image and video generation), and the demand for high-quality\ncontent (e.g., 2K/4K videos) is rapidly increasing across various domains.\nHowever, generating ultra-high-resolution videos on existing\nstandard-resolution (e.g., 720p) platforms remains challenging due to the\nexcessive re-training requirements and prohibitively high computational and\nmemory costs. To this end, we introduce SuperGen, an efficient tile-based\nframework for ultra-high-resolution video generation. SuperGen features a novel\ntraining-free algorithmic innovation with tiling to successfully support a wide\nrange of resolutions without additional training efforts while significantly\nreducing both memory footprint and computational complexity. Moreover, SuperGen\nincorporates a tile-tailored, adaptive, region-aware caching strategy that\naccelerates video generation by exploiting redundancy across denoising steps\nand spatial regions. SuperGen also integrates cache-guided,\ncommunication-minimized tile parallelism for enhanced throughput and minimized\nlatency. Evaluations demonstrate that SuperGen harvests the maximum performance\ngains while achieving high output quality across various benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have recently achieved remarkable success in generative\ntasks (e.g., image and video generation), and the demand for high-quality\ncontent (e.g., 2K/4K videos) is rapidly increasing across various domains.\nHowever, generating ultra-high-resolution videos on existing\nstandard-resolution (e.g., 720p) platforms remains challenging due to the\nexcessive re-training requirements and prohibitively high computational and\nmemory costs. To this end, we introduce SuperGen, an efficient tile-based\nframework for ultra-high-resolution video generation. SuperGen features a novel\ntraining-free algorithmic innovation with tiling to successfully support a wide\nrange of resolutions without additional training efforts while significantly\nreducing both memory footprint and computational complexity. Moreover, SuperGen\nincorporates a tile-tailored, adaptive, region-aware caching strategy that\naccelerates video generation by exploiting redundancy across denoising steps\nand spatial regions. SuperGen also integrates cache-guided,\ncommunication-minimized tile parallelism for enhanced throughput and minimized\nlatency. Evaluations demonstrate that SuperGen harvests the maximum performance\ngains while achieving high output quality across various benchmarks."
                },
                "authors": [
                    {
                        "name": "Fanjiang Ye"
                    },
                    {
                        "name": "Zepeng Zhao"
                    },
                    {
                        "name": "Yi Mu"
                    },
                    {
                        "name": "Jucheng Shen"
                    },
                    {
                        "name": "Renjie Li"
                    },
                    {
                        "name": "Kaijian Wang"
                    },
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Triston Cao"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "T. S. Eugene Ng"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    },
                    {
                        "name": "Yuke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Wang"
                },
                "author": "Yuke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16212v2",
                "updated": "2025-08-25T03:07:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    7,
                    2,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-22T08:36:58Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    36,
                    58,
                    4,
                    234,
                    0
                ],
                "title": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free\n  Cache Reuse for Diffusion Transformer Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free\n  Cache Reuse for Diffusion Transformer Models"
                },
                "summary": "Diffusion models have emerged as a powerful paradigm for generative tasks\nsuch as image synthesis and video generation, with Transformer architectures\nfurther enhancing performance. However, the high computational cost of\ndiffusion Transformers-stemming from a large number of sampling steps and\ncomplex per-step computations-presents significant challenges for real-time\ndeployment. In this paper, we introduce OmniCache, a training-free acceleration\nmethod that exploits the global redundancy inherent in the denoising process.\nUnlike existing methods that determine caching strategies based on inter-step\nsimilarities and tend to prioritize reusing later sampling steps, our approach\noriginates from the sampling perspective of DIT models. We systematically\nanalyze the model's sampling trajectories and strategically distribute cache\nreuse across the entire sampling process. This global perspective enables more\neffective utilization of cached computations throughout the diffusion\ntrajectory, rather than concentrating reuse within limited segments of the\nsampling procedure. In addition, during cache reuse, we dynamically estimate\nthe corresponding noise and filter it out to reduce its impact on the sampling\ndirection. Extensive experiments demonstrate that our approach accelerates the\nsampling process while maintaining competitive generative quality, offering a\npromising and practical solution for efficient deployment of diffusion-based\ngenerative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a powerful paradigm for generative tasks\nsuch as image synthesis and video generation, with Transformer architectures\nfurther enhancing performance. However, the high computational cost of\ndiffusion Transformers-stemming from a large number of sampling steps and\ncomplex per-step computations-presents significant challenges for real-time\ndeployment. In this paper, we introduce OmniCache, a training-free acceleration\nmethod that exploits the global redundancy inherent in the denoising process.\nUnlike existing methods that determine caching strategies based on inter-step\nsimilarities and tend to prioritize reusing later sampling steps, our approach\noriginates from the sampling perspective of DIT models. We systematically\nanalyze the model's sampling trajectories and strategically distribute cache\nreuse across the entire sampling process. This global perspective enables more\neffective utilization of cached computations throughout the diffusion\ntrajectory, rather than concentrating reuse within limited segments of the\nsampling procedure. In addition, during cache reuse, we dynamically estimate\nthe corresponding noise and filter it out to reduce its impact on the sampling\ndirection. Extensive experiments demonstrate that our approach accelerates the\nsampling process while maintaining competitive generative quality, offering a\npromising and practical solution for efficient deployment of diffusion-based\ngenerative models."
                },
                "authors": [
                    {
                        "name": "Huanpeng Chu"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Guanyu Fen"
                    },
                    {
                        "name": "Yutao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yutao Zhang"
                },
                "author": "Yutao Zhang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17624v1",
                "updated": "2025-08-25T03:05:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    5,
                    16,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T03:05:16Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    5,
                    16,
                    0,
                    237,
                    0
                ],
                "title": "ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters\n  at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters\n  at Scale"
                },
                "summary": "Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large\nlanguage models to enhance their task-specific performance by selectively\ntuning the top-activated experts for the task. Serving these fine-tuned models\nat scale is challenging: deploying merged models in isolation is prohibitively\nresource-hungry, while existing multi-adapter serving systems with LoRA-style\nadditive updates are incompatible with ESFT's expert-oriented paradigm. We\npresent ExpertWeave, a system that serves multiple ESFT adapters concurrently\nover a single shared MoE base model, drastically reducing the memory footprint\nand improving resource utilization. To seamlessly integrate into existing\ninference pipelines for MoE models with non-intrusive modifications and minimal\nlatency overhead, ExpertWeave introduces a virtual-memory-assisted expert\nweight manager that co-locates base-model and adapter experts without incurring\nmemory overhead from fragmentation, and a fused kernel for batched rerouting to\nenable lightweight redirection of tokens to the appropriate experts at runtime.\nOur evaluations show that ExpertWeave can simultaneously serve multiple\nadapters of a 16B MoE model on a single accelerator where the baseline runs out\nof memory, or provides up to 94x more KV cache capacity and achieves up to 18%\nhigher throughput while using comparable resources, all without compromising\nmodel accuracy. ExpertWeave maintains low overhead even when scaling to 20\nadapters, with a 4-11% latency increase compared with serving the base model\nalone. Source code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large\nlanguage models to enhance their task-specific performance by selectively\ntuning the top-activated experts for the task. Serving these fine-tuned models\nat scale is challenging: deploying merged models in isolation is prohibitively\nresource-hungry, while existing multi-adapter serving systems with LoRA-style\nadditive updates are incompatible with ESFT's expert-oriented paradigm. We\npresent ExpertWeave, a system that serves multiple ESFT adapters concurrently\nover a single shared MoE base model, drastically reducing the memory footprint\nand improving resource utilization. To seamlessly integrate into existing\ninference pipelines for MoE models with non-intrusive modifications and minimal\nlatency overhead, ExpertWeave introduces a virtual-memory-assisted expert\nweight manager that co-locates base-model and adapter experts without incurring\nmemory overhead from fragmentation, and a fused kernel for batched rerouting to\nenable lightweight redirection of tokens to the appropriate experts at runtime.\nOur evaluations show that ExpertWeave can simultaneously serve multiple\nadapters of a 16B MoE model on a single accelerator where the baseline runs out\nof memory, or provides up to 94x more KV cache capacity and achieves up to 18%\nhigher throughput while using comparable resources, all without compromising\nmodel accuracy. ExpertWeave maintains low overhead even when scaling to 20\nadapters, with a 4-11% latency increase compared with serving the base model\nalone. Source code will be released soon."
                },
                "authors": [
                    {
                        "name": "Ge Shi"
                    },
                    {
                        "name": "Hanieh Sadri"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00052v1",
                "updated": "2025-08-25T02:58:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    2,
                    58,
                    39,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T02:58:39Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    2,
                    58,
                    39,
                    0,
                    237,
                    0
                ],
                "title": "Lightning Fast Caching-based Parallel Denoising Prediction for\n  Accelerating Talking Head Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightning Fast Caching-based Parallel Denoising Prediction for\n  Accelerating Talking Head Generation"
                },
                "summary": "Diffusion-based talking head models generate high-quality, photorealistic\nvideos but suffer from slow inference, limiting practical applications.\nExisting acceleration methods for general diffusion models fail to exploit the\ntemporal and spatial redundancies unique to talking head generation. In this\npaper, we propose a task-specific framework addressing these inefficiencies\nthrough two key innovations. First, we introduce Lightning-fast Caching-based\nParallel denoising prediction (LightningCP), caching static features to bypass\nmost model layers in inference time. We also enable parallel prediction using\ncached features and estimated noisy latents as inputs, efficiently bypassing\nsequential sampling. Second, we propose Decoupled Foreground Attention (DFA) to\nfurther accelerate attention computations, exploiting the spatial decoupling in\ntalking head videos to restrict attention to dynamic foreground regions.\nAdditionally, we remove reference features in certain layers to bring extra\nspeedup. Extensive experiments demonstrate that our framework significantly\nimproves inference speed while preserving video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based talking head models generate high-quality, photorealistic\nvideos but suffer from slow inference, limiting practical applications.\nExisting acceleration methods for general diffusion models fail to exploit the\ntemporal and spatial redundancies unique to talking head generation. In this\npaper, we propose a task-specific framework addressing these inefficiencies\nthrough two key innovations. First, we introduce Lightning-fast Caching-based\nParallel denoising prediction (LightningCP), caching static features to bypass\nmost model layers in inference time. We also enable parallel prediction using\ncached features and estimated noisy latents as inputs, efficiently bypassing\nsequential sampling. Second, we propose Decoupled Foreground Attention (DFA) to\nfurther accelerate attention computations, exploiting the spatial decoupling in\ntalking head videos to restrict attention to dynamic foreground regions.\nAdditionally, we remove reference features in certain layers to bring extra\nspeedup. Extensive experiments demonstrate that our framework significantly\nimproves inference speed while preserving video quality."
                },
                "authors": [
                    {
                        "name": "Jianzhi Long"
                    },
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rongcheng Tu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15881v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15881v2",
                "updated": "2025-08-25T02:24:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    2,
                    24,
                    20,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-21T15:25:40Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    15,
                    25,
                    40,
                    3,
                    233,
                    0
                ],
                "title": "TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated\n  Prefill and Decode Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated\n  Prefill and Decode Inference"
                },
                "summary": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses\nkey-value states into a low-rank latent vector, caching only this vector to\nreduce memory. In tensor parallelism (TP), however, attention heads are\ncomputed across multiple devices, and each device must load the full cache,\neroding the advantage of MLA over Grouped Query Attention (GQA). We propose\nTensor-Parallel Latent Attention (TPLA): a scheme that partitions both the\nlatent representation and each head's input dimension across devices, performs\nattention independently per shard, and then combines results with an\nall-reduce. TPLA preserves the benefits of a compressed KV cache while\nunlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in\nTPLA still leverages the full latent representation, maintaining stronger\nrepresentational capacity. TPLA is drop-in compatible with models pre-trained\nusing MLA: it supports MLA-style prefilling and enables efficient\ntensor-parallel decoding without retraining. Applying simple orthogonal\ntransforms -- e.g., the Hadamard transform or PCA -- before TP slicing further\nmitigates cross-shard interference, yielding minimal accuracy degradation. By\nreducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x\nand 1.93x speedups, respectively, at a 32K-token context length while\nmaintaining performance on commonsense and LongBench benchmarks. TPLA can be\nimplemented with FlashAttention-3, enabling practical end-to-end acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses\nkey-value states into a low-rank latent vector, caching only this vector to\nreduce memory. In tensor parallelism (TP), however, attention heads are\ncomputed across multiple devices, and each device must load the full cache,\neroding the advantage of MLA over Grouped Query Attention (GQA). We propose\nTensor-Parallel Latent Attention (TPLA): a scheme that partitions both the\nlatent representation and each head's input dimension across devices, performs\nattention independently per shard, and then combines results with an\nall-reduce. TPLA preserves the benefits of a compressed KV cache while\nunlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in\nTPLA still leverages the full latent representation, maintaining stronger\nrepresentational capacity. TPLA is drop-in compatible with models pre-trained\nusing MLA: it supports MLA-style prefilling and enables efficient\ntensor-parallel decoding without retraining. Applying simple orthogonal\ntransforms -- e.g., the Hadamard transform or PCA -- before TP slicing further\nmitigates cross-shard interference, yielding minimal accuracy degradation. By\nreducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x\nand 1.93x speedups, respectively, at a 32K-token context length while\nmaintaining performance on commonsense and LongBench benchmarks. TPLA can be\nimplemented with FlashAttention-3, enabling practical end-to-end acceleration."
                },
                "authors": [
                    {
                        "name": "Xiaojuan Tang"
                    },
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15881v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15881v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17593v1",
                "updated": "2025-08-25T01:33:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    1,
                    33,
                    18,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T01:33:18Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    1,
                    33,
                    18,
                    0,
                    237,
                    0
                ],
                "title": "Zen-Attention: A Compiler Framework for Dynamic Attention Folding on AMD\n  NPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zen-Attention: A Compiler Framework for Dynamic Attention Folding on AMD\n  NPUs"
                },
                "summary": "Transformer-based deep learning models are increasingly deployed on energy,\nand DRAM bandwidth constrained devices such as laptops and gaming consoles,\nwhich presents significant challenges in meeting the latency requirements of\nthe models. The industry is turning to neural processing units (NPUs) for\nsuperior performance-per-watt (perf/watt); however, efficiently mapping dynamic\nattention layers to the NPUs remains a challenging task. For optimizing\nperf/watt, AMD XDNA NPUs employ software managed caches and share system memory\nwith host. This requires substantial engineering effort to unlock efficient\ntiling, buffer allocation, and data movement to extract the maximum efficiency\nfrom the device. This paper introduces Zen-Attention, a framework that\noptimizes DRAM bandwidth utilization in the attention layer of models by\nsystematically exploring the complex design space of layer folding, tiling, and\ndata-movement on the interconnect, and the tensor layouts to come up with an\noptimal solution. Our evaluation includes comparative analysis of end-to-end\nmodel latency and specific attention latency in each model. We demonstrate how\nthe framework enhances mapping capabilities by varying input dimensions, which\nrequire padding and masking in the attention block. For representative\ntransformer models, the Zen-Attention Framework achieves up to 4x improvement\nin the latency of the attention block and up to 32% improvement in end-to-end\nnetwork latency compared to the baseline Unfolded- approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based deep learning models are increasingly deployed on energy,\nand DRAM bandwidth constrained devices such as laptops and gaming consoles,\nwhich presents significant challenges in meeting the latency requirements of\nthe models. The industry is turning to neural processing units (NPUs) for\nsuperior performance-per-watt (perf/watt); however, efficiently mapping dynamic\nattention layers to the NPUs remains a challenging task. For optimizing\nperf/watt, AMD XDNA NPUs employ software managed caches and share system memory\nwith host. This requires substantial engineering effort to unlock efficient\ntiling, buffer allocation, and data movement to extract the maximum efficiency\nfrom the device. This paper introduces Zen-Attention, a framework that\noptimizes DRAM bandwidth utilization in the attention layer of models by\nsystematically exploring the complex design space of layer folding, tiling, and\ndata-movement on the interconnect, and the tensor layouts to come up with an\noptimal solution. Our evaluation includes comparative analysis of end-to-end\nmodel latency and specific attention latency in each model. We demonstrate how\nthe framework enhances mapping capabilities by varying input dimensions, which\nrequire padding and masking in the attention block. For representative\ntransformer models, the Zen-Attention Framework achieves up to 4x improvement\nin the latency of the attention block and up to 32% improvement in end-to-end\nnetwork latency compared to the baseline Unfolded- approaches."
                },
                "authors": [
                    {
                        "name": "Aadesh Deshmukh"
                    },
                    {
                        "name": "Venkata Yaswanth Raparti"
                    },
                    {
                        "name": "Samuel Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Hsu"
                },
                "author": "Samuel Hsu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09040v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09040v3",
                "updated": "2025-08-25T00:15:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    0,
                    15,
                    27,
                    0,
                    237,
                    0
                ],
                "published": "2025-05-14T00:41:44Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    41,
                    44,
                    2,
                    134,
                    0
                ],
                "title": "RT-Cache: Training-Free Retrieval for Real-Time Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RT-Cache: Training-Free Retrieval for Real-Time Manipulation"
                },
                "summary": "Real robots are expected to repeat the same behavior in new environments with\nvery little new data, yet modern controllers either incur heavy per-step\ninference or require deployment-time fine-tuning. We propose RT-Cache, a\ntraining-free retrieval-as-control pipeline that caches diverse image action\ntrajectories in a unified vector memory and, at test time, embeds the current\nframe to retrieve and replay multi-step snippets, replacing per-step model\ncalls. A hierarchical search keeps lookups sub-second at million scale,\nshifting cost from compute to storage and enabling real-time control on modest\nGPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher\nsuccess and lower completion time than strong retrieval baselines\n(approximately x2 higher success and ~30% faster in our settings), and a\nsingle-episode anchoring study shows immediate adaptation to a more complex,\ncontact-rich task without fine-tuning. RT-Cache turns experience into an\nappend-only memory, offering a simple, scalable path to few-shot deployment\ntoday and a foundation for multimodal keys and optional integration with\nhigh-level policies. Project page: https://rt-cache.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real robots are expected to repeat the same behavior in new environments with\nvery little new data, yet modern controllers either incur heavy per-step\ninference or require deployment-time fine-tuning. We propose RT-Cache, a\ntraining-free retrieval-as-control pipeline that caches diverse image action\ntrajectories in a unified vector memory and, at test time, embeds the current\nframe to retrieve and replay multi-step snippets, replacing per-step model\ncalls. A hierarchical search keeps lookups sub-second at million scale,\nshifting cost from compute to storage and enabling real-time control on modest\nGPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher\nsuccess and lower completion time than strong retrieval baselines\n(approximately x2 higher success and ~30% faster in our settings), and a\nsingle-episode anchoring study shows immediate adaptation to a more complex,\ncontact-rich task without fine-tuning. RT-Cache turns experience into an\nappend-only memory, offering a simple, scalable path to few-shot deployment\ntoday and a foundation for multimodal keys and optional integration with\nhigh-level policies. Project page: https://rt-cache.github.io/."
                },
                "authors": [
                    {
                        "name": "Owen Kwon"
                    },
                    {
                        "name": "Abraham George"
                    },
                    {
                        "name": "Alison Bartsch"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "arxiv_comment": "8 pages, 6 figures. 2025 IEEE-RAS 24th International Conference on\n  Humanoid Robots",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09040v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09040v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v3",
                "updated": "2025-08-24T22:09:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    22,
                    9,
                    57,
                    6,
                    236,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs"
                },
                "summary": "Long-range tasks demand reasoning over long inputs. However, existing\nsolutions are limited, e.g., long-context models require large compute budgets,\nparameter-efficient fine-tuning (PEFT) needs training data, and\nretrieval-augmented generation (RAG) entails complex task-specific designs.\nThough in-context approaches overcome many of these issues, methods with\nshort-context LLMs are inefficient, trading context for processing more tokens.\nWe introduce PRISM, a highly token-efficient in-context method based on\nstructured schemas that outperforms baselines on diverse tasks with 4x shorter\ncontexts. This approach produces concise outputs and efficiently leverages\nkey-value (KV) caches to reduce costs by up to 54%. PRISM scales down to tiny\ncontexts without increasing costs or sacrificing quality, and generalizes to\nnew tasks with minimal effort by generating schemas from task descriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks demand reasoning over long inputs. However, existing\nsolutions are limited, e.g., long-context models require large compute budgets,\nparameter-efficient fine-tuning (PEFT) needs training data, and\nretrieval-augmented generation (RAG) entails complex task-specific designs.\nThough in-context approaches overcome many of these issues, methods with\nshort-context LLMs are inefficient, trading context for processing more tokens.\nWe introduce PRISM, a highly token-efficient in-context method based on\nstructured schemas that outperforms baselines on diverse tasks with 4x shorter\ncontexts. This approach produces concise outputs and efficiently leverages\nkey-value (KV) caches to reduce costs by up to 54%. PRISM scales down to tiny\ncontexts without increasing costs or sacrificing quality, and generalizes to\nnew tasks with minimal effort by generating schemas from task descriptions."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "Published as a conference paper at EMNLP 2025. 28 pages, 7 figures, 5\n  tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17518v1",
                "updated": "2025-08-24T20:51:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    20,
                    51,
                    6,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T20:51:06Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    20,
                    51,
                    6,
                    6,
                    236,
                    0
                ],
                "title": "Evaluating Compiler Optimization Impacts on zkVM Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Compiler Optimization Impacts on zkVM Performance"
                },
                "summary": "Zero-knowledge proofs (ZKPs) are the cornerstone of programmable\ncryptography. They enable (1) privacy-preserving and verifiable computation\nacross blockchains, and (2) an expanding range of off-chain applications such\nas credential schemes. Zero-knowledge virtual machines (zkVMs) lower the\nbarrier by turning ZKPs into a drop-in backend for standard compilation\npipelines. This lets developers write proof-generating programs in conventional\nlanguages (e.g., Rust or C++) instead of hand-crafting arithmetic circuits.\nHowever, these VMs inherit compiler infrastructures tuned for traditional\narchitectures rather than for proof systems. In particular, standard compiler\noptimizations assume features that are absent in zkVMs, including cache\nlocality, branch prediction, or instruction-level parallelism. Therefore, their\nimpact on proof generation is questionable.\n  We present the first systematic study of the impact of compiler optimizations\non zkVMs. We evaluate 64 LLVM passes, six standard optimization levels, and an\nunoptimized baseline across 58 benchmarks on two RISC-V-based zkVMs (RISC Zero\nand SP1). While standard LLVM optimization levels do improve zkVM performance\n(over 40\\%), their impact is far smaller than on traditional CPUs, since their\ndecisions rely on hardware features rather than proof constraints. Guided by a\nfine-grained pass-level analysis, we~\\emph{slightly} refine a small set of LLVM\npasses to be zkVM-aware, improving zkVM execution time by up to 45\\% (average\n+4.6\\% on RISC Zero, +1\\% on SP1) and achieving consistent proving-time gains.\nOur work highlights the potential of compiler-level optimizations for zkVM\nperformance and opens new direction for zkVM-specific passes, backends, and\nsuperoptimizers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-knowledge proofs (ZKPs) are the cornerstone of programmable\ncryptography. They enable (1) privacy-preserving and verifiable computation\nacross blockchains, and (2) an expanding range of off-chain applications such\nas credential schemes. Zero-knowledge virtual machines (zkVMs) lower the\nbarrier by turning ZKPs into a drop-in backend for standard compilation\npipelines. This lets developers write proof-generating programs in conventional\nlanguages (e.g., Rust or C++) instead of hand-crafting arithmetic circuits.\nHowever, these VMs inherit compiler infrastructures tuned for traditional\narchitectures rather than for proof systems. In particular, standard compiler\noptimizations assume features that are absent in zkVMs, including cache\nlocality, branch prediction, or instruction-level parallelism. Therefore, their\nimpact on proof generation is questionable.\n  We present the first systematic study of the impact of compiler optimizations\non zkVMs. We evaluate 64 LLVM passes, six standard optimization levels, and an\nunoptimized baseline across 58 benchmarks on two RISC-V-based zkVMs (RISC Zero\nand SP1). While standard LLVM optimization levels do improve zkVM performance\n(over 40\\%), their impact is far smaller than on traditional CPUs, since their\ndecisions rely on hardware features rather than proof constraints. Guided by a\nfine-grained pass-level analysis, we~\\emph{slightly} refine a small set of LLVM\npasses to be zkVM-aware, improving zkVM execution time by up to 45\\% (average\n+4.6\\% on RISC Zero, +1\\% on SP1) and achieving consistent proving-time gains.\nOur work highlights the potential of compiler-level optimizations for zkVM\nperformance and opens new direction for zkVM-specific passes, backends, and\nsuperoptimizers."
                },
                "authors": [
                    {
                        "name": "Thomas Gassmann"
                    },
                    {
                        "name": "Stefanos Chaliasos"
                    },
                    {
                        "name": "Thodoris Sotiropoulos"
                    },
                    {
                        "name": "Zhendong Su"
                    }
                ],
                "author_detail": {
                    "name": "Zhendong Su"
                },
                "author": "Zhendong Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17496v1",
                "updated": "2025-08-24T19:28:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    19,
                    28,
                    22,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T19:28:22Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    19,
                    28,
                    22,
                    6,
                    236,
                    0
                ],
                "title": "Practical Insertion-Only Convex Hull",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical Insertion-Only Convex Hull"
                },
                "summary": "Convex hull data structures are fundamental in computational geometry. We\nstudy insertion-only data structures, supporting various containment and\nintersection queries. When $P$ is sorted by $x$- or $y$-coordinate, convex\nhulls can be constructed in linear time using classical algorithms such as\nGraham scan. We investigate a variety of methods tailored to the insertion-only\nsetting. We explore a broad selection of trade-offs involving robustness,\nmemory access patterns, and space usage, providing an extensive evaluation of\nboth existing and novel techniques. Logarithmic-time methods rely on\npointer-based tree structures, which suffer in practice due to poor memory\nlocality. Motivated by this, we develop a vector-based solution inspired by\nOvermars' logarithmic method. Our structure has worse asymptotic bounds,\nsupporting queries in $O(\\log^2 n)$ time, but stores data in $O(\\log n)$\ncontiguous vectors, greatly improving cache performance.\n  Through empirical evaluation on real-world and synthetic data sets, we\nuncover surprising trends. Let $h$ denote the size of the convex hull. We show\nthat a na\\\"ive $O(h)$ insertion-only algorithm based on Graham scan\nconsistently outperforms both theoretical and practical state-of-the-art\nmethods under realistic workloads, even on data sets with rather large convex\nhulls. While tree-based methods with $O(\\log h)$ update times offer solid\ntheoretical guarantees, they are never optimal in practice. In contrast, our\nvector-based logarithmic method, despite its theoretically inferior bounds, is\nhighly competitive across all tested scenarios. It is optimal whenever the\nconvex hull becomes large.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convex hull data structures are fundamental in computational geometry. We\nstudy insertion-only data structures, supporting various containment and\nintersection queries. When $P$ is sorted by $x$- or $y$-coordinate, convex\nhulls can be constructed in linear time using classical algorithms such as\nGraham scan. We investigate a variety of methods tailored to the insertion-only\nsetting. We explore a broad selection of trade-offs involving robustness,\nmemory access patterns, and space usage, providing an extensive evaluation of\nboth existing and novel techniques. Logarithmic-time methods rely on\npointer-based tree structures, which suffer in practice due to poor memory\nlocality. Motivated by this, we develop a vector-based solution inspired by\nOvermars' logarithmic method. Our structure has worse asymptotic bounds,\nsupporting queries in $O(\\log^2 n)$ time, but stores data in $O(\\log n)$\ncontiguous vectors, greatly improving cache performance.\n  Through empirical evaluation on real-world and synthetic data sets, we\nuncover surprising trends. Let $h$ denote the size of the convex hull. We show\nthat a na\\\"ive $O(h)$ insertion-only algorithm based on Graham scan\nconsistently outperforms both theoretical and practical state-of-the-art\nmethods under realistic workloads, even on data sets with rather large convex\nhulls. While tree-based methods with $O(\\log h)$ update times offer solid\ntheoretical guarantees, they are never optimal in practice. In contrast, our\nvector-based logarithmic method, despite its theoretically inferior bounds, is\nhighly competitive across all tested scenarios. It is optimal whenever the\nconvex hull becomes large."
                },
                "authors": [
                    {
                        "name": "Ivor van der Hoog"
                    },
                    {
                        "name": "Henrik Reinstdtler"
                    },
                    {
                        "name": "Eva Rotenberg"
                    }
                ],
                "author_detail": {
                    "name": "Eva Rotenberg"
                },
                "author": "Eva Rotenberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17445v1",
                "updated": "2025-08-24T16:52:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    52,
                    37,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T16:52:37Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    52,
                    37,
                    6,
                    236,
                    0
                ],
                "title": "TreePO: Bridging the Gap of Policy Optimization and Efficacy and\n  Inference Efficiency with Heuristic Tree-based Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreePO: Bridging the Gap of Policy Optimization and Efficacy and\n  Inference Efficiency with Heuristic Tree-based Modeling"
                },
                "summary": "Recent advancements in aligning large language models via reinforcement\nlearning have achieved remarkable gains in solving complex reasoning problems,\nbut at the cost of expensive on-policy rollouts and limited exploration of\ndiverse reasoning paths. In this work, we introduce TreePO, involving a\nself-guided rollout algorithm that views sequence generation as a\ntree-structured searching process. Composed of dynamic tree sampling policy and\nfixed-length segment decoding, TreePO leverages local uncertainty to warrant\nadditional branches. By amortizing computation across common prefixes and\npruning low-value paths early, TreePO essentially reduces the per-update\ncompute burden while preserving or enhancing exploration diversity. Key\ncontributions include: (1) a segment-wise sampling algorithm that alleviates\nthe KV cache burden through contiguous segments and spawns new branches along\nwith an early-stop mechanism; (2) a tree-based segment-level advantage\nestimation that considers both global and local proximal policy optimization.\nand (3) analysis on the effectiveness of probability and quality-driven dynamic\ndivergence and fallback strategy. We empirically validate the performance gain\nof TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours\nfrom 22\\% up to 43\\% of the sampling design for the trained models, meanwhile\nshowing up to 40\\% reduction at trajectory-level and 35\\% at token-level\nsampling compute for the existing models. While offering a free lunch of\ninference efficiency, TreePO reveals a practical path toward scaling RL-based\npost-training with fewer samples and less compute. Home page locates at\nhttps://m-a-p.ai/TreePO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in aligning large language models via reinforcement\nlearning have achieved remarkable gains in solving complex reasoning problems,\nbut at the cost of expensive on-policy rollouts and limited exploration of\ndiverse reasoning paths. In this work, we introduce TreePO, involving a\nself-guided rollout algorithm that views sequence generation as a\ntree-structured searching process. Composed of dynamic tree sampling policy and\nfixed-length segment decoding, TreePO leverages local uncertainty to warrant\nadditional branches. By amortizing computation across common prefixes and\npruning low-value paths early, TreePO essentially reduces the per-update\ncompute burden while preserving or enhancing exploration diversity. Key\ncontributions include: (1) a segment-wise sampling algorithm that alleviates\nthe KV cache burden through contiguous segments and spawns new branches along\nwith an early-stop mechanism; (2) a tree-based segment-level advantage\nestimation that considers both global and local proximal policy optimization.\nand (3) analysis on the effectiveness of probability and quality-driven dynamic\ndivergence and fallback strategy. We empirically validate the performance gain\nof TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours\nfrom 22\\% up to 43\\% of the sampling design for the trained models, meanwhile\nshowing up to 40\\% reduction at trajectory-level and 35\\% at token-level\nsampling compute for the existing models. While offering a free lunch of\ninference efficiency, TreePO reveals a practical path toward scaling RL-based\npost-training with fewer samples and less compute. Home page locates at\nhttps://m-a-p.ai/TreePO."
                },
                "authors": [
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Qingshui Gu"
                    },
                    {
                        "name": "Zhoufutu Wen"
                    },
                    {
                        "name": "Ziniu Li"
                    },
                    {
                        "name": "Tianshun Xing"
                    },
                    {
                        "name": "Shuyue Guo"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Wei Shen"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wenhao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhao Huang"
                },
                "author": "Wenhao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17434v1",
                "updated": "2025-08-24T16:17:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    17,
                    33,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T16:17:33Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    17,
                    33,
                    6,
                    236,
                    0
                ],
                "title": "TinySR: Pruning Diffusion for Real-World Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinySR: Pruning Diffusion for Real-World Image Super-Resolution"
                },
                "summary": "Real-world image super-resolution (Real-ISR) focuses on recovering\nhigh-quality images from low-resolution inputs that suffer from complex\ndegradations like noise, blur, and compression. Recently, diffusion models\n(DMs) have shown great potential in this area by leveraging strong generative\npriors to restore fine details. However, their iterative denoising process\nincurs high computational overhead, posing challenges for real-time\napplications. Although one-step distillation methods, such as OSEDiff and\nTSD-SR, offer faster inference, they remain fundamentally constrained by their\nlarge, over-parameterized model architectures. In this work, we present TinySR,\na compact yet effective diffusion model specifically designed for Real-ISR that\nachieves real-time performance while maintaining perceptual quality. We\nintroduce a Dynamic Inter-block Activation and an Expansion-Corrosion Strategy\nto facilitate more effective decision-making in depth pruning. We achieve VAE\ncompression through channel pruning, attention removal and lightweight SepConv.\nWe eliminate time- and prompt-related modules and perform pre-caching\ntechniques to further speed up the model. TinySR significantly reduces\ncomputational cost and model size, achieving up to 5.68x speedup and 83%\nparameter reduction compared to its teacher TSD-SR, while still providing high\nquality results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world image super-resolution (Real-ISR) focuses on recovering\nhigh-quality images from low-resolution inputs that suffer from complex\ndegradations like noise, blur, and compression. Recently, diffusion models\n(DMs) have shown great potential in this area by leveraging strong generative\npriors to restore fine details. However, their iterative denoising process\nincurs high computational overhead, posing challenges for real-time\napplications. Although one-step distillation methods, such as OSEDiff and\nTSD-SR, offer faster inference, they remain fundamentally constrained by their\nlarge, over-parameterized model architectures. In this work, we present TinySR,\na compact yet effective diffusion model specifically designed for Real-ISR that\nachieves real-time performance while maintaining perceptual quality. We\nintroduce a Dynamic Inter-block Activation and an Expansion-Corrosion Strategy\nto facilitate more effective decision-making in depth pruning. We achieve VAE\ncompression through channel pruning, attention removal and lightweight SepConv.\nWe eliminate time- and prompt-related modules and perform pre-caching\ntechniques to further speed up the model. TinySR significantly reduces\ncomputational cost and model size, achieving up to 5.68x speedup and 83%\nparameter reduction compared to its teacher TSD-SR, while still providing high\nquality results."
                },
                "authors": [
                    {
                        "name": "Linwei Dong"
                    },
                    {
                        "name": "Qingnan Fan"
                    },
                    {
                        "name": "Yuhang Yu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Jinwei Chen"
                    },
                    {
                        "name": "Yawei Luo"
                    },
                    {
                        "name": "Changqing Zou"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zou"
                },
                "author": "Changqing Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17356v1",
                "updated": "2025-08-24T13:30:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    13,
                    30,
                    0,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T13:30:00Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    13,
                    30,
                    0,
                    6,
                    236,
                    0
                ],
                "title": "DiCache: Let Diffusion Model Determine Its Own Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiCache: Let Diffusion Model Determine Its Own Cache"
                },
                "summary": "Recent years have witnessed the rapid development of acceleration techniques\nfor diffusion models, especially caching-based acceleration methods. These\nstudies seek to answer two fundamental questions: \"When to cache\" and \"How to\nuse cache\", typically relying on predefined empirical laws or dataset-level\npriors to determine the timing of caching and utilizing handcrafted rules for\nleveraging multi-step caches. However, given the highly dynamic nature of the\ndiffusion process, they often exhibit limited generalizability and fail on\noutlier samples. In this paper, a strong correlation is revealed between the\nvariation patterns of the shallow-layer feature differences in the diffusion\nmodel and those of final model outputs. Moreover, we have observed that the\nfeatures from different model layers form similar trajectories. Based on these\nobservations, we present DiCache, a novel training-free adaptive caching\nstrategy for accelerating diffusion models at runtime, answering both when and\nhow to cache within a unified framework. Specifically, DiCache is composed of\ntwo principal components: (1) Online Probe Profiling Scheme leverages a\nshallow-layer online probe to obtain a stable prior for the caching error in\nreal time, enabling the model to autonomously determine caching schedules. (2)\nDynamic Cache Trajectory Alignment combines multi-step caches based on\nshallow-layer probe feature trajectory to better approximate the current\nfeature, facilitating higher visual quality. Extensive experiments validate\nDiCache's capability in achieving higher efficiency and improved visual\nfidelity over state-of-the-art methods on various leading diffusion models\nincluding WAN 2.1, HunyuanVideo for video generation, and Flux for image\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed the rapid development of acceleration techniques\nfor diffusion models, especially caching-based acceleration methods. These\nstudies seek to answer two fundamental questions: \"When to cache\" and \"How to\nuse cache\", typically relying on predefined empirical laws or dataset-level\npriors to determine the timing of caching and utilizing handcrafted rules for\nleveraging multi-step caches. However, given the highly dynamic nature of the\ndiffusion process, they often exhibit limited generalizability and fail on\noutlier samples. In this paper, a strong correlation is revealed between the\nvariation patterns of the shallow-layer feature differences in the diffusion\nmodel and those of final model outputs. Moreover, we have observed that the\nfeatures from different model layers form similar trajectories. Based on these\nobservations, we present DiCache, a novel training-free adaptive caching\nstrategy for accelerating diffusion models at runtime, answering both when and\nhow to cache within a unified framework. Specifically, DiCache is composed of\ntwo principal components: (1) Online Probe Profiling Scheme leverages a\nshallow-layer online probe to obtain a stable prior for the caching error in\nreal time, enabling the model to autonomously determine caching schedules. (2)\nDynamic Cache Trajectory Alignment combines multi-step caches based on\nshallow-layer probe feature trajectory to better approximate the current\nfeature, facilitating higher visual quality. Extensive experiments validate\nDiCache's capability in achieving higher efficiency and improved visual\nfidelity over state-of-the-art methods on various leading diffusion models\nincluding WAN 2.1, HunyuanVideo for video generation, and Flux for image\ngeneration."
                },
                "authors": [
                    {
                        "name": "Jiazi Bu"
                    },
                    {
                        "name": "Pengyang Ling"
                    },
                    {
                        "name": "Yujie Zhou"
                    },
                    {
                        "name": "Yibin Wang"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiaqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Wang"
                },
                "author": "Jiaqi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17219v1",
                "updated": "2025-08-24T05:45:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    5,
                    45,
                    16,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T05:45:16Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    5,
                    45,
                    16,
                    6,
                    236,
                    0
                ],
                "title": "TokenLake: A Unified Segment-level Prefix Cache Pool for Fine-grained\n  Elastic Long-Context LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenLake: A Unified Segment-level Prefix Cache Pool for Fine-grained\n  Elastic Long-Context LLM Serving"
                },
                "summary": "Prefix caching is crucial to accelerate multi-turn interactions and requests\nwith shared prefixes. At the cluster level, existing prefix caching systems are\ntightly coupled with request scheduling to optimize cache efficiency and\ncomputation performance together, leading to load imbalance, data redundancy,\nand memory fragmentation of caching systems across instances. To address these\nissues, memory pooling is promising to shield the scheduler from the underlying\ncache management so that it can focus on the computation optimization. However,\nbecause existing prefix caching systems only transfer increasingly longer\nprefix caches between instances, they cannot achieve low-latency memory\npooling.\n  To address these problems, we propose a unified segment-level prefix cache\npool, TokenLake. It uses a declarative cache interface to expose requests'\nquery tensors, prefix caches, and cache-aware operations to TokenLake for\nefficient pooling. Powered by this abstraction, TokenLake can manage prefix\ncache at the segment level with a heavy-hitter-aware load balancing algorithm\nto achieve better cache load balance, deduplication, and defragmentation.\nTokenLake also transparently minimizes the communication volume of query\ntensors and new caches. Based on TokenLake, the scheduler can schedule requests\nelastically by using existing techniques without considering prefix cache\nmanagement. Evaluations on real-world workloads show that TokenLake can improve\nthroughput by up to 2.6$\\times$ and 2.0$\\times$ and boost hit rate by\n2.0$\\times$ and 2.1$\\times$, compared to state-of-the-art cache-aware routing\nand cache-centric PD-disaggregation solutions, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefix caching is crucial to accelerate multi-turn interactions and requests\nwith shared prefixes. At the cluster level, existing prefix caching systems are\ntightly coupled with request scheduling to optimize cache efficiency and\ncomputation performance together, leading to load imbalance, data redundancy,\nand memory fragmentation of caching systems across instances. To address these\nissues, memory pooling is promising to shield the scheduler from the underlying\ncache management so that it can focus on the computation optimization. However,\nbecause existing prefix caching systems only transfer increasingly longer\nprefix caches between instances, they cannot achieve low-latency memory\npooling.\n  To address these problems, we propose a unified segment-level prefix cache\npool, TokenLake. It uses a declarative cache interface to expose requests'\nquery tensors, prefix caches, and cache-aware operations to TokenLake for\nefficient pooling. Powered by this abstraction, TokenLake can manage prefix\ncache at the segment level with a heavy-hitter-aware load balancing algorithm\nto achieve better cache load balance, deduplication, and defragmentation.\nTokenLake also transparently minimizes the communication volume of query\ntensors and new caches. Based on TokenLake, the scheduler can schedule requests\nelastically by using existing techniques without considering prefix cache\nmanagement. Evaluations on real-world workloads show that TokenLake can improve\nthroughput by up to 2.6$\\times$ and 2.0$\\times$ and boost hit rate by\n2.0$\\times$ and 2.1$\\times$, compared to state-of-the-art cache-aware routing\nand cache-centric PD-disaggregation solutions, respectively."
                },
                "authors": [
                    {
                        "name": "Bingyang Wu"
                    },
                    {
                        "name": "Zili Zhang"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Guanzhe Huang"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14148v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14148v2",
                "updated": "2025-08-23T20:28:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    20,
                    28,
                    45,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-19T16:56:51Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    56,
                    51,
                    1,
                    231,
                    0
                ],
                "title": "DPad: Efficient Diffusion Language Models with Suffix Dropout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DPad: Efficient Diffusion Language Models with Suffix Dropout"
                },
                "summary": "Diffusion-based Large Language Models (dLLMs) parallelize text generation by\nframing decoding as a denoising process, but suffer from high computational\noverhead since they predict all future suffix tokens at each step while\nretaining only a small fraction. We propose Diffusion Scratchpad (DPad), a\ntraining-free method that restricts attention to a small set of nearby suffix\ntokens, preserving fidelity while eliminating redundancy. DPad integrates two\nstrategies: (i) a sliding window, which maintains a fixed-length suffix window,\nand (ii) distance-decay dropout, which deterministically removes distant suffix\ntokens before attention computation. This simple design is compatible with\nexisting optimizations such as prefix caching and can be implemented with only\na few lines of code. Comprehensive evaluations across multiple benchmarks on\nLLaDA-1.5 and Dream models demonstrate that DPad delivers up to\n$\\mathbf{61.4\\times}$ speedup over vanilla dLLMs while maintaining comparable\naccuracy, highlighting its potential for efficient and scalable long-sequence\ninference. Our code is available at https://github.com/Crys-Chen/DPad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based Large Language Models (dLLMs) parallelize text generation by\nframing decoding as a denoising process, but suffer from high computational\noverhead since they predict all future suffix tokens at each step while\nretaining only a small fraction. We propose Diffusion Scratchpad (DPad), a\ntraining-free method that restricts attention to a small set of nearby suffix\ntokens, preserving fidelity while eliminating redundancy. DPad integrates two\nstrategies: (i) a sliding window, which maintains a fixed-length suffix window,\nand (ii) distance-decay dropout, which deterministically removes distant suffix\ntokens before attention computation. This simple design is compatible with\nexisting optimizations such as prefix caching and can be implemented with only\na few lines of code. Comprehensive evaluations across multiple benchmarks on\nLLaDA-1.5 and Dream models demonstrate that DPad delivers up to\n$\\mathbf{61.4\\times}$ speedup over vanilla dLLMs while maintaining comparable\naccuracy, highlighting its potential for efficient and scalable long-sequence\ninference. Our code is available at https://github.com/Crys-Chen/DPad."
                },
                "authors": [
                    {
                        "name": "Xinhua Chen"
                    },
                    {
                        "name": "Sitao Huang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Chiyue Wei"
                    },
                    {
                        "name": "Yintao He"
                    },
                    {
                        "name": "Jianyi Zhang"
                    },
                    {
                        "name": "Hai \"Helen\" Li"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14148v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14148v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17137v1",
                "updated": "2025-08-23T20:28:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    20,
                    28,
                    32,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-23T20:28:32Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    20,
                    28,
                    32,
                    5,
                    235,
                    0
                ],
                "title": "MoE-Beyond: Learning-Based Expert Activation Prediction on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Beyond: Learning-Based Expert Activation Prediction on Edge Devices"
                },
                "summary": "The deployment of large-scale Mixture-of-Experts (MoE) models on edge devices\npresents significant challenges due to memory constraints. While MoE\narchitectures enable efficient utilization of computational resources by\nactivating only a subset of experts per inference, they require careful memory\nmanagement to operate efficiently in resource-constrained environments.\nTraditional heuristic-based expert caching strategies such as MoE-Infinity\nstruggle to maintain high cache hit rates as models parameters scale. In this\nwork, we introduce MoE-Beyond, a learning-based expert activation predictor\ntrained to predict expert activations during autoregressive decoding. By\nframing the task as a multi-label sequence prediction problem, we train a\nlightweight transformer model on 66 million expert activation traces extracted\nfrom LDJnr-Puffin dataset [5] using DeepSeek-V2-Chat-Lite MoE. Our predictor\ngeneralizes effectively across unseen prompts from WebGLM-QA dataset [6],\nachieving 97.5% accuracy and an 86.6% F1-score. Simulation results show that\nMoE-Beyond improves GPU cache hit rate from 17% to 72% when only 10% of experts\nfit in GPU cache, outperforming heuristic baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large-scale Mixture-of-Experts (MoE) models on edge devices\npresents significant challenges due to memory constraints. While MoE\narchitectures enable efficient utilization of computational resources by\nactivating only a subset of experts per inference, they require careful memory\nmanagement to operate efficiently in resource-constrained environments.\nTraditional heuristic-based expert caching strategies such as MoE-Infinity\nstruggle to maintain high cache hit rates as models parameters scale. In this\nwork, we introduce MoE-Beyond, a learning-based expert activation predictor\ntrained to predict expert activations during autoregressive decoding. By\nframing the task as a multi-label sequence prediction problem, we train a\nlightweight transformer model on 66 million expert activation traces extracted\nfrom LDJnr-Puffin dataset [5] using DeepSeek-V2-Chat-Lite MoE. Our predictor\ngeneralizes effectively across unseen prompts from WebGLM-QA dataset [6],\nachieving 97.5% accuracy and an 86.6% F1-score. Simulation results show that\nMoE-Beyond improves GPU cache hit rate from 17% to 72% when only 10% of experts\nfit in GPU cache, outperforming heuristic baselines."
                },
                "authors": [
                    {
                        "name": "Nishant Gavhane"
                    },
                    {
                        "name": "Arush Mehrotra"
                    },
                    {
                        "name": "Rohit Chawla"
                    },
                    {
                        "name": "Peter Proenca"
                    }
                ],
                "author_detail": {
                    "name": "Peter Proenca"
                },
                "author": "Peter Proenca",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17125v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17125v1",
                "updated": "2025-08-23T19:58:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    19,
                    58,
                    18,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-23T19:58:18Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    19,
                    58,
                    18,
                    5,
                    235,
                    0
                ],
                "title": "VQL: An End-to-End Context-Aware Vector Quantization Attention for\n  Ultra-Long User Behavior Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQL: An End-to-End Context-Aware Vector Quantization Attention for\n  Ultra-Long User Behavior Modeling"
                },
                "summary": "In large-scale recommender systems, ultra-long user behavior sequences encode\nrich signals of evolving interests. Extending sequence length generally\nimproves accuracy, but directly modeling such sequences in production is\ninfeasible due to latency and memory constraints. Existing solutions fall into\ntwo categories: (1) top-k retrieval, which truncates the sequence and may\ndiscard most attention mass when L >> k; and (2) encoder-based compression,\nwhich preserves coverage but often over-compresses and fails to incorporate key\ncontext such as temporal gaps or target-aware signals. Neither class achieves a\ngood balance of low-loss compression, context awareness, and efficiency.\n  We propose VQL, a context-aware Vector Quantization Attention framework for\nultra-long behavior modeling, with three innovations. (1) Key-only\nquantization: only attention keys are quantized, while values remain intact; we\nprove that softmax normalization yields an error bound independent of sequence\nlength, and a codebook loss directly supervises quantization quality. This also\nenables L-free inference via offline caches. (2) Multi-scale quantization:\nattention heads are partitioned into groups, each with its own small codebook,\nwhich reduces quantization error while keeping cache size fixed. (3) Efficient\ncontext injection: static features (e.g., item category, modality) are directly\nintegrated, and relative position is modeled via a separable temporal kernel.\nAll context is injected without enlarging the codebook, so cached\nrepresentations remain query-independent.\n  Experiments on three large-scale datasets (KuaiRand-1K, KuaiRec, TMALL) show\nthat VQL consistently outperforms strong baselines, achieving higher accuracy\nwhile reducing inference latency, establishing a new state of the art in\nbalancing accuracy and efficiency for ultra-long sequence recommendation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-scale recommender systems, ultra-long user behavior sequences encode\nrich signals of evolving interests. Extending sequence length generally\nimproves accuracy, but directly modeling such sequences in production is\ninfeasible due to latency and memory constraints. Existing solutions fall into\ntwo categories: (1) top-k retrieval, which truncates the sequence and may\ndiscard most attention mass when L >> k; and (2) encoder-based compression,\nwhich preserves coverage but often over-compresses and fails to incorporate key\ncontext such as temporal gaps or target-aware signals. Neither class achieves a\ngood balance of low-loss compression, context awareness, and efficiency.\n  We propose VQL, a context-aware Vector Quantization Attention framework for\nultra-long behavior modeling, with three innovations. (1) Key-only\nquantization: only attention keys are quantized, while values remain intact; we\nprove that softmax normalization yields an error bound independent of sequence\nlength, and a codebook loss directly supervises quantization quality. This also\nenables L-free inference via offline caches. (2) Multi-scale quantization:\nattention heads are partitioned into groups, each with its own small codebook,\nwhich reduces quantization error while keeping cache size fixed. (3) Efficient\ncontext injection: static features (e.g., item category, modality) are directly\nintegrated, and relative position is modeled via a separable temporal kernel.\nAll context is injected without enlarging the codebook, so cached\nrepresentations remain query-independent.\n  Experiments on three large-scale datasets (KuaiRand-1K, KuaiRec, TMALL) show\nthat VQL consistently outperforms strong baselines, achieving higher accuracy\nwhile reducing inference latency, establishing a new state of the art in\nbalancing accuracy and efficiency for ultra-long sequence recommendation."
                },
                "authors": [
                    {
                        "name": "Kaiyuan Li"
                    },
                    {
                        "name": "Yongxiang Tang"
                    },
                    {
                        "name": "Yanhua Cheng"
                    },
                    {
                        "name": "Yong Bai"
                    },
                    {
                        "name": "Yanxiang Zeng"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Xialong Liu"
                    },
                    {
                        "name": "Peng Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Jiang"
                },
                "author": "Peng Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17125v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17032v1",
                "updated": "2025-08-23T14:20:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    14,
                    20,
                    6,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-23T14:20:06Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    14,
                    20,
                    6,
                    5,
                    235,
                    0
                ],
                "title": "Learned Structure in CARTRIDGES: Keys as Shareable Routers in\n  Self-Studied Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned Structure in CARTRIDGES: Keys as Shareable Routers in\n  Self-Studied Representations"
                },
                "summary": "A bottleneck for long-context LLM inference is the linearly growing KV cache.\nRecent work has proposed CARTRIDGES, an approach which leverages offline\ncompute to train a much smaller KV cache than is typically required for a full\ndocument (up to 40x less memory usage at inference time). In this paper, we\npresent the first mechanistic exploration of the learned CARTRIDGE key-value\ncache structure. In particular, we propose that (1) CARTRIDGE keys act as\nstable, shareable retrieval routers for the compressed corpora and (2) most of\nthe learned compression occurs within the CARTRIDGE value vectors. We present\nempirical evidence of our routing theory across tasks, model families, and\nmodel sizes; for example, we can ablate the learned CARTRIDGE key vectors\nbetween tasks with little performance loss. Finally, we propose a slight\nimprovement in initialization called Sampled Chunk Initialization (SCI). We\nsuggest that SCI can lead to faster CARTRIDGE convergence than previously\ndemonstrated in the literature. Our findings lay the groundwork for broader\nempirical study of CARTRIDGE training optimization which may be crucial for\nfurther scaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A bottleneck for long-context LLM inference is the linearly growing KV cache.\nRecent work has proposed CARTRIDGES, an approach which leverages offline\ncompute to train a much smaller KV cache than is typically required for a full\ndocument (up to 40x less memory usage at inference time). In this paper, we\npresent the first mechanistic exploration of the learned CARTRIDGE key-value\ncache structure. In particular, we propose that (1) CARTRIDGE keys act as\nstable, shareable retrieval routers for the compressed corpora and (2) most of\nthe learned compression occurs within the CARTRIDGE value vectors. We present\nempirical evidence of our routing theory across tasks, model families, and\nmodel sizes; for example, we can ablate the learned CARTRIDGE key vectors\nbetween tasks with little performance loss. Finally, we propose a slight\nimprovement in initialization called Sampled Chunk Initialization (SCI). We\nsuggest that SCI can lead to faster CARTRIDGE convergence than previously\ndemonstrated in the literature. Our findings lay the groundwork for broader\nempirical study of CARTRIDGE training optimization which may be crucial for\nfurther scaling."
                },
                "authors": [
                    {
                        "name": "Maurizio Diaz"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Diaz"
                },
                "author": "Maurizio Diaz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16984v1",
                "updated": "2025-08-23T10:35:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    10,
                    35,
                    16,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-23T10:35:16Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    10,
                    35,
                    16,
                    5,
                    235,
                    0
                ],
                "title": "HiCache: Training-free Acceleration of Diffusion Models via Hermite\n  Polynomial-based Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiCache: Training-free Acceleration of Diffusion Models via Hermite\n  Polynomial-based Feature Caching"
                },
                "summary": "Diffusion models have achieved remarkable success in content generation but\nsuffer from prohibitive computational costs due to iterative sampling. While\nrecent feature caching methods tend to accelerate inference through temporal\nextrapolation, these methods still suffer from server quality loss due to the\nfailure in modeling the complex dynamics of feature evolution. To solve this\nproblem, this paper presents HiCache, a training-free acceleration framework\nthat fundamentally improves feature prediction by aligning mathematical tools\nwith empirical properties. Our key insight is that feature derivative\napproximations in Diffusion Transformers exhibit multivariate Gaussian\ncharacteristics, motivating the use of Hermite polynomials-the potentially\ntheoretically optimal basis for Gaussian-correlated processes. Besides, We\nfurther introduce a dual-scaling mechanism that ensures numerical stability\nwhile preserving predictive accuracy. Extensive experiments demonstrate\nHiCache's superiority: achieving 6.24x speedup on FLUX.1-dev while exceeding\nbaseline quality, maintaining strong performance across text-to-image, video\ngeneration, and super-resolution tasks. Core implementation is provided in the\nappendix, with complete code to be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have achieved remarkable success in content generation but\nsuffer from prohibitive computational costs due to iterative sampling. While\nrecent feature caching methods tend to accelerate inference through temporal\nextrapolation, these methods still suffer from server quality loss due to the\nfailure in modeling the complex dynamics of feature evolution. To solve this\nproblem, this paper presents HiCache, a training-free acceleration framework\nthat fundamentally improves feature prediction by aligning mathematical tools\nwith empirical properties. Our key insight is that feature derivative\napproximations in Diffusion Transformers exhibit multivariate Gaussian\ncharacteristics, motivating the use of Hermite polynomials-the potentially\ntheoretically optimal basis for Gaussian-correlated processes. Besides, We\nfurther introduce a dual-scaling mechanism that ensures numerical stability\nwhile preserving predictive accuracy. Extensive experiments demonstrate\nHiCache's superiority: achieving 6.24x speedup on FLUX.1-dev while exceeding\nbaseline quality, maintaining strong performance across text-to-image, video\ngeneration, and super-resolution tasks. Core implementation is provided in the\nappendix, with complete code to be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Liang Feng"
                    },
                    {
                        "name": "Shikang Zheng"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03182v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03182v2",
                "updated": "2025-08-23T08:40:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    8,
                    40,
                    52,
                    5,
                    235,
                    0
                ],
                "published": "2025-03-05T04:54:50Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    4,
                    54,
                    50,
                    2,
                    64,
                    0
                ],
                "title": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism"
                },
                "summary": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation."
                },
                "authors": [
                    {
                        "name": "Xinyuan Lin"
                    },
                    {
                        "name": "Chenlu Li"
                    },
                    {
                        "name": "Zongle Huang"
                    },
                    {
                        "name": "Chunyu Wang"
                    },
                    {
                        "name": "Bo Xiao"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Shishi Duan"
                    },
                    {
                        "name": "Yongpan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yongpan Liu"
                },
                "author": "Yongpan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03182v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03182v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20776v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20776v2",
                "updated": "2025-08-22T08:45:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    45,
                    4,
                    4,
                    234,
                    0
                ],
                "published": "2025-05-27T06:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences"
                },
                "summary": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. First, SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models. To improve draft accuracy\nand speed on long inputs without retraining, we propose Cross-model Retrieval,\na novel KV cache eviction strategy that uses the target model's attention\nscores to dynamically select relevant context for the draft model. Extensive\nevaluations on three long-context understanding datasets show that SpecExtend\naccelerates standard tree-based speculative decoding by up to 2.22x for inputs\nup to 16K tokens, providing an effective solution for speculative decoding of\nlong sequences. Our code is available at https://github.com/jycha98/SpecExtend .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. First, SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models. To improve draft accuracy\nand speed on long inputs without retraining, we propose Cross-model Retrieval,\na novel KV cache eviction strategy that uses the target model's attention\nscores to dynamically select relevant context for the draft model. Extensive\nevaluations on three long-context understanding datasets show that SpecExtend\naccelerates standard tree-based speculative decoding by up to 2.22x for inputs\nup to 16K tokens, providing an effective solution for speculative decoding of\nlong sequences. Our code is available at https://github.com/jycha98/SpecExtend ."
                },
                "authors": [
                    {
                        "name": "Jungyoub Cha"
                    },
                    {
                        "name": "Hyunjong Kim"
                    },
                    {
                        "name": "Sungzoon Cho"
                    }
                ],
                "author_detail": {
                    "name": "Sungzoon Cho"
                },
                "author": "Sungzoon Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20776v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20776v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16211v1",
                "updated": "2025-08-22T08:34:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    34,
                    3,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T08:34:03Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    34,
                    3,
                    4,
                    234,
                    0
                ],
                "title": "Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion\n  Transformers"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated exceptional performance in\nhigh-fidelity image and video generation. To reduce their substantial\ncomputational costs, feature caching techniques have been proposed to\naccelerate inference by reusing hidden representations from previous timesteps.\nHowever, current methods often struggle to maintain generation quality at high\nacceleration ratios, where prediction errors increase sharply due to the\ninherent instability of long-step forecasting. In this work, we adopt an\nordinary differential equation (ODE) perspective on the hidden-feature\nsequence, modeling layer representations along the trajectory as a feature-ODE.\nWe attribute the degradation of existing caching strategies to their inability\nto robustly integrate historical features under large skipping intervals. To\naddress this, we propose FoCa (Forecast-then-Calibrate), which treats feature\ncaching as a feature-ODE solving problem. Extensive experiments on image\nsynthesis, video generation, and super-resolution tasks demonstrate the\neffectiveness of FoCa, especially under aggressive acceleration. Without\nadditional training, FoCa achieves near-lossless speedups of 5.50 times on\nFLUX, 6.45 times on HunyuanVideo, 3.17 times on Inf-DiT, and maintains high\nquality with a 4.53 times speedup on DiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated exceptional performance in\nhigh-fidelity image and video generation. To reduce their substantial\ncomputational costs, feature caching techniques have been proposed to\naccelerate inference by reusing hidden representations from previous timesteps.\nHowever, current methods often struggle to maintain generation quality at high\nacceleration ratios, where prediction errors increase sharply due to the\ninherent instability of long-step forecasting. In this work, we adopt an\nordinary differential equation (ODE) perspective on the hidden-feature\nsequence, modeling layer representations along the trajectory as a feature-ODE.\nWe attribute the degradation of existing caching strategies to their inability\nto robustly integrate historical features under large skipping intervals. To\naddress this, we propose FoCa (Forecast-then-Calibrate), which treats feature\ncaching as a feature-ODE solving problem. Extensive experiments on image\nsynthesis, video generation, and super-resolution tasks demonstrate the\neffectiveness of FoCa, especially under aggressive acceleration. Without\nadditional training, FoCa achieves near-lossless speedups of 5.50 times on\nFLUX, 6.45 times on HunyuanVideo, 3.17 times on Inf-DiT, and maintains high\nquality with a 4.53 times speedup on DiT."
                },
                "authors": [
                    {
                        "name": "Shikang Zheng"
                    },
                    {
                        "name": "Liang Feng"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16184v1",
                "updated": "2025-08-22T07:57:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    57,
                    28,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T07:57:28Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    57,
                    28,
                    4,
                    234,
                    0
                ],
                "title": "Joint Cache Placement and Routing in Satellite-Terrestrial Edge\n  Computing Network: A GNN-Enabled DRL Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Cache Placement and Routing in Satellite-Terrestrial Edge\n  Computing Network: A GNN-Enabled DRL Approach"
                },
                "summary": "In this letter, we investigate the problem of joint content caching and\nrouting in satellite-terrestrial edge computing networks (STECNs) to improve\ncaching service for geographically distributed users. To handle the challenges\narising from dynamic low Earth orbit (LEO) satellite topologies and\nheterogeneous content demands, we propose a learning-based framework that\nintegrates graph neural networks (GNNs) with deep reinforcement learning (DRL).\nThe satellite network is represented as a dynamic graph, where GNNs are\nembedded within the DRL agent to capture spatial and topological dependencies\nand support routing-aware decision-making. The caching strategy is optimized by\nformulating the problem as a Markov decision process (MDP) and applying soft\nactor-critic (SAC) algorithm. Simulation results demonstrate that our approach\nsignificantly improves the delivery success rate and reduces communication\ntraffic cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this letter, we investigate the problem of joint content caching and\nrouting in satellite-terrestrial edge computing networks (STECNs) to improve\ncaching service for geographically distributed users. To handle the challenges\narising from dynamic low Earth orbit (LEO) satellite topologies and\nheterogeneous content demands, we propose a learning-based framework that\nintegrates graph neural networks (GNNs) with deep reinforcement learning (DRL).\nThe satellite network is represented as a dynamic graph, where GNNs are\nembedded within the DRL agent to capture spatial and topological dependencies\nand support routing-aware decision-making. The caching strategy is optimized by\nformulating the problem as a Markov decision process (MDP) and applying soft\nactor-critic (SAC) algorithm. Simulation results demonstrate that our approach\nsignificantly improves the delivery success rate and reduces communication\ntraffic cost."
                },
                "authors": [
                    {
                        "name": "Yuhao Zheng"
                    },
                    {
                        "name": "Ting You"
                    },
                    {
                        "name": "Kejia Peng"
                    },
                    {
                        "name": "Chang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Liu"
                },
                "author": "Chang Liu",
                "arxiv_comment": "5 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16134v1",
                "updated": "2025-08-22T06:55:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    55,
                    45,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T06:55:45Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    55,
                    45,
                    4,
                    234,
                    0
                ],
                "title": "CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing"
                },
                "summary": "Large Language Models (LLMs) confront significant memory challenges due to\nthe escalating KV cache with increasing sequence length. As a crucial\ntechnique, existing cross-layer KV cache sharing methods either necessitate\nmodified model architectures with subsequent pre-training or incur significant\nperformance degradation at high compression rates. To mitigate these\nchallenges, we propose CommonKV, a training-free method for cross-layer KV\ncache compression through adjacent parameters sharing. Inspired by the high\nsimilarity observed in cross-layer hidden states, we utilize Singular Value\nDecomposition (SVD) to achieve weight sharing across adjacent parameters,\nresulting in a more easily mergeable latent KV cache. Furthermore, we also\nintroduce an adaptive budget allocation strategy. It dynamically assigns\ncompression budgets based on cosine similarity, ensuring that dissimilar caches\nare not over-compressed. Experiments across multiple backbone models and\nbenchmarks including LongBench and Ruler demonstrate that the proposed method\nconsistently outperforms existing low-rank and cross-layer approaches at\nvarious compression ratios. Moreover, we find that the benefits of CommonKV are\northogonal to other quantization and eviction methods. By integrating these\napproaches, we can ultimately achieve a 98\\% compression ratio without\nsignificant performance loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) confront significant memory challenges due to\nthe escalating KV cache with increasing sequence length. As a crucial\ntechnique, existing cross-layer KV cache sharing methods either necessitate\nmodified model architectures with subsequent pre-training or incur significant\nperformance degradation at high compression rates. To mitigate these\nchallenges, we propose CommonKV, a training-free method for cross-layer KV\ncache compression through adjacent parameters sharing. Inspired by the high\nsimilarity observed in cross-layer hidden states, we utilize Singular Value\nDecomposition (SVD) to achieve weight sharing across adjacent parameters,\nresulting in a more easily mergeable latent KV cache. Furthermore, we also\nintroduce an adaptive budget allocation strategy. It dynamically assigns\ncompression budgets based on cosine similarity, ensuring that dissimilar caches\nare not over-compressed. Experiments across multiple backbone models and\nbenchmarks including LongBench and Ruler demonstrate that the proposed method\nconsistently outperforms existing low-rank and cross-layer approaches at\nvarious compression ratios. Moreover, we find that the benefits of CommonKV are\northogonal to other quantization and eviction methods. By integrating these\napproaches, we can ultimately achieve a 98\\% compression ratio without\nsignificant performance loss."
                },
                "authors": [
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Haoyu Qiao"
                    },
                    {
                        "name": "Lujun Li"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16121v1",
                "updated": "2025-08-22T06:28:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    28,
                    24,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T06:28:24Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    28,
                    24,
                    4,
                    234,
                    0
                ],
                "title": "Lightweight and Fast Real-time Image Enhancement via Decomposition of\n  the Spatial-aware Lookup Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight and Fast Real-time Image Enhancement via Decomposition of\n  the Spatial-aware Lookup Tables"
                },
                "summary": "The image enhancement methods based on 3D lookup tables (3D LUTs) efficiently\nreduce both model size and runtime by interpolating pre-calculated values at\nthe vertices. However, the 3D LUT methods have a limitation due to their lack\nof spatial information, as they convert color values on a point-by-point basis.\nAlthough spatial-aware 3D LUT methods address this limitation, they introduce\nadditional modules that require a substantial number of parameters, leading to\nincreased runtime as image resolution increases. To address this issue, we\npropose a method for generating image-adaptive LUTs by focusing on the\nredundant parts of the tables. Our efficient framework decomposes a 3D LUT into\na linear sum of low-dimensional LUTs and employs singular value decomposition\n(SVD). Furthermore, we enhance the modules for spatial feature fusion to be\nmore cache-efficient. Extensive experimental results demonstrate that our model\neffectively decreases both the number of parameters and runtime while\nmaintaining spatial awareness and performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The image enhancement methods based on 3D lookup tables (3D LUTs) efficiently\nreduce both model size and runtime by interpolating pre-calculated values at\nthe vertices. However, the 3D LUT methods have a limitation due to their lack\nof spatial information, as they convert color values on a point-by-point basis.\nAlthough spatial-aware 3D LUT methods address this limitation, they introduce\nadditional modules that require a substantial number of parameters, leading to\nincreased runtime as image resolution increases. To address this issue, we\npropose a method for generating image-adaptive LUTs by focusing on the\nredundant parts of the tables. Our efficient framework decomposes a 3D LUT into\na linear sum of low-dimensional LUTs and employs singular value decomposition\n(SVD). Furthermore, we enhance the modules for spatial feature fusion to be\nmore cache-efficient. Extensive experimental results demonstrate that our model\neffectively decreases both the number of parameters and runtime while\nmaintaining spatial awareness and performance."
                },
                "authors": [
                    {
                        "name": "Wontae Kim"
                    },
                    {
                        "name": "Keuntek Lee"
                    },
                    {
                        "name": "Nam Ik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Nam Ik Cho"
                },
                "author": "Nam Ik Cho",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.04442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04442v1",
                "updated": "2025-09-04T17:59:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    59,
                    6,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T17:59:06Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    59,
                    6,
                    3,
                    247,
                    0
                ],
                "title": "Delta Activations: A Representation for Finetuned Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delta Activations: A Representation for Finetuned Large Language Models"
                },
                "summary": "The success of powerful open source Large Language Models (LLMs) has enabled\nthe community to create a vast collection of post-trained models adapted to\nspecific tasks and domains. However, navigating and understanding these models\nremains challenging due to inconsistent metadata and unstructured repositories.\nWe introduce Delta Activations, a method to represent finetuned models as\nvector embeddings by measuring shifts in their internal activations relative to\na base model. This representation allows for effective clustering by domain and\ntask, revealing structure in the model landscape. Delta Activations also\ndemonstrate desirable properties: it is robust across finetuning settings and\nexhibits an additive property when finetuning datasets are mixed. In addition,\nwe show that Delta Activations can embed tasks via few-shot finetuning, and\nfurther explore its use for model selection and merging. We hope Delta\nActivations can facilitate the practice of reusing publicly available models.\nCode is available at https://github.com/OscarXZQ/delta_activations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of powerful open source Large Language Models (LLMs) has enabled\nthe community to create a vast collection of post-trained models adapted to\nspecific tasks and domains. However, navigating and understanding these models\nremains challenging due to inconsistent metadata and unstructured repositories.\nWe introduce Delta Activations, a method to represent finetuned models as\nvector embeddings by measuring shifts in their internal activations relative to\na base model. This representation allows for effective clustering by domain and\ntask, revealing structure in the model landscape. Delta Activations also\ndemonstrate desirable properties: it is robust across finetuning settings and\nexhibits an additive property when finetuning datasets are mixed. In addition,\nwe show that Delta Activations can embed tasks via few-shot finetuning, and\nfurther explore its use for model selection and merging. We hope Delta\nActivations can facilitate the practice of reusing publicly available models.\nCode is available at https://github.com/OscarXZQ/delta_activations."
                },
                "authors": [
                    {
                        "name": "Zhiqiu Xu"
                    },
                    {
                        "name": "Amish Sethi"
                    },
                    {
                        "name": "Mayur Naik"
                    },
                    {
                        "name": "Ser-Nam Lim"
                    }
                ],
                "author_detail": {
                    "name": "Ser-Nam Lim"
                },
                "author": "Ser-Nam Lim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12736v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12736v2",
                "updated": "2025-09-04T17:56:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    56,
                    24,
                    3,
                    247,
                    0
                ],
                "published": "2024-11-19T18:58:03Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    58,
                    3,
                    1,
                    324,
                    0
                ],
                "title": "ACING: Actor-Critic for Instruction Learning in Black-Box LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACING: Actor-Critic for Instruction Learning in Black-Box LLMs"
                },
                "summary": "The effectiveness of Large Language Models (LLMs) in solving tasks depends\nsignificantly on the quality of their instructions, which often require\nsubstantial human effort to craft. This underscores the need for automated\ninstruction optimization. However, optimizing instructions is particularly\nchallenging when working with black-box LLMs, where model parameters and\ngradients are inaccessible. We introduce ACING, an actor-critic reinforcement\nlearning framework that formulates instruction optimization as a stateless,\ncontinuous-action problem, enabling exploration of infinite instruction spaces\nusing only black-box feedback. ACING automatically discovers prompts that\noutperform human-written prompts in 76% of instruction-induction tasks, with\ngains of up to 33 points and a 10-point median improvement over the best\nautomatic baseline in 33 tasks spanning instruction-induction, summarization,\nand chain-of-thought reasoning. Extensive ablations highlight its robustness\nand efficiency. An implementation of ACING is available at\nhttps://github.com/salmakh1/ACING.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effectiveness of Large Language Models (LLMs) in solving tasks depends\nsignificantly on the quality of their instructions, which often require\nsubstantial human effort to craft. This underscores the need for automated\ninstruction optimization. However, optimizing instructions is particularly\nchallenging when working with black-box LLMs, where model parameters and\ngradients are inaccessible. We introduce ACING, an actor-critic reinforcement\nlearning framework that formulates instruction optimization as a stateless,\ncontinuous-action problem, enabling exploration of infinite instruction spaces\nusing only black-box feedback. ACING automatically discovers prompts that\noutperform human-written prompts in 76% of instruction-induction tasks, with\ngains of up to 33 points and a 10-point median improvement over the best\nautomatic baseline in 33 tasks spanning instruction-induction, summarization,\nand chain-of-thought reasoning. Extensive ablations highlight its robustness\nand efficiency. An implementation of ACING is available at\nhttps://github.com/salmakh1/ACING."
                },
                "authors": [
                    {
                        "name": "Salma Kharrat"
                    },
                    {
                        "name": "Fares Fourati"
                    },
                    {
                        "name": "Marco Canini"
                    }
                ],
                "author_detail": {
                    "name": "Marco Canini"
                },
                "author": "Marco Canini",
                "arxiv_comment": "Accepted at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12736v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12736v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04439v1",
                "updated": "2025-09-04T17:54:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    54,
                    19,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T17:54:19Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    54,
                    19,
                    3,
                    247,
                    0
                ],
                "title": "ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory"
                },
                "summary": "While inference-time scaling enables LLMs to carry out increasingly long and\ncapable reasoning traces, the patterns and insights uncovered during these\ntraces are immediately discarded once the context window is reset for a new\nquery. External memory is a natural way to persist these discoveries, and\nrecent work has shown clear benefits for reasoning-intensive tasks. We see an\nopportunity to make such memories more broadly reusable and scalable by moving\nbeyond instance-based memory entries (e.g. exact query/response pairs, or\nsummaries tightly coupled with the original problem context) toward\nconcept-level memory: reusable, modular abstractions distilled from solution\ntraces and stored in natural language. For future queries, relevant concepts\nare selectively retrieved and integrated into the prompt, enabling test-time\ncontinual learning without weight updates. Our design introduces new strategies\nfor abstracting takeaways from rollouts and retrieving entries for new queries,\npromoting reuse and allowing memory to expand with additional experiences. On\nthe challenging ARC-AGI benchmark, our method yields a 7.5% relative gain over\na strong no-memory baseline with performance continuing to scale with inference\ncompute. We find abstract concepts to be the most consistent memory design,\noutscoring the baseline at all tested inference compute scales. Moreover, we\nconfirm that dynamically updating memory during test-time outperforms an\notherwise identical fixed memory setting with additional attempts, supporting\nthe hypothesis that solving more problems and abstracting more patterns to\nmemory enables further solutions in a form of self-improvement. Code available\nat https://github.com/matt-seb-ho/arc_memo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While inference-time scaling enables LLMs to carry out increasingly long and\ncapable reasoning traces, the patterns and insights uncovered during these\ntraces are immediately discarded once the context window is reset for a new\nquery. External memory is a natural way to persist these discoveries, and\nrecent work has shown clear benefits for reasoning-intensive tasks. We see an\nopportunity to make such memories more broadly reusable and scalable by moving\nbeyond instance-based memory entries (e.g. exact query/response pairs, or\nsummaries tightly coupled with the original problem context) toward\nconcept-level memory: reusable, modular abstractions distilled from solution\ntraces and stored in natural language. For future queries, relevant concepts\nare selectively retrieved and integrated into the prompt, enabling test-time\ncontinual learning without weight updates. Our design introduces new strategies\nfor abstracting takeaways from rollouts and retrieving entries for new queries,\npromoting reuse and allowing memory to expand with additional experiences. On\nthe challenging ARC-AGI benchmark, our method yields a 7.5% relative gain over\na strong no-memory baseline with performance continuing to scale with inference\ncompute. We find abstract concepts to be the most consistent memory design,\noutscoring the baseline at all tested inference compute scales. Moreover, we\nconfirm that dynamically updating memory during test-time outperforms an\notherwise identical fixed memory setting with additional attempts, supporting\nthe hypothesis that solving more problems and abstracting more patterns to\nmemory enables further solutions in a form of self-improvement. Code available\nat https://github.com/matt-seb-ho/arc_memo."
                },
                "authors": [
                    {
                        "name": "Matthew Ho"
                    },
                    {
                        "name": "Chen Si"
                    },
                    {
                        "name": "Zhaoxiang Feng"
                    },
                    {
                        "name": "Fangxu Yu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Zhiting Hu"
                    },
                    {
                        "name": "Lianhui Qin"
                    }
                ],
                "author_detail": {
                    "name": "Lianhui Qin"
                },
                "author": "Lianhui Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04437v1",
                "updated": "2025-09-04T17:53:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    53,
                    45,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T17:53:45Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    53,
                    45,
                    3,
                    247,
                    0
                ],
                "title": "From Lines to Shapes: Geometric-Constrained Segmentation of X-Ray\n  Collimators via Hough Transform",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Lines to Shapes: Geometric-Constrained Segmentation of X-Ray\n  Collimators via Hough Transform"
                },
                "summary": "Collimation in X-ray imaging restricts exposure to the region-of-interest\n(ROI) and minimizes the radiation dose applied to the patient. The detection of\ncollimator shadows is an essential image-based preprocessing step in digital\nradiography posing a challenge when edges get obscured by scattered X-ray\nradiation. Regardless, the prior knowledge that collimation forms\npolygonal-shaped shadows is evident. For this reason, we introduce a deep\nlearning-based segmentation that is inherently constrained to its geometry. We\nachieve this by incorporating a differentiable Hough transform-based network to\ndetect the collimation borders and enhance its capability to extract the\ninformation about the ROI center. During inference, we combine the information\nof both tasks to enable the generation of refined, line-constrained\nsegmentation masks. We demonstrate robust reconstruction of collimated regions\nachieving median Hausdorff distances of 4.3-5.0mm on diverse test sets of real\nXray images. While this application involves at most four shadow borders, our\nmethod is not fundamentally limited by a specific number of edges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collimation in X-ray imaging restricts exposure to the region-of-interest\n(ROI) and minimizes the radiation dose applied to the patient. The detection of\ncollimator shadows is an essential image-based preprocessing step in digital\nradiography posing a challenge when edges get obscured by scattered X-ray\nradiation. Regardless, the prior knowledge that collimation forms\npolygonal-shaped shadows is evident. For this reason, we introduce a deep\nlearning-based segmentation that is inherently constrained to its geometry. We\nachieve this by incorporating a differentiable Hough transform-based network to\ndetect the collimation borders and enhance its capability to extract the\ninformation about the ROI center. During inference, we combine the information\nof both tasks to enable the generation of refined, line-constrained\nsegmentation masks. We demonstrate robust reconstruction of collimated regions\nachieving median Hausdorff distances of 4.3-5.0mm on diverse test sets of real\nXray images. While this application involves at most four shadow borders, our\nmethod is not fundamentally limited by a specific number of edges."
                },
                "authors": [
                    {
                        "name": "Benjamin El-Zein"
                    },
                    {
                        "name": "Dominik Eckert"
                    },
                    {
                        "name": "Andreas Fieselmann"
                    },
                    {
                        "name": "Christopher Syben"
                    },
                    {
                        "name": "Ludwig Ritschl"
                    },
                    {
                        "name": "Steffen Kappler"
                    },
                    {
                        "name": "Sebastian Stober"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Stober"
                },
                "author": "Sebastian Stober",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06020v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06020v2",
                "updated": "2025-09-04T17:50:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    50,
                    13,
                    3,
                    247,
                    0
                ],
                "published": "2025-08-08T05:14:47Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    5,
                    14,
                    47,
                    4,
                    220,
                    0
                ],
                "title": "Mergers Fall Short: Non-merger Channels Required for Galactic Heavy\n  Element Production",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mergers Fall Short: Non-merger Channels Required for Galactic Heavy\n  Element Production"
                },
                "summary": "Since the discovery of the binary neutron star merger GW170817 and its\nassociated kilonova, neutron star mergers have been established as a key\nproduction channel for $r$-process elements in the Universe. However, various\nlines of evidence, including the $r$-process abundances inferred from stellar\nspectra of Milky Way disk stars, suggest that additional channels are needed to\nfully account for the $r$-process enrichment in the Milky Way. Neutron\nstar-black hole mergers and fast-merging binary neutron star systems are among\nthe leading alternative candidates. In this paper, we combine\ngravitational-wave observations from LIGO-Virgo-KAGRA with data from short\ngamma-ray bursts, Galactic pulsars, and Galactic [Eu/Fe] versus [Fe/H]\nabundance observations to assess the contribution of these mergers to\n$r$-process enrichment in the Galactic disk. We find that neither neutron\nstar-black hole mergers nor fast-merging binary neutron star populations can\nserve as the dominant additional channel without generating strong tension with\nexisting observations and theoretical expectations. These results constrain the\nviable sources of Galactic $r$-process enrichment and underscore the necessity\nof non-merger production channels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the discovery of the binary neutron star merger GW170817 and its\nassociated kilonova, neutron star mergers have been established as a key\nproduction channel for $r$-process elements in the Universe. However, various\nlines of evidence, including the $r$-process abundances inferred from stellar\nspectra of Milky Way disk stars, suggest that additional channels are needed to\nfully account for the $r$-process enrichment in the Milky Way. Neutron\nstar-black hole mergers and fast-merging binary neutron star systems are among\nthe leading alternative candidates. In this paper, we combine\ngravitational-wave observations from LIGO-Virgo-KAGRA with data from short\ngamma-ray bursts, Galactic pulsars, and Galactic [Eu/Fe] versus [Fe/H]\nabundance observations to assess the contribution of these mergers to\n$r$-process enrichment in the Galactic disk. We find that neither neutron\nstar-black hole mergers nor fast-merging binary neutron star populations can\nserve as the dominant additional channel without generating strong tension with\nexisting observations and theoretical expectations. These results constrain the\nviable sources of Galactic $r$-process enrichment and underscore the necessity\nof non-merger production channels."
                },
                "authors": [
                    {
                        "name": "Muhammed Saleem"
                    },
                    {
                        "name": "Hsin-Yu Chen"
                    },
                    {
                        "name": "Daniel M. Siegel"
                    },
                    {
                        "name": "Philippe Landry"
                    },
                    {
                        "name": "Jocelyn S. Read"
                    },
                    {
                        "name": "Kaile Wang"
                    }
                ],
                "author_detail": {
                    "name": "Kaile Wang"
                },
                "author": "Kaile Wang",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06020v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06020v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14326v2",
                "updated": "2025-09-04T17:49:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    49,
                    22,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-24T08:39:50Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    39,
                    50,
                    4,
                    24,
                    0
                ],
                "title": "Assessing Large Language Models in Comprehending and Verifying\n  Concurrent Programs across Memory Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Large Language Models in Comprehending and Verifying\n  Concurrent Programs across Memory Models"
                },
                "summary": "As concurrent programming becomes increasingly prevalent, effectively\nidentifying and addressing concurrency issues such as data races and deadlocks\nis critical. This study evaluates the performance of several leading large\nlanguage models (LLMs), including GPT-3.5-turbo, GPT-4, GPT-4o, GPT-4o-mini,\nand Mistral-AI's Large2, in understanding and analyzing concurrency issues\nwithin software programs. Given that relaxed memory models, such as Total Store\nOrder (TSO) and Partial Store Order (PSO), are widely implemented and adapted\nin modern systems, supported even by commodity architectures like ARM and x86,\nour evaluation focuses not only on sequentially consistent memory models but\nalso on these relaxed memory models. Specifically, we assess two main aspects:\nthe models' capacity to detect concurrency problems under a sequentially\nconsistent memory model and their ability to verify the correctness conditions\nof concurrent programs across both sequentially consistent and relaxed memory\nmodels. To do this, we leverage SV-COMP's pthread tests and 25 ARM Litmus tests\ndesigned to evaluate Total Store Order (TSO) and Partial Store Order (PSO)\nmemory models. The experimental results reveal that GPT-4, GPT-4o, and\nMistral-AI's Large2 demonstrate a robust understanding of concurrency issues,\neffectively identifying data races and deadlocks when assessed under a\nsequentially consistent memory model. However, despite its superior\nperformance, all selected LLMs face significant challenges verifying program\ncorrectness under relaxed memory models. These LLMs exhibit limitations in\naccurately capturing memory ordering constraints, and their current\ncapabilities fall short in verifying even small programs in these complex\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As concurrent programming becomes increasingly prevalent, effectively\nidentifying and addressing concurrency issues such as data races and deadlocks\nis critical. This study evaluates the performance of several leading large\nlanguage models (LLMs), including GPT-3.5-turbo, GPT-4, GPT-4o, GPT-4o-mini,\nand Mistral-AI's Large2, in understanding and analyzing concurrency issues\nwithin software programs. Given that relaxed memory models, such as Total Store\nOrder (TSO) and Partial Store Order (PSO), are widely implemented and adapted\nin modern systems, supported even by commodity architectures like ARM and x86,\nour evaluation focuses not only on sequentially consistent memory models but\nalso on these relaxed memory models. Specifically, we assess two main aspects:\nthe models' capacity to detect concurrency problems under a sequentially\nconsistent memory model and their ability to verify the correctness conditions\nof concurrent programs across both sequentially consistent and relaxed memory\nmodels. To do this, we leverage SV-COMP's pthread tests and 25 ARM Litmus tests\ndesigned to evaluate Total Store Order (TSO) and Partial Store Order (PSO)\nmemory models. The experimental results reveal that GPT-4, GPT-4o, and\nMistral-AI's Large2 demonstrate a robust understanding of concurrency issues,\neffectively identifying data races and deadlocks when assessed under a\nsequentially consistent memory model. However, despite its superior\nperformance, all selected LLMs face significant challenges verifying program\ncorrectness under relaxed memory models. These LLMs exhibit limitations in\naccurately capturing memory ordering constraints, and their current\ncapabilities fall short in verifying even small programs in these complex\nscenarios."
                },
                "authors": [
                    {
                        "name": "Ridhi Jain"
                    },
                    {
                        "name": "Rahul Purandare"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Purandare"
                },
                "author": "Rahul Purandare",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03312v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03312v2",
                "updated": "2025-09-04T17:49:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    49,
                    20,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-03T13:42:14Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    13,
                    42,
                    14,
                    2,
                    246,
                    0
                ],
                "title": "AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?"
                },
                "summary": "Large Language Model (LLM)-based agentic systems, often comprising multiple\nmodels, complex tool invocations, and orchestration protocols, substantially\noutperform monolithic agents. Yet this very sophistication amplifies their\nfragility, making them more prone to system failure. Pinpointing the specific\nagent or step responsible for an error within long execution traces defines the\ntask of agentic system failure attribution. Current state-of-the-art reasoning\nLLMs, however, remain strikingly inadequate for this challenge, with accuracy\ngenerally below 10%. To address this gap, we propose AgenTracer, the first\nautomated framework for annotating failed multi-agent trajectories via\ncounterfactual replay and programmed fault injection, producing the curated\ndataset TracerTraj. Leveraging this resource, we develop AgenTracer-8B, a\nlightweight failure tracer trained with multi-granular reinforcement learning,\ncapable of efficiently diagnosing errors in verbose multi-agent interactions.\nOn the Who&When benchmark, AgenTracer-8B outperforms giant proprietary LLMs\nlike Gemini-2.5-Pro and Claude-4-Sonnet by up to 18.18%, setting a new standard\nin LLM agentic failure attribution. More importantly, AgenTracer-8B delivers\nactionable feedback to off-the-shelf multi-agent systems like MetaGPT and MaAS\nwith 4.8-14.2% performance gains, empowering self-correcting and self-evolving\nagentic AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agentic systems, often comprising multiple\nmodels, complex tool invocations, and orchestration protocols, substantially\noutperform monolithic agents. Yet this very sophistication amplifies their\nfragility, making them more prone to system failure. Pinpointing the specific\nagent or step responsible for an error within long execution traces defines the\ntask of agentic system failure attribution. Current state-of-the-art reasoning\nLLMs, however, remain strikingly inadequate for this challenge, with accuracy\ngenerally below 10%. To address this gap, we propose AgenTracer, the first\nautomated framework for annotating failed multi-agent trajectories via\ncounterfactual replay and programmed fault injection, producing the curated\ndataset TracerTraj. Leveraging this resource, we develop AgenTracer-8B, a\nlightweight failure tracer trained with multi-granular reinforcement learning,\ncapable of efficiently diagnosing errors in verbose multi-agent interactions.\nOn the Who&When benchmark, AgenTracer-8B outperforms giant proprietary LLMs\nlike Gemini-2.5-Pro and Claude-4-Sonnet by up to 18.18%, setting a new standard\nin LLM agentic failure attribution. More importantly, AgenTracer-8B delivers\nactionable feedback to off-the-shelf multi-agent systems like MetaGPT and MaAS\nwith 4.8-14.2% performance gains, empowering self-correcting and self-evolving\nagentic AI."
                },
                "authors": [
                    {
                        "name": "Guibin Zhang"
                    },
                    {
                        "name": "Junhao Wang"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Shuicheng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Shuicheng Yan"
                },
                "author": "Shuicheng Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03312v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03312v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05122v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05122v2",
                "updated": "2025-09-04T17:41:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    41,
                    13,
                    3,
                    247,
                    0
                ],
                "published": "2025-05-08T10:51:13Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    10,
                    51,
                    13,
                    3,
                    128,
                    0
                ],
                "title": "Text2Cypher: Data Pruning using Hard Example Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text2Cypher: Data Pruning using Hard Example Selection"
                },
                "summary": "Database query languages such as SQL for relational databases and Cypher for\ngraph databases have been widely adopted. Recent advancements in large language\nmodels (LLMs) enable natural language interactions with databases through\nmodels like Text2SQL and Text2Cypher. Fine-tuning these models typically\nrequires large, diverse datasets containing non-trivial examples. However, as\ndataset size increases, the cost of fine-tuning also rises. This makes smaller,\nhigh-quality datasets essential for reducing costs for the same or better\nperformance. In this paper, we propose five hard-example selection techniques\nfor pruning the Text2Cypher dataset, aiming to preserve or improve performance\nwhile reducing resource usage. Our results show that these hard-example\nselection approaches can halve training time and costs with minimal impact on\nperformance, and demonstrates that hard-example selection provides a\ncost-effective solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Database query languages such as SQL for relational databases and Cypher for\ngraph databases have been widely adopted. Recent advancements in large language\nmodels (LLMs) enable natural language interactions with databases through\nmodels like Text2SQL and Text2Cypher. Fine-tuning these models typically\nrequires large, diverse datasets containing non-trivial examples. However, as\ndataset size increases, the cost of fine-tuning also rises. This makes smaller,\nhigh-quality datasets essential for reducing costs for the same or better\nperformance. In this paper, we propose five hard-example selection techniques\nfor pruning the Text2Cypher dataset, aiming to preserve or improve performance\nwhile reducing resource usage. Our results show that these hard-example\nselection approaches can halve training time and costs with minimal impact on\nperformance, and demonstrates that hard-example selection provides a\ncost-effective solution."
                },
                "authors": [
                    {
                        "name": "Makbule Gulcin Ozsoy"
                    }
                ],
                "author_detail": {
                    "name": "Makbule Gulcin Ozsoy"
                },
                "author": "Makbule Gulcin Ozsoy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05122v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05122v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04415v1",
                "updated": "2025-09-04T17:37:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    37,
                    35,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T17:37:35Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    37,
                    35,
                    3,
                    247,
                    0
                ],
                "title": "Interpretable Clustering with Adaptive Heterogeneous Causal Structure\n  Learning in Mixed Observational Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Clustering with Adaptive Heterogeneous Causal Structure\n  Learning in Mixed Observational Data"
                },
                "summary": "Understanding causal heterogeneity is essential for scientific discovery in\ndomains such as biology and medicine. However, existing methods lack causal\nawareness, with insufficient modeling of heterogeneity, confounding, and\nobservational constraints, leading to poor interpretability and difficulty\ndistinguishing true causal heterogeneity from spurious associations. We propose\nan unsupervised framework, HCL (Interpretable Causal Mechanism-Aware Clustering\nwith Adaptive Heterogeneous Causal Structure Learning), that jointly infers\nlatent clusters and their associated causal structures from mixed-type\nobservational data without requiring temporal ordering, environment labels,\ninterventions or other prior knowledge. HCL relaxes the homogeneity and\nsufficiency assumptions by introducing an equivalent representation that\nencodes both structural heterogeneity and confounding. It further develops a\nbi-directional iterative strategy to alternately refine causal clustering and\nstructure learning, along with a self-supervised regularization that balance\ncross-cluster universality and specificity. Together, these components enable\nconvergence toward interpretable, heterogeneous causal patterns. Theoretically,\nwe show identifiability of heterogeneous causal structures under mild\nconditions. Empirically, HCL achieves superior performance in both clustering\nand structure learning tasks, and recovers biologically meaningful mechanisms\nin real-world single-cell perturbation data, demonstrating its utility for\ndiscovering interpretable, mechanism-level causal heterogeneity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding causal heterogeneity is essential for scientific discovery in\ndomains such as biology and medicine. However, existing methods lack causal\nawareness, with insufficient modeling of heterogeneity, confounding, and\nobservational constraints, leading to poor interpretability and difficulty\ndistinguishing true causal heterogeneity from spurious associations. We propose\nan unsupervised framework, HCL (Interpretable Causal Mechanism-Aware Clustering\nwith Adaptive Heterogeneous Causal Structure Learning), that jointly infers\nlatent clusters and their associated causal structures from mixed-type\nobservational data without requiring temporal ordering, environment labels,\ninterventions or other prior knowledge. HCL relaxes the homogeneity and\nsufficiency assumptions by introducing an equivalent representation that\nencodes both structural heterogeneity and confounding. It further develops a\nbi-directional iterative strategy to alternately refine causal clustering and\nstructure learning, along with a self-supervised regularization that balance\ncross-cluster universality and specificity. Together, these components enable\nconvergence toward interpretable, heterogeneous causal patterns. Theoretically,\nwe show identifiability of heterogeneous causal structures under mild\nconditions. Empirically, HCL achieves superior performance in both clustering\nand structure learning tasks, and recovers biologically meaningful mechanisms\nin real-world single-cell perturbation data, demonstrating its utility for\ndiscovering interpretable, mechanism-level causal heterogeneity."
                },
                "authors": [
                    {
                        "name": "Wenrui Li"
                    },
                    {
                        "name": "Qinghao Zhang"
                    },
                    {
                        "name": "Xiaowo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowo Wang"
                },
                "author": "Xiaowo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04873v2",
                "updated": "2025-09-04T17:30:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    30,
                    11,
                    3,
                    247,
                    0
                ],
                "published": "2025-04-07T09:28:50Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    9,
                    28,
                    50,
                    0,
                    97,
                    0
                ],
                "title": "Closed-Loop Neural Operator-Based Observer of Traffic Density",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Closed-Loop Neural Operator-Based Observer of Traffic Density"
                },
                "summary": "We consider the problem of traffic density estimation with sparse\nmeasurements from stationary roadside sensors. Our approach uses Fourier neural\noperators to learn macroscopic traffic flow dynamics from high-fidelity data.\nDuring inference, the operator functions as an open-loop predictor of traffic\nevolution. To close the loop, we couple the open-loop operator with a\ncorrection operator that combines the predicted density with sparse\nmeasurements from the sensors. Simulations with the SUMO software indicate\nthat, compared to open-loop observers, the proposed closed-loop observer\nexhibits classical closed-loop properties such as robustness to noise and\nultimate boundedness of the error. This shows the advantages of combining\nlearned physics with real-time corrections, and opens avenues for accurate,\nefficient, and interpretable data-driven observers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of traffic density estimation with sparse\nmeasurements from stationary roadside sensors. Our approach uses Fourier neural\noperators to learn macroscopic traffic flow dynamics from high-fidelity data.\nDuring inference, the operator functions as an open-loop predictor of traffic\nevolution. To close the loop, we couple the open-loop operator with a\ncorrection operator that combines the predicted density with sparse\nmeasurements from the sensors. Simulations with the SUMO software indicate\nthat, compared to open-loop observers, the proposed closed-loop observer\nexhibits classical closed-loop properties such as robustness to noise and\nultimate boundedness of the error. This shows the advantages of combining\nlearned physics with real-time corrections, and opens avenues for accurate,\nefficient, and interpretable data-driven observers."
                },
                "authors": [
                    {
                        "name": "Alice Harting"
                    },
                    {
                        "name": "Karl Henrik Johansson"
                    },
                    {
                        "name": "Matthieu Barreau"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Barreau"
                },
                "author": "Matthieu Barreau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04406v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04406v1",
                "updated": "2025-09-04T17:24:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    24,
                    31,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T17:24:31Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    24,
                    31,
                    3,
                    247,
                    0
                ],
                "title": "Few-step Flow for 3D Generation via Marginal-Data Transport Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-step Flow for 3D Generation via Marginal-Data Transport Distillation"
                },
                "summary": "Flow-based 3D generation models typically require dozens of sampling steps\nduring inference. Though few-step distillation methods, particularly\nConsistency Models (CMs), have achieved substantial advancements in\naccelerating 2D diffusion models, they remain under-explored for more complex\n3D generation tasks. In this study, we propose a novel framework, MDT-dist, for\nfew-step 3D flow distillation. Our approach is built upon a primary objective:\ndistilling the pretrained model to learn the Marginal-Data Transport. Directly\nlearning this objective needs to integrate the velocity fields, while this\nintegral is intractable to be implemented. Therefore, we propose two\noptimizable objectives, Velocity Matching (VM) and Velocity Distillation (VD),\nto equivalently convert the optimization target from the transport level to the\nvelocity and the distribution level respectively. Velocity Matching (VM) learns\nto stably match the velocity fields between the student and the teacher, but\ninevitably provides biased gradient estimates. Velocity Distillation (VD)\nfurther enhances the optimization process by leveraging the learned velocity\nfields to perform probability density distillation. When evaluated on the\npioneer 3D generation framework TRELLIS, our method reduces sampling steps of\neach flow transformer from 25 to 1 or 2, achieving 0.68s (1 step x 2) and 0.94s\n(2 steps x 2) latency with 9.0x and 6.5x speedup on A800, while preserving high\nvisual and geometric fidelity. Extensive experiments demonstrate that our\nmethod significantly outperforms existing CM distillation methods, and enables\nTRELLIS to achieve superior performance in few-step 3D generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow-based 3D generation models typically require dozens of sampling steps\nduring inference. Though few-step distillation methods, particularly\nConsistency Models (CMs), have achieved substantial advancements in\naccelerating 2D diffusion models, they remain under-explored for more complex\n3D generation tasks. In this study, we propose a novel framework, MDT-dist, for\nfew-step 3D flow distillation. Our approach is built upon a primary objective:\ndistilling the pretrained model to learn the Marginal-Data Transport. Directly\nlearning this objective needs to integrate the velocity fields, while this\nintegral is intractable to be implemented. Therefore, we propose two\noptimizable objectives, Velocity Matching (VM) and Velocity Distillation (VD),\nto equivalently convert the optimization target from the transport level to the\nvelocity and the distribution level respectively. Velocity Matching (VM) learns\nto stably match the velocity fields between the student and the teacher, but\ninevitably provides biased gradient estimates. Velocity Distillation (VD)\nfurther enhances the optimization process by leveraging the learned velocity\nfields to perform probability density distillation. When evaluated on the\npioneer 3D generation framework TRELLIS, our method reduces sampling steps of\neach flow transformer from 25 to 1 or 2, achieving 0.68s (1 step x 2) and 0.94s\n(2 steps x 2) latency with 9.0x and 6.5x speedup on A800, while preserving high\nvisual and geometric fidelity. Extensive experiments demonstrate that our\nmethod significantly outperforms existing CM distillation methods, and enables\nTRELLIS to achieve superior performance in few-step 3D generation."
                },
                "authors": [
                    {
                        "name": "Zanwei Zhou"
                    },
                    {
                        "name": "Taoran Yi"
                    },
                    {
                        "name": "Jiemin Fang"
                    },
                    {
                        "name": "Chen Yang"
                    },
                    {
                        "name": "Lingxi Xie"
                    },
                    {
                        "name": "Xinggang Wang"
                    },
                    {
                        "name": "Wei Shen"
                    },
                    {
                        "name": "Qi Tian"
                    }
                ],
                "author_detail": {
                    "name": "Qi Tian"
                },
                "author": "Qi Tian",
                "arxiv_comment": "Project page: https://github.com/Zanue/MDT-dist",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04406v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04406v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01185v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01185v2",
                "updated": "2025-09-04T17:22:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    22,
                    16,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-01T07:08:45Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    7,
                    8,
                    45,
                    0,
                    244,
                    0
                ],
                "title": "Modular Techniques for Synthetic Long-Context Data Generation in\n  Language Model Training and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modular Techniques for Synthetic Long-Context Data Generation in\n  Language Model Training and Evaluation"
                },
                "summary": "The ability of large language models (LLMs) to process and reason over long\ntextual inputs is critical for a wide range of real-world applications.\nHowever, progress in this area is significantly constrained by the absence of\nhigh-quality, diverse, and verifiable long-context datasets suitable for both\ntraining and evaluation. This work introduces a modular, extensible framework\nfor synthetic long-context data generation via prompt-based interaction with\nLLMs. The framework supports multiple training and alignment objectives,\nincluding Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO),\nand Group Relative Policy Optimization (GRPO). It encompasses four core\ngeneration paradigms: multi-turn conversational dialogues, document-grounded\ninput-output pairs, verifiable instruction-response tasks, and long-context\nreasoning examples. Through templated prompting, a model-agnostic architecture,\nand metadata-enriched outputs, the proposed approach facilitates scalable,\ncontrollable, and purpose-aligned dataset creation for advancing long-context\ncapabilities in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability of large language models (LLMs) to process and reason over long\ntextual inputs is critical for a wide range of real-world applications.\nHowever, progress in this area is significantly constrained by the absence of\nhigh-quality, diverse, and verifiable long-context datasets suitable for both\ntraining and evaluation. This work introduces a modular, extensible framework\nfor synthetic long-context data generation via prompt-based interaction with\nLLMs. The framework supports multiple training and alignment objectives,\nincluding Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO),\nand Group Relative Policy Optimization (GRPO). It encompasses four core\ngeneration paradigms: multi-turn conversational dialogues, document-grounded\ninput-output pairs, verifiable instruction-response tasks, and long-context\nreasoning examples. Through templated prompting, a model-agnostic architecture,\nand metadata-enriched outputs, the proposed approach facilitates scalable,\ncontrollable, and purpose-aligned dataset creation for advancing long-context\ncapabilities in LLMs."
                },
                "authors": [
                    {
                        "name": "Seganrasan Subramanian"
                    },
                    {
                        "name": "Abhigya Verma"
                    }
                ],
                "author_detail": {
                    "name": "Abhigya Verma"
                },
                "author": "Abhigya Verma",
                "arxiv_comment": "26 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01185v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01185v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04405v1",
                "updated": "2025-09-04T17:18:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    18,
                    4,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T17:18:04Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    18,
                    4,
                    3,
                    247,
                    0
                ],
                "title": "Generation of Lognormal Synthetic Lyman-$$ Forest Spectra for\n  $P_{1D}$ Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generation of Lognormal Synthetic Lyman-$$ Forest Spectra for\n  $P_{1D}$ Analysis"
                },
                "summary": "The one-dimensional flux power spectrum (P1D) of the Lyman-$\\alpha$ forest\nprobes small-scale structure in the intergalactic medium (IGM) and is therefore\nsensitive to a variety of cosmological and astrophysical parameters. These\ninclude the amplitude and shape of the matter power spectrum, the thermal\nhistory of the IGM, the sum of neutrino masses, and potential small-scale\nfluctuations due to the nature of dark matter. However, P1D is also highly\nsensitive to observational and instrumental systematics, making accurate\nsynthetic spectra essential for validating analyses and quantifying these\neffects, especially in high-volume surveys like the Dark Energy Spectroscopic\nInstrument (DESI). We present an efficient lognormal mock framework for\ngenerating one-dimensional Lyman-$\\alpha$ forest spectra tailored for P1D\nanalysis. Our method captures the redshift evolution of the mean transmitted\nflux and the scale-dependent shape and amplitude of the one-dimensional flux\npower spectrum by tuning Gaussian field correlations and transformation\nparameters. Across the DESI Early Data Release (EDR) redshift range ($2.0 \\leq\nz \\leq 3.8$), and a wide range of scales ($10^{-4}$ s km$^{-1} \\leq k \\leq 1.0$\ns km$^{-1}$), our mocks recover the mean flux evolution with redshift to\nsub-percent accuracy, and the P1D at the percent level. Additionally, we\ndiscuss potential extensions of this framework, such as the incorporation of\nastrophysical contaminants, continuum uncertainties, and instrumental effects.\nSuch improvements would expand its utility in ongoing and upcoming surveys and\nenable a broader range of validation efforts and systematics studies for P1D\ninference and precision cosmology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The one-dimensional flux power spectrum (P1D) of the Lyman-$\\alpha$ forest\nprobes small-scale structure in the intergalactic medium (IGM) and is therefore\nsensitive to a variety of cosmological and astrophysical parameters. These\ninclude the amplitude and shape of the matter power spectrum, the thermal\nhistory of the IGM, the sum of neutrino masses, and potential small-scale\nfluctuations due to the nature of dark matter. However, P1D is also highly\nsensitive to observational and instrumental systematics, making accurate\nsynthetic spectra essential for validating analyses and quantifying these\neffects, especially in high-volume surveys like the Dark Energy Spectroscopic\nInstrument (DESI). We present an efficient lognormal mock framework for\ngenerating one-dimensional Lyman-$\\alpha$ forest spectra tailored for P1D\nanalysis. Our method captures the redshift evolution of the mean transmitted\nflux and the scale-dependent shape and amplitude of the one-dimensional flux\npower spectrum by tuning Gaussian field correlations and transformation\nparameters. Across the DESI Early Data Release (EDR) redshift range ($2.0 \\leq\nz \\leq 3.8$), and a wide range of scales ($10^{-4}$ s km$^{-1} \\leq k \\leq 1.0$\ns km$^{-1}$), our mocks recover the mean flux evolution with redshift to\nsub-percent accuracy, and the P1D at the percent level. Additionally, we\ndiscuss potential extensions of this framework, such as the incorporation of\nastrophysical contaminants, continuum uncertainties, and instrumental effects.\nSuch improvements would expand its utility in ongoing and upcoming surveys and\nenable a broader range of validation efforts and systematics studies for P1D\ninference and precision cosmology."
                },
                "authors": [
                    {
                        "name": "Meagan Herbold"
                    },
                    {
                        "name": "Naim Gksel Karaayl"
                    },
                    {
                        "name": "Paul Martini"
                    }
                ],
                "author_detail": {
                    "name": "Paul Martini"
                },
                "author": "Paul Martini",
                "arxiv_comment": "21 pages, 5 figures, 1 table, prepared for submission to JCAP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04404v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04404v1",
                "updated": "2025-09-04T17:16:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    16,
                    26,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T17:16:26Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    16,
                    26,
                    3,
                    247,
                    0
                ],
                "title": "No Thoughts Just AI: Biased LLM Recommendations Limit Human Agency in\n  Resume Screening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Thoughts Just AI: Biased LLM Recommendations Limit Human Agency in\n  Resume Screening"
                },
                "summary": "In this study, we conduct a resume-screening experiment (N=528) where people\ncollaborate with simulated AI models exhibiting race-based preferences (bias)\nto evaluate candidates for 16 high and low status occupations. Simulated AI\nbias approximates factual and counterfactual estimates of racial bias in\nreal-world AI systems. We investigate people's preferences for White, Black,\nHispanic, and Asian candidates (represented through names and affinity groups\non quality-controlled resumes) across 1,526 scenarios and measure their\nunconscious associations between race and status using implicit association\ntests (IATs), which predict discriminatory hiring decisions but have not been\ninvestigated in human-AI collaboration. When making decisions without AI or\nwith AI that exhibits no race-based preferences, people select all candidates\nat equal rates. However, when interacting with AI favoring a particular group,\npeople also favor those candidates up to 90% of the time, indicating a\nsignificant behavioral shift. The likelihood of selecting candidates whose\nidentities do not align with common race-status stereotypes can increase by 13%\nif people complete an IAT before conducting resume screening. Finally, even if\npeople think AI recommendations are low quality or not important, their\ndecisions are still vulnerable to AI bias under certain circumstances. This\nwork has implications for people's autonomy in AI-HITL scenarios, AI and work,\ndesign and evaluation of AI hiring systems, and strategies for mitigating bias\nin collaborative decision-making tasks. In particular, organizational and\nregulatory policy should acknowledge the complex nature of AI-HITL decision\nmaking when implementing these systems, educating people who use them, and\ndetermining which are subject to oversight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we conduct a resume-screening experiment (N=528) where people\ncollaborate with simulated AI models exhibiting race-based preferences (bias)\nto evaluate candidates for 16 high and low status occupations. Simulated AI\nbias approximates factual and counterfactual estimates of racial bias in\nreal-world AI systems. We investigate people's preferences for White, Black,\nHispanic, and Asian candidates (represented through names and affinity groups\non quality-controlled resumes) across 1,526 scenarios and measure their\nunconscious associations between race and status using implicit association\ntests (IATs), which predict discriminatory hiring decisions but have not been\ninvestigated in human-AI collaboration. When making decisions without AI or\nwith AI that exhibits no race-based preferences, people select all candidates\nat equal rates. However, when interacting with AI favoring a particular group,\npeople also favor those candidates up to 90% of the time, indicating a\nsignificant behavioral shift. The likelihood of selecting candidates whose\nidentities do not align with common race-status stereotypes can increase by 13%\nif people complete an IAT before conducting resume screening. Finally, even if\npeople think AI recommendations are low quality or not important, their\ndecisions are still vulnerable to AI bias under certain circumstances. This\nwork has implications for people's autonomy in AI-HITL scenarios, AI and work,\ndesign and evaluation of AI hiring systems, and strategies for mitigating bias\nin collaborative decision-making tasks. In particular, organizational and\nregulatory policy should acknowledge the complex nature of AI-HITL decision\nmaking when implementing these systems, educating people who use them, and\ndetermining which are subject to oversight."
                },
                "authors": [
                    {
                        "name": "Kyra Wilson"
                    },
                    {
                        "name": "Mattea Sim"
                    },
                    {
                        "name": "Anna-Maria Gueorguieva"
                    },
                    {
                        "name": "Aylin Caliskan"
                    }
                ],
                "author_detail": {
                    "name": "Aylin Caliskan"
                },
                "author": "Aylin Caliskan",
                "arxiv_comment": "Published in Proceedings of the 2025 AAAI/ACM Conference on AI,\n  Ethics, and Society; code available at\n  https://github.com/kyrawilson/No-Thoughts-Just-AI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04404v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04404v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.4.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04399v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04399v1",
                "updated": "2025-09-04T17:10:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    10,
                    15,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T17:10:15Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    10,
                    15,
                    3,
                    247,
                    0
                ],
                "title": "Leveraging Equivariances and Symmetries in the Control Barrier Function\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Equivariances and Symmetries in the Control Barrier Function\n  Synthesis"
                },
                "summary": "The synthesis of Control Barrier Functions (CBFs) often involves demanding\ncomputations or a meticulous construction. However, structural properties of\nthe system dynamics and constraints have the potential to mitigate these\nchallenges. In this paper, we explore how equivariances in the dynamics,\nloosely speaking a form of symmetry, can be leveraged in the CBF synthesis.\nAlthough CBFs are generally not inherently symmetric, we show how equivariances\nin the dynamics and symmetries in the constraints induce symmetries in CBFs\nderived through reachability analysis. This insight allows us to infer their\nCBF values across the entire domain from their values on a subset, leading to\nsignificant computational savings. Interestingly, equivariances can be even\nleveraged to the CBF synthesis for non-symmetric constraints. Specifically, we\nshow how a partially known CBF can be leveraged together with equivariances to\nconstruct a CBF for various new constraints. Throughout the paper, we provide\nexamples illustrating the theoretical findings. Furthermore, a numerical study\ninvestigates the computational gains from invoking equivariances into the CBF\nsynthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The synthesis of Control Barrier Functions (CBFs) often involves demanding\ncomputations or a meticulous construction. However, structural properties of\nthe system dynamics and constraints have the potential to mitigate these\nchallenges. In this paper, we explore how equivariances in the dynamics,\nloosely speaking a form of symmetry, can be leveraged in the CBF synthesis.\nAlthough CBFs are generally not inherently symmetric, we show how equivariances\nin the dynamics and symmetries in the constraints induce symmetries in CBFs\nderived through reachability analysis. This insight allows us to infer their\nCBF values across the entire domain from their values on a subset, leading to\nsignificant computational savings. Interestingly, equivariances can be even\nleveraged to the CBF synthesis for non-symmetric constraints. Specifically, we\nshow how a partially known CBF can be leveraged together with equivariances to\nconstruct a CBF for various new constraints. Throughout the paper, we provide\nexamples illustrating the theoretical findings. Furthermore, a numerical study\ninvestigates the computational gains from invoking equivariances into the CBF\nsynthesis."
                },
                "authors": [
                    {
                        "name": "Adrian Wiltz"
                    },
                    {
                        "name": "Dimos V. Dimarogonas"
                    }
                ],
                "author_detail": {
                    "name": "Dimos V. Dimarogonas"
                },
                "author": "Dimos V. Dimarogonas",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04399v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04398v1",
                "updated": "2025-09-04T17:10:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    10,
                    1,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T17:10:01Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    10,
                    1,
                    3,
                    247,
                    0
                ],
                "title": "IPA: An Information-Preserving Input Projection Framework for Efficient\n  Foundation Model Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IPA: An Information-Preserving Input Projection Framework for Efficient\n  Foundation Model Adaptation"
                },
                "summary": "Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, reduce\nadaptation cost by injecting low-rank updates into pretrained weights. However,\nLoRA's down-projection is randomly initialized and data-agnostic, discarding\npotentially useful information. Prior analyses show that this projection\nchanges little during training, while the up-projection carries most of the\nadaptation, making the random input compression a performance bottleneck. We\npropose IPA, a feature-aware projection framework that explicitly preserves\ninformation in the reduced hidden space. In the linear case, we instantiate IPA\nwith algorithms approximating top principal components, enabling efficient\nprojector pretraining with negligible inference overhead. Across language and\nvision benchmarks, IPA consistently improves over LoRA and DoRA, achieving on\naverage 1.5 points higher accuracy on commonsense reasoning and 2.3 points on\nVTAB-1k, while matching full LoRA performance with roughly half the trainable\nparameters when the projection is frozen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, reduce\nadaptation cost by injecting low-rank updates into pretrained weights. However,\nLoRA's down-projection is randomly initialized and data-agnostic, discarding\npotentially useful information. Prior analyses show that this projection\nchanges little during training, while the up-projection carries most of the\nadaptation, making the random input compression a performance bottleneck. We\npropose IPA, a feature-aware projection framework that explicitly preserves\ninformation in the reduced hidden space. In the linear case, we instantiate IPA\nwith algorithms approximating top principal components, enabling efficient\nprojector pretraining with negligible inference overhead. Across language and\nvision benchmarks, IPA consistently improves over LoRA and DoRA, achieving on\naverage 1.5 points higher accuracy on commonsense reasoning and 2.3 points on\nVTAB-1k, while matching full LoRA performance with roughly half the trainable\nparameters when the projection is frozen."
                },
                "authors": [
                    {
                        "name": "Yuan Yin"
                    },
                    {
                        "name": "Shashanka Venkataramanan"
                    },
                    {
                        "name": "Tuan-Hung Vu"
                    },
                    {
                        "name": "Andrei Bursuc"
                    },
                    {
                        "name": "Matthieu Cord"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Cord"
                },
                "author": "Matthieu Cord",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04392v1",
                "updated": "2025-09-04T17:03:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    3,
                    58,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T17:03:58Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    3,
                    58,
                    3,
                    247,
                    0
                ],
                "title": "Denoising GER: A Noise-Robust Generative Error Correction with LLM for\n  Speech Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Denoising GER: A Noise-Robust Generative Error Correction with LLM for\n  Speech Recognition"
                },
                "summary": "In recent years, large language models (LLM) have made significant progress\nin the task of generation error correction (GER) for automatic speech\nrecognition (ASR) post-processing. However, in complex noisy environments, they\nstill face challenges such as poor adaptability and low information\nutilization, resulting in limited effectiveness of GER. To address these\nissues, this paper proposes a noise-robust multi-modal GER framework (Denoising\nGER). The framework enhances the model's adaptability to different noisy\nscenarios through a noise-adaptive acoustic encoder and optimizes the\nintegration of multi-modal information via a heterogeneous feature compensation\ndynamic fusion (HFCDF) mechanism, improving the LLM's utilization of\nmulti-modal information. Additionally, reinforcement learning (RL) training\nstrategies are introduced to enhance the model's predictive capabilities.\nExperimental results demonstrate that Denoising GER significantly improves\naccuracy and robustness in noisy environments and exhibits good generalization\nabilities in unseen noise scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLM) have made significant progress\nin the task of generation error correction (GER) for automatic speech\nrecognition (ASR) post-processing. However, in complex noisy environments, they\nstill face challenges such as poor adaptability and low information\nutilization, resulting in limited effectiveness of GER. To address these\nissues, this paper proposes a noise-robust multi-modal GER framework (Denoising\nGER). The framework enhances the model's adaptability to different noisy\nscenarios through a noise-adaptive acoustic encoder and optimizes the\nintegration of multi-modal information via a heterogeneous feature compensation\ndynamic fusion (HFCDF) mechanism, improving the LLM's utilization of\nmulti-modal information. Additionally, reinforcement learning (RL) training\nstrategies are introduced to enhance the model's predictive capabilities.\nExperimental results demonstrate that Denoising GER significantly improves\naccuracy and robustness in noisy environments and exhibits good generalization\nabilities in unseen noise scenarios."
                },
                "authors": [
                    {
                        "name": "Yanyan Liu"
                    },
                    {
                        "name": "Minqiang Xu"
                    },
                    {
                        "name": "Yihao Chen"
                    },
                    {
                        "name": "Liang He"
                    },
                    {
                        "name": "Lei Fang"
                    },
                    {
                        "name": "Sian Fang"
                    },
                    {
                        "name": "Lin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Lin Liu"
                },
                "author": "Lin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04377v1",
                "updated": "2025-09-04T16:40:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    40,
                    1,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T16:40:01Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    40,
                    1,
                    3,
                    247,
                    0
                ],
                "title": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient\n  Large Language Model Inference"
                },
                "summary": "KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks."
                },
                "authors": [
                    {
                        "name": "Krishna Teja Chitty-Venkata"
                    },
                    {
                        "name": "Jie Ye"
                    },
                    {
                        "name": "Xian-He Sun"
                    },
                    {
                        "name": "Anthony Kougkas"
                    },
                    {
                        "name": "Murali Emani"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    },
                    {
                        "name": "Bogdan Nicolae"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Nicolae"
                },
                "author": "Bogdan Nicolae",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.20017v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.20017v2",
                "updated": "2025-09-04T16:38:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    38,
                    32,
                    3,
                    247,
                    0
                ],
                "published": "2023-10-30T21:12:28Z",
                "published_parsed": [
                    2023,
                    10,
                    30,
                    21,
                    12,
                    28,
                    0,
                    303,
                    0
                ],
                "title": "Spatially resolving superconductivity in type-II superconductors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatially resolving superconductivity in type-II superconductors"
                },
                "summary": "Superconductivity is identified by the emergence of a macroscopic\nzero-resistance state, typically inferred from a vanishing four-probe voltage\nat finite current. That inference assumes spatially uniform conduction-e.g., at\nleast one continuous superconducting path between the current leads and voltage\nelectrodes that sample a finite potential gradient-and can fail if the drive\ncurrent bypasses the electrodes or if narrow filaments short the current\ncontacts. Here we introduce a methodology to test these assumptions in\nsuperconductors, by using spatially resolved measurements of local variations\nin dc using cryogenic conductive atomic-force microscopy (cAFM). Using\nFe(Se,Te) as a model system, we find that despite bulk measurements consistent\nwith a homogeneous superconducting state, the material exhibits a heterogeneous\nconducting landscape: micrometre-scale superconducting regions coexist with\nrelatively insulating areas. We further show that cAFM resolves conductance\nfluctuations at 20 K (> TC) that vary between repeated scans, consistent with\nexpectations for short-lived, pre-formed Cooper pairs in the BCS-BEC crossover\nregime. These results establish cAFM as a practical tool to validate\nassumptions underlying four-probe transport and underscore the need for direct\nspatial probes in materials whose macroscopic response can conceal nanoscale\ninhomogeneity. Accurate identification of macroscopic properties is critical\nfor materials classes like superconductors that are defined by their\nmacroscopic properties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superconductivity is identified by the emergence of a macroscopic\nzero-resistance state, typically inferred from a vanishing four-probe voltage\nat finite current. That inference assumes spatially uniform conduction-e.g., at\nleast one continuous superconducting path between the current leads and voltage\nelectrodes that sample a finite potential gradient-and can fail if the drive\ncurrent bypasses the electrodes or if narrow filaments short the current\ncontacts. Here we introduce a methodology to test these assumptions in\nsuperconductors, by using spatially resolved measurements of local variations\nin dc using cryogenic conductive atomic-force microscopy (cAFM). Using\nFe(Se,Te) as a model system, we find that despite bulk measurements consistent\nwith a homogeneous superconducting state, the material exhibits a heterogeneous\nconducting landscape: micrometre-scale superconducting regions coexist with\nrelatively insulating areas. We further show that cAFM resolves conductance\nfluctuations at 20 K (> TC) that vary between repeated scans, consistent with\nexpectations for short-lived, pre-formed Cooper pairs in the BCS-BEC crossover\nregime. These results establish cAFM as a practical tool to validate\nassumptions underlying four-probe transport and underscore the need for direct\nspatial probes in materials whose macroscopic response can conceal nanoscale\ninhomogeneity. Accurate identification of macroscopic properties is critical\nfor materials classes like superconductors that are defined by their\nmacroscopic properties."
                },
                "authors": [
                    {
                        "name": "Donald M. Evans"
                    },
                    {
                        "name": "Michele Conroy"
                    },
                    {
                        "name": "Lukas Puntigam"
                    },
                    {
                        "name": "Dorina Croitori"
                    },
                    {
                        "name": "Lilian Prodan"
                    },
                    {
                        "name": "Marin Alexe"
                    },
                    {
                        "name": "James O. Douglas"
                    },
                    {
                        "name": "Baptiste Gault"
                    },
                    {
                        "name": "Vladimir Tsurkan"
                    }
                ],
                "author_detail": {
                    "name": "Vladimir Tsurkan"
                },
                "author": "Vladimir Tsurkan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.20017v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.20017v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04373v1",
                "updated": "2025-09-04T16:32:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    32,
                    18,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T16:32:18Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    32,
                    18,
                    3,
                    247,
                    0
                ],
                "title": "Measuring Bias or Measuring the Task: Understanding the Brittle Nature\n  of LLM Gender Biases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Bias or Measuring the Task: Understanding the Brittle Nature\n  of LLM Gender Biases"
                },
                "summary": "As LLMs are increasingly applied in socially impactful settings, concerns\nabout gender bias have prompted growing efforts both to measure and mitigate\nsuch bias. These efforts often rely on evaluation tasks that differ from\nnatural language distributions, as they typically involve carefully constructed\ntask prompts that overtly or covertly signal the presence of gender\nbias-related content. In this paper, we examine how signaling the evaluative\npurpose of a task impacts measured gender bias in LLMs. Concretely, we test\nmodels under prompt conditions that (1) make the testing context salient, and\n(2) make gender-focused content salient. We then assess prompt sensitivity\nacross four task formats with both token-probability and discrete-choice\nmetrics. We find that even minor prompt changes can substantially alter bias\noutcomes, sometimes reversing their direction entirely. Discrete-choice metrics\nfurther tend to amplify bias relative to probabilistic measures. These findings\ndo not only highlight the brittleness of LLM gender bias evaluations but open a\nnew puzzle for the NLP benchmarking and development community: To what extent\ncan well-controlled testing designs trigger LLM ``testing mode'' performance,\nand what does this mean for the ecological validity of future benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs are increasingly applied in socially impactful settings, concerns\nabout gender bias have prompted growing efforts both to measure and mitigate\nsuch bias. These efforts often rely on evaluation tasks that differ from\nnatural language distributions, as they typically involve carefully constructed\ntask prompts that overtly or covertly signal the presence of gender\nbias-related content. In this paper, we examine how signaling the evaluative\npurpose of a task impacts measured gender bias in LLMs. Concretely, we test\nmodels under prompt conditions that (1) make the testing context salient, and\n(2) make gender-focused content salient. We then assess prompt sensitivity\nacross four task formats with both token-probability and discrete-choice\nmetrics. We find that even minor prompt changes can substantially alter bias\noutcomes, sometimes reversing their direction entirely. Discrete-choice metrics\nfurther tend to amplify bias relative to probabilistic measures. These findings\ndo not only highlight the brittleness of LLM gender bias evaluations but open a\nnew puzzle for the NLP benchmarking and development community: To what extent\ncan well-controlled testing designs trigger LLM ``testing mode'' performance,\nand what does this mean for the ecological validity of future benchmarks."
                },
                "authors": [
                    {
                        "name": "Bufan Gao"
                    },
                    {
                        "name": "Elisa Kreiss"
                    }
                ],
                "author_detail": {
                    "name": "Elisa Kreiss"
                },
                "author": "Elisa Kreiss",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07900v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07900v2",
                "updated": "2025-09-04T16:23:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    23,
                    2,
                    3,
                    247,
                    0
                ],
                "published": "2025-06-09T16:16:50Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    16,
                    16,
                    50,
                    0,
                    160,
                    0
                ],
                "title": "MiniCPM4: Ultra-Efficient LLMs on End Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniCPM4: Ultra-Efficient LLMs on End Devices"
                },
                "summary": "This paper introduces MiniCPM4, a highly efficient large language model (LLM)\ndesigned explicitly for end-side devices. We achieve this efficiency through\nsystematic innovation in four key dimensions: model architecture, training\ndata, training algorithms, and inference systems. Specifically, in terms of\nmodel architecture, we propose InfLLM v2, a trainable sparse attention\nmechanism that accelerates both prefilling and decoding phases for long-context\nprocessing. Regarding training data, we propose UltraClean, an efficient and\naccurate pre-training data filtering and generation strategy, and UltraChat v2,\na comprehensive supervised fine-tuning dataset. These datasets enable\nsatisfactory model performance to be achieved using just 8 trillion training\ntokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient\npre-training strategy search, and improve existing post-training methods by\nintroducing chunk-wise rollout for load-balanced reinforcement learning and\ndata-efficient tenary LLM, BitCPM. Regarding inference systems, we propose\nCPM.cu that integrates sparse attention, model quantization, and speculative\nsampling to achieve efficient prefilling and decoding. To meet diverse\non-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B\nparameters, respectively. Furthermore, we construct a hybrid reasoning model,\nMiniCPM4.1, which can be used in both deep reasoning mode and non-reasoning\nmode. Evaluation results demonstrate that MiniCPM4 and MiniCPM4.1 outperform\nsimilar-sized open-source models across benchmarks, with the 8B variants\nshowing significant speed improvements on long sequence understanding and\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces MiniCPM4, a highly efficient large language model (LLM)\ndesigned explicitly for end-side devices. We achieve this efficiency through\nsystematic innovation in four key dimensions: model architecture, training\ndata, training algorithms, and inference systems. Specifically, in terms of\nmodel architecture, we propose InfLLM v2, a trainable sparse attention\nmechanism that accelerates both prefilling and decoding phases for long-context\nprocessing. Regarding training data, we propose UltraClean, an efficient and\naccurate pre-training data filtering and generation strategy, and UltraChat v2,\na comprehensive supervised fine-tuning dataset. These datasets enable\nsatisfactory model performance to be achieved using just 8 trillion training\ntokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient\npre-training strategy search, and improve existing post-training methods by\nintroducing chunk-wise rollout for load-balanced reinforcement learning and\ndata-efficient tenary LLM, BitCPM. Regarding inference systems, we propose\nCPM.cu that integrates sparse attention, model quantization, and speculative\nsampling to achieve efficient prefilling and decoding. To meet diverse\non-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B\nparameters, respectively. Furthermore, we construct a hybrid reasoning model,\nMiniCPM4.1, which can be used in both deep reasoning mode and non-reasoning\nmode. Evaluation results demonstrate that MiniCPM4 and MiniCPM4.1 outperform\nsimilar-sized open-source models across benchmarks, with the 8B variants\nshowing significant speed improvements on long sequence understanding and\ngeneration."
                },
                "authors": [
                    {
                        "name": "MiniCPM Team"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Yuxuan Li"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Yuzhuo Bai"
                    },
                    {
                        "name": "Jie Cai"
                    },
                    {
                        "name": "Haotian Chen"
                    },
                    {
                        "name": "Wentong Chen"
                    },
                    {
                        "name": "Xin Cong"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Shengda Fan"
                    },
                    {
                        "name": "Yewei Fang"
                    },
                    {
                        "name": "Zixuan Fu"
                    },
                    {
                        "name": "Wenyu Guan"
                    },
                    {
                        "name": "Yitong Guan"
                    },
                    {
                        "name": "Junshao Guo"
                    },
                    {
                        "name": "Yufeng Han"
                    },
                    {
                        "name": "Bingxiang He"
                    },
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Baoxi Ji"
                    },
                    {
                        "name": "Cunliang Kong"
                    },
                    {
                        "name": "Qiuzuo Li"
                    },
                    {
                        "name": "Siyuan Li"
                    },
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Yishan Li"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Dan Liu"
                    },
                    {
                        "name": "Biyuan Lin"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Xiang Long"
                    },
                    {
                        "name": "Quanyu Lu"
                    },
                    {
                        "name": "Yaxi Lu"
                    },
                    {
                        "name": "Peiyan Luo"
                    },
                    {
                        "name": "Hongya Lyu"
                    },
                    {
                        "name": "Litu Ou"
                    },
                    {
                        "name": "Yinxu Pan"
                    },
                    {
                        "name": "Lushi Pu"
                    },
                    {
                        "name": "Zekai Qu"
                    },
                    {
                        "name": "Qundong Shi"
                    },
                    {
                        "name": "Zijun Song"
                    },
                    {
                        "name": "Jiayuan Su"
                    },
                    {
                        "name": "Zhou Su"
                    },
                    {
                        "name": "Ao Sun"
                    },
                    {
                        "name": "Xianghui Sun"
                    },
                    {
                        "name": "Peijun Tang"
                    },
                    {
                        "name": "Fangzheng Wang"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Yudong Wang"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Yesai Wu"
                    },
                    {
                        "name": "Zhenyu Xiao"
                    },
                    {
                        "name": "Jie Xie"
                    },
                    {
                        "name": "Zihao Xie"
                    },
                    {
                        "name": "Xiaoyue Xu"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Jiarui Yuan"
                    },
                    {
                        "name": "Jinqian Zhang"
                    },
                    {
                        "name": "Kaihuo Zhang"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Linyue Zhang"
                    },
                    {
                        "name": "Xueren Zhang"
                    },
                    {
                        "name": "Yudi Zhang"
                    },
                    {
                        "name": "Hengyu Zhao"
                    },
                    {
                        "name": "Weilin Zhao"
                    },
                    {
                        "name": "Weilun Zhao"
                    },
                    {
                        "name": "Yuanqian Zhao"
                    },
                    {
                        "name": "Zhi Zheng"
                    },
                    {
                        "name": "Chuyue Zhou"
                    },
                    {
                        "name": "Ge Zhou"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Yanghao Zhou"
                    },
                    {
                        "name": "Zihan Zhou"
                    },
                    {
                        "name": "Zixuan Zhou"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Guoyang Zeng"
                    },
                    {
                        "name": "Chao Jia"
                    },
                    {
                        "name": "Dahai Li"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "MiniCPM4 Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07900v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07900v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01747v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01747v3",
                "updated": "2025-09-04T16:22:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    22,
                    32,
                    3,
                    247,
                    0
                ],
                "published": "2024-11-04T02:08:59Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    8,
                    59,
                    0,
                    309,
                    0
                ],
                "title": "DynaSaur: Large Language Agents Beyond Predefined Actions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynaSaur: Large Language Agents Beyond Predefined Actions"
                },
                "summary": "Existing LLM agent systems typically select actions from a fixed and\npredefined set at every step. While this approach is effective in closed,\nnarrowly scoped environments, it presents two major challenges for real-world,\nopen-ended scenarios: (1) it significantly restricts the planning and acting\ncapabilities of LLM agents, and (2) it requires substantial human effort to\nenumerate and implement all possible actions, which is impractical in complex\nenvironments with a vast number of potential actions. To address these\nlimitations, we propose an LLM agent framework that can dynamically create and\ncompose actions as needed. In this framework, the agent interacts with its\nenvironment by generating and executing programs written in a general-purpose\nprogramming language. Moreover, generated actions are accumulated over time for\nfuture reuse. Our extensive experiments across multiple benchmarks show that\nthis framework significantly improves flexibility and outperforms prior methods\nthat rely on a fixed action set. Notably, it enables LLM agents to adapt and\nrecover in scenarios where predefined actions are insufficient or fail due to\nunforeseen edge cases. Our code can be found in\nhttps://github.com/adobe-research/dynasaur.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM agent systems typically select actions from a fixed and\npredefined set at every step. While this approach is effective in closed,\nnarrowly scoped environments, it presents two major challenges for real-world,\nopen-ended scenarios: (1) it significantly restricts the planning and acting\ncapabilities of LLM agents, and (2) it requires substantial human effort to\nenumerate and implement all possible actions, which is impractical in complex\nenvironments with a vast number of potential actions. To address these\nlimitations, we propose an LLM agent framework that can dynamically create and\ncompose actions as needed. In this framework, the agent interacts with its\nenvironment by generating and executing programs written in a general-purpose\nprogramming language. Moreover, generated actions are accumulated over time for\nfuture reuse. Our extensive experiments across multiple benchmarks show that\nthis framework significantly improves flexibility and outperforms prior methods\nthat rely on a fixed action set. Notably, it enables LLM agents to adapt and\nrecover in scenarios where predefined actions are insufficient or fail due to\nunforeseen edge cases. Our code can be found in\nhttps://github.com/adobe-research/dynasaur."
                },
                "authors": [
                    {
                        "name": "Dang Nguyen"
                    },
                    {
                        "name": "Viet Dac Lai"
                    },
                    {
                        "name": "Seunghyun Yoon"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Handong Zhao"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Puneet Mathur"
                    },
                    {
                        "name": "Nedim Lipka"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Tianyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhou"
                },
                "author": "Tianyi Zhou",
                "arxiv_comment": "Published as a conference paper at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01747v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01747v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04356v1",
                "updated": "2025-09-04T16:18:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    18,
                    4,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T16:18:04Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    18,
                    4,
                    3,
                    247,
                    0
                ],
                "title": "SRWToolkit: An Open Source Wizard of Oz Toolkit to Create Social Robotic\n  Avatars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SRWToolkit: An Open Source Wizard of Oz Toolkit to Create Social Robotic\n  Avatars"
                },
                "summary": "We present SRWToolkit, an open-source Wizard of Oz toolkit designed to\nfacilitate the rapid prototyping of social robotic avatars powered by local\nlarge language models (LLMs). Our web-based toolkit enables multimodal\ninteraction through text input, button-activated speech, and wake-word command.\nThe toolkit offers real-time configuration of avatar appearance, behavior,\nlanguage, and voice via an intuitive control panel. In contrast to prior works\nthat rely on cloud-based LLM services, SRWToolkit emphasizes modularity and\nensures on-device functionality through local LLM inference. In our small-scale\nuser study ($n=11$), participants created and interacted with diverse robotic\nroles (hospital receptionist, mathematics teacher, and driving assistant),\nwhich demonstrated positive outcomes in the toolkit's usability, trust, and\nuser experience. The toolkit enables rapid and efficient development of robot\ncharacters customized to researchers' needs, supporting scalable research in\nhuman-robot interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present SRWToolkit, an open-source Wizard of Oz toolkit designed to\nfacilitate the rapid prototyping of social robotic avatars powered by local\nlarge language models (LLMs). Our web-based toolkit enables multimodal\ninteraction through text input, button-activated speech, and wake-word command.\nThe toolkit offers real-time configuration of avatar appearance, behavior,\nlanguage, and voice via an intuitive control panel. In contrast to prior works\nthat rely on cloud-based LLM services, SRWToolkit emphasizes modularity and\nensures on-device functionality through local LLM inference. In our small-scale\nuser study ($n=11$), participants created and interacted with diverse robotic\nroles (hospital receptionist, mathematics teacher, and driving assistant),\nwhich demonstrated positive outcomes in the toolkit's usability, trust, and\nuser experience. The toolkit enables rapid and efficient development of robot\ncharacters customized to researchers' needs, supporting scalable research in\nhuman-robot interaction."
                },
                "authors": [
                    {
                        "name": "Atikkhan Faridkhan Nilgar"
                    },
                    {
                        "name": "Kristof Van Laerhoven"
                    },
                    {
                        "name": "Ayub Kinoti"
                    }
                ],
                "author_detail": {
                    "name": "Ayub Kinoti"
                },
                "author": "Ayub Kinoti",
                "arxiv_journal_ref": "2025 International Conference on Social Robotics (ICSR)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04915v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04915v3",
                "updated": "2025-09-04T16:15:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    15,
                    3,
                    3,
                    247,
                    0
                ],
                "published": "2024-02-07T14:41:17Z",
                "published_parsed": [
                    2024,
                    2,
                    7,
                    14,
                    41,
                    17,
                    2,
                    38,
                    0
                ],
                "title": "Moco: A Learnable Meta Optimizer for Combinatorial Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moco: A Learnable Meta Optimizer for Combinatorial Optimization"
                },
                "summary": "Relevant combinatorial optimization problems (COPs) are often NP-hard. While\nthey have been tackled mainly via handcrafted heuristics in the past, advances\nin neural networks have motivated the development of general methods to learn\nheuristics from data. Many approaches utilize a neural network to directly\nconstruct a solution, but are limited in further improving based on already\nconstructed solutions at inference time. Our approach, Moco, defines a\nlightweight solution construction procedure, guided by a single continuous\nvector $\\theta$ (called heatmap) and learns a neural network to update $\\theta$\nfor a single instance of a COP at inference time. The update is based on\nvarious features of the current search state. The training procedure is budget\naware, targeting the overall best solution found during the entire search. Moco\nis a fully learnable meta optimizer not utilizing problem specific heuristics\nor requiring optimal solutions for training. We test Moco on the Traveling\nSalesman Problem (TSP) and Maximum Independent Set (MIS) and show that it\nsignificantly improves over other heatmap based methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relevant combinatorial optimization problems (COPs) are often NP-hard. While\nthey have been tackled mainly via handcrafted heuristics in the past, advances\nin neural networks have motivated the development of general methods to learn\nheuristics from data. Many approaches utilize a neural network to directly\nconstruct a solution, but are limited in further improving based on already\nconstructed solutions at inference time. Our approach, Moco, defines a\nlightweight solution construction procedure, guided by a single continuous\nvector $\\theta$ (called heatmap) and learns a neural network to update $\\theta$\nfor a single instance of a COP at inference time. The update is based on\nvarious features of the current search state. The training procedure is budget\naware, targeting the overall best solution found during the entire search. Moco\nis a fully learnable meta optimizer not utilizing problem specific heuristics\nor requiring optimal solutions for training. We test Moco on the Traveling\nSalesman Problem (TSP) and Maximum Independent Set (MIS) and show that it\nsignificantly improves over other heatmap based methods."
                },
                "authors": [
                    {
                        "name": "Tim Dernedde"
                    },
                    {
                        "name": "Daniela Thyssens"
                    },
                    {
                        "name": "Sren Dittrich"
                    },
                    {
                        "name": "Maximilian Stubbemann"
                    },
                    {
                        "name": "Lars Schmidt-Thieme"
                    }
                ],
                "author_detail": {
                    "name": "Lars Schmidt-Thieme"
                },
                "author": "Lars Schmidt-Thieme",
                "arxiv_doi": "10.1007/978-981-96-8180-8_19",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-981-96-8180-8_19",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.04915v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04915v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "20 pages, 2 figures. A prior version was published in Advances in\n  Knowledge Discovery and Data Mining. PAKDD 2025. Lecture Notes in Computer\n  Science, vol 15872. Springer, Singapore",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04348v1",
                "updated": "2025-09-04T16:05:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    5,
                    54,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T16:05:54Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    5,
                    54,
                    3,
                    247,
                    0
                ],
                "title": "GWTC-4.0: Constraints on the Cosmic Expansion Rate and Modified\n  Gravitational-wave Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GWTC-4.0: Constraints on the Cosmic Expansion Rate and Modified\n  Gravitational-wave Propagation"
                },
                "summary": "We analyze data from 142 of the 218 gravitational-wave (GW) sources in the\nfourth LIGO-Virgo-KAGRA Collaboration (LVK) Gravitational-Wave Transient\nCatalog (GWTC-4.0) to estimate the Hubble constant $H_0$ jointly with the\npopulation properties of merging compact binaries. We measure the luminosity\ndistance and redshifted masses of GW sources directly; in contrast, we infer GW\nsource redshifts statistically through i) location of features in the compact\nobject mass spectrum and merger rate evolution, and ii) identifying potential\nhost galaxies in the GW localization volume. Probing the relationship between\nsource luminosity distances and redshifts obtained in this way yields\nconstraints on cosmological parameters. We also constrain parameterized\ndeviations from general relativity which affect GW propagation, specifically\nthose modifying the dependence of a GW signal on the source luminosity\ndistance. Assuming our fiducial model for the source-frame mass distribution\nand using GW candidates detected up to the end of the fourth observing run\n(O4a), together with the GLADE+ all-sky galaxy catalog, we estimate $H_0 =\n76.6^{+13.0}_{-9.5} (76.6^{+25.2}_{-14.0})$ km s$^{-1}$ Mpc$^{-1}$. This value\nis reported as a median with 68.3% (90%) symmetric credible interval, and\nincludes combination with the $H_0$ measurement from GW170817 and its\nelectromagnetic counterpart. Using a parametrization of modified GW propagation\nin terms of the magnitude parameter $\\Xi_0$, we estimate $\\Xi_0 =\n1.2^{+0.8}_{-0.4} (1.2^{+2.4}_{-0.5})$, where $\\Xi_0 = 1$ recovers the behavior\nof general relativity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyze data from 142 of the 218 gravitational-wave (GW) sources in the\nfourth LIGO-Virgo-KAGRA Collaboration (LVK) Gravitational-Wave Transient\nCatalog (GWTC-4.0) to estimate the Hubble constant $H_0$ jointly with the\npopulation properties of merging compact binaries. We measure the luminosity\ndistance and redshifted masses of GW sources directly; in contrast, we infer GW\nsource redshifts statistically through i) location of features in the compact\nobject mass spectrum and merger rate evolution, and ii) identifying potential\nhost galaxies in the GW localization volume. Probing the relationship between\nsource luminosity distances and redshifts obtained in this way yields\nconstraints on cosmological parameters. We also constrain parameterized\ndeviations from general relativity which affect GW propagation, specifically\nthose modifying the dependence of a GW signal on the source luminosity\ndistance. Assuming our fiducial model for the source-frame mass distribution\nand using GW candidates detected up to the end of the fourth observing run\n(O4a), together with the GLADE+ all-sky galaxy catalog, we estimate $H_0 =\n76.6^{+13.0}_{-9.5} (76.6^{+25.2}_{-14.0})$ km s$^{-1}$ Mpc$^{-1}$. This value\nis reported as a median with 68.3% (90%) symmetric credible interval, and\nincludes combination with the $H_0$ measurement from GW170817 and its\nelectromagnetic counterpart. Using a parametrization of modified GW propagation\nin terms of the magnitude parameter $\\Xi_0$, we estimate $\\Xi_0 =\n1.2^{+0.8}_{-0.4} (1.2^{+2.4}_{-0.5})$, where $\\Xi_0 = 1$ recovers the behavior\nof general relativity."
                },
                "authors": [
                    {
                        "name": "The LIGO Scientific Collaboration"
                    },
                    {
                        "name": "the Virgo Collaboration"
                    },
                    {
                        "name": "the KAGRA Collaboration"
                    }
                ],
                "author_detail": {
                    "name": "the KAGRA Collaboration"
                },
                "author": "the KAGRA Collaboration",
                "arxiv_comment": "As part of the Astrophysical Journal Letters Focus Issue on the\n  Gravitational Wave Transient Catalog",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04343v1",
                "updated": "2025-09-04T16:03:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    3,
                    3,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T16:03:03Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    3,
                    3,
                    3,
                    247,
                    0
                ],
                "title": "Psychologically Enhanced AI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Psychologically Enhanced AI Agents"
                },
                "summary": "We introduce MBTI-in-Thoughts, a framework for enhancing the effectiveness of\nLarge Language Model (LLM) agents through psychologically grounded personality\nconditioning. Drawing on the Myers-Briggs Type Indicator (MBTI), our method\nprimes agents with distinct personality archetypes via prompt engineering,\nenabling control over behavior along two foundational axes of human psychology,\ncognition and affect. We show that such personality priming yields consistent,\ninterpretable behavioral biases across diverse tasks: emotionally expressive\nagents excel in narrative generation, while analytically primed agents adopt\nmore stable strategies in game-theoretic settings. Our framework supports\nexperimenting with structured multi-agent communication protocols and reveals\nthat self-reflection prior to interaction improves cooperation and reasoning\nquality. To ensure trait persistence, we integrate the official 16Personalities\ntest for automated verification. While our focus is on MBTI, we show that our\napproach generalizes seamlessly to other psychological frameworks such as Big\nFive, HEXACO, or Enneagram. By bridging psychological theory and LLM behavior\ndesign, we establish a foundation for psychologically enhanced AI agents\nwithout any fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MBTI-in-Thoughts, a framework for enhancing the effectiveness of\nLarge Language Model (LLM) agents through psychologically grounded personality\nconditioning. Drawing on the Myers-Briggs Type Indicator (MBTI), our method\nprimes agents with distinct personality archetypes via prompt engineering,\nenabling control over behavior along two foundational axes of human psychology,\ncognition and affect. We show that such personality priming yields consistent,\ninterpretable behavioral biases across diverse tasks: emotionally expressive\nagents excel in narrative generation, while analytically primed agents adopt\nmore stable strategies in game-theoretic settings. Our framework supports\nexperimenting with structured multi-agent communication protocols and reveals\nthat self-reflection prior to interaction improves cooperation and reasoning\nquality. To ensure trait persistence, we integrate the official 16Personalities\ntest for automated verification. While our focus is on MBTI, we show that our\napproach generalizes seamlessly to other psychological frameworks such as Big\nFive, HEXACO, or Enneagram. By bridging psychological theory and LLM behavior\ndesign, we establish a foundation for psychologically enhanced AI agents\nwithout any fine-tuning."
                },
                "authors": [
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Shriram Chandran"
                    },
                    {
                        "name": "Robert Gerstenberger"
                    },
                    {
                        "name": "Mathis Lindner"
                    },
                    {
                        "name": "Marcin Chrapek"
                    },
                    {
                        "name": "Sebastian Hermann Martschat"
                    },
                    {
                        "name": "Taraneh Ghandi"
                    },
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Hubert Niewiadomski"
                    },
                    {
                        "name": "Piotr Nyczyk"
                    },
                    {
                        "name": "Jrgen Mller"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04340v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04340v1",
                "updated": "2025-09-04T15:59:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    59,
                    45,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T15:59:45Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    59,
                    45,
                    3,
                    247,
                    0
                ],
                "title": "Write on Paper, Wrong in Practice: Why LLMs Still Struggle with Writing\n  Clinical Notes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Write on Paper, Wrong in Practice: Why LLMs Still Struggle with Writing\n  Clinical Notes"
                },
                "summary": "Large Language Models (LLMs) are often proposed as tools to streamline\nclinical documentation, a task viewed as both high-volume and low-risk.\nHowever, even seemingly straightforward applications of LLMs raise complex\nsociotechnical considerations to translate into practice. This case study,\nconducted at KidsAbility, a pediatric rehabilitation facility in Ontario,\nCanada examined the use of LLMs to support occupational therapists in reducing\ndocumentation burden.We conducted a qualitative study involving 20 clinicians\nwho participated in pilot programs using two AI technologies: a general-purpose\nproprietary LLM and a bespoke model fine-tuned on proprietary historical\ndocumentation.\n  Our findings reveal that documentation challenges are sociotechnical in\nnature, shaped by clinical workflows, organizational policies, and system\nconstraints. Four key themes emerged: (1) the heterogeneity of workflows, (2)\nthe documentation burden is systemic and not directly linked to the creation of\nany single type of documentation, (3) the need for flexible tools and clinician\nautonomy, and (4) effective implementation requires mutual learning between\nclinicians and AI systems.\n  While LLMs show promise in easing documentation tasks, their success will\ndepend on flexible, adaptive integration that supports clinician autonomy.\nBeyond technical performance, sustained adoption will require training programs\nand implementation strategies that reflect the complexity of clinical\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are often proposed as tools to streamline\nclinical documentation, a task viewed as both high-volume and low-risk.\nHowever, even seemingly straightforward applications of LLMs raise complex\nsociotechnical considerations to translate into practice. This case study,\nconducted at KidsAbility, a pediatric rehabilitation facility in Ontario,\nCanada examined the use of LLMs to support occupational therapists in reducing\ndocumentation burden.We conducted a qualitative study involving 20 clinicians\nwho participated in pilot programs using two AI technologies: a general-purpose\nproprietary LLM and a bespoke model fine-tuned on proprietary historical\ndocumentation.\n  Our findings reveal that documentation challenges are sociotechnical in\nnature, shaped by clinical workflows, organizational policies, and system\nconstraints. Four key themes emerged: (1) the heterogeneity of workflows, (2)\nthe documentation burden is systemic and not directly linked to the creation of\nany single type of documentation, (3) the need for flexible tools and clinician\nautonomy, and (4) effective implementation requires mutual learning between\nclinicians and AI systems.\n  While LLMs show promise in easing documentation tasks, their success will\ndepend on flexible, adaptive integration that supports clinician autonomy.\nBeyond technical performance, sustained adoption will require training programs\nand implementation strategies that reflect the complexity of clinical\nenvironments."
                },
                "authors": [
                    {
                        "name": "Kristina L. Kupferschmidt"
                    },
                    {
                        "name": "Kieran O'Doherty"
                    },
                    {
                        "name": "Joshua A. Skorburg"
                    }
                ],
                "author_detail": {
                    "name": "Joshua A. Skorburg"
                },
                "author": "Joshua A. Skorburg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04340v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04340v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14723v2",
                "updated": "2025-09-04T15:58:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    58,
                    17,
                    3,
                    247,
                    0
                ],
                "published": "2025-08-20T14:05:18Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    5,
                    18,
                    2,
                    232,
                    0
                ],
                "title": "Transplant Then Regenerate: A New Paradigm for Text Data Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transplant Then Regenerate: A New Paradigm for Text Data Augmentation"
                },
                "summary": "Data augmentation is a critical technique in deep learning. Traditional\nmethods like Back-translation typically focus on lexical-level rephrasing,\nwhich primarily produces variations with the same semantics. While large\nlanguage models (LLMs) have enhanced text augmentation by their \"knowledge\nemergence\" capability, controlling the style and structure of these outputs\nremains challenging and requires meticulous prompt engineering. In this paper,\nwe propose LMTransplant, a novel text augmentation paradigm leveraging LLMs.\nThe core idea of LMTransplant is transplant-then-regenerate: incorporating seed\ntext into a context expanded by LLM, and asking the LLM to regenerate a variant\nbased on the expanded context. This strategy allows the model to create more\ndiverse and creative content-level variants by fully leveraging the knowledge\nembedded in LLMs, while preserving the core attributes of the original text. We\nevaluate LMTransplant across various text-related tasks, demonstrating its\nsuperior performance over existing text augmentation methods. Moreover,\nLMTransplant demonstrates exceptional scalability as the size of augmented data\ngrows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data augmentation is a critical technique in deep learning. Traditional\nmethods like Back-translation typically focus on lexical-level rephrasing,\nwhich primarily produces variations with the same semantics. While large\nlanguage models (LLMs) have enhanced text augmentation by their \"knowledge\nemergence\" capability, controlling the style and structure of these outputs\nremains challenging and requires meticulous prompt engineering. In this paper,\nwe propose LMTransplant, a novel text augmentation paradigm leveraging LLMs.\nThe core idea of LMTransplant is transplant-then-regenerate: incorporating seed\ntext into a context expanded by LLM, and asking the LLM to regenerate a variant\nbased on the expanded context. This strategy allows the model to create more\ndiverse and creative content-level variants by fully leveraging the knowledge\nembedded in LLMs, while preserving the core attributes of the original text. We\nevaluate LMTransplant across various text-related tasks, demonstrating its\nsuperior performance over existing text augmentation methods. Moreover,\nLMTransplant demonstrates exceptional scalability as the size of augmented data\ngrows."
                },
                "authors": [
                    {
                        "name": "Guangzhan Wang"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Beijun Shen"
                    },
                    {
                        "name": "Xiaodong Gu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodong Gu"
                },
                "author": "Xiaodong Gu",
                "arxiv_comment": "Accepted by EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04336v1",
                "updated": "2025-09-04T15:55:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    55,
                    42,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T15:55:42Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    55,
                    42,
                    3,
                    247,
                    0
                ],
                "title": "Gravitational-wave inference at GPU speed: A bilby-like nested sampling\n  kernel within blackjax-ns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational-wave inference at GPU speed: A bilby-like nested sampling\n  kernel within blackjax-ns"
                },
                "summary": "We present a GPU-accelerated implementation of the gravitational-wave\nBayesian inference pipeline for parameter estimation and model comparison.\nSpecifically, we implement the `acceptance-walk' sampling method, a cornerstone\nalgorithm for gravitational-wave inference within the bilby and dynesty\nframework. By integrating this trusted kernel with the vectorized blackjax-ns\nframework, we achieve typical speedups of 20-40x for aligned spin binary black\nhole analyses, while recovering posteriors and evidences that are statistically\nidentical to the original CPU implementation. This faithful re-implementation\nof a community-standard algorithm establishes a foundational benchmark for\ngravitational-wave inference. It quantifies the performance gains attributable\nsolely to the architectural shift to GPUs, creating a vital reference against\nwhich future parallel sampling algorithms can be rigorously assessed. This\nallows for a clear distinction between algorithmic innovation and the inherent\nspeedup from hardware. Our work provides a validated community tool for\nperforming GPU-accelerated nested sampling in gravitational-wave data analyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a GPU-accelerated implementation of the gravitational-wave\nBayesian inference pipeline for parameter estimation and model comparison.\nSpecifically, we implement the `acceptance-walk' sampling method, a cornerstone\nalgorithm for gravitational-wave inference within the bilby and dynesty\nframework. By integrating this trusted kernel with the vectorized blackjax-ns\nframework, we achieve typical speedups of 20-40x for aligned spin binary black\nhole analyses, while recovering posteriors and evidences that are statistically\nidentical to the original CPU implementation. This faithful re-implementation\nof a community-standard algorithm establishes a foundational benchmark for\ngravitational-wave inference. It quantifies the performance gains attributable\nsolely to the architectural shift to GPUs, creating a vital reference against\nwhich future parallel sampling algorithms can be rigorously assessed. This\nallows for a clear distinction between algorithmic innovation and the inherent\nspeedup from hardware. Our work provides a validated community tool for\nperforming GPU-accelerated nested sampling in gravitational-wave data analyses."
                },
                "authors": [
                    {
                        "name": "Metha Prathaban"
                    },
                    {
                        "name": "David Yallup"
                    },
                    {
                        "name": "James Alvey"
                    },
                    {
                        "name": "Ming Yang"
                    },
                    {
                        "name": "Will Templeton"
                    },
                    {
                        "name": "Will Handley"
                    }
                ],
                "author_detail": {
                    "name": "Will Handley"
                },
                "author": "Will Handley",
                "arxiv_comment": "13 total pages, 15 total figures (main text: 11 pages, 12 figures;\n  appendix: 2 pages, 3 figures)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04316v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04316v2",
                "updated": "2025-09-04T15:53:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    53,
                    10,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-08T07:28:10Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    28,
                    10,
                    2,
                    8,
                    0
                ],
                "title": "Small Changes, Large Consequences: Analyzing the Allocational Fairness\n  of LLMs in Hiring Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Changes, Large Consequences: Analyzing the Allocational Fairness\n  of LLMs in Hiring Contexts"
                },
                "summary": "Large language models (LLMs) are increasingly being deployed in high-stakes\napplications like hiring, yet their potential for unfair decision-making\nremains understudied in generative and retrieval settings. In this work, we\nexamine the allocational fairness of LLM-based hiring systems through two tasks\nthat reflect actual HR usage: resume summarization and applicant ranking. By\nconstructing a synthetic resume dataset with controlled perturbations and\ncurating job postings, we investigate whether model behavior differs across\ndemographic groups. Our findings reveal that generated summaries exhibit\nmeaningful differences more frequently for race than for gender perturbations.\nModels also display non-uniform retrieval selection patterns across demographic\ngroups and exhibit high ranking sensitivity to both gender and race\nperturbations. Surprisingly, retrieval models can show comparable sensitivity\nto both demographic and non-demographic changes, suggesting that fairness\nissues may stem from broader model brittleness. Overall, our results indicate\nthat LLM-based hiring systems, especially in the retrieval stage, can exhibit\nnotable biases that lead to discriminatory outcomes in real-world contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being deployed in high-stakes\napplications like hiring, yet their potential for unfair decision-making\nremains understudied in generative and retrieval settings. In this work, we\nexamine the allocational fairness of LLM-based hiring systems through two tasks\nthat reflect actual HR usage: resume summarization and applicant ranking. By\nconstructing a synthetic resume dataset with controlled perturbations and\ncurating job postings, we investigate whether model behavior differs across\ndemographic groups. Our findings reveal that generated summaries exhibit\nmeaningful differences more frequently for race than for gender perturbations.\nModels also display non-uniform retrieval selection patterns across demographic\ngroups and exhibit high ranking sensitivity to both gender and race\nperturbations. Surprisingly, retrieval models can show comparable sensitivity\nto both demographic and non-demographic changes, suggesting that fairness\nissues may stem from broader model brittleness. Overall, our results indicate\nthat LLM-based hiring systems, especially in the retrieval stage, can exhibit\nnotable biases that lead to discriminatory outcomes in real-world contexts."
                },
                "authors": [
                    {
                        "name": "Preethi Seshadri"
                    },
                    {
                        "name": "Hongyu Chen"
                    },
                    {
                        "name": "Sameer Singh"
                    },
                    {
                        "name": "Seraphina Goldfarb-Tarrant"
                    }
                ],
                "author_detail": {
                    "name": "Seraphina Goldfarb-Tarrant"
                },
                "author": "Seraphina Goldfarb-Tarrant",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04316v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04316v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06742v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06742v3",
                "updated": "2025-09-04T15:43:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    43,
                    39,
                    3,
                    247,
                    0
                ],
                "published": "2025-06-07T10:10:56Z",
                "published_parsed": [
                    2025,
                    6,
                    7,
                    10,
                    10,
                    56,
                    5,
                    158,
                    0
                ],
                "title": "LADSG: Label-Anonymized Distillation and Similar Gradient Substitution\n  for Label Privacy in Vertical Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LADSG: Label-Anonymized Distillation and Similar Gradient Substitution\n  for Label Privacy in Vertical Federated Learning"
                },
                "summary": "Vertical Federated Learning (VFL) has emerged as a promising paradigm for\ncollaborative model training across distributed feature spaces, which enables\nprivacy-preserving learning without sharing raw data. However, recent studies\nhave confirmed the feasibility of label inference attacks by internal\nadversaries. By strategically exploiting gradient vectors and semantic\nembeddings, attackers-through passive, active, or direct attacks-can accurately\nreconstruct private labels, leading to catastrophic data leakage. Existing\ndefenses, which typically address isolated leakage vectors or are designed for\nspecific types of attacks, remain vulnerable to emerging hybrid attacks that\nexploit multiple pathways simultaneously. To bridge this gap, we propose\nLabel-Anonymized Defense with Substitution Gradient (LADSG), a unified and\nlightweight defense framework for VFL. LADSG first anonymizes true labels via\nsoft distillation to reduce semantic exposure, then generates\nsemantically-aligned substitute gradients to disrupt gradient-based leakage,\nand finally filters anomalous updates through gradient norm detection. It is\nscalable and compatible with standard VFL pipelines. Extensive experiments on\nsix real-world datasets show that LADSG reduces the success rates of all three\ntypes of label inference attacks by 30-60% with minimal computational overhead,\ndemonstrating its practical effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vertical Federated Learning (VFL) has emerged as a promising paradigm for\ncollaborative model training across distributed feature spaces, which enables\nprivacy-preserving learning without sharing raw data. However, recent studies\nhave confirmed the feasibility of label inference attacks by internal\nadversaries. By strategically exploiting gradient vectors and semantic\nembeddings, attackers-through passive, active, or direct attacks-can accurately\nreconstruct private labels, leading to catastrophic data leakage. Existing\ndefenses, which typically address isolated leakage vectors or are designed for\nspecific types of attacks, remain vulnerable to emerging hybrid attacks that\nexploit multiple pathways simultaneously. To bridge this gap, we propose\nLabel-Anonymized Defense with Substitution Gradient (LADSG), a unified and\nlightweight defense framework for VFL. LADSG first anonymizes true labels via\nsoft distillation to reduce semantic exposure, then generates\nsemantically-aligned substitute gradients to disrupt gradient-based leakage,\nand finally filters anomalous updates through gradient norm detection. It is\nscalable and compatible with standard VFL pipelines. Extensive experiments on\nsix real-world datasets show that LADSG reduces the success rates of all three\ntypes of label inference attacks by 30-60% with minimal computational overhead,\ndemonstrating its practical effectiveness."
                },
                "authors": [
                    {
                        "name": "Zeyu Yan"
                    },
                    {
                        "name": "Yanfei Yao"
                    },
                    {
                        "name": "Xuanbing Wen"
                    },
                    {
                        "name": "Shixiong Zhang"
                    },
                    {
                        "name": "Juli Zhang"
                    },
                    {
                        "name": "Kai Fan"
                    }
                ],
                "author_detail": {
                    "name": "Kai Fan"
                },
                "author": "Kai Fan",
                "arxiv_comment": "20 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06742v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06742v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.07655v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.07655v2",
                "updated": "2025-09-04T15:43:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    43,
                    33,
                    3,
                    247,
                    0
                ],
                "published": "2023-07-14T23:20:36Z",
                "published_parsed": [
                    2023,
                    7,
                    14,
                    23,
                    20,
                    36,
                    4,
                    195,
                    0
                ],
                "title": "Measurement dependence in a Bell inequality arising from the dynamics of\n  hidden variables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurement dependence in a Bell inequality arising from the dynamics of\n  hidden variables"
                },
                "summary": "Bell inequalities rely on an assumption that the probabilities of adopting\nconfigurations of hidden variables describing a system prior to measurement are\nindependent of the choice of measured physical property, also known as\nmeasurement independence. Weakening this assumption could alter the\ninequalities to accommodate experimental data whilst maintaining local\ninteractions. A natural avenue for achieving this would be to model measurement\nas a dynamical process involving an interaction between the system and its\nenvironment (the measurement apparatus), that drives the hidden variables\ntowards attractors representing measurement outcomes of the observable.\nImplementing such hidden variable dynamics, we can infer from observed\ncorrelations the hidden variable probability distributions before measurement,\nwhich differ according to which measurement settings were chosen. We explore\nvarious models of the dynamics of the hidden variables under measurement,\nrevealing features that can create measurement dependence and others that can\nnot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bell inequalities rely on an assumption that the probabilities of adopting\nconfigurations of hidden variables describing a system prior to measurement are\nindependent of the choice of measured physical property, also known as\nmeasurement independence. Weakening this assumption could alter the\ninequalities to accommodate experimental data whilst maintaining local\ninteractions. A natural avenue for achieving this would be to model measurement\nas a dynamical process involving an interaction between the system and its\nenvironment (the measurement apparatus), that drives the hidden variables\ntowards attractors representing measurement outcomes of the observable.\nImplementing such hidden variable dynamics, we can infer from observed\ncorrelations the hidden variable probability distributions before measurement,\nwhich differ according to which measurement settings were chosen. We explore\nvarious models of the dynamics of the hidden variables under measurement,\nrevealing features that can create measurement dependence and others that can\nnot."
                },
                "authors": [
                    {
                        "name": "Sophia M. Walls"
                    },
                    {
                        "name": "Ian J. Ford"
                    }
                ],
                "author_detail": {
                    "name": "Ian J. Ford"
                },
                "author": "Ian J. Ford",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.07655v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.07655v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04324v1",
                "updated": "2025-09-04T15:42:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    42,
                    36,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T15:42:36Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    42,
                    36,
                    3,
                    247,
                    0
                ],
                "title": "OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent\n  Detection"
                },
                "summary": "Grasping assistance is essential for restoring autonomy in individuals with\nmotor impairments, particularly in unstructured environments where object\ncategories and user intentions are diverse and unpredictable. We present\nOVGrasp, a hierarchical control framework for soft exoskeleton-based grasp\nassistance that integrates RGB-D vision, open-vocabulary prompts, and voice\ncommands to enable robust multimodal interaction. To enhance generalization in\nopen environments, OVGrasp incorporates a vision-language foundation model with\nan open-vocabulary mechanism, allowing zero-shot detection of previously unseen\nobjects without retraining. A multimodal decision-maker further fuses spatial\nand linguistic cues to infer user intent, such as grasp or release, in\nmulti-object scenarios. We deploy the complete framework on a custom\negocentric-view wearable exoskeleton and conduct systematic evaluations on 15\nobjects across three grasp types. Experimental results with ten participants\ndemonstrate that OVGrasp achieves a grasping ability score (GAS) of 87.00%,\noutperforming state-of-the-art baselines and achieving improved kinematic\nalignment with natural hand motion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grasping assistance is essential for restoring autonomy in individuals with\nmotor impairments, particularly in unstructured environments where object\ncategories and user intentions are diverse and unpredictable. We present\nOVGrasp, a hierarchical control framework for soft exoskeleton-based grasp\nassistance that integrates RGB-D vision, open-vocabulary prompts, and voice\ncommands to enable robust multimodal interaction. To enhance generalization in\nopen environments, OVGrasp incorporates a vision-language foundation model with\nan open-vocabulary mechanism, allowing zero-shot detection of previously unseen\nobjects without retraining. A multimodal decision-maker further fuses spatial\nand linguistic cues to infer user intent, such as grasp or release, in\nmulti-object scenarios. We deploy the complete framework on a custom\negocentric-view wearable exoskeleton and conduct systematic evaluations on 15\nobjects across three grasp types. Experimental results with ten participants\ndemonstrate that OVGrasp achieves a grasping ability score (GAS) of 87.00%,\noutperforming state-of-the-art baselines and achieving improved kinematic\nalignment with natural hand motion."
                },
                "authors": [
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Shan Luo"
                    },
                    {
                        "name": "Letizia Gionfrida"
                    }
                ],
                "author_detail": {
                    "name": "Letizia Gionfrida"
                },
                "author": "Letizia Gionfrida",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07809v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07809v2",
                "updated": "2025-09-04T15:41:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    41,
                    36,
                    3,
                    247,
                    0
                ],
                "published": "2025-08-11T09:49:01Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    49,
                    1,
                    0,
                    223,
                    0
                ],
                "title": "EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning"
                },
                "summary": "Reinforcement learning with verifiable reward (RLVR) has become a promising\nparadigm for post-training large language models (LLMs) to improve their\nreasoning capability. However, when the rollout accuracy is low on hard\nproblems, the reward becomes sparse, limiting learning efficiency and causing\nexploration bottlenecks. Existing approaches either rely on stronger LLMs for\ndistillation or filter out difficult problems, which limits scalability or\nrestricts reasoning improvement through exploration.\n  We propose EvoCoT, a self-evolving curriculum learning framework based on\ntwo-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the\nexploration space by self-generating and verifying CoT trajectories, then\ngradually shortens them to expand the space in a controlled way. This enables\nLLMs to stably learn from initially unsolved hard problems under sparse\nrewards. We apply EvoCoT to multiple LLM families, including Qwen, DeepSeek,\nand Llama. Experiments show that EvoCoT enables LLMs to solve previously\nunsolved problems, improves reasoning capability without external CoT\nsupervision, and is compatible with various RL fine-tuning methods. We release\nthe source code to support future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable reward (RLVR) has become a promising\nparadigm for post-training large language models (LLMs) to improve their\nreasoning capability. However, when the rollout accuracy is low on hard\nproblems, the reward becomes sparse, limiting learning efficiency and causing\nexploration bottlenecks. Existing approaches either rely on stronger LLMs for\ndistillation or filter out difficult problems, which limits scalability or\nrestricts reasoning improvement through exploration.\n  We propose EvoCoT, a self-evolving curriculum learning framework based on\ntwo-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the\nexploration space by self-generating and verifying CoT trajectories, then\ngradually shortens them to expand the space in a controlled way. This enables\nLLMs to stably learn from initially unsolved hard problems under sparse\nrewards. We apply EvoCoT to multiple LLM families, including Qwen, DeepSeek,\nand Llama. Experiments show that EvoCoT enables LLMs to solve previously\nunsolved problems, improves reasoning capability without external CoT\nsupervision, and is compatible with various RL fine-tuning methods. We release\nthe source code to support future research."
                },
                "authors": [
                    {
                        "name": "Huanyu Liu"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Chang Yu"
                    },
                    {
                        "name": "Taozhi Chen"
                    },
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Lecheng Wang"
                    },
                    {
                        "name": "XiaoLong Hu"
                    },
                    {
                        "name": "Ge Li"
                    }
                ],
                "author_detail": {
                    "name": "Ge Li"
                },
                "author": "Ge Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07809v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07809v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02846v2",
                "updated": "2025-09-04T15:40:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    40,
                    37,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-02T21:31:32Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    21,
                    31,
                    32,
                    1,
                    245,
                    0
                ],
                "title": "Towards Reasoning for PDE Foundation Models: A Reward-Model-Driven\n  Inference-Time-Scaling Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Reasoning for PDE Foundation Models: A Reward-Model-Driven\n  Inference-Time-Scaling Algorithm"
                },
                "summary": "Partial Differential Equations (PDEs) are the bedrock for modern\ncomputational sciences and engineering, and inherently computationally\nexpensive. While PDE foundation models have shown much promise for simulating\nsuch complex spatio-temporal phenomena, existing models remain constrained by\nthe pretraining datasets and struggle with auto-regressive rollout performance,\nespecially in out-of-distribution (OOD) cases. Furthermore, they have\nsignificant compute and training data requirements which hamper their use in\nmany critical applications. Inspired by recent advances in ``thinking\"\nstrategies used in large language models (LLMs), we introduce the first\ntest-time computing (TTC) strategy for PDEs that utilizes computational\nresources during inference to achieve more accurate predictions with fewer\ntraining samples and smaller models. We accomplish this with two types of\nreward models that evaluate predictions of a stochastic based model for\nspatio-temporal consistency. We demonstrate this method on compressible\nEuler-equation simulations from the PDEGym benchmark and show that TTC captures\nimproved predictions relative to standard non-adaptive auto-regressive\ninference. This TTC framework marks a foundational step towards more advanced\nreasoning algorithms or PDE modeling, inluding building\nreinforcement-learning-based approaches, potentially transforming computational\nworkflows in physics and engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partial Differential Equations (PDEs) are the bedrock for modern\ncomputational sciences and engineering, and inherently computationally\nexpensive. While PDE foundation models have shown much promise for simulating\nsuch complex spatio-temporal phenomena, existing models remain constrained by\nthe pretraining datasets and struggle with auto-regressive rollout performance,\nespecially in out-of-distribution (OOD) cases. Furthermore, they have\nsignificant compute and training data requirements which hamper their use in\nmany critical applications. Inspired by recent advances in ``thinking\"\nstrategies used in large language models (LLMs), we introduce the first\ntest-time computing (TTC) strategy for PDEs that utilizes computational\nresources during inference to achieve more accurate predictions with fewer\ntraining samples and smaller models. We accomplish this with two types of\nreward models that evaluate predictions of a stochastic based model for\nspatio-temporal consistency. We demonstrate this method on compressible\nEuler-equation simulations from the PDEGym benchmark and show that TTC captures\nimproved predictions relative to standard non-adaptive auto-regressive\ninference. This TTC framework marks a foundational step towards more advanced\nreasoning algorithms or PDE modeling, inluding building\nreinforcement-learning-based approaches, potentially transforming computational\nworkflows in physics and engineering."
                },
                "authors": [
                    {
                        "name": "Siddharth Mansingh"
                    },
                    {
                        "name": "James Amarel"
                    },
                    {
                        "name": "Ragib Arnab"
                    },
                    {
                        "name": "Arvind Mohan"
                    },
                    {
                        "name": "Kamaljeet Singh"
                    },
                    {
                        "name": "Gerd J. Kunde"
                    },
                    {
                        "name": "Nicolas Hengartner"
                    },
                    {
                        "name": "Benjamin Migliori"
                    },
                    {
                        "name": "Emily Casleton"
                    },
                    {
                        "name": "Nathan A. Debardeleben"
                    },
                    {
                        "name": "Ayan Biswas"
                    },
                    {
                        "name": "Diane Oyen"
                    },
                    {
                        "name": "Earl Lawrence"
                    }
                ],
                "author_detail": {
                    "name": "Earl Lawrence"
                },
                "author": "Earl Lawrence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02761v2",
                "updated": "2025-09-04T15:30:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    30,
                    53,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-02T19:06:56Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    19,
                    6,
                    56,
                    1,
                    245,
                    0
                ],
                "title": "Plan Verification for LLM-Based Embodied Task Completion Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plan Verification for LLM-Based Embodied Task Completion Agents"
                },
                "summary": "Large language model (LLM) based task plans and corresponding human\ndemonstrations for embodied AI may be noisy, with unnecessary actions,\nredundant navigation, and logical errors that reduce policy quality. We propose\nan iterative verification framework in which a Judge LLM critiques action\nsequences and a Planner LLM applies the revisions, yielding progressively\ncleaner and more spatially coherent trajectories. Unlike rule-based approaches,\nour method relies on natural language prompting, enabling broad generalization\nacross error types including irrelevant actions, contradictions, and missing\nsteps. On a set of manually annotated actions from the TEACh embodied AI\ndataset, our framework achieves up to 90% recall and 100% precision across four\nstate-of-the-art LLMs (GPT o4-mini, DeepSeek-R1, Gemini 2.5, LLaMA 4 Scout).\nThe refinement loop converges quickly, with 96.5% of sequences requiring at\nmost three iterations, while improving both temporal efficiency and spatial\naction organization. Crucially, the method preserves human error-recovery\npatterns rather than collapsing them, supporting future work on robust\ncorrective behavior. By establishing plan verification as a reliable LLM\ncapability for spatial planning and action refinement, we provide a scalable\npath to higher-quality training data for imitation learning in embodied AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) based task plans and corresponding human\ndemonstrations for embodied AI may be noisy, with unnecessary actions,\nredundant navigation, and logical errors that reduce policy quality. We propose\nan iterative verification framework in which a Judge LLM critiques action\nsequences and a Planner LLM applies the revisions, yielding progressively\ncleaner and more spatially coherent trajectories. Unlike rule-based approaches,\nour method relies on natural language prompting, enabling broad generalization\nacross error types including irrelevant actions, contradictions, and missing\nsteps. On a set of manually annotated actions from the TEACh embodied AI\ndataset, our framework achieves up to 90% recall and 100% precision across four\nstate-of-the-art LLMs (GPT o4-mini, DeepSeek-R1, Gemini 2.5, LLaMA 4 Scout).\nThe refinement loop converges quickly, with 96.5% of sequences requiring at\nmost three iterations, while improving both temporal efficiency and spatial\naction organization. Crucially, the method preserves human error-recovery\npatterns rather than collapsing them, supporting future work on robust\ncorrective behavior. By establishing plan verification as a reliable LLM\ncapability for spatial planning and action refinement, we provide a scalable\npath to higher-quality training data for imitation learning in embodied AI."
                },
                "authors": [
                    {
                        "name": "Ananth Hariharan"
                    },
                    {
                        "name": "Vardhan Dongre"
                    },
                    {
                        "name": "Dilek Hakkani-Tr"
                    },
                    {
                        "name": "Gokhan Tur"
                    }
                ],
                "author_detail": {
                    "name": "Gokhan Tur"
                },
                "author": "Gokhan Tur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01163v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01163v5",
                "updated": "2025-09-04T15:26:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    26,
                    7,
                    3,
                    247,
                    0
                ],
                "published": "2024-10-02T01:34:02Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    1,
                    34,
                    2,
                    2,
                    276,
                    0
                ],
                "title": "Perturbation-Robust Predictive Modeling of Social Effects by Network\n  Subspace Generalized Linear Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perturbation-Robust Predictive Modeling of Social Effects by Network\n  Subspace Generalized Linear Models"
                },
                "summary": "Network-linked data, where multivariate observations are interconnected by a\nnetwork, are becoming increasingly prevalent in fields such as sociology and\nbiology. These data often exhibit inherent noise and complex relational\nstructures, complicating conventional modeling and statistical inference.\nMotivated by empirical challenges in analyzing such data sets, this paper\nintroduces a family of network subspace generalized linear models designed for\nanalyzing noisy, network-linked data. We propose a model inference method based\non subspace-constrained maximum likelihood, which emphasizes flexibility in\ncapturing network effects and provides a robust inference framework against\nnetwork perturbations. We establish the asymptotic distributions of the\nestimators under network perturbations, demonstrating the method's accuracy\nthrough extensive simulations involving random network models and\ndeep-learning-based embedding algorithms. The proposed methodology is applied\nto a comprehensive analysis of a large-scale study on school conflicts, where\nit identifies significant social effects, offering meaningful and interpretable\ninsights into student behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network-linked data, where multivariate observations are interconnected by a\nnetwork, are becoming increasingly prevalent in fields such as sociology and\nbiology. These data often exhibit inherent noise and complex relational\nstructures, complicating conventional modeling and statistical inference.\nMotivated by empirical challenges in analyzing such data sets, this paper\nintroduces a family of network subspace generalized linear models designed for\nanalyzing noisy, network-linked data. We propose a model inference method based\non subspace-constrained maximum likelihood, which emphasizes flexibility in\ncapturing network effects and provides a robust inference framework against\nnetwork perturbations. We establish the asymptotic distributions of the\nestimators under network perturbations, demonstrating the method's accuracy\nthrough extensive simulations involving random network models and\ndeep-learning-based embedding algorithms. The proposed methodology is applied\nto a comprehensive analysis of a large-scale study on school conflicts, where\nit identifies significant social effects, offering meaningful and interpretable\ninsights into student behaviors."
                },
                "authors": [
                    {
                        "name": "Jianxiang Wang"
                    },
                    {
                        "name": "Can M. Le"
                    },
                    {
                        "name": "Tianxi Li"
                    }
                ],
                "author_detail": {
                    "name": "Tianxi Li"
                },
                "author": "Tianxi Li",
                "arxiv_comment": "78 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01163v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01163v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04310v1",
                "updated": "2025-09-04T15:23:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    23,
                    58,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T15:23:58Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    23,
                    58,
                    3,
                    247,
                    0
                ],
                "title": "EvoEmo: Towards Evolved Emotional Policies for LLM Agents in Multi-Turn\n  Negotiation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvoEmo: Towards Evolved Emotional Policies for LLM Agents in Multi-Turn\n  Negotiation"
                },
                "summary": "Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models\n(LLMs) has demonstrated that agents can engage in \\textit{complex},\n\\textit{multi-turn} negotiations, opening new avenues for agentic AI. However,\nexisting LLM agents largely overlook the functional role of emotions in such\nnegotiations, instead generating passive, preference-driven emotional responses\nthat make them vulnerable to manipulation and strategic exploitation by\nadversarial counterparts. To address this gap, we present EvoEmo, an\nevolutionary reinforcement learning framework that optimizes dynamic emotional\nexpression in negotiations. EvoEmo models emotional state transitions as a\nMarkov Decision Process and employs population-based genetic optimization to\nevolve high-reward emotion policies across diverse negotiation scenarios. We\nfurther propose an evaluation framework with two baselines -- vanilla\nstrategies and fixed-emotion strategies -- for benchmarking emotion-aware\nnegotiation. Extensive experiments and ablation studies show that EvoEmo\nconsistently outperforms both baselines, achieving higher success rates, higher\nefficiency, and increased buyer savings. This findings highlight the importance\nof adaptive emotional expression in enabling more effective LLM agents for\nmulti-turn negotiation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models\n(LLMs) has demonstrated that agents can engage in \\textit{complex},\n\\textit{multi-turn} negotiations, opening new avenues for agentic AI. However,\nexisting LLM agents largely overlook the functional role of emotions in such\nnegotiations, instead generating passive, preference-driven emotional responses\nthat make them vulnerable to manipulation and strategic exploitation by\nadversarial counterparts. To address this gap, we present EvoEmo, an\nevolutionary reinforcement learning framework that optimizes dynamic emotional\nexpression in negotiations. EvoEmo models emotional state transitions as a\nMarkov Decision Process and employs population-based genetic optimization to\nevolve high-reward emotion policies across diverse negotiation scenarios. We\nfurther propose an evaluation framework with two baselines -- vanilla\nstrategies and fixed-emotion strategies -- for benchmarking emotion-aware\nnegotiation. Extensive experiments and ablation studies show that EvoEmo\nconsistently outperforms both baselines, achieving higher success rates, higher\nefficiency, and increased buyer savings. This findings highlight the importance\nof adaptive emotional expression in enabling more effective LLM agents for\nmulti-turn negotiation."
                },
                "authors": [
                    {
                        "name": "Yunbo Long"
                    },
                    {
                        "name": "Liming Xu"
                    },
                    {
                        "name": "Lukas Beckenbauer"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Alexandra Brintrup"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Brintrup"
                },
                "author": "Alexandra Brintrup",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13755v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13755v2",
                "updated": "2025-09-04T15:21:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    21,
                    27,
                    3,
                    247,
                    0
                ],
                "published": "2025-08-19T11:51:40Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    51,
                    40,
                    1,
                    231,
                    0
                ],
                "title": "Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with\n  Adaptive Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with\n  Adaptive Exploration"
                },
                "summary": "Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a\npowerful paradigm for unlocking reasoning capabilities in large language\nmodels, yet its full potential is hindered by two under-explored dimensions:\nDepth-the hardest problem a model can sample; Breadth-the number of instances\nconsumed in a single iteration. We dissect the popular GRPO algorithm and\nreveal a systematic bias: the cumulative-advantage disproportionately weights\nsamples with medium accuracy, while down-weighting the low-accuracy instances\nthat are crucial for pushing reasoning boundaries. To rectify the depth\nneglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which\nre-weights hard problems through targeted multi-stage rollouts, thereby\nincreasing the number of positive rollouts for hard problems. Empirically,\nnaively enlarging rollout size only accelerates convergence and even hurts\nPass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra\ninference cost at convergence. Just as we adaptively expanded the depth of\nexploration, we now ask whether aggressively scaling the breadth of training\ndata can further amplify reasoning gains. To this end, we intensely scale batch\nsize and replace PPO's mini-batch iterations with full-batch updates over\nmultiple epochs. Increasing breadth significantly enhances Pass@1 performance.\nLarge-breadth training sustains high token-level entropy, indicating continued\nexploration and reduced gradient noise. We further present DARS-B, which\naugments DARS with large breadth, and demonstrate simultaneous gains in Pass@K\nand Pass@1. The results confirm that breadth and adaptive exploration across\ndepth operate as orthogonal dimensions in RLVR, which are key to unleashing the\nreasoning power of RLVR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a\npowerful paradigm for unlocking reasoning capabilities in large language\nmodels, yet its full potential is hindered by two under-explored dimensions:\nDepth-the hardest problem a model can sample; Breadth-the number of instances\nconsumed in a single iteration. We dissect the popular GRPO algorithm and\nreveal a systematic bias: the cumulative-advantage disproportionately weights\nsamples with medium accuracy, while down-weighting the low-accuracy instances\nthat are crucial for pushing reasoning boundaries. To rectify the depth\nneglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which\nre-weights hard problems through targeted multi-stage rollouts, thereby\nincreasing the number of positive rollouts for hard problems. Empirically,\nnaively enlarging rollout size only accelerates convergence and even hurts\nPass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra\ninference cost at convergence. Just as we adaptively expanded the depth of\nexploration, we now ask whether aggressively scaling the breadth of training\ndata can further amplify reasoning gains. To this end, we intensely scale batch\nsize and replace PPO's mini-batch iterations with full-batch updates over\nmultiple epochs. Increasing breadth significantly enhances Pass@1 performance.\nLarge-breadth training sustains high token-level entropy, indicating continued\nexploration and reduced gradient noise. We further present DARS-B, which\naugments DARS with large breadth, and demonstrate simultaneous gains in Pass@K\nand Pass@1. The results confirm that breadth and adaptive exploration across\ndepth operate as orthogonal dimensions in RLVR, which are key to unleashing the\nreasoning power of RLVR."
                },
                "authors": [
                    {
                        "name": "Zhicheng Yang"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Yinya Huang"
                    },
                    {
                        "name": "Yongxin Wang"
                    },
                    {
                        "name": "Dongchun Xie"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Xiaodan Liang"
                    },
                    {
                        "name": "Jing Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Tang"
                },
                "author": "Jing Tang",
                "arxiv_comment": "16 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13755v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13755v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04304v1",
                "updated": "2025-09-04T15:17:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    17,
                    50,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T15:17:50Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    17,
                    50,
                    3,
                    247,
                    0
                ],
                "title": "Facts Fade Fast: Evaluating Memorization of Outdated Medical Knowledge\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facts Fade Fast: Evaluating Memorization of Outdated Medical Knowledge\n  in Large Language Models"
                },
                "summary": "The growing capabilities of Large Language Models (LLMs) show significant\npotential to enhance healthcare by assisting medical researchers and\nphysicians. However, their reliance on static training data is a major risk\nwhen medical recommendations evolve with new research and developments. When\nLLMs memorize outdated medical knowledge, they can provide harmful advice or\nfail at clinical reasoning tasks. To investigate this problem, we introduce two\nnovel question-answering (QA) datasets derived from systematic reviews:\nMedRevQA (16,501 QA pairs covering general biomedical knowledge) and\nMedChangeQA (a subset of 512 QA pairs where medical consensus has changed over\ntime). Our evaluation of eight prominent LLMs on the datasets reveals\nconsistent reliance on outdated knowledge across all models. We additionally\nanalyze the influence of obsolete pre-training data and training strategies to\nexplain this phenomenon and propose future directions for mitigation, laying\nthe groundwork for developing more current and reliable medical AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing capabilities of Large Language Models (LLMs) show significant\npotential to enhance healthcare by assisting medical researchers and\nphysicians. However, their reliance on static training data is a major risk\nwhen medical recommendations evolve with new research and developments. When\nLLMs memorize outdated medical knowledge, they can provide harmful advice or\nfail at clinical reasoning tasks. To investigate this problem, we introduce two\nnovel question-answering (QA) datasets derived from systematic reviews:\nMedRevQA (16,501 QA pairs covering general biomedical knowledge) and\nMedChangeQA (a subset of 512 QA pairs where medical consensus has changed over\ntime). Our evaluation of eight prominent LLMs on the datasets reveals\nconsistent reliance on outdated knowledge across all models. We additionally\nanalyze the influence of obsolete pre-training data and training strategies to\nexplain this phenomenon and propose future directions for mitigation, laying\nthe groundwork for developing more current and reliable medical AI systems."
                },
                "authors": [
                    {
                        "name": "Juraj Vladika"
                    },
                    {
                        "name": "Mahdi Dhaini"
                    },
                    {
                        "name": "Florian Matthes"
                    }
                ],
                "author_detail": {
                    "name": "Florian Matthes"
                },
                "author": "Florian Matthes",
                "arxiv_comment": "Accepted to Findings of EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21080v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21080v4",
                "updated": "2025-09-04T15:06:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    6,
                    27,
                    3,
                    247,
                    0
                ],
                "published": "2025-03-27T01:41:34Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    1,
                    41,
                    34,
                    3,
                    86,
                    0
                ],
                "title": "EQ-Knight: A Memory-Augmented LLM Agent for Strategic Affective Gaming\n  in Debt Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EQ-Knight: A Memory-Augmented LLM Agent for Strategic Affective Gaming\n  in Debt Recovery"
                },
                "summary": "Large language model-based chatbots have enhanced engagement in financial\nnegotiations, but their overreliance on passive empathy introduces critical\nrisks in credit collection. While empathy-driven approaches preserve client\nsatisfaction in benign cases, they fail catastrophically against dishonest\ndebtors--individuals who exploit conciliatory tactics to manipulate terms or\nevade repayment. Blindly prioritizing \"customer experience\" in such scenarios\nleads to creditor vulnerabilities: revenue leakage, moral hazard, and systemic\nexploitation. To address this, we propose EQ-Knight, an LLM agent that\ndynamically optimizes emotional strategy to defend creditor interests. Unlike\nnaive empathy-centric bots, EQ-Knight integrates emotion memory and\ngame-theoretic reasoning, powered by a Hidden Markov Model (HMM) to track and\npredict debtor emotional states. By analyzing both real-time and historical\nemotional cues, EQ-Knight strategically counters negative emotions (e.g.,\naggression, feigned distress) while preserving productive debtor relationships.\nExperiments demonstrate EQ-Knight's superiority over conventional LLM\nnegotiators: it achieves a 32\\% reduction in concession losses without\ncompromising recovery rates, particularly in adversarial cases where debtors\nweaponize negative emotions (e.g., intimidation, guilt-tripping) to coerce\nconcessions. For credit agencies, EQ-Knight transforms LLMs from high-risk\n\"people-pleasers\" into strategic emotion-defenders--balancing emotional\nintelligence with tactical rigor to enforce accountability and deter\nexploitation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model-based chatbots have enhanced engagement in financial\nnegotiations, but their overreliance on passive empathy introduces critical\nrisks in credit collection. While empathy-driven approaches preserve client\nsatisfaction in benign cases, they fail catastrophically against dishonest\ndebtors--individuals who exploit conciliatory tactics to manipulate terms or\nevade repayment. Blindly prioritizing \"customer experience\" in such scenarios\nleads to creditor vulnerabilities: revenue leakage, moral hazard, and systemic\nexploitation. To address this, we propose EQ-Knight, an LLM agent that\ndynamically optimizes emotional strategy to defend creditor interests. Unlike\nnaive empathy-centric bots, EQ-Knight integrates emotion memory and\ngame-theoretic reasoning, powered by a Hidden Markov Model (HMM) to track and\npredict debtor emotional states. By analyzing both real-time and historical\nemotional cues, EQ-Knight strategically counters negative emotions (e.g.,\naggression, feigned distress) while preserving productive debtor relationships.\nExperiments demonstrate EQ-Knight's superiority over conventional LLM\nnegotiators: it achieves a 32\\% reduction in concession losses without\ncompromising recovery rates, particularly in adversarial cases where debtors\nweaponize negative emotions (e.g., intimidation, guilt-tripping) to coerce\nconcessions. For credit agencies, EQ-Knight transforms LLMs from high-risk\n\"people-pleasers\" into strategic emotion-defenders--balancing emotional\nintelligence with tactical rigor to enforce accountability and deter\nexploitation."
                },
                "authors": [
                    {
                        "name": "Yunbo Long"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Liming Xu"
                    },
                    {
                        "name": "Alexandra Brintrup"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Brintrup"
                },
                "author": "Alexandra Brintrup",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21080v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21080v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04292v1",
                "updated": "2025-09-04T15:03:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    3,
                    2,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T15:03:02Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    3,
                    2,
                    3,
                    247,
                    0
                ],
                "title": "Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow\n  Real Instructions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow\n  Real Instructions?"
                },
                "summary": "Large Language Models (LLMs) achieve strong performance on diverse tasks but\noften exhibit cognitive inertia, struggling to follow instructions that\nconflict with the standardized patterns learned during supervised fine-tuning\n(SFT). To evaluate this limitation, we propose Inverse IFEval, a benchmark that\nmeasures models Counter-intuitive Abilitytheir capacity to override\ntraining-induced biases and comply with adversarial instructions. Inverse\nIFEval introduces eight types of such challenges, including Question\nCorrection, Intentional Textual Flaws, Code without Comments, and\nCounterfactual Answering. Using a human-in-the-loop pipeline, we construct a\ndataset of 1012 high-quality Chinese and English questions across 23 domains,\nevaluated under an optimized LLM-as-a-Judge framework. Experiments on existing\nleading LLMs demonstrate the necessity of our proposed Inverse IFEval\nbenchmark. Our findings emphasize that future alignment efforts should not only\npursue fluency and factual correctness but also account for adaptability under\nunconventional contexts. We hope that Inverse IFEval serves as both a\ndiagnostic tool and a foundation for developing methods that mitigate cognitive\ninertia, reduce overfitting to narrow patterns, and ultimately enhance the\ninstruction-following reliability of LLMs in diverse and unpredictable\nreal-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) achieve strong performance on diverse tasks but\noften exhibit cognitive inertia, struggling to follow instructions that\nconflict with the standardized patterns learned during supervised fine-tuning\n(SFT). To evaluate this limitation, we propose Inverse IFEval, a benchmark that\nmeasures models Counter-intuitive Abilitytheir capacity to override\ntraining-induced biases and comply with adversarial instructions. Inverse\nIFEval introduces eight types of such challenges, including Question\nCorrection, Intentional Textual Flaws, Code without Comments, and\nCounterfactual Answering. Using a human-in-the-loop pipeline, we construct a\ndataset of 1012 high-quality Chinese and English questions across 23 domains,\nevaluated under an optimized LLM-as-a-Judge framework. Experiments on existing\nleading LLMs demonstrate the necessity of our proposed Inverse IFEval\nbenchmark. Our findings emphasize that future alignment efforts should not only\npursue fluency and factual correctness but also account for adaptability under\nunconventional contexts. We hope that Inverse IFEval serves as both a\ndiagnostic tool and a foundation for developing methods that mitigate cognitive\ninertia, reduce overfitting to narrow patterns, and ultimately enhance the\ninstruction-following reliability of LLMs in diverse and unpredictable\nreal-world scenarios."
                },
                "authors": [
                    {
                        "name": "Qinyan Zhang"
                    },
                    {
                        "name": "Xinping Lei"
                    },
                    {
                        "name": "Ruijie Miao"
                    },
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Haojie Fan"
                    },
                    {
                        "name": "Le Chang"
                    },
                    {
                        "name": "Jiafan Hou"
                    },
                    {
                        "name": "Dingling Zhang"
                    },
                    {
                        "name": "Zhongfei Hou"
                    },
                    {
                        "name": "Ziqiang Yang"
                    },
                    {
                        "name": "Changxin Pu"
                    },
                    {
                        "name": "Fei Hu"
                    },
                    {
                        "name": "Jingkai Liu"
                    },
                    {
                        "name": "Mengyun Liu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xiang Gao"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "Zaiyuan Wang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wenhao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhao Huang"
                },
                "author": "Wenhao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02799v2",
                "updated": "2025-09-04T14:59:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    14,
                    59,
                    48,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-02T20:02:00Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    20,
                    2,
                    0,
                    1,
                    245,
                    0
                ],
                "title": "Data-driven mean-field within whole-brain models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven mean-field within whole-brain models"
                },
                "summary": "Mean-field models provide a link between microscopic neuronal activity and\nmacroscopic brain dynamics. Their derivation depends on simplifying\nassumptions, such as all-to-all connectivity, limiting their biological\nrealism. To overcome this, we introduce a data-driven framework in which a\nmulti-layer perceptron (MLP) learns the macroscopic dynamics directly from\nsimulations of a network of spiking neurons. The network connection probability\nserves here as a new parameter, inaccessible to purely analytical treatment,\nwhich is validated against ground truth analytical solutions. Through\nbifurcation analysis on the trained MLP, we demonstrate the existence of new\ncusp bifurcation that systematically reshapes the system's phase diagram in a\ndegenerate manner with synaptic coupling. By integrating this data-driven\nmean-field model into a whole-brain computational framework, we show that it\nextends beyond the macroscopic emergent dynamics generated by the analytical\nmodel. For validation, we use simulation-based inference on synthetic\nfunctional magnetic resonance imaging (fMRI) data and demonstrate accurate\nparameter recovery for the novel mean-field model, while the current\nstate-of-the-art models lead to biased estimates. This work presents a flexible\nand generic framework for building more realistic whole-brain models, bridging\nthe gap between microscale mechanisms and macroscopic brain recordings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mean-field models provide a link between microscopic neuronal activity and\nmacroscopic brain dynamics. Their derivation depends on simplifying\nassumptions, such as all-to-all connectivity, limiting their biological\nrealism. To overcome this, we introduce a data-driven framework in which a\nmulti-layer perceptron (MLP) learns the macroscopic dynamics directly from\nsimulations of a network of spiking neurons. The network connection probability\nserves here as a new parameter, inaccessible to purely analytical treatment,\nwhich is validated against ground truth analytical solutions. Through\nbifurcation analysis on the trained MLP, we demonstrate the existence of new\ncusp bifurcation that systematically reshapes the system's phase diagram in a\ndegenerate manner with synaptic coupling. By integrating this data-driven\nmean-field model into a whole-brain computational framework, we show that it\nextends beyond the macroscopic emergent dynamics generated by the analytical\nmodel. For validation, we use simulation-based inference on synthetic\nfunctional magnetic resonance imaging (fMRI) data and demonstrate accurate\nparameter recovery for the novel mean-field model, while the current\nstate-of-the-art models lead to biased estimates. This work presents a flexible\nand generic framework for building more realistic whole-brain models, bridging\nthe gap between microscale mechanisms and macroscopic brain recordings."
                },
                "authors": [
                    {
                        "name": "Martin Breyton"
                    },
                    {
                        "name": "Viktor Sip"
                    },
                    {
                        "name": "Marmaduke Woodman"
                    },
                    {
                        "name": "Meysam Hashemi"
                    },
                    {
                        "name": "Spase Petkoski"
                    },
                    {
                        "name": "Viktor Jirsa"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Jirsa"
                },
                "author": "Viktor Jirsa",
                "arxiv_comment": "12 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14791v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14791v4",
                "updated": "2025-09-04T14:58:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    14,
                    58,
                    9,
                    3,
                    247,
                    0
                ],
                "published": "2025-02-20T18:11:38Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    11,
                    38,
                    3,
                    51,
                    0
                ],
                "title": "Rapid Word Learning Through Meta In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid Word Learning Through Meta In-Context Learning"
                },
                "summary": "Humans can quickly learn a new word from a few illustrative examples, and\nthen systematically and flexibly use it in novel contexts. Yet the abilities of\ncurrent language models for few-shot word learning, and methods for improving\nthese abilities, are underexplored. In this study, we introduce a novel method,\nMeta-training for IN-context learNing Of Words (Minnow). This method trains\nlanguage models to generate new examples of a word's usage given a few\nin-context examples, using a special placeholder token to represent the new\nword. This training is repeated on many new words to develop a general\nword-learning ability. We find that training models from scratch with Minnow on\nhuman-scale child-directed language enables strong few-shot word learning,\ncomparable to a large language model (LLM) pre-trained on orders of magnitude\nmore data. Furthermore, through discriminative and generative evaluations, we\ndemonstrate that finetuning pre-trained LLMs with Minnow improves their ability\nto discriminate between new words, identify syntactic categories of new words,\nand generate reasonable new usages and definitions for new words, based on one\nor a few in-context examples. These findings highlight the data efficiency of\nMinnow and its potential to improve language model performance in word learning\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans can quickly learn a new word from a few illustrative examples, and\nthen systematically and flexibly use it in novel contexts. Yet the abilities of\ncurrent language models for few-shot word learning, and methods for improving\nthese abilities, are underexplored. In this study, we introduce a novel method,\nMeta-training for IN-context learNing Of Words (Minnow). This method trains\nlanguage models to generate new examples of a word's usage given a few\nin-context examples, using a special placeholder token to represent the new\nword. This training is repeated on many new words to develop a general\nword-learning ability. We find that training models from scratch with Minnow on\nhuman-scale child-directed language enables strong few-shot word learning,\ncomparable to a large language model (LLM) pre-trained on orders of magnitude\nmore data. Furthermore, through discriminative and generative evaluations, we\ndemonstrate that finetuning pre-trained LLMs with Minnow improves their ability\nto discriminate between new words, identify syntactic categories of new words,\nand generate reasonable new usages and definitions for new words, based on one\nor a few in-context examples. These findings highlight the data efficiency of\nMinnow and its potential to improve language model performance in word learning\ntasks."
                },
                "authors": [
                    {
                        "name": "Wentao Wang"
                    },
                    {
                        "name": "Guangyuan Jiang"
                    },
                    {
                        "name": "Tal Linzen"
                    },
                    {
                        "name": "Brenden M. Lake"
                    }
                ],
                "author_detail": {
                    "name": "Brenden M. Lake"
                },
                "author": "Brenden M. Lake",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14791v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14791v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07967v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07967v2",
                "updated": "2025-09-04T14:43:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    14,
                    43,
                    59,
                    3,
                    247,
                    0
                ],
                "published": "2025-07-10T17:47:40Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "title": "Synthesizing Sun-as-a-star flare spectra from high-resolution solar\n  observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing Sun-as-a-star flare spectra from high-resolution solar\n  observations"
                },
                "summary": "Spatially resolved observations of the Sun and the astronomical sample size\nof stellar bodies are the respective key strengths of solar and stellar\nobservations. However, the large difference in object brightness between the\nSun and other stars has led to distinctly different instrumentation and\nmethodologies between the two fields. We produce and analyze synthetic\nfull-disk spectra derived from 19 small area field-of-view optical observations\nof solar flares acquired by the Swedish 1-m Solar Telescope (SST) between 2011\nand 2024. These are used to investigate what can and cannot be inferred about\nphysical processes on the Sun from Sun-as-a-star observations. The recently\nreleased Numerical Empirical Sun-as-a-Star Integrator (NESSI) code provides\nsynthetic full-disk integrated spectral line emission based on smaller\nfield-of-view input, accounting for center-to-limb variations and differential\nrotation. We use this code to generate pseudo-Sun-as-a-star spectra from the\nSST observations. ...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatially resolved observations of the Sun and the astronomical sample size\nof stellar bodies are the respective key strengths of solar and stellar\nobservations. However, the large difference in object brightness between the\nSun and other stars has led to distinctly different instrumentation and\nmethodologies between the two fields. We produce and analyze synthetic\nfull-disk spectra derived from 19 small area field-of-view optical observations\nof solar flares acquired by the Swedish 1-m Solar Telescope (SST) between 2011\nand 2024. These are used to investigate what can and cannot be inferred about\nphysical processes on the Sun from Sun-as-a-star observations. The recently\nreleased Numerical Empirical Sun-as-a-Star Integrator (NESSI) code provides\nsynthetic full-disk integrated spectral line emission based on smaller\nfield-of-view input, accounting for center-to-limb variations and differential\nrotation. We use this code to generate pseudo-Sun-as-a-star spectra from the\nSST observations. ..."
                },
                "authors": [
                    {
                        "name": "M. De Wilde"
                    },
                    {
                        "name": "A. G. M. Pietrow"
                    },
                    {
                        "name": "M. K. Druett"
                    },
                    {
                        "name": "A. Pastor Yabar"
                    },
                    {
                        "name": "J. Koza"
                    },
                    {
                        "name": "I. Kontogiannis"
                    },
                    {
                        "name": "O. Andriienko"
                    },
                    {
                        "name": "A. Berlicki"
                    },
                    {
                        "name": "A. R. Brunvoll"
                    },
                    {
                        "name": "J. de la Cruz Rodrguez"
                    },
                    {
                        "name": "J. T. Faber"
                    },
                    {
                        "name": "R. Joshi"
                    },
                    {
                        "name": "D. Kuridze"
                    },
                    {
                        "name": "D. Nbrega-Siverio"
                    },
                    {
                        "name": "L. H. M. Rouppe van der Voort"
                    },
                    {
                        "name": "J. Rybk"
                    },
                    {
                        "name": "E. Scullion"
                    },
                    {
                        "name": "A. M. Silva"
                    },
                    {
                        "name": "Z. Vashalomidze"
                    },
                    {
                        "name": "A. Vicente Arvalo"
                    },
                    {
                        "name": "A. Winiewska"
                    },
                    {
                        "name": "R. Yadav"
                    },
                    {
                        "name": "T. V. Zaqarashvili"
                    },
                    {
                        "name": "J. Zbinden"
                    },
                    {
                        "name": "E. S. yre"
                    }
                ],
                "author_detail": {
                    "name": "E. S. yre"
                },
                "author": "E. S. yre",
                "arxiv_doi": "10.1051/0004-6361/202554870",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202554870",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.07967v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07967v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in A&A. 22 pages, 22 figures",
                "arxiv_journal_ref": "A&A 700, A275 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08193v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08193v2",
                "updated": "2025-09-04T14:42:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    14,
                    42,
                    6,
                    3,
                    247,
                    0
                ],
                "published": "2025-08-11T17:12:55Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    12,
                    55,
                    0,
                    223,
                    0
                ],
                "title": "Street-Level AI: Are Large Language Models Ready for Real-World\n  Judgments?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Street-Level AI: Are Large Language Models Ready for Real-World\n  Judgments?"
                },
                "summary": "A surge of recent work explores the ethical and societal implications of\nlarge-scale AI models that make \"moral\" judgments. Much of this literature\nfocuses either on alignment with human judgments through various thought\nexperiments or on the group fairness implications of AI judgments. However, the\nmost immediate and likely use of AI is to help or fully replace the so-called\nstreet-level bureaucrats, the individuals deciding to allocate scarce social\nresources or approve benefits. There is a rich history underlying how\nprinciples of local justice determine how society decides on prioritization\nmechanisms in such domains. In this paper, we examine how well LLM judgments\nalign with human judgments, as well as with socially and politically determined\nvulnerability scoring systems currently used in the domain of homelessness\nresource allocation. Crucially, we use real data on those needing services\n(maintaining strict confidentiality by only using local large models) to\nperform our analyses. We find that LLM prioritizations are extremely\ninconsistent in several ways: internally on different runs, between different\nLLMs, and between LLMs and the vulnerability scoring systems. At the same time,\nLLMs demonstrate qualitative consistency with lay human judgments in pairwise\ntesting. Findings call into question the readiness of current generation AI\nsystems for naive integration in high-stakes societal decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A surge of recent work explores the ethical and societal implications of\nlarge-scale AI models that make \"moral\" judgments. Much of this literature\nfocuses either on alignment with human judgments through various thought\nexperiments or on the group fairness implications of AI judgments. However, the\nmost immediate and likely use of AI is to help or fully replace the so-called\nstreet-level bureaucrats, the individuals deciding to allocate scarce social\nresources or approve benefits. There is a rich history underlying how\nprinciples of local justice determine how society decides on prioritization\nmechanisms in such domains. In this paper, we examine how well LLM judgments\nalign with human judgments, as well as with socially and politically determined\nvulnerability scoring systems currently used in the domain of homelessness\nresource allocation. Crucially, we use real data on those needing services\n(maintaining strict confidentiality by only using local large models) to\nperform our analyses. We find that LLM prioritizations are extremely\ninconsistent in several ways: internally on different runs, between different\nLLMs, and between LLMs and the vulnerability scoring systems. At the same time,\nLLMs demonstrate qualitative consistency with lay human judgments in pairwise\ntesting. Findings call into question the readiness of current generation AI\nsystems for naive integration in high-stakes societal decision-making."
                },
                "authors": [
                    {
                        "name": "Gaurab Pokharel"
                    },
                    {
                        "name": "Shafkat Farabi"
                    },
                    {
                        "name": "Patrick J. Fowler"
                    },
                    {
                        "name": "Sanmay Das"
                    }
                ],
                "author_detail": {
                    "name": "Sanmay Das"
                },
                "author": "Sanmay Das",
                "arxiv_comment": "This work has been accepted for publication as a full paper at the\n  AAAI/ACM Conference on AI, Ethics, and Society (AIES 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08193v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08193v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04260v1",
                "updated": "2025-09-04T14:38:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    14,
                    38,
                    28,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T14:38:28Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    14,
                    38,
                    28,
                    3,
                    247,
                    0
                ],
                "title": "An Empirical Study of Vulnerabilities in Python Packages and Their\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study of Vulnerabilities in Python Packages and Their\n  Detection"
                },
                "summary": "In the rapidly evolving software development landscape, Python stands out for\nits simplicity, versatility, and extensive ecosystem. Python packages, as units\nof organization, reusability, and distribution, have become a pressing concern,\nhighlighted by the considerable number of vulnerability reports. As a scripting\nlanguage, Python often cooperates with other languages for performance or\ninteroperability. This adds complexity to the vulnerabilities inherent to\nPython packages, and the effectiveness of current vulnerability detection tools\nremains underexplored. This paper addresses these gaps by introducing PyVul,\nthe first comprehensive benchmark suite of Python-package vulnerabilities.\nPyVul includes 1,157 publicly reported, developer-verified vulnerabilities,\neach linked to its affected packages. To accommodate diverse detection\ntechniques, it provides annotations at both commit and function levels. An\nLLM-assisted data cleansing method is incorporated to improve label accuracy,\nachieving 100% commit-level and 94% function-level accuracy, establishing PyVul\nas the most precise large-scale Python vulnerability benchmark. We further\ncarry out a distribution analysis of PyVul, which demonstrates that\nvulnerabilities in Python packages involve multiple programming languages and\nexhibit a wide variety of types. Moreover, our analysis reveals that\nmulti-lingual Python packages are potentially more susceptible to\nvulnerabilities. Evaluation of state-of-the-art detectors using this benchmark\nreveals a significant discrepancy between the capabilities of existing tools\nand the demands of effectively identifying real-world security issues in Python\npackages. Additionally, we conduct an empirical review of the top-ranked CWEs\nobserved in Python packages, to diagnose the fine-grained limitations of\ncurrent detection tools and highlight the necessity for future advancements in\nthe field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving software development landscape, Python stands out for\nits simplicity, versatility, and extensive ecosystem. Python packages, as units\nof organization, reusability, and distribution, have become a pressing concern,\nhighlighted by the considerable number of vulnerability reports. As a scripting\nlanguage, Python often cooperates with other languages for performance or\ninteroperability. This adds complexity to the vulnerabilities inherent to\nPython packages, and the effectiveness of current vulnerability detection tools\nremains underexplored. This paper addresses these gaps by introducing PyVul,\nthe first comprehensive benchmark suite of Python-package vulnerabilities.\nPyVul includes 1,157 publicly reported, developer-verified vulnerabilities,\neach linked to its affected packages. To accommodate diverse detection\ntechniques, it provides annotations at both commit and function levels. An\nLLM-assisted data cleansing method is incorporated to improve label accuracy,\nachieving 100% commit-level and 94% function-level accuracy, establishing PyVul\nas the most precise large-scale Python vulnerability benchmark. We further\ncarry out a distribution analysis of PyVul, which demonstrates that\nvulnerabilities in Python packages involve multiple programming languages and\nexhibit a wide variety of types. Moreover, our analysis reveals that\nmulti-lingual Python packages are potentially more susceptible to\nvulnerabilities. Evaluation of state-of-the-art detectors using this benchmark\nreveals a significant discrepancy between the capabilities of existing tools\nand the demands of effectively identifying real-world security issues in Python\npackages. Additionally, we conduct an empirical review of the top-ranked CWEs\nobserved in Python packages, to diagnose the fine-grained limitations of\ncurrent detection tools and highlight the necessity for future advancements in\nthe field."
                },
                "authors": [
                    {
                        "name": "Haowei Quan"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Xinzhe Li"
                    },
                    {
                        "name": "Terry Yue Zhuo"
                    },
                    {
                        "name": "Xiao Chen"
                    },
                    {
                        "name": "Xiaoning Du"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoning Du"
                },
                "author": "Xiaoning Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13958v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13958v2",
                "updated": "2025-09-04T14:24:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    14,
                    24,
                    19,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-21T06:25:21Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    6,
                    25,
                    21,
                    1,
                    21,
                    0
                ],
                "title": "A Survey of Graph Retrieval-Augmented Generation for Customized Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Graph Retrieval-Augmented Generation for Customized Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in a\nwide range of tasks, yet their application to specialized domains remains\nchallenging due to the need for deep expertise. Retrieval-Augmented generation\n(RAG) has emerged as a promising solution to customize LLMs for professional\nfields by seamlessly integrating external knowledge bases, enabling real-time\naccess to domain-specific expertise during inference. Despite its potential,\ntraditional RAG systems, based on flat text retrieval, face three critical\nchallenges: (i) complex query understanding in professional contexts, (ii)\ndifficulties in knowledge integration across distributed sources, and (iii)\nsystem efficiency bottlenecks at scale. This survey presents a systematic\nanalysis of Graph-based Retrieval-Augmented Generation (GraphRAG), a new\nparadigm that revolutionizes domain-specific LLM applications. GraphRAG\naddresses traditional RAG limitations through three key innovations: (i)\ngraph-structured knowledge representation that explicitly captures entity\nrelationships and domain hierarchies, (ii) efficient graph-based retrieval\ntechniques that enable context-preserving knowledge retrieval with multihop\nreasoning ability, and (iii) structure-aware knowledge integration algorithms\nthat leverage retrieved knowledge for accurate and logical coherent generation\nof LLMs. In this survey, we systematically analyze the technical foundations of\nGraphRAG and examine current implementations across various professional\ndomains, identifying key technical challenges and promising research\ndirections. All the related resources of GraphRAG, including research papers,\nopen-source data, and projects, are collected for the community in\nhttps://github.com/DEEP-PolyU/Awesome-GraphRAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in a\nwide range of tasks, yet their application to specialized domains remains\nchallenging due to the need for deep expertise. Retrieval-Augmented generation\n(RAG) has emerged as a promising solution to customize LLMs for professional\nfields by seamlessly integrating external knowledge bases, enabling real-time\naccess to domain-specific expertise during inference. Despite its potential,\ntraditional RAG systems, based on flat text retrieval, face three critical\nchallenges: (i) complex query understanding in professional contexts, (ii)\ndifficulties in knowledge integration across distributed sources, and (iii)\nsystem efficiency bottlenecks at scale. This survey presents a systematic\nanalysis of Graph-based Retrieval-Augmented Generation (GraphRAG), a new\nparadigm that revolutionizes domain-specific LLM applications. GraphRAG\naddresses traditional RAG limitations through three key innovations: (i)\ngraph-structured knowledge representation that explicitly captures entity\nrelationships and domain hierarchies, (ii) efficient graph-based retrieval\ntechniques that enable context-preserving knowledge retrieval with multihop\nreasoning ability, and (iii) structure-aware knowledge integration algorithms\nthat leverage retrieved knowledge for accurate and logical coherent generation\nof LLMs. In this survey, we systematically analyze the technical foundations of\nGraphRAG and examine current implementations across various professional\ndomains, identifying key technical challenges and promising research\ndirections. All the related resources of GraphRAG, including research papers,\nopen-source data, and projects, are collected for the community in\nhttps://github.com/DEEP-PolyU/Awesome-GraphRAG."
                },
                "authors": [
                    {
                        "name": "Qinggang Zhang"
                    },
                    {
                        "name": "Shengyuan Chen"
                    },
                    {
                        "name": "Yuanchen Bei"
                    },
                    {
                        "name": "Zheng Yuan"
                    },
                    {
                        "name": "Huachi Zhou"
                    },
                    {
                        "name": "Zijin Hong"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Yilin Xiao"
                    },
                    {
                        "name": "Chuang Zhou"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Xiao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Huang"
                },
                "author": "Xiao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13958v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13958v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04250v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04250v1",
                "updated": "2025-09-04T14:23:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    14,
                    23,
                    35,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T14:23:35Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    14,
                    23,
                    35,
                    3,
                    247,
                    0
                ],
                "title": "How many patients could we save with LLM priors?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How many patients could we save with LLM priors?"
                },
                "summary": "Imagine a world where clinical trials need far fewer patients to achieve the\nsame statistical power, thanks to the knowledge encoded in large language\nmodels (LLMs). We present a novel framework for hierarchical Bayesian modeling\nof adverse events in multi-center clinical trials, leveraging LLM-informed\nprior distributions. Unlike data augmentation approaches that generate\nsynthetic data points, our methodology directly obtains parametric priors from\nthe model. Our approach systematically elicits informative priors for\nhyperparameters in hierarchical Bayesian models using a pre-trained LLM,\nenabling the incorporation of external clinical expertise directly into\nBayesian safety modeling. Through comprehensive temperature sensitivity\nanalysis and rigorous cross-validation on real-world clinical trial data, we\ndemonstrate that LLM-derived priors consistently improve predictive performance\ncompared to traditional meta-analytical approaches. This methodology paves the\nway for more efficient and expert-informed clinical trial design, enabling\nsubstantial reductions in the number of patients required to achieve robust\nsafety assessment and with the potential to transform drug safety monitoring\nand regulatory decision making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imagine a world where clinical trials need far fewer patients to achieve the\nsame statistical power, thanks to the knowledge encoded in large language\nmodels (LLMs). We present a novel framework for hierarchical Bayesian modeling\nof adverse events in multi-center clinical trials, leveraging LLM-informed\nprior distributions. Unlike data augmentation approaches that generate\nsynthetic data points, our methodology directly obtains parametric priors from\nthe model. Our approach systematically elicits informative priors for\nhyperparameters in hierarchical Bayesian models using a pre-trained LLM,\nenabling the incorporation of external clinical expertise directly into\nBayesian safety modeling. Through comprehensive temperature sensitivity\nanalysis and rigorous cross-validation on real-world clinical trial data, we\ndemonstrate that LLM-derived priors consistently improve predictive performance\ncompared to traditional meta-analytical approaches. This methodology paves the\nway for more efficient and expert-informed clinical trial design, enabling\nsubstantial reductions in the number of patients required to achieve robust\nsafety assessment and with the potential to transform drug safety monitoring\nand regulatory decision making."
                },
                "authors": [
                    {
                        "name": "Shota Arai"
                    },
                    {
                        "name": "David Selby"
                    },
                    {
                        "name": "Andrew Vargo"
                    },
                    {
                        "name": "Sebastian Vollmer"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Vollmer"
                },
                "author": "Sebastian Vollmer",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04250v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11701v2",
                "updated": "2025-09-04T14:12:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    14,
                    12,
                    36,
                    3,
                    247,
                    0
                ],
                "published": "2025-05-16T21:09:36Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    21,
                    9,
                    36,
                    4,
                    136,
                    0
                ],
                "title": "DMN-Guided Prompting: A Framework for Controlling LLM Behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DMN-Guided Prompting: A Framework for Controlling LLM Behavior"
                },
                "summary": "Large Language Models (LLMs) have shown considerable potential in automating\ndecision logic within knowledge-intensive processes. However, their\neffectiveness largely depends on the strategy and quality of prompting. Since\ndecision logic is typically embedded in prompts, it becomes challenging for end\nusers to modify or refine it. Decision Model and Notation (DMN) offers a\nstandardized graphical approach for defining decision logic in a structured,\nuser-friendly manner. This paper introduces a DMN-guided prompting framework\nthat breaks down complex decision logic into smaller, manageable components,\nguiding LLMs through structured decision pathways. We implemented the framework\nin a graduate-level course where students submitted assignments. The\nassignments and DMN models representing feedback instructions served as inputs\nto our framework. The instructor evaluated the generated feedback and labeled\nit for performance assessment. Our approach demonstrated promising results,\noutperforming chain-of-thought (CoT) prompting in our case study. Students also\nresponded positively to the generated feedback, reporting high levels of\nperceived usefulness in a survey based on the Technology Acceptance Model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown considerable potential in automating\ndecision logic within knowledge-intensive processes. However, their\neffectiveness largely depends on the strategy and quality of prompting. Since\ndecision logic is typically embedded in prompts, it becomes challenging for end\nusers to modify or refine it. Decision Model and Notation (DMN) offers a\nstandardized graphical approach for defining decision logic in a structured,\nuser-friendly manner. This paper introduces a DMN-guided prompting framework\nthat breaks down complex decision logic into smaller, manageable components,\nguiding LLMs through structured decision pathways. We implemented the framework\nin a graduate-level course where students submitted assignments. The\nassignments and DMN models representing feedback instructions served as inputs\nto our framework. The instructor evaluated the generated feedback and labeled\nit for performance assessment. Our approach demonstrated promising results,\noutperforming chain-of-thought (CoT) prompting in our case study. Students also\nresponded positively to the generated feedback, reporting high levels of\nperceived usefulness in a survey based on the Technology Acceptance Model."
                },
                "authors": [
                    {
                        "name": "Shaghayegh Abedi"
                    },
                    {
                        "name": "Amin Jalali"
                    }
                ],
                "author_detail": {
                    "name": "Amin Jalali"
                },
                "author": "Amin Jalali",
                "arxiv_comment": "Large Language Models, Decision Model and Notation, Automated\n  Feedback, Prompt Engineering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04231v1",
                "updated": "2025-09-04T14:08:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    14,
                    8,
                    18,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T14:08:18Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    14,
                    8,
                    18,
                    3,
                    247,
                    0
                ],
                "title": "Conformalized Multiple Testing under Unknown Null Distribution with\n  Symmetric Errors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformalized Multiple Testing under Unknown Null Distribution with\n  Symmetric Errors"
                },
                "summary": "This article addresses a fundamental concern, first raised by Efron (2004),\nregarding the selection of null distributions in large-scale multiple testing.\nIn modern data-intensive applications involving thousands or even millions of\nhypotheses, the theoretical null distribution of the test statistics often\ndeviates from the true underlying null distribution, severely compromising the\nfalse discovery rate (FDR) analysis. We propose a conformalized empirical Bayes\nmethod using self-calibrated empirical null samples (SENS) for both one-sample\nand two-sample multiple testing problems. The new framework not only sidesteps\nthe use of potentially erroneous theoretical null distributions, which is\ncommon in conventional practice, but also mitigates the impact of estimation\nerrors in the unknown null distribution on the validity of FDR control, a\nchallenge frequently encountered in the empirical Bayes FDR literature. In\ncontrast to the empirical Bayes approaches (cf. Efron, 2004; Jin and Cai, 2007;\nSun and Cai, 2007) that rely on Gaussian assumptions for the null models, SENS\nimposes only a weak condition on the symmetry of the error distribution, and\nleverages conformal tools to achieve FDR control in finite samples. Moreover,\nSENS incorporates structural insights from empirical Bayes into inference,\nexhibiting higher power compared to frequentist model-free methods. We conduct\nan in-depth analysis to establish a novel optimality theory for SENS under\nEfron's two-group model and demonstrate its superiority over existing empirical\nBayes FDR methods and recent model-free FDR methods through numerical\nexperiments on both simulated and real data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article addresses a fundamental concern, first raised by Efron (2004),\nregarding the selection of null distributions in large-scale multiple testing.\nIn modern data-intensive applications involving thousands or even millions of\nhypotheses, the theoretical null distribution of the test statistics often\ndeviates from the true underlying null distribution, severely compromising the\nfalse discovery rate (FDR) analysis. We propose a conformalized empirical Bayes\nmethod using self-calibrated empirical null samples (SENS) for both one-sample\nand two-sample multiple testing problems. The new framework not only sidesteps\nthe use of potentially erroneous theoretical null distributions, which is\ncommon in conventional practice, but also mitigates the impact of estimation\nerrors in the unknown null distribution on the validity of FDR control, a\nchallenge frequently encountered in the empirical Bayes FDR literature. In\ncontrast to the empirical Bayes approaches (cf. Efron, 2004; Jin and Cai, 2007;\nSun and Cai, 2007) that rely on Gaussian assumptions for the null models, SENS\nimposes only a weak condition on the symmetry of the error distribution, and\nleverages conformal tools to achieve FDR control in finite samples. Moreover,\nSENS incorporates structural insights from empirical Bayes into inference,\nexhibiting higher power compared to frequentist model-free methods. We conduct\nan in-depth analysis to establish a novel optimality theory for SENS under\nEfron's two-group model and demonstrate its superiority over existing empirical\nBayes FDR methods and recent model-free FDR methods through numerical\nexperiments on both simulated and real data."
                },
                "authors": [
                    {
                        "name": "Yang Tian"
                    },
                    {
                        "name": "Zinan Zhao"
                    },
                    {
                        "name": "Wenguang Sun"
                    }
                ],
                "author_detail": {
                    "name": "Wenguang Sun"
                },
                "author": "Wenguang Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15161v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15161v4",
                "updated": "2025-09-04T14:07:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    14,
                    7,
                    56,
                    3,
                    247,
                    0
                ],
                "published": "2024-07-21T13:33:08Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    13,
                    33,
                    8,
                    6,
                    203,
                    0
                ],
                "title": "FFHFlow: Diverse and Uncertainty-Aware Dexterous Grasp Generation via\n  Flow Variational Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FFHFlow: Diverse and Uncertainty-Aware Dexterous Grasp Generation via\n  Flow Variational Inference"
                },
                "summary": "Synthesizing diverse, uncertainty-aware grasps for multi-fingered hands from\npartial observations remains a critical challenge in robot learning. Prior\ngenerative methods struggle to model the intricate grasp distribution of\ndexterous hands and often fail to reason about shape uncertainty inherent in\npartial point clouds, leading to unreliable or overly conservative grasps. We\npropose FFHFlow, a flow-based variational framework that generates diverse,\nrobust multi-finger grasps while explicitly quantifying perceptual uncertainty\nin the partial point clouds. Our approach leverages a normalizing flow-based\ndeep latent variable model to learn a hierarchical grasp manifold, overcoming\nthe mode collapse and rigid prior limitations of conditional Variational\nAutoencoders (cVAEs). By exploiting the invertibility and exact likelihoods of\nflows, FFHFlow introspects shape uncertainty in partial observations and\nidentifies novel object structures, enabling risk-aware grasp synthesis. To\nfurther enhance reliability, we integrate a discriminative grasp evaluator with\nthe flow likelihoods, formulating an uncertainty-aware ranking strategy that\nprioritizes grasps robust to shape ambiguity. Extensive experiments in\nsimulation and real-world setups demonstrate that FFHFlow outperforms\nstate-of-the-art baselines (including diffusion models) in grasp diversity and\nsuccess rate, while achieving run-time efficient sampling. We also showcase its\npractical value in cluttered and confined environments, where diversity-driven\nsampling excels by mitigating collisions (Project Page:\nhttps://sites.google.com/view/ffhflow/home/).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing diverse, uncertainty-aware grasps for multi-fingered hands from\npartial observations remains a critical challenge in robot learning. Prior\ngenerative methods struggle to model the intricate grasp distribution of\ndexterous hands and often fail to reason about shape uncertainty inherent in\npartial point clouds, leading to unreliable or overly conservative grasps. We\npropose FFHFlow, a flow-based variational framework that generates diverse,\nrobust multi-finger grasps while explicitly quantifying perceptual uncertainty\nin the partial point clouds. Our approach leverages a normalizing flow-based\ndeep latent variable model to learn a hierarchical grasp manifold, overcoming\nthe mode collapse and rigid prior limitations of conditional Variational\nAutoencoders (cVAEs). By exploiting the invertibility and exact likelihoods of\nflows, FFHFlow introspects shape uncertainty in partial observations and\nidentifies novel object structures, enabling risk-aware grasp synthesis. To\nfurther enhance reliability, we integrate a discriminative grasp evaluator with\nthe flow likelihoods, formulating an uncertainty-aware ranking strategy that\nprioritizes grasps robust to shape ambiguity. Extensive experiments in\nsimulation and real-world setups demonstrate that FFHFlow outperforms\nstate-of-the-art baselines (including diffusion models) in grasp diversity and\nsuccess rate, while achieving run-time efficient sampling. We also showcase its\npractical value in cluttered and confined environments, where diversity-driven\nsampling excels by mitigating collisions (Project Page:\nhttps://sites.google.com/view/ffhflow/home/)."
                },
                "authors": [
                    {
                        "name": "Qian Feng"
                    },
                    {
                        "name": "Jianxiang Feng"
                    },
                    {
                        "name": "Zhaopeng Chen"
                    },
                    {
                        "name": "Rudolph Triebel"
                    },
                    {
                        "name": "Alois Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois Knoll"
                },
                "author": "Alois Knoll",
                "arxiv_comment": "First two authors contributed equally, whose ordering decided via\n  coin-tossing. Accepted for CoRL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15161v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15161v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03562v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03562v2",
                "updated": "2025-09-04T14:07:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    14,
                    7,
                    26,
                    3,
                    247,
                    0
                ],
                "published": "2024-11-05T23:55:23Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    23,
                    55,
                    23,
                    1,
                    310,
                    0
                ],
                "title": "Kolb-Based Experiential Learning for Generalist Agents with Human-Level\n  Kaggle Data Science Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kolb-Based Experiential Learning for Generalist Agents with Human-Level\n  Kaggle Data Science Performance"
                },
                "summary": "Human expertise emerges through iterative cycles of interaction, reflection,\nand internal model updating, which are central to cognitive theories such as\nKolb's experiential learning and Vygotsky's zone of proximal development. In\ncontrast, current AI systems, particularly LLM agents, rely on static\npre-training or rigid workflows, lacking mechanisms for continual adaptation.\nRecent studies identified early cognitive traits in LLM agents (reflection,\nrevision, and self-correction) suggesting foundational elements of human-like\nexperiential learning. Thus the key question: Can we design LLM agents capable\nof structured, cognitively grounded learning similar to human processes? In\nresponse, we propose a computational framework of Kolb's learning cycle with\nVygotsky's ZPD for autonomous agents. Our architecture separates extrinsic\n(environment interaction) and intrinsic (internal reflection/abstraction)\nfunctions, enabling cognitively grounded scaffolded learning, where the agent\ninitially learns within structured environments, followed by open-ended\ngeneralisation. This approach empowers agents to master complex tasks ; domains\nthat traditional fine-tuning or simple reflective methods could not tackle\neffectively. Its potential is powerfully demonstrated via direct comparison\nwith humans in real-world Kaggle data science competitions. Learning fully\nautomated data science code generation across 81 tasks, our system, Agent K,\ndemonstrated the ability to perform the entire workflow autonomously, achieving\nan Elo-MMR score of 1694, beyond median score of the Kaggle Masters (the top 2%\namong 200,000 users) of our study. With 9 gold, 8 silver, and 12 bronze medals\nlevel performance - including 4 gold and 4 silver on prize-awarding\ncompetitions - Agent K is the 1st AI system to successfully integrate Kolb- and\nVygotsky-inspired human cognitive learning, marking a major step toward\ngeneralist AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human expertise emerges through iterative cycles of interaction, reflection,\nand internal model updating, which are central to cognitive theories such as\nKolb's experiential learning and Vygotsky's zone of proximal development. In\ncontrast, current AI systems, particularly LLM agents, rely on static\npre-training or rigid workflows, lacking mechanisms for continual adaptation.\nRecent studies identified early cognitive traits in LLM agents (reflection,\nrevision, and self-correction) suggesting foundational elements of human-like\nexperiential learning. Thus the key question: Can we design LLM agents capable\nof structured, cognitively grounded learning similar to human processes? In\nresponse, we propose a computational framework of Kolb's learning cycle with\nVygotsky's ZPD for autonomous agents. Our architecture separates extrinsic\n(environment interaction) and intrinsic (internal reflection/abstraction)\nfunctions, enabling cognitively grounded scaffolded learning, where the agent\ninitially learns within structured environments, followed by open-ended\ngeneralisation. This approach empowers agents to master complex tasks ; domains\nthat traditional fine-tuning or simple reflective methods could not tackle\neffectively. Its potential is powerfully demonstrated via direct comparison\nwith humans in real-world Kaggle data science competitions. Learning fully\nautomated data science code generation across 81 tasks, our system, Agent K,\ndemonstrated the ability to perform the entire workflow autonomously, achieving\nan Elo-MMR score of 1694, beyond median score of the Kaggle Masters (the top 2%\namong 200,000 users) of our study. With 9 gold, 8 silver, and 12 bronze medals\nlevel performance - including 4 gold and 4 silver on prize-awarding\ncompetitions - Agent K is the 1st AI system to successfully integrate Kolb- and\nVygotsky-inspired human cognitive learning, marking a major step toward\ngeneralist AI."
                },
                "authors": [
                    {
                        "name": "Antoine Grosnit"
                    },
                    {
                        "name": "Alexandre Maraval"
                    },
                    {
                        "name": "Refinath S N"
                    },
                    {
                        "name": "Zichao Zhao"
                    },
                    {
                        "name": "James Dora"
                    },
                    {
                        "name": "Giuseppe Paolo"
                    },
                    {
                        "name": "Albert Thomas"
                    },
                    {
                        "name": "Jonas Gonzalez"
                    },
                    {
                        "name": "Abhineet Kumar"
                    },
                    {
                        "name": "Khyati Khandelwal"
                    },
                    {
                        "name": "Abdelhakim Benechehab"
                    },
                    {
                        "name": "Hamza Cherkaoui"
                    },
                    {
                        "name": "Youssef Attia El-Hili"
                    },
                    {
                        "name": "Kun Shao"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Jun Yao"
                    },
                    {
                        "name": "Balzs Kgl"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03562v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03562v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11110v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11110v4",
                "updated": "2025-09-04T14:01:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    14,
                    1,
                    21,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-19T16:53:26Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    16,
                    53,
                    26,
                    6,
                    19,
                    0
                ],
                "title": "Chain-of-Reasoning: Towards Unified Mathematical Reasoning in Large\n  Language Models via a Multi-Paradigm Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Reasoning: Towards Unified Mathematical Reasoning in Large\n  Language Models via a Multi-Paradigm Perspective"
                },
                "summary": "Large Language Models (LLMs) have made notable progress in mathematical\nreasoning, yet often rely on single-paradigm reasoning, limiting their\neffectiveness across diverse tasks. We introduce Chain-of-Reasoning (CoR), a\nnovel unified framework integrating multiple reasoning paradigms--Natural\nLanguage Reasoning (NLR), Algorithmic Reasoning (AR), and Symbolic Reasoning\n(SR)--to enable synergistic collaboration. CoR generates multiple potential\nanswers via different reasoning paradigms and synthesizes them into a coherent\nfinal solution. We propose a Progressive Paradigm Training (PPT) strategy for\nmodels to progressively master these paradigms, leading to CoR-Math-7B.\nExperimental results demonstrate that CoR-Math-7B significantly outperforms\ncurrent SOTA models, achieving up to a 41.0% absolute improvement over GPT-4o\nin theorem proving and a 15.0% improvement over RL-based methods on the MATH\nbenchmark in arithmetic tasks. These results show the enhanced mathematical\ncomprehension ability of our model, enabling zero-shot generalization across\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made notable progress in mathematical\nreasoning, yet often rely on single-paradigm reasoning, limiting their\neffectiveness across diverse tasks. We introduce Chain-of-Reasoning (CoR), a\nnovel unified framework integrating multiple reasoning paradigms--Natural\nLanguage Reasoning (NLR), Algorithmic Reasoning (AR), and Symbolic Reasoning\n(SR)--to enable synergistic collaboration. CoR generates multiple potential\nanswers via different reasoning paradigms and synthesizes them into a coherent\nfinal solution. We propose a Progressive Paradigm Training (PPT) strategy for\nmodels to progressively master these paradigms, leading to CoR-Math-7B.\nExperimental results demonstrate that CoR-Math-7B significantly outperforms\ncurrent SOTA models, achieving up to a 41.0% absolute improvement over GPT-4o\nin theorem proving and a 15.0% improvement over RL-based methods on the MATH\nbenchmark in arithmetic tasks. These results show the enhanced mathematical\ncomprehension ability of our model, enabling zero-shot generalization across\ntasks."
                },
                "authors": [
                    {
                        "name": "Yiyao Yu"
                    },
                    {
                        "name": "Yuxiang Zhang"
                    },
                    {
                        "name": "Dongdong Zhang"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Hengyuan Zhang"
                    },
                    {
                        "name": "Xingxing Zhang"
                    },
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Mahmoud Khademi"
                    },
                    {
                        "name": "Hany Awadalla"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "Accepted to ACL 2025 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11110v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11110v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04223v1",
                "updated": "2025-09-04T13:54:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    54,
                    25,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T13:54:25Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    54,
                    25,
                    3,
                    247,
                    0
                ],
                "title": "Making neural networks understand internal heat transfer using\n  Fourier-transformed thermal diffusion wave fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making neural networks understand internal heat transfer using\n  Fourier-transformed thermal diffusion wave fields"
                },
                "summary": "Heat propagation is governed by phonon interactions and mathematically\ndescribed by partial differential equations (PDEs), which link thermal\ntransport to the intrinsic properties of materials. Conventional experimental\ntechniques infer thermal responses based on surface emissions, limiting their\nability to fully resolve subsurface structures and internal heat distribution.\nAdditionally, existing thermal tomographic techniques can only shoot one frame\nfrom each layer. Physics-informed neural networks (PINNs) have recently emerged\nas powerful tools for solving inverse problems in heat transfer by integrating\nobservational data with physical constraints. However, standard PINNs are\nprimarily focused on fitting the given external temperature data, without\nexplicit knowledge of the unknown internal temperature distribution. In this\nstudy, we introduce a Helmholtz-informed neural network (HINN) to predict\ninternal temperature distributions without requiring internal measurements. The\ntime-domain heat diffusion equation was converted to the frequency-domain and\nbecomes the pseudo-Helmholtz equation. HINN embeds this pseudo-Helmholtz\nequation into the learning framework, leveraging both real and imaginary\ncomponents of the thermal field. Finally, an inverse Fourier transform brings\nreal-part and imagery-part back to the time-domain and can be used to map 3D\nthermal fields with interior defects. Furthermore, a truncated operation was\nconducted to improve computational efficiency, and the principle of conjugate\nsymmetry was employed for repairing the discarded data. This approach\nsignificantly enhances predictive accuracy and computational efficiency. Our\nresults demonstrate that HINN outperforms state-of-the-art PINNs and inverse\nheat solvers, offering a novel solution for non-invasive thermography in\napplications spanning materials science, biomedical diagnostics, and\nnondestructive evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heat propagation is governed by phonon interactions and mathematically\ndescribed by partial differential equations (PDEs), which link thermal\ntransport to the intrinsic properties of materials. Conventional experimental\ntechniques infer thermal responses based on surface emissions, limiting their\nability to fully resolve subsurface structures and internal heat distribution.\nAdditionally, existing thermal tomographic techniques can only shoot one frame\nfrom each layer. Physics-informed neural networks (PINNs) have recently emerged\nas powerful tools for solving inverse problems in heat transfer by integrating\nobservational data with physical constraints. However, standard PINNs are\nprimarily focused on fitting the given external temperature data, without\nexplicit knowledge of the unknown internal temperature distribution. In this\nstudy, we introduce a Helmholtz-informed neural network (HINN) to predict\ninternal temperature distributions without requiring internal measurements. The\ntime-domain heat diffusion equation was converted to the frequency-domain and\nbecomes the pseudo-Helmholtz equation. HINN embeds this pseudo-Helmholtz\nequation into the learning framework, leveraging both real and imaginary\ncomponents of the thermal field. Finally, an inverse Fourier transform brings\nreal-part and imagery-part back to the time-domain and can be used to map 3D\nthermal fields with interior defects. Furthermore, a truncated operation was\nconducted to improve computational efficiency, and the principle of conjugate\nsymmetry was employed for repairing the discarded data. This approach\nsignificantly enhances predictive accuracy and computational efficiency. Our\nresults demonstrate that HINN outperforms state-of-the-art PINNs and inverse\nheat solvers, offering a novel solution for non-invasive thermography in\napplications spanning materials science, biomedical diagnostics, and\nnondestructive evaluation."
                },
                "authors": [
                    {
                        "name": "Pengfei Zhu"
                    },
                    {
                        "name": "Hai Zhang"
                    },
                    {
                        "name": "Clemente Ibarra-Castanedo"
                    },
                    {
                        "name": "Xavier Maldague"
                    },
                    {
                        "name": "Andreas Mandelis"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Mandelis"
                },
                "author": "Andreas Mandelis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13287v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13287v6",
                "updated": "2025-09-04T13:51:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    51,
                    56,
                    3,
                    247,
                    0
                ],
                "published": "2024-10-17T07:33:35Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    7,
                    33,
                    35,
                    3,
                    291,
                    0
                ],
                "title": "PAK-UCB Contextual Bandit: An Online Learning Approach to Prompt-Aware\n  Selection of Generative Models and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAK-UCB Contextual Bandit: An Online Learning Approach to Prompt-Aware\n  Selection of Generative Models and LLMs"
                },
                "summary": "Selecting a sample generation scheme from multiple prompt-based generative\nmodels, including large language models (LLMs) and prompt-guided image and\nvideo generation models, is typically addressed by choosing the model that\nmaximizes an averaged evaluation score. However, this score-based selection\noverlooks the possibility that different models achieve the best generation\nperformance for different types of text prompts. An online identification of\nthe best generation model for various input prompts can reduce the costs\nassociated with querying sub-optimal models. In this work, we explore the\npossibility of varying rankings of text-based generative models for different\ntext prompts and propose an online learning framework to predict the best data\ngeneration model for a given input prompt. The proposed PAK-UCB algorithm\naddresses a contextual bandit (CB) setting with shared context variables across\nthe arms, utilizing the generated data to update kernel-based functions that\npredict the score of each model available for unseen text prompts.\nAdditionally, we leverage random Fourier features (RFF) to accelerate the\nonline learning process of PAK-UCB. Our numerical experiments on real and\nsimulated text-to-image and image-to-text generative models show that RFF-UCB\nperforms successfully in identifying the best generation model across different\nsample types. The code is available at:\ngithub.com/yannxiaoyanhu/dgm-online-select.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selecting a sample generation scheme from multiple prompt-based generative\nmodels, including large language models (LLMs) and prompt-guided image and\nvideo generation models, is typically addressed by choosing the model that\nmaximizes an averaged evaluation score. However, this score-based selection\noverlooks the possibility that different models achieve the best generation\nperformance for different types of text prompts. An online identification of\nthe best generation model for various input prompts can reduce the costs\nassociated with querying sub-optimal models. In this work, we explore the\npossibility of varying rankings of text-based generative models for different\ntext prompts and propose an online learning framework to predict the best data\ngeneration model for a given input prompt. The proposed PAK-UCB algorithm\naddresses a contextual bandit (CB) setting with shared context variables across\nthe arms, utilizing the generated data to update kernel-based functions that\npredict the score of each model available for unseen text prompts.\nAdditionally, we leverage random Fourier features (RFF) to accelerate the\nonline learning process of PAK-UCB. Our numerical experiments on real and\nsimulated text-to-image and image-to-text generative models show that RFF-UCB\nperforms successfully in identifying the best generation model across different\nsample types. The code is available at:\ngithub.com/yannxiaoyanhu/dgm-online-select."
                },
                "authors": [
                    {
                        "name": "Xiaoyan Hu"
                    },
                    {
                        "name": "Ho-fung Leung"
                    },
                    {
                        "name": "Farzan Farnia"
                    }
                ],
                "author_detail": {
                    "name": "Farzan Farnia"
                },
                "author": "Farzan Farnia",
                "arxiv_comment": "accepted to ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13287v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13287v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00401v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00401v2",
                "updated": "2025-09-04T13:51:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    51,
                    30,
                    3,
                    247,
                    0
                ],
                "published": "2025-08-01T08:02:35Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    2,
                    35,
                    4,
                    213,
                    0
                ],
                "title": "Theory of Mind Using Active Inference: A Framework for Multi-Agent\n  Cooperation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theory of Mind Using Active Inference: A Framework for Multi-Agent\n  Cooperation"
                },
                "summary": "Theory of Mind (ToM) -- the ability to understand that others can have\ndiffering knowledge and goals -- enables agents to reason about others' beliefs\nwhile planning their own actions. We present a novel approach to multi-agent\ncooperation by implementing ToM within active inference. Unlike previous active\ninference approaches to multi-agent cooperation, our method neither relies on\ntask-specific shared generative models nor requires explicit communication. In\nour framework, ToM-equipped agents maintain distinct representations of their\nown and others' beliefs and goals. ToM agents then use an extended and adapted\nversion of the sophisticated inference tree-based planning algorithm to\nsystematically explore joint policy spaces through recursive reasoning. We\nevaluate our approach through collision avoidance and foraging simulations.\nResults suggest that ToM agents cooperate better compared to non-ToM\ncounterparts by being able to avoid collisions and reduce redundant efforts.\nCrucially, ToM agents accomplish this by inferring others' beliefs solely from\nobservable behaviour and considering them when planning their own actions. Our\napproach shows potential for generalisable and scalable multi-agent systems\nwhile providing computational insights into ToM mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theory of Mind (ToM) -- the ability to understand that others can have\ndiffering knowledge and goals -- enables agents to reason about others' beliefs\nwhile planning their own actions. We present a novel approach to multi-agent\ncooperation by implementing ToM within active inference. Unlike previous active\ninference approaches to multi-agent cooperation, our method neither relies on\ntask-specific shared generative models nor requires explicit communication. In\nour framework, ToM-equipped agents maintain distinct representations of their\nown and others' beliefs and goals. ToM agents then use an extended and adapted\nversion of the sophisticated inference tree-based planning algorithm to\nsystematically explore joint policy spaces through recursive reasoning. We\nevaluate our approach through collision avoidance and foraging simulations.\nResults suggest that ToM agents cooperate better compared to non-ToM\ncounterparts by being able to avoid collisions and reduce redundant efforts.\nCrucially, ToM agents accomplish this by inferring others' beliefs solely from\nobservable behaviour and considering them when planning their own actions. Our\napproach shows potential for generalisable and scalable multi-agent systems\nwhile providing computational insights into ToM mechanisms."
                },
                "authors": [
                    {
                        "name": "Riddhi J. Pitliya"
                    },
                    {
                        "name": "Ozan atal"
                    },
                    {
                        "name": "Toon Van de Maele"
                    },
                    {
                        "name": "Corrado Pezzato"
                    },
                    {
                        "name": "Tim Verbelen"
                    }
                ],
                "author_detail": {
                    "name": "Tim Verbelen"
                },
                "author": "Tim Verbelen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00401v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00401v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04198v1",
                "updated": "2025-09-04T13:22:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    22,
                    44,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T13:22:44Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    22,
                    44,
                    3,
                    247,
                    0
                ],
                "title": "Are LLM Agents the New RPA? A Comparative Study with RPA Across\n  Enterprise Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLM Agents the New RPA? A Comparative Study with RPA Across\n  Enterprise Workflows"
                },
                "summary": "The emergence of large language models (LLMs) has introduced a new paradigm\nin automation: LLM agents or Agentic Automation with Computer Use (AACU).\nUnlike traditional Robotic Process Automation (RPA), which relies on rule-based\nworkflows and scripting, AACU enables intelligent agents to perform tasks\nthrough natural language instructions and autonomous interaction with user\ninterfaces. This study investigates whether AACU can serve as a viable\nalternative to RPA in enterprise workflow automation. We conducted controlled\nexperiments across three standard RPA challenges data entry, monitoring, and\ndocument extraction comparing RPA (via UiPath) and AACU (via Anthropic's\nComputer Use Agent) in terms of speed, reliability, and development effort.\nResults indicate that RPA outperforms AACU in execution speed and reliability,\nparticularly in repetitive, stable environments. However, AACU significantly\nreduces development time and adapts more flexibly to dynamic interfaces. While\ncurrent AACU implementations are not yet production-ready, their promise in\nrapid prototyping and lightweight automation is evident. Future research should\nexplore multi-agent orchestration, hybrid RPA-AACU architectures, and more\nrobust evaluation across industries and platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) has introduced a new paradigm\nin automation: LLM agents or Agentic Automation with Computer Use (AACU).\nUnlike traditional Robotic Process Automation (RPA), which relies on rule-based\nworkflows and scripting, AACU enables intelligent agents to perform tasks\nthrough natural language instructions and autonomous interaction with user\ninterfaces. This study investigates whether AACU can serve as a viable\nalternative to RPA in enterprise workflow automation. We conducted controlled\nexperiments across three standard RPA challenges data entry, monitoring, and\ndocument extraction comparing RPA (via UiPath) and AACU (via Anthropic's\nComputer Use Agent) in terms of speed, reliability, and development effort.\nResults indicate that RPA outperforms AACU in execution speed and reliability,\nparticularly in repetitive, stable environments. However, AACU significantly\nreduces development time and adapts more flexibly to dynamic interfaces. While\ncurrent AACU implementations are not yet production-ready, their promise in\nrapid prototyping and lightweight automation is evident. Future research should\nexplore multi-agent orchestration, hybrid RPA-AACU architectures, and more\nrobust evaluation across industries and platforms."
                },
                "authors": [
                    {
                        "name": "Petr Prcha"
                    },
                    {
                        "name": "Michaela Matoukov"
                    },
                    {
                        "name": "Jan Strnad"
                    }
                ],
                "author_detail": {
                    "name": "Jan Strnad"
                },
                "author": "Jan Strnad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v2",
                "updated": "2025-09-04T13:14:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    14,
                    33,
                    3,
                    247,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04191v1",
                "updated": "2025-09-04T13:13:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    13,
                    57,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T13:13:57Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    13,
                    57,
                    3,
                    247,
                    0
                ],
                "title": "KubeGuard: LLM-Assisted Kubernetes Hardening via Configuration Files and\n  Runtime Logs Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KubeGuard: LLM-Assisted Kubernetes Hardening via Configuration Files and\n  Runtime Logs Analysis"
                },
                "summary": "The widespread adoption of Kubernetes (K8s) for orchestrating cloud-native\napplications has introduced significant security challenges, such as\nmisconfigured resources and overly permissive configurations. Failing to\naddress these issues can result in unauthorized access, privilege escalation,\nand lateral movement within clusters. Most existing K8s security solutions\nfocus on detecting misconfigurations, typically through static analysis or\nanomaly detection. In contrast, this paper presents KubeGuard, a novel runtime\nlog-driven recommender framework aimed at mitigating risks by addressing overly\npermissive configurations. KubeGuard is designed to harden K8s environments\nthrough two complementary tasks: Resource Creation and Resource Refinement. It\nleverages large language models (LLMs) to analyze manifests and runtime logs\nreflecting actual system behavior, using modular prompt-chaining workflows.\nThis approach enables KubeGuard to create least-privilege configurations for\nnew resources and refine existing manifests to reduce the attack surface.\nKubeGuard's output manifests are presented as recommendations that users (e.g.,\ndevelopers and operators) can review and adopt to enhance cluster security. Our\nevaluation demonstrates that KubeGuard effectively generates and refines K8s\nmanifests for Roles, NetworkPolicies, and Deployments, leveraging both\nproprietary and open-source LLMs. The high precision, recall, and F1-scores\naffirm KubeGuard's practicality as a framework that translates runtime\nobservability into actionable, least-privilege configuration guidance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of Kubernetes (K8s) for orchestrating cloud-native\napplications has introduced significant security challenges, such as\nmisconfigured resources and overly permissive configurations. Failing to\naddress these issues can result in unauthorized access, privilege escalation,\nand lateral movement within clusters. Most existing K8s security solutions\nfocus on detecting misconfigurations, typically through static analysis or\nanomaly detection. In contrast, this paper presents KubeGuard, a novel runtime\nlog-driven recommender framework aimed at mitigating risks by addressing overly\npermissive configurations. KubeGuard is designed to harden K8s environments\nthrough two complementary tasks: Resource Creation and Resource Refinement. It\nleverages large language models (LLMs) to analyze manifests and runtime logs\nreflecting actual system behavior, using modular prompt-chaining workflows.\nThis approach enables KubeGuard to create least-privilege configurations for\nnew resources and refine existing manifests to reduce the attack surface.\nKubeGuard's output manifests are presented as recommendations that users (e.g.,\ndevelopers and operators) can review and adopt to enhance cluster security. Our\nevaluation demonstrates that KubeGuard effectively generates and refines K8s\nmanifests for Roles, NetworkPolicies, and Deployments, leveraging both\nproprietary and open-source LLMs. The high precision, recall, and F1-scores\naffirm KubeGuard's practicality as a framework that translates runtime\nobservability into actionable, least-privilege configuration guidance."
                },
                "authors": [
                    {
                        "name": "Omri Sgan Cohen"
                    },
                    {
                        "name": "Ehud Malul"
                    },
                    {
                        "name": "Yair Meidan"
                    },
                    {
                        "name": "Dudu Mimran"
                    },
                    {
                        "name": "Yuval Elovici"
                    },
                    {
                        "name": "Asaf Shabtai"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Shabtai"
                },
                "author": "Asaf Shabtai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13744v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13744v8",
                "updated": "2025-09-04T13:08:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    8,
                    42,
                    3,
                    247,
                    0
                ],
                "published": "2025-02-19T14:07:37Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    7,
                    37,
                    2,
                    50,
                    0
                ],
                "title": "The Risk-Neutral Equivalent Pricing of Model-Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Risk-Neutral Equivalent Pricing of Model-Uncertainty"
                },
                "summary": "Existing approaches to asset-pricing under model-uncertainty adapt classical\nutility-maximization frameworks and seek theoretical comprehensiveness. We move\ntoward practice by considering binary model-risks and by emphasizing\n'constraints' over 'preference'. This decomposes viable economic asset-pricing\ninto that of model and non-model risks separately, leading to a unique and\nconvenient model-risk pricing formula. Its parameter, a dynamically conserved\nconstant of model-risk inference, allows an integrated representation of\nex-ante risk-pricing and bias such that their ex-post impacts are disentangled\nvia well-known anomalies, Momentum and Low-Risk, whose risk-reward patterns\nacquire a fresh significance: peak-reward reveals ex-ante risk-premia, and\npeak-location, bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing approaches to asset-pricing under model-uncertainty adapt classical\nutility-maximization frameworks and seek theoretical comprehensiveness. We move\ntoward practice by considering binary model-risks and by emphasizing\n'constraints' over 'preference'. This decomposes viable economic asset-pricing\ninto that of model and non-model risks separately, leading to a unique and\nconvenient model-risk pricing formula. Its parameter, a dynamically conserved\nconstant of model-risk inference, allows an integrated representation of\nex-ante risk-pricing and bias such that their ex-post impacts are disentangled\nvia well-known anomalies, Momentum and Low-Risk, whose risk-reward patterns\nacquire a fresh significance: peak-reward reveals ex-ante risk-premia, and\npeak-location, bias."
                },
                "authors": [
                    {
                        "name": "Ken Kangda Wren"
                    }
                ],
                "author_detail": {
                    "name": "Ken Kangda Wren"
                },
                "author": "Ken Kangda Wren",
                "arxiv_comment": "20 pages of main text (including title and abstract), 8 pages of\n  Appendix and Bibliography",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13744v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13744v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.MF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.MF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04185v1",
                "updated": "2025-09-04T13:02:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    2,
                    39,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T13:02:39Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    2,
                    39,
                    3,
                    247,
                    0
                ],
                "title": "Set Block Decoding is a Language Model Inference Accelerator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Set Block Decoding is a Language Model Inference Accelerator"
                },
                "summary": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training."
                },
                "authors": [
                    {
                        "name": "Itai Gat"
                    },
                    {
                        "name": "Heli Ben-Hamu"
                    },
                    {
                        "name": "Marton Havasi"
                    },
                    {
                        "name": "Daniel Haziza"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Gabriel Synnaeve"
                    },
                    {
                        "name": "David Lopez-Paz"
                    },
                    {
                        "name": "Brian Karrer"
                    },
                    {
                        "name": "Yaron Lipman"
                    }
                ],
                "author_detail": {
                    "name": "Yaron Lipman"
                },
                "author": "Yaron Lipman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04183v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04183v1",
                "updated": "2025-09-04T12:59:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    59,
                    24,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T12:59:24Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    59,
                    24,
                    3,
                    247,
                    0
                ],
                "title": "MAGneT: Coordinated Multi-Agent Generation of Synthetic Multi-Turn\n  Mental Health Counseling Sessions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGneT: Coordinated Multi-Agent Generation of Synthetic Multi-Turn\n  Mental Health Counseling Sessions"
                },
                "summary": "The growing demand for scalable psychological counseling highlights the need\nfor fine-tuning open-source Large Language Models (LLMs) with high-quality,\nprivacy-compliant data, yet such data remains scarce. Here we introduce MAGneT,\na novel multi-agent framework for synthetic psychological counseling session\ngeneration that decomposes counselor response generation into coordinated\nsub-tasks handled by specialized LLM agents, each modeling a key psychological\ntechnique. Unlike prior single-agent approaches, MAGneT better captures the\nstructure and nuance of real counseling. In addition, we address\ninconsistencies in prior evaluation protocols by proposing a unified evaluation\nframework integrating diverse automatic and expert metrics. Furthermore, we\nexpand the expert evaluations from four aspects of counseling in previous works\nto nine aspects, enabling a more thorough and robust assessment of data\nquality. Empirical results show that MAGneT significantly outperforms existing\nmethods in quality, diversity, and therapeutic alignment of the generated\ncounseling sessions, improving general counseling skills by 3.2% and\nCBT-specific skills by 4.3% on average on cognitive therapy rating scale\n(CTRS). Crucially, experts prefer MAGneT-generated sessions in 77.2% of cases\non average across all aspects. Moreover, fine-tuning an open-source model on\nMAGneT-generated sessions shows better performance, with improvements of 6.3%\non general counseling skills and 7.3% on CBT-specific skills on average on CTRS\nover those fine-tuned with sessions generated by baseline methods. We also make\nour code and data public.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for scalable psychological counseling highlights the need\nfor fine-tuning open-source Large Language Models (LLMs) with high-quality,\nprivacy-compliant data, yet such data remains scarce. Here we introduce MAGneT,\na novel multi-agent framework for synthetic psychological counseling session\ngeneration that decomposes counselor response generation into coordinated\nsub-tasks handled by specialized LLM agents, each modeling a key psychological\ntechnique. Unlike prior single-agent approaches, MAGneT better captures the\nstructure and nuance of real counseling. In addition, we address\ninconsistencies in prior evaluation protocols by proposing a unified evaluation\nframework integrating diverse automatic and expert metrics. Furthermore, we\nexpand the expert evaluations from four aspects of counseling in previous works\nto nine aspects, enabling a more thorough and robust assessment of data\nquality. Empirical results show that MAGneT significantly outperforms existing\nmethods in quality, diversity, and therapeutic alignment of the generated\ncounseling sessions, improving general counseling skills by 3.2% and\nCBT-specific skills by 4.3% on average on cognitive therapy rating scale\n(CTRS). Crucially, experts prefer MAGneT-generated sessions in 77.2% of cases\non average across all aspects. Moreover, fine-tuning an open-source model on\nMAGneT-generated sessions shows better performance, with improvements of 6.3%\non general counseling skills and 7.3% on CBT-specific skills on average on CTRS\nover those fine-tuned with sessions generated by baseline methods. We also make\nour code and data public."
                },
                "authors": [
                    {
                        "name": "Aishik Mandal"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "arxiv_comment": "25 pages, 29 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04183v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04183v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05338v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05338v3",
                "updated": "2025-09-04T12:56:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    56,
                    16,
                    3,
                    247,
                    0
                ],
                "published": "2024-12-06T02:12:22Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    2,
                    12,
                    22,
                    4,
                    341,
                    0
                ],
                "title": "Theoretical Radio Signals from Radio-Band Gravitational Waves Converted\n  from the Neutron Star Magnetic Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theoretical Radio Signals from Radio-Band Gravitational Waves Converted\n  from the Neutron Star Magnetic Field"
                },
                "summary": "Gravitational waves (GWs) can convert into electromagnetic waves in the\npresence of a magnetic field via the Gertsenshtein-Zeldovich (GZ) effect. The\ncharacteristics of the magnetic field substantially affect this conversion\nprobability. This paper confirms that strong magnetic fields in neutron stars\nsignificantly enhance the conversion probability, facilitating detectable radio\nsignatures of very high-frequency (VHF,\n$\\left(10^6-10^{11}\\mathrm{~Hz}\\right)$) gravitational waves. We theoretically\nidentify two distinct signatures using single-dish telescopes (FAST, TMRT, QTT,\nGBT) and interferometers (SKA1/2-MID): transient signals from burst-like\ngravitational wave sources and persistent signals from cosmological background\ngravitational wave sources. These signatures are mapped to graviton spectral\nlines derived from quantum field theory by incorporating spin-2 and mass\nconstraints, resulting in smooth, featureless profiles that are critical for\ndistinguishing gravitational wave signals from astrophysical foregrounds. FAST\nattains a characteristic strain bound of $h_c<10^{-23}$, approaching $10^{-24}$\nin the frequency range of $1-3\\mathrm{~GHz}$ with a 6-hour observation period.\nThis performance exceeds the $5 \\sigma$ detection thresholds for GWs\noriginating from primordial black holes (PBHs) and nears the limits set by Big\nBang nucleosynthesis. Additionally, projections for SKA2-MID indicate even\ngreater sensitivity. Detecting such gravitational waves would improve our\ncomprehension of cosmological models, refine the parameter spaces for\nprimordial black holes, and function as a test for quantum field theory. This\napproach addresses significant deficiencies in VHF GW research, improving\ndetection sensitivity and facilitating the advancement of next-generation radio\ntelescopes such as FASTA and SKA, which feature larger fields of view and\nenhanced gain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational waves (GWs) can convert into electromagnetic waves in the\npresence of a magnetic field via the Gertsenshtein-Zeldovich (GZ) effect. The\ncharacteristics of the magnetic field substantially affect this conversion\nprobability. This paper confirms that strong magnetic fields in neutron stars\nsignificantly enhance the conversion probability, facilitating detectable radio\nsignatures of very high-frequency (VHF,\n$\\left(10^6-10^{11}\\mathrm{~Hz}\\right)$) gravitational waves. We theoretically\nidentify two distinct signatures using single-dish telescopes (FAST, TMRT, QTT,\nGBT) and interferometers (SKA1/2-MID): transient signals from burst-like\ngravitational wave sources and persistent signals from cosmological background\ngravitational wave sources. These signatures are mapped to graviton spectral\nlines derived from quantum field theory by incorporating spin-2 and mass\nconstraints, resulting in smooth, featureless profiles that are critical for\ndistinguishing gravitational wave signals from astrophysical foregrounds. FAST\nattains a characteristic strain bound of $h_c<10^{-23}$, approaching $10^{-24}$\nin the frequency range of $1-3\\mathrm{~GHz}$ with a 6-hour observation period.\nThis performance exceeds the $5 \\sigma$ detection thresholds for GWs\noriginating from primordial black holes (PBHs) and nears the limits set by Big\nBang nucleosynthesis. Additionally, projections for SKA2-MID indicate even\ngreater sensitivity. Detecting such gravitational waves would improve our\ncomprehension of cosmological models, refine the parameter spaces for\nprimordial black holes, and function as a test for quantum field theory. This\napproach addresses significant deficiencies in VHF GW research, improving\ndetection sensitivity and facilitating the advancement of next-generation radio\ntelescopes such as FASTA and SKA, which feature larger fields of view and\nenhanced gain."
                },
                "authors": [
                    {
                        "name": "Wei Hong"
                    },
                    {
                        "name": "Zhen-Zhao Tao"
                    },
                    {
                        "name": "Peng He"
                    },
                    {
                        "name": "Tong-Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong-Jie Zhang"
                },
                "author": "Tong-Jie Zhang",
                "arxiv_doi": "10.3847/1538-4357/adf19a",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/adf19a",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.05338v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05338v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "40 pages, 19 figures, 4 tables. Accepted for publication in ApJ. We\n  infer two novel types of the converted radio signals: transient and\n  persistent signals. Considering the mass and spin of the graviton, the\n  expected spectral line shape of the graviton is derived. FAST is the most\n  sensitive telescope to detect VHFGWs in a single-dish telescope. In addition,\n  SKA2-MID has greater detection potential",
                "arxiv_journal_ref": "ApJ, 990: 156 (29pp), 2025",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04169v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04169v1",
                "updated": "2025-09-04T12:43:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    43,
                    45,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T12:43:45Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    43,
                    45,
                    3,
                    247,
                    0
                ],
                "title": "Privacy Risks in Time Series Forecasting: User- and Record-Level\n  Membership Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy Risks in Time Series Forecasting: User- and Record-Level\n  Membership Inference"
                },
                "summary": "Membership inference attacks (MIAs) aim to determine whether specific data\nwere used to train a model. While extensively studied on classification models,\ntheir impact on time series forecasting remains largely unexplored. We address\nthis gap by introducing two new attacks: (i) an adaptation of multivariate\nLiRA, a state-of-the-art MIA originally developed for classification models, to\nthe time-series forecasting setting, and (ii) a novel end-to-end learning\napproach called Deep Time Series (DTS) attack. We benchmark these methods\nagainst adapted versions of other leading attacks from the classification\nsetting.\n  We evaluate all attacks in realistic settings on the TUH-EEG and ELD\ndatasets, targeting two strong forecasting architectures, LSTM and the\nstate-of-the-art N-HiTS, under both record- and user-level threat models. Our\nresults show that forecasting models are vulnerable, with user-level attacks\noften achieving perfect detection. The proposed methods achieve the strongest\nperformance in several settings, establishing new baselines for privacy risk\nassessment in time series forecasting. Furthermore, vulnerability increases\nwith longer prediction horizons and smaller training populations, echoing\ntrends observed in large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership inference attacks (MIAs) aim to determine whether specific data\nwere used to train a model. While extensively studied on classification models,\ntheir impact on time series forecasting remains largely unexplored. We address\nthis gap by introducing two new attacks: (i) an adaptation of multivariate\nLiRA, a state-of-the-art MIA originally developed for classification models, to\nthe time-series forecasting setting, and (ii) a novel end-to-end learning\napproach called Deep Time Series (DTS) attack. We benchmark these methods\nagainst adapted versions of other leading attacks from the classification\nsetting.\n  We evaluate all attacks in realistic settings on the TUH-EEG and ELD\ndatasets, targeting two strong forecasting architectures, LSTM and the\nstate-of-the-art N-HiTS, under both record- and user-level threat models. Our\nresults show that forecasting models are vulnerable, with user-level attacks\noften achieving perfect detection. The proposed methods achieve the strongest\nperformance in several settings, establishing new baselines for privacy risk\nassessment in time series forecasting. Furthermore, vulnerability increases\nwith longer prediction horizons and smaller training populations, echoing\ntrends observed in large language models."
                },
                "authors": [
                    {
                        "name": "Nicolas Johansson"
                    },
                    {
                        "name": "Tobias Olsson"
                    },
                    {
                        "name": "Daniel Nilsson"
                    },
                    {
                        "name": "Johan stman"
                    },
                    {
                        "name": "Fazeleh Hoseini"
                    }
                ],
                "author_detail": {
                    "name": "Fazeleh Hoseini"
                },
                "arxiv_affiliation": "AI Sweden",
                "author": "Fazeleh Hoseini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04169v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04169v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03951v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03951v3",
                "updated": "2025-09-04T12:43:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    43,
                    29,
                    3,
                    247,
                    0
                ],
                "published": "2024-07-04T14:08:50Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    14,
                    8,
                    50,
                    3,
                    186,
                    0
                ],
                "title": "Uncertainty-Guided Likelihood Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty-Guided Likelihood Tree Search"
                },
                "summary": "Tree search is a fundamental tool for planning, as many sequential\ndecision-making problems can be framed as searching over tree-structured\nspaces. We propose an uncertainty-guided tree search algorithm for settings\nwhere the reward function is a log-likelihood function of the paths. Due to the\ncombinatorial explosion of the tree size, the set of paths for which one can\nobtain rewards is sparse, particularly when the likelihood is obtained through\nexpensive evaluations, such as by querying a large language model. We address\nthis challenge by deriving an probabilistic search heuristic based on\nregularity assumptions for the likelihood. Unlike existing tree search methods,\nthe proposed method can perform backtracking and trade-off exploration with\nexploitation, and yet does not require expensive roll-outs, or sophisticated\nBayesian inference. Through extensive on-model and off-model experiments on\ntimely, large-scale practical applications, we demonstrate that our method\nidentifies paths with high likelihood while requiring fewer costly evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree search is a fundamental tool for planning, as many sequential\ndecision-making problems can be framed as searching over tree-structured\nspaces. We propose an uncertainty-guided tree search algorithm for settings\nwhere the reward function is a log-likelihood function of the paths. Due to the\ncombinatorial explosion of the tree size, the set of paths for which one can\nobtain rewards is sparse, particularly when the likelihood is obtained through\nexpensive evaluations, such as by querying a large language model. We address\nthis challenge by deriving an probabilistic search heuristic based on\nregularity assumptions for the likelihood. Unlike existing tree search methods,\nthe proposed method can perform backtracking and trade-off exploration with\nexploitation, and yet does not require expensive roll-outs, or sophisticated\nBayesian inference. Through extensive on-model and off-model experiments on\ntimely, large-scale practical applications, we demonstrate that our method\nidentifies paths with high likelihood while requiring fewer costly evaluations."
                },
                "authors": [
                    {
                        "name": "Julia Grosse"
                    },
                    {
                        "name": "Ruotian Wu"
                    },
                    {
                        "name": "Ahmad Rashid"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Philipp Hennig"
                    },
                    {
                        "name": "Pascal Poupart"
                    },
                    {
                        "name": "Agustinus Kristiadi"
                    }
                ],
                "author_detail": {
                    "name": "Agustinus Kristiadi"
                },
                "author": "Agustinus Kristiadi",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03951v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03951v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12065v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12065v3",
                "updated": "2025-09-04T12:43:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    43,
                    14,
                    3,
                    247,
                    0
                ],
                "published": "2025-02-17T17:34:48Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    17,
                    34,
                    48,
                    0,
                    48,
                    0
                ],
                "title": "Autoformalization in the Wild: Assessing LLMs on Real-World Mathematical\n  Definitions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoformalization in the Wild: Assessing LLMs on Real-World Mathematical\n  Definitions"
                },
                "summary": "Thanks to their linguistic capabilities, LLMs offer an opportunity to bridge\nthe gap between informal mathematics and formal languages through\nautoformalization. However, it is still unclear how well LLMs generalize to\nsophisticated and naturally occurring mathematical statements. To address this\ngap, we investigate the task of autoformalizing real-world mathematical\ndefinitions: a critical component of mathematical discourse. Specifically, we\nintroduce two novel resources for autoformalization, collecting definitions\nfrom Wikipedia (Def_Wiki) and arXiv papers (Def_ArXiv). We then systematically\nevaluate a range of LLMs, analyzing their ability to formalize definitions into\nIsabelle/HOL. Furthermore, we investigate strategies to enhance LLMs'\nperformance including refinement through external feedback from Proof\nAssistants, and formal definition grounding, where we augment LLMs'\nformalizations through relevant contextual elements from formal mathematical\nlibraries. Our findings reveal that definitions present a greater challenge\ncompared to existing benchmarks, such as miniF2F. In particular, we found that\nLLMs still struggle with self-correction, and aligning with relevant\nmathematical libraries. At the same time, structured refinement methods and\ndefinition grounding strategies yield notable improvements of up to 16% on\nself-correction capabilities and 43% on the reduction of undefined errors,\nhighlighting promising directions for enhancing LLM-based autoformalization in\nreal-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thanks to their linguistic capabilities, LLMs offer an opportunity to bridge\nthe gap between informal mathematics and formal languages through\nautoformalization. However, it is still unclear how well LLMs generalize to\nsophisticated and naturally occurring mathematical statements. To address this\ngap, we investigate the task of autoformalizing real-world mathematical\ndefinitions: a critical component of mathematical discourse. Specifically, we\nintroduce two novel resources for autoformalization, collecting definitions\nfrom Wikipedia (Def_Wiki) and arXiv papers (Def_ArXiv). We then systematically\nevaluate a range of LLMs, analyzing their ability to formalize definitions into\nIsabelle/HOL. Furthermore, we investigate strategies to enhance LLMs'\nperformance including refinement through external feedback from Proof\nAssistants, and formal definition grounding, where we augment LLMs'\nformalizations through relevant contextual elements from formal mathematical\nlibraries. Our findings reveal that definitions present a greater challenge\ncompared to existing benchmarks, such as miniF2F. In particular, we found that\nLLMs still struggle with self-correction, and aligning with relevant\nmathematical libraries. At the same time, structured refinement methods and\ndefinition grounding strategies yield notable improvements of up to 16% on\nself-correction capabilities and 43% on the reduction of undefined errors,\nhighlighting promising directions for enhancing LLM-based autoformalization in\nreal-world scenarios."
                },
                "authors": [
                    {
                        "name": "Lan Zhang"
                    },
                    {
                        "name": "Marco Valentino"
                    },
                    {
                        "name": "Andre Freitas"
                    }
                ],
                "author_detail": {
                    "name": "Andre Freitas"
                },
                "author": "Andre Freitas",
                "arxiv_comment": "EMNLP 2025 Camera-Ready Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12065v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12065v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03317v2",
                "updated": "2025-09-04T12:40:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    40,
                    40,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-03T13:50:45Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    13,
                    50,
                    45,
                    2,
                    246,
                    0
                ],
                "title": "Bayesian Additive Regression Trees for functional ANOVA model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Additive Regression Trees for functional ANOVA model"
                },
                "summary": "Bayesian Additive Regression Trees (BART) is a powerful statistical model\nthat leverages the strengths of Bayesian inference and regression trees. It has\nreceived significant attention for capturing complex non-linear relationships\nand interactions among predictors. However, the accuracy of BART often comes at\nthe cost of interpretability. To address this limitation, we propose ANOVA\nBayesian Additive Regression Trees (ANOVA-BART), a novel extension of BART\nbased on the functional ANOVA decomposition, which is used to decompose the\nvariability of a function into different interactions, each representing the\ncontribution of a different set of covariates or factors. Our proposed\nANOVA-BART enhances interpretability, preserves and extends the theoretical\nguarantees of BART, and achieves superior predictive performance. Specifically,\nwe establish that the posterior concentration rate of ANOVA-BART is nearly\nminimax optimal, and further provides the same convergence rates for each\ninteraction that are not available for BART. Moreover, comprehensive\nexperiments confirm that ANOVA-BART surpasses BART in both accuracy and\nuncertainty quantification, while also demonstrating its effectiveness in\ncomponent selection. These results suggest that ANOVA-BART offers a compelling\nalternative to BART by balancing predictive accuracy, interpretability, and\ntheoretical consistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Additive Regression Trees (BART) is a powerful statistical model\nthat leverages the strengths of Bayesian inference and regression trees. It has\nreceived significant attention for capturing complex non-linear relationships\nand interactions among predictors. However, the accuracy of BART often comes at\nthe cost of interpretability. To address this limitation, we propose ANOVA\nBayesian Additive Regression Trees (ANOVA-BART), a novel extension of BART\nbased on the functional ANOVA decomposition, which is used to decompose the\nvariability of a function into different interactions, each representing the\ncontribution of a different set of covariates or factors. Our proposed\nANOVA-BART enhances interpretability, preserves and extends the theoretical\nguarantees of BART, and achieves superior predictive performance. Specifically,\nwe establish that the posterior concentration rate of ANOVA-BART is nearly\nminimax optimal, and further provides the same convergence rates for each\ninteraction that are not available for BART. Moreover, comprehensive\nexperiments confirm that ANOVA-BART surpasses BART in both accuracy and\nuncertainty quantification, while also demonstrating its effectiveness in\ncomponent selection. These results suggest that ANOVA-BART offers a compelling\nalternative to BART by balancing predictive accuracy, interpretability, and\ntheoretical consistency."
                },
                "authors": [
                    {
                        "name": "Seokhun Park"
                    },
                    {
                        "name": "Insung Kong"
                    },
                    {
                        "name": "Yongdai Kim"
                    }
                ],
                "author_detail": {
                    "name": "Yongdai Kim"
                },
                "author": "Yongdai Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04162v1",
                "updated": "2025-09-04T12:35:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    35,
                    26,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T12:35:26Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    35,
                    26,
                    3,
                    247,
                    0
                ],
                "title": "Real Time FPGA Based Transformers & VLMs for Vision Tasks: SOTA Designs\n  and Optimizations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real Time FPGA Based Transformers & VLMs for Vision Tasks: SOTA Designs\n  and Optimizations"
                },
                "summary": "Transformers and vision-language models (VLMs) have emerged as dominant\narchitectures in computer vision and multimodal AI, offering state-of-the-art\nperformance in tasks such as image classification, object detection, visual\nquestion answering, and caption generation. However, their high computational\ncomplexity, large memory footprints, and irregular data access patterns present\nsignificant challenges for deployment in latency- and power-constrained\nenvironments. Field-programmable gate arrays (FPGAs) provide an attractive\nhardware platform for such workloads due to their reconfigurability,\nfine-grained parallelism, and potential for energy-efficient acceleration. This\npaper presents a comprehensive review of design trade-offs, optimization\nstrategies, and implementation challenges for FPGA-based inference of\ntransformers and VLMs. We examine critical factors such as device-class\nselection, memory subsystem constraints, dataflow orchestration, quantization\nstrategies, sparsity exploitation, and toolchain choices, alongside\nmodality-specific issues unique to VLMs, including heterogeneous compute\nbalancing and cross-attention memory management. Additionally, we discuss\nemerging trends in hardware-algorithm co-design, highlighting innovations in\nattention mechanisms, compression, and modular overlays to improve efficiency\nand adaptability. Practical issues such as runtime flexibility, verification\noverhead, and the absence of standardized FPGA multimodal benchmarks are also\nconsidered. Finally, we outline future directions toward scalable, portable,\nand reconfigurable FPGA solutions that adapt to evolving model architectures\nwhile sustaining high utilization and predictable performance. This synthesis\noffers both a technical foundation and a forward-looking perspective to help\nbridge the gap between advanced multimodal AI models and efficient FPGA\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers and vision-language models (VLMs) have emerged as dominant\narchitectures in computer vision and multimodal AI, offering state-of-the-art\nperformance in tasks such as image classification, object detection, visual\nquestion answering, and caption generation. However, their high computational\ncomplexity, large memory footprints, and irregular data access patterns present\nsignificant challenges for deployment in latency- and power-constrained\nenvironments. Field-programmable gate arrays (FPGAs) provide an attractive\nhardware platform for such workloads due to their reconfigurability,\nfine-grained parallelism, and potential for energy-efficient acceleration. This\npaper presents a comprehensive review of design trade-offs, optimization\nstrategies, and implementation challenges for FPGA-based inference of\ntransformers and VLMs. We examine critical factors such as device-class\nselection, memory subsystem constraints, dataflow orchestration, quantization\nstrategies, sparsity exploitation, and toolchain choices, alongside\nmodality-specific issues unique to VLMs, including heterogeneous compute\nbalancing and cross-attention memory management. Additionally, we discuss\nemerging trends in hardware-algorithm co-design, highlighting innovations in\nattention mechanisms, compression, and modular overlays to improve efficiency\nand adaptability. Practical issues such as runtime flexibility, verification\noverhead, and the absence of standardized FPGA multimodal benchmarks are also\nconsidered. Finally, we outline future directions toward scalable, portable,\nand reconfigurable FPGA solutions that adapt to evolving model architectures\nwhile sustaining high utilization and predictable performance. This synthesis\noffers both a technical foundation and a forward-looking perspective to help\nbridge the gap between advanced multimodal AI models and efficient FPGA\ndeployment."
                },
                "authors": [
                    {
                        "name": "Safa Mohammed Sali"
                    },
                    {
                        "name": "Mahmoud Meribout"
                    },
                    {
                        "name": "Ashiyana Abdul Majeed"
                    }
                ],
                "author_detail": {
                    "name": "Ashiyana Abdul Majeed"
                },
                "author": "Ashiyana Abdul Majeed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.18661v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.18661v2",
                "updated": "2025-09-04T12:28:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    28,
                    54,
                    3,
                    247,
                    0
                ],
                "published": "2024-03-27T15:04:15Z",
                "published_parsed": [
                    2024,
                    3,
                    27,
                    15,
                    4,
                    15,
                    2,
                    87,
                    0
                ],
                "title": "A machine-learning pipeline for real-time detection of gravitational\n  waves from compact binary coalescences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A machine-learning pipeline for real-time detection of gravitational\n  waves from compact binary coalescences"
                },
                "summary": "The promise of multi-messenger astronomy relies on the rapid detection of\ngravitational waves at very low latencies ($\\mathcal{O}$(1\\,s)) in order to\nmaximize the amount of time available for follow-up observations. In recent\nyears, neural-networks have demonstrated robust non-linear modeling\ncapabilities and millisecond-scale inference at a comparatively small\ncomputational footprint, making them an attractive family of algorithms in this\ncontext. However, integration of these algorithms into the gravitational-wave\nastrophysics research ecosystem has proven non-trivial. Here, we present the\nfirst fully machine learning-based pipeline for the detection of gravitational\nwaves from compact binary coalescences (CBCs) running in low-latency. We\ndemonstrate this pipeline to have a fraction of the latency of traditional\nmatched filtering search pipelines while achieving state-of-the-art sensitivity\nto higher-mass stellar binary black holes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promise of multi-messenger astronomy relies on the rapid detection of\ngravitational waves at very low latencies ($\\mathcal{O}$(1\\,s)) in order to\nmaximize the amount of time available for follow-up observations. In recent\nyears, neural-networks have demonstrated robust non-linear modeling\ncapabilities and millisecond-scale inference at a comparatively small\ncomputational footprint, making them an attractive family of algorithms in this\ncontext. However, integration of these algorithms into the gravitational-wave\nastrophysics research ecosystem has proven non-trivial. Here, we present the\nfirst fully machine learning-based pipeline for the detection of gravitational\nwaves from compact binary coalescences (CBCs) running in low-latency. We\ndemonstrate this pipeline to have a fraction of the latency of traditional\nmatched filtering search pipelines while achieving state-of-the-art sensitivity\nto higher-mass stellar binary black holes."
                },
                "authors": [
                    {
                        "name": "Ethan Marx"
                    },
                    {
                        "name": "William Benoit"
                    },
                    {
                        "name": "Alec Gunny"
                    },
                    {
                        "name": "Rafia Omer"
                    },
                    {
                        "name": "Deep Chatterjee"
                    },
                    {
                        "name": "Ricco C. Venterea"
                    },
                    {
                        "name": "Lauren Wills"
                    },
                    {
                        "name": "Muhammed Saleem"
                    },
                    {
                        "name": "Eric Moreno"
                    },
                    {
                        "name": "Ryan Raikman"
                    },
                    {
                        "name": "Ekaterina Govorkova"
                    },
                    {
                        "name": "Dylan Rankin"
                    },
                    {
                        "name": "Michael W. Coughlin"
                    },
                    {
                        "name": "Philip Harris"
                    },
                    {
                        "name": "Erik Katsavounidis"
                    }
                ],
                "author_detail": {
                    "name": "Erik Katsavounidis"
                },
                "author": "Erik Katsavounidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.18661v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.18661v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04153v1",
                "updated": "2025-09-04T12:28:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    28,
                    36,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T12:28:36Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    28,
                    36,
                    3,
                    247,
                    0
                ],
                "title": "Real Time FPGA Based CNNs for Detection, Classification, and Tracking in\n  Autonomous Systems: State of the Art Designs and Optimizations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real Time FPGA Based CNNs for Detection, Classification, and Tracking in\n  Autonomous Systems: State of the Art Designs and Optimizations"
                },
                "summary": "This paper presents a comprehensive review of recent advances in deploying\nconvolutional neural networks (CNNs) for object detection, classification, and\ntracking on Field Programmable Gate Arrays (FPGAs). With the increasing demand\nfor real-time computer vision applications in domains such as autonomous\nvehicles, robotics, and surveillance, FPGAs have emerged as a powerful\nalternative to GPUs and ASICs due to their reconfigurability, low power\nconsumption, and deterministic latency. We critically examine state-of-the-art\nFPGA implementations of CNN-based vision tasks, covering algorithmic\ninnovations, hardware acceleration techniques, and the integration of\noptimization strategies like pruning, quantization, and sparsity-aware methods\nto maximize performance within hardware constraints. This survey also explores\nthe landscape of modern FPGA platforms, including classical LUT-DSP based\narchitectures, System-on-Chip (SoC) FPGAs, and Adaptive Compute Acceleration\nPlatforms (ACAPs), comparing their capabilities in handling deep learning\nworkloads. Furthermore, we review available software development tools such as\nVitis AI, FINN, and Intel FPGA AI Suite, which significantly streamline the\ndesign and deployment of AI models on FPGAs. The paper uniquely discusses\nhybrid architecture that combine GPUs and FPGAs for collaborative acceleration\nof AI inference, addressing challenges related to energy efficiency and\nthroughput. Additionally, we highlight hardware-software co-design practices,\ndataflow optimizations, and pipelined processing techniques essential for\nreal-time inference on resource-constrained devices. Through this survey,\nresearchers and engineers are equipped with insights to develop\nnext-generation, power-efficient, and high-performance vision systems optimized\nfor FPGA deployment in edge and embedded applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive review of recent advances in deploying\nconvolutional neural networks (CNNs) for object detection, classification, and\ntracking on Field Programmable Gate Arrays (FPGAs). With the increasing demand\nfor real-time computer vision applications in domains such as autonomous\nvehicles, robotics, and surveillance, FPGAs have emerged as a powerful\nalternative to GPUs and ASICs due to their reconfigurability, low power\nconsumption, and deterministic latency. We critically examine state-of-the-art\nFPGA implementations of CNN-based vision tasks, covering algorithmic\ninnovations, hardware acceleration techniques, and the integration of\noptimization strategies like pruning, quantization, and sparsity-aware methods\nto maximize performance within hardware constraints. This survey also explores\nthe landscape of modern FPGA platforms, including classical LUT-DSP based\narchitectures, System-on-Chip (SoC) FPGAs, and Adaptive Compute Acceleration\nPlatforms (ACAPs), comparing their capabilities in handling deep learning\nworkloads. Furthermore, we review available software development tools such as\nVitis AI, FINN, and Intel FPGA AI Suite, which significantly streamline the\ndesign and deployment of AI models on FPGAs. The paper uniquely discusses\nhybrid architecture that combine GPUs and FPGAs for collaborative acceleration\nof AI inference, addressing challenges related to energy efficiency and\nthroughput. Additionally, we highlight hardware-software co-design practices,\ndataflow optimizations, and pipelined processing techniques essential for\nreal-time inference on resource-constrained devices. Through this survey,\nresearchers and engineers are equipped with insights to develop\nnext-generation, power-efficient, and high-performance vision systems optimized\nfor FPGA deployment in edge and embedded applications."
                },
                "authors": [
                    {
                        "name": "Safa Mohammed Sali"
                    },
                    {
                        "name": "Mahmoud Meribout"
                    },
                    {
                        "name": "Ashiyana Abdul Majeed"
                    }
                ],
                "author_detail": {
                    "name": "Ashiyana Abdul Majeed"
                },
                "author": "Ashiyana Abdul Majeed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04152v1",
                "updated": "2025-09-04T12:25:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    25,
                    14,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T12:25:14Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    25,
                    14,
                    3,
                    247,
                    0
                ],
                "title": "TAGAL: Tabular Data Generation using Agentic LLM Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAGAL: Tabular Data Generation using Agentic LLM Methods"
                },
                "summary": "The generation of data is a common approach to improve the performance of\nmachine learning tasks, among which is the training of models for\nclassification. In this paper, we present TAGAL, a collection of methods able\nto generate synthetic tabular data using an agentic workflow. The methods\nleverage Large Language Models (LLMs) for an automatic and iterative process\nthat uses feedback to improve the generated data without any further LLM\ntraining. The use of LLMs also allows for the addition of external knowledge in\nthe generation process. We evaluate TAGAL across diverse datasets and different\naspects of quality for the generated data. We look at the utility of downstream\nML models, both by training classifiers on synthetic data only and by combining\nreal and synthetic data. Moreover, we compare the similarities between the real\nand the generated data. We show that TAGAL is able to perform on par with\nstate-of-the-art approaches that require LLM training and generally outperforms\nother training-free approaches. These findings highlight the potential of\nagentic workflow and open new directions for LLM-based data generation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation of data is a common approach to improve the performance of\nmachine learning tasks, among which is the training of models for\nclassification. In this paper, we present TAGAL, a collection of methods able\nto generate synthetic tabular data using an agentic workflow. The methods\nleverage Large Language Models (LLMs) for an automatic and iterative process\nthat uses feedback to improve the generated data without any further LLM\ntraining. The use of LLMs also allows for the addition of external knowledge in\nthe generation process. We evaluate TAGAL across diverse datasets and different\naspects of quality for the generated data. We look at the utility of downstream\nML models, both by training classifiers on synthetic data only and by combining\nreal and synthetic data. Moreover, we compare the similarities between the real\nand the generated data. We show that TAGAL is able to perform on par with\nstate-of-the-art approaches that require LLM training and generally outperforms\nother training-free approaches. These findings highlight the potential of\nagentic workflow and open new directions for LLM-based data generation methods."
                },
                "authors": [
                    {
                        "name": "Benot Ronval"
                    },
                    {
                        "name": "Pierre Dupont"
                    },
                    {
                        "name": "Siegfried Nijssen"
                    }
                ],
                "author_detail": {
                    "name": "Siegfried Nijssen"
                },
                "author": "Siegfried Nijssen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06911v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06911v2",
                "updated": "2025-09-04T12:21:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    21,
                    13,
                    3,
                    247,
                    0
                ],
                "published": "2024-05-11T04:47:50Z",
                "published_parsed": [
                    2024,
                    5,
                    11,
                    4,
                    47,
                    50,
                    5,
                    132,
                    0
                ],
                "title": "Replication Study and Benchmarking of Real-Time Object Detection Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Replication Study and Benchmarking of Real-Time Object Detection Models"
                },
                "summary": "This work examines the reproducibility and benchmarking of state-of-the-art\nreal-time object detection models. As object detection models are often used in\nreal-world contexts, such as robotics, where inference time is paramount,\nsimply measuring models' accuracy is not enough to compare them. We thus\ncompare a large variety of object detection models' accuracy and inference\nspeed on multiple graphics cards. In addition to this large benchmarking\nattempt, we also reproduce the following models from scratch using PyTorch on\nthe MS COCO 2017 dataset: DETR, RTMDet, ViTDet and YOLOv7. More importantly, we\npropose a unified training and evaluation pipeline, based on MMDetection's\nfeatures, to better compare models. Our implementation of DETR and ViTDet could\nnot achieve accuracy or speed performances comparable to what is declared in\nthe original papers. On the other hand, reproduced RTMDet and YOLOv7 could\nmatch such performances. Studied papers are also found to be generally lacking\nfor reproducibility purposes. As for MMDetection pretrained models, speed\nperformances are severely reduced with limited computing resources (larger,\nmore accurate models even more so). Moreover, results exhibit a strong\ntrade-off between accuracy and speed, prevailed by anchor-free models - notably\nRTMDet or YOLOx models. The code used is this paper and all the experiments is\navailable in the repository at https://github.com/willGuimont/segdet_mlcr2024.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work examines the reproducibility and benchmarking of state-of-the-art\nreal-time object detection models. As object detection models are often used in\nreal-world contexts, such as robotics, where inference time is paramount,\nsimply measuring models' accuracy is not enough to compare them. We thus\ncompare a large variety of object detection models' accuracy and inference\nspeed on multiple graphics cards. In addition to this large benchmarking\nattempt, we also reproduce the following models from scratch using PyTorch on\nthe MS COCO 2017 dataset: DETR, RTMDet, ViTDet and YOLOv7. More importantly, we\npropose a unified training and evaluation pipeline, based on MMDetection's\nfeatures, to better compare models. Our implementation of DETR and ViTDet could\nnot achieve accuracy or speed performances comparable to what is declared in\nthe original papers. On the other hand, reproduced RTMDet and YOLOv7 could\nmatch such performances. Studied papers are also found to be generally lacking\nfor reproducibility purposes. As for MMDetection pretrained models, speed\nperformances are severely reduced with limited computing resources (larger,\nmore accurate models even more so). Moreover, results exhibit a strong\ntrade-off between accuracy and speed, prevailed by anchor-free models - notably\nRTMDet or YOLOx models. The code used is this paper and all the experiments is\navailable in the repository at https://github.com/willGuimont/segdet_mlcr2024."
                },
                "authors": [
                    {
                        "name": "Pierre-Luc Asselin"
                    },
                    {
                        "name": "Vincent Coulombe"
                    },
                    {
                        "name": "William Guimont-Martin"
                    },
                    {
                        "name": "William Larrive-Hardy"
                    }
                ],
                "author_detail": {
                    "name": "William Larrive-Hardy"
                },
                "author": "William Larrive-Hardy",
                "arxiv_comment": "Authors are presented in alphabetical order, each having equal\n  contribution to the work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06911v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06911v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09454v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09454v4",
                "updated": "2025-09-04T12:20:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    20,
                    43,
                    3,
                    247,
                    0
                ],
                "published": "2025-03-12T14:57:08Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    57,
                    8,
                    2,
                    71,
                    0
                ],
                "title": "Explicit Learning and the LLM in Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explicit Learning and the LLM in Machine Translation"
                },
                "summary": "This study explores an LLM's ability to learn new languages using\nexplanations found in a grammar book, a process we term \"explicit learning.\" To\nrigorously assess this ability, we design controlled translation experiments\nbetween English and constructed languages generated, through specific\ncryptographic means, from Latin or French. Contrary to previous studies, our\nresults demonstrate that LLMs do possess a measurable capacity for explicit\nlearning. This ability, however, diminishes as the complexity of the linguistic\nphenomena to be learned increases. Supervised fine-tuning on ad hoc chains of\nthought significantly enhances LLM performance but struggles to generalize to\ntypologically novel or more complex linguistic features. These findings point\nto the need for more diverse training sets and alternative fine-tuning\nstrategies to further improve explicit learning by LLMs, benefiting\nlow-resource languages typically described in grammar books but lacking\nextensive corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores an LLM's ability to learn new languages using\nexplanations found in a grammar book, a process we term \"explicit learning.\" To\nrigorously assess this ability, we design controlled translation experiments\nbetween English and constructed languages generated, through specific\ncryptographic means, from Latin or French. Contrary to previous studies, our\nresults demonstrate that LLMs do possess a measurable capacity for explicit\nlearning. This ability, however, diminishes as the complexity of the linguistic\nphenomena to be learned increases. Supervised fine-tuning on ad hoc chains of\nthought significantly enhances LLM performance but struggles to generalize to\ntypologically novel or more complex linguistic features. These findings point\nto the need for more diverse training sets and alternative fine-tuning\nstrategies to further improve explicit learning by LLMs, benefiting\nlow-resource languages typically described in grammar books but lacking\nextensive corpora."
                },
                "authors": [
                    {
                        "name": "Malik Marmonier"
                    },
                    {
                        "name": "Rachel Bawden"
                    },
                    {
                        "name": "Benot Sagot"
                    }
                ],
                "author_detail": {
                    "name": "Benot Sagot"
                },
                "author": "Benot Sagot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09454v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09454v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04145v1",
                "updated": "2025-09-04T12:15:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    15,
                    55,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T12:15:55Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    15,
                    55,
                    3,
                    247,
                    0
                ],
                "title": "Hyper Diffusion Avatars: Dynamic Human Avatar Generation using Network\n  Weight Space Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyper Diffusion Avatars: Dynamic Human Avatar Generation using Network\n  Weight Space Diffusion"
                },
                "summary": "Creating human avatars is a highly desirable yet challenging task. Recent\nadvancements in radiance field rendering have achieved unprecedented\nphotorealism and real-time performance for personalized dynamic human avatars.\nHowever, these approaches are typically limited to person-specific rendering\nmodels trained on multi-view video data for a single individual, limiting their\nability to generalize across different identities. On the other hand,\ngenerative approaches leveraging prior knowledge from pre-trained 2D diffusion\nmodels can produce cartoonish, static human avatars, which are animated through\nsimple skeleton-based articulation. Therefore, the avatars generated by these\nmethods suffer from lower rendering quality compared to person-specific\nrendering methods and fail to capture pose-dependent deformations such as cloth\nwrinkles. In this paper, we propose a novel approach that unites the strengths\nof person-specific rendering and diffusion-based generative modeling to enable\ndynamic human avatar generation with both high photorealism and realistic\npose-dependent deformations. Our method follows a two-stage pipeline: first, we\noptimize a set of person-specific UNets, with each network representing a\ndynamic human avatar that captures intricate pose-dependent deformations. In\nthe second stage, we train a hyper diffusion model over the optimized network\nweights. During inference, our method generates network weights for real-time,\ncontrollable rendering of dynamic human avatars. Using a large-scale,\ncross-identity, multi-view video dataset, we demonstrate that our approach\noutperforms state-of-the-art human avatar generation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating human avatars is a highly desirable yet challenging task. Recent\nadvancements in radiance field rendering have achieved unprecedented\nphotorealism and real-time performance for personalized dynamic human avatars.\nHowever, these approaches are typically limited to person-specific rendering\nmodels trained on multi-view video data for a single individual, limiting their\nability to generalize across different identities. On the other hand,\ngenerative approaches leveraging prior knowledge from pre-trained 2D diffusion\nmodels can produce cartoonish, static human avatars, which are animated through\nsimple skeleton-based articulation. Therefore, the avatars generated by these\nmethods suffer from lower rendering quality compared to person-specific\nrendering methods and fail to capture pose-dependent deformations such as cloth\nwrinkles. In this paper, we propose a novel approach that unites the strengths\nof person-specific rendering and diffusion-based generative modeling to enable\ndynamic human avatar generation with both high photorealism and realistic\npose-dependent deformations. Our method follows a two-stage pipeline: first, we\noptimize a set of person-specific UNets, with each network representing a\ndynamic human avatar that captures intricate pose-dependent deformations. In\nthe second stage, we train a hyper diffusion model over the optimized network\nweights. During inference, our method generates network weights for real-time,\ncontrollable rendering of dynamic human avatars. Using a large-scale,\ncross-identity, multi-view video dataset, we demonstrate that our approach\noutperforms state-of-the-art human avatar generation methods."
                },
                "authors": [
                    {
                        "name": "Dongliang Cao"
                    },
                    {
                        "name": "Guoxing Sun"
                    },
                    {
                        "name": "Marc Habermann"
                    },
                    {
                        "name": "Florian Bernard"
                    }
                ],
                "author_detail": {
                    "name": "Florian Bernard"
                },
                "author": "Florian Bernard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04394v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04394v4",
                "updated": "2025-09-04T12:13:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    13,
                    47,
                    3,
                    247,
                    0
                ],
                "published": "2024-12-05T18:09:41Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    9,
                    41,
                    3,
                    340,
                    0
                ],
                "title": "Bayesian Quantum Amplitude Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Quantum Amplitude Estimation"
                },
                "summary": "We present BAE, a problem-tailored and noise-aware Bayesian algorithm for\nquantum amplitude estimation. In a fault tolerant scenario, BAE is capable of\nsaturating the Heisenberg limit; if device noise is present, BAE can\ndynamically characterize it and self-adapt. We further propose aBAE, an\nannealed variant of BAE drawing on methods from statistical inference, to\nenhance robustness. Our proposals are parallelizable in both quantum and\nclassical components, offer tools for fast noise model assessment, and can\nleverage preexisting information. Additionally, they accommodate experimental\nlimitations and preferred cost trade-offs. We propose a robust benchmark for\namplitude estimation algorithms and use it to test BAE against other\napproaches, demonstrating its competitive performance in both noisy and\nnoiseless scenarios. In both cases, it achieves lower error than any other\nalgorithm as a function of the cost. In the presence of decoherence, it is\ncapable of learning when other algorithms fail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present BAE, a problem-tailored and noise-aware Bayesian algorithm for\nquantum amplitude estimation. In a fault tolerant scenario, BAE is capable of\nsaturating the Heisenberg limit; if device noise is present, BAE can\ndynamically characterize it and self-adapt. We further propose aBAE, an\nannealed variant of BAE drawing on methods from statistical inference, to\nenhance robustness. Our proposals are parallelizable in both quantum and\nclassical components, offer tools for fast noise model assessment, and can\nleverage preexisting information. Additionally, they accommodate experimental\nlimitations and preferred cost trade-offs. We propose a robust benchmark for\namplitude estimation algorithms and use it to test BAE against other\napproaches, demonstrating its competitive performance in both noisy and\nnoiseless scenarios. In both cases, it achieves lower error than any other\nalgorithm as a function of the cost. In the presence of decoherence, it is\ncapable of learning when other algorithms fail."
                },
                "authors": [
                    {
                        "name": "Alexandra Rama"
                    },
                    {
                        "name": "Luis Paulo Santos"
                    }
                ],
                "author_detail": {
                    "name": "Luis Paulo Santos"
                },
                "author": "Luis Paulo Santos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04394v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04394v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04139v1",
                "updated": "2025-09-04T12:11:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    11,
                    3,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T12:11:03Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    11,
                    3,
                    3,
                    247,
                    0
                ],
                "title": "Enhancing Technical Documents Retrieval for RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Technical Documents Retrieval for RAG"
                },
                "summary": "In this paper, we introduce Technical-Embeddings, a novel framework designed\nto optimize semantic retrieval in technical documentation, with applications in\nboth hardware and software development. Our approach addresses the challenges\nof understanding and retrieving complex technical content by leveraging the\ncapabilities of Large Language Models (LLMs). First, we enhance user queries by\ngenerating expanded representations that better capture user intent and improve\ndataset diversity, thereby enriching the fine-tuning process for embedding\nmodels. Second, we apply summary extraction techniques to encode essential\ncontextual information, refining the representation of technical documents. To\nfurther enhance retrieval performance, we fine-tune a bi-encoder BERT model\nusing soft prompting, incorporating separate learning parameters for queries\nand document context to capture fine-grained semantic nuances. We evaluate our\napproach on two public datasets, RAG-EDA and Rust-Docs-QA, demonstrating that\nTechnical-Embeddings significantly outperforms baseline models in both\nprecision and recall. Our findings highlight the effectiveness of integrating\nquery expansion and contextual summarization to enhance information access and\ncomprehension in technical domains. This work advances the state of\nRetrieval-Augmented Generation (RAG) systems, offering new avenues for\nefficient and accurate technical document retrieval in engineering and product\ndevelopment workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Technical-Embeddings, a novel framework designed\nto optimize semantic retrieval in technical documentation, with applications in\nboth hardware and software development. Our approach addresses the challenges\nof understanding and retrieving complex technical content by leveraging the\ncapabilities of Large Language Models (LLMs). First, we enhance user queries by\ngenerating expanded representations that better capture user intent and improve\ndataset diversity, thereby enriching the fine-tuning process for embedding\nmodels. Second, we apply summary extraction techniques to encode essential\ncontextual information, refining the representation of technical documents. To\nfurther enhance retrieval performance, we fine-tune a bi-encoder BERT model\nusing soft prompting, incorporating separate learning parameters for queries\nand document context to capture fine-grained semantic nuances. We evaluate our\napproach on two public datasets, RAG-EDA and Rust-Docs-QA, demonstrating that\nTechnical-Embeddings significantly outperforms baseline models in both\nprecision and recall. Our findings highlight the effectiveness of integrating\nquery expansion and contextual summarization to enhance information access and\ncomprehension in technical domains. This work advances the state of\nRetrieval-Augmented Generation (RAG) systems, offering new avenues for\nefficient and accurate technical document retrieval in engineering and product\ndevelopment workflows."
                },
                "authors": [
                    {
                        "name": "Songjiang Lai"
                    },
                    {
                        "name": "Tsun-Hin Cheung"
                    },
                    {
                        "name": "Ka-Chun Fung"
                    },
                    {
                        "name": "Kaiwen Xue"
                    },
                    {
                        "name": "Kwan-Ho Lin"
                    },
                    {
                        "name": "Yan-Ming Choi"
                    },
                    {
                        "name": "Vincent Ng"
                    },
                    {
                        "name": "Kin-Man Lam"
                    }
                ],
                "author_detail": {
                    "name": "Kin-Man Lam"
                },
                "author": "Kin-Man Lam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00454v2",
                "updated": "2025-09-04T11:57:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    57,
                    46,
                    3,
                    247,
                    0
                ],
                "published": "2025-08-01T09:26:01Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    9,
                    26,
                    1,
                    4,
                    213,
                    0
                ],
                "title": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges"
                },
                "summary": "Evaluating the conversational abilities of large language models (LLMs)\nremains a challenging task. Current mainstream approaches primarily rely on the\n\"LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator to\nassess dialogue quality. However, such methods often suffer from various\nbiases, which undermine the reliability and consistency of the evaluation\nresults. To mitigate these biases, recent methods employ multiple LLMs as\njudges and aggregate their judgments to select the optimal assessment. Although\neffective, this multi-judge approach incurs significant computational overhead\nduring inference. In this paper, we propose an efficient multi-turn dialogue\nevaluator that captures the collective wisdom of multiple LLM judges by\naggregating their preference knowledge into a single model. Our approach\npreserves the advantages of diverse multi-judge feedback while drastically\nreducing the evaluation cost, enabling fast and flexible dialogue quality\nassessment. Extensive experiments on seven single rating and pairwise\ncomparison dialogue evaluation benchmarks demonstrate that our method\noutperforms existing baselines across diverse scenarios, showcasing its\nefficiency and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the conversational abilities of large language models (LLMs)\nremains a challenging task. Current mainstream approaches primarily rely on the\n\"LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator to\nassess dialogue quality. However, such methods often suffer from various\nbiases, which undermine the reliability and consistency of the evaluation\nresults. To mitigate these biases, recent methods employ multiple LLMs as\njudges and aggregate their judgments to select the optimal assessment. Although\neffective, this multi-judge approach incurs significant computational overhead\nduring inference. In this paper, we propose an efficient multi-turn dialogue\nevaluator that captures the collective wisdom of multiple LLM judges by\naggregating their preference knowledge into a single model. Our approach\npreserves the advantages of diverse multi-judge feedback while drastically\nreducing the evaluation cost, enabling fast and flexible dialogue quality\nassessment. Extensive experiments on seven single rating and pairwise\ncomparison dialogue evaluation benchmarks demonstrate that our method\noutperforms existing baselines across diverse scenarios, showcasing its\nefficiency and robustness."
                },
                "authors": [
                    {
                        "name": "Yuqi Tang"
                    },
                    {
                        "name": "Kehua Feng"
                    },
                    {
                        "name": "Yunfeng Wang"
                    },
                    {
                        "name": "Zhiwen Chen"
                    },
                    {
                        "name": "Chengfei Lv"
                    },
                    {
                        "name": "Gang Yu"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Keyan Ding"
                    }
                ],
                "author_detail": {
                    "name": "Keyan Ding"
                },
                "author": "Keyan Ding",
                "arxiv_comment": "15 pages, 2 pages, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08570v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08570v3",
                "updated": "2025-09-04T11:54:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    54,
                    13,
                    3,
                    247,
                    0
                ],
                "published": "2025-06-10T08:37:45Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    8,
                    37,
                    45,
                    1,
                    161,
                    0
                ],
                "title": "Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling\n  Paradigms for Text-to-Music Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling\n  Paradigms for Text-to-Music Generation"
                },
                "summary": "Recent progress in text-to-music generation has enabled models to synthesize\nhigh-quality musical segments, full compositions, and even respond to\nfine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA)\nsystems differ significantly in many dimensions, such as training datasets,\nmodeling paradigms, and architectural choices. This diversity complicates\nefforts to evaluate models fairly and identify which design choices influence\nperformance the most. While factors like data and architecture are important,\nin this study we focus exclusively on the modeling paradigm. We conduct a\nsystematic empirical analysis to isolate its effects, offering insights into\nassociated trade-offs and emergent behaviors that can guide future\ntext-to-music generation systems. Specifically, we compare the two arguably\nmost common modeling paradigms: auto-regressive decoding and conditional\nflow-matching. We conduct a controlled comparison by training all models from\nscratch using identical datasets, training configurations, and similar backbone\narchitectures. Performance is evaluated across multiple axes, including\ngeneration quality, robustness to inference configurations, scalability,\nadherence to both textual and temporally aligned conditioning, and editing\ncapabilities in the form of audio inpainting. This comparative study sheds\nlight on distinct strengths and limitations of each paradigm, providing\nactionable insights that can inform future architectural and training decisions\nin the evolving landscape of text-to-music generation. Audio sampled examples\nare available at: https://huggingface.co/spaces/ortal1602/ARvsFM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in text-to-music generation has enabled models to synthesize\nhigh-quality musical segments, full compositions, and even respond to\nfine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA)\nsystems differ significantly in many dimensions, such as training datasets,\nmodeling paradigms, and architectural choices. This diversity complicates\nefforts to evaluate models fairly and identify which design choices influence\nperformance the most. While factors like data and architecture are important,\nin this study we focus exclusively on the modeling paradigm. We conduct a\nsystematic empirical analysis to isolate its effects, offering insights into\nassociated trade-offs and emergent behaviors that can guide future\ntext-to-music generation systems. Specifically, we compare the two arguably\nmost common modeling paradigms: auto-regressive decoding and conditional\nflow-matching. We conduct a controlled comparison by training all models from\nscratch using identical datasets, training configurations, and similar backbone\narchitectures. Performance is evaluated across multiple axes, including\ngeneration quality, robustness to inference configurations, scalability,\nadherence to both textual and temporally aligned conditioning, and editing\ncapabilities in the form of audio inpainting. This comparative study sheds\nlight on distinct strengths and limitations of each paradigm, providing\nactionable insights that can inform future architectural and training decisions\nin the evolving landscape of text-to-music generation. Audio sampled examples\nare available at: https://huggingface.co/spaces/ortal1602/ARvsFM"
                },
                "authors": [
                    {
                        "name": "Or Tal"
                    },
                    {
                        "name": "Felix Kreuk"
                    },
                    {
                        "name": "Yossi Adi"
                    }
                ],
                "author_detail": {
                    "name": "Yossi Adi"
                },
                "author": "Yossi Adi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08570v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08570v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01909v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01909v2",
                "updated": "2025-09-04T11:54:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    54,
                    6,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-02T03:04:27Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    3,
                    4,
                    27,
                    1,
                    245,
                    0
                ],
                "title": "Oyster-I: Beyond Refusal -- Constructive Safety Alignment for\n  Responsible Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oyster-I: Beyond Refusal -- Constructive Safety Alignment for\n  Responsible Language Models"
                },
                "summary": "Large language models (LLMs) typically deploy safety mechanisms to prevent\nharmful content generation. Most current approaches focus narrowly on risks\nposed by malicious actors, often framing risks as adversarial events and\nrelying on defensive refusals. However, in real-world settings, risks also come\nfrom non-malicious users seeking help while under psychological distress (e.g.,\nself-harm intentions). In such cases, the model's response can strongly\ninfluence the user's next actions. Simple refusals may lead them to repeat,\nescalate, or move to unsafe platforms, creating worse outcomes. We introduce\nConstructive Safety Alignment (CSA), a human-centric paradigm that protects\nagainst malicious misuse while actively guiding vulnerable users toward safe\nand helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic\nanticipation of user reactions, fine-grained risk boundary discovery, and\ninterpretable reasoning control, turning safety into a trust-building process.\nOy1 achieves state-of-the-art safety among open models while retaining high\ngeneral capabilities. On our Constructive Benchmark, it shows strong\nconstructive engagement, close to GPT-5, and unmatched robustness on the\nStrata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from\nrefusal-first to guidance-first safety, CSA redefines the model-user\nrelationship, aiming for systems that are not just safe, but meaningfully\nhelpful. We release Oy1, code, and the benchmark to support responsible,\nuser-centered AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) typically deploy safety mechanisms to prevent\nharmful content generation. Most current approaches focus narrowly on risks\nposed by malicious actors, often framing risks as adversarial events and\nrelying on defensive refusals. However, in real-world settings, risks also come\nfrom non-malicious users seeking help while under psychological distress (e.g.,\nself-harm intentions). In such cases, the model's response can strongly\ninfluence the user's next actions. Simple refusals may lead them to repeat,\nescalate, or move to unsafe platforms, creating worse outcomes. We introduce\nConstructive Safety Alignment (CSA), a human-centric paradigm that protects\nagainst malicious misuse while actively guiding vulnerable users toward safe\nand helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic\nanticipation of user reactions, fine-grained risk boundary discovery, and\ninterpretable reasoning control, turning safety into a trust-building process.\nOy1 achieves state-of-the-art safety among open models while retaining high\ngeneral capabilities. On our Constructive Benchmark, it shows strong\nconstructive engagement, close to GPT-5, and unmatched robustness on the\nStrata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from\nrefusal-first to guidance-first safety, CSA redefines the model-user\nrelationship, aiming for systems that are not just safe, but meaningfully\nhelpful. We release Oy1, code, and the benchmark to support responsible,\nuser-centered AI."
                },
                "authors": [
                    {
                        "name": "Ranjie Duan"
                    },
                    {
                        "name": "Jiexi Liu"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Shiji Zhao"
                    },
                    {
                        "name": "Ruoxi Cheng"
                    },
                    {
                        "name": "Fengxiang Wang"
                    },
                    {
                        "name": "Cheng Wei"
                    },
                    {
                        "name": "Yong Xie"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Defeng Li"
                    },
                    {
                        "name": "Yinpeng Dong"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Yuefeng Chen"
                    },
                    {
                        "name": "Chongwen Wang"
                    },
                    {
                        "name": "Xingjun Ma"
                    },
                    {
                        "name": "Xingxing Wei"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Jun Zhu"
                    },
                    {
                        "name": "Xinfeng Li"
                    },
                    {
                        "name": "Yitong Sun"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Jinzhao Hu"
                    },
                    {
                        "name": "Sha Xu"
                    },
                    {
                        "name": "Yitong Yang"
                    },
                    {
                        "name": "Jialing Tao"
                    },
                    {
                        "name": "Hui Xue"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xue"
                },
                "author": "Hui Xue",
                "arxiv_comment": "Technical Report Code & Model weights available:\n  https://github.com/Alibaba-AAIG/Oyster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01909v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01909v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04126v1",
                "updated": "2025-09-04T11:44:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    44,
                    28,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T11:44:28Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    44,
                    28,
                    3,
                    247,
                    0
                ],
                "title": "MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image\n  Generation"
                },
                "summary": "Text-to-image diffusion models have achieved remarkable image quality, but\nthey still struggle with complex, multiele ment prompts, and limited stylistic\ndiversity. To address these limitations, we propose a Multi-Expert Planning and\nGen eration Framework (MEPG) that synergistically integrates position- and\nstyle-aware large language models (LLMs) with spatial-semantic expert modules.\nThe framework comprises two core components: (1) a Position-Style-Aware (PSA)\nmodule that utilizes a supervised fine-tuned LLM to decom pose input prompts\ninto precise spatial coordinates and style encoded semantic instructions; and\n(2) a Multi-Expert Dif fusion (MED) module that implements cross-region genera\ntion through dynamic expert routing across both local regions and global areas.\nDuring the generation process for each lo cal region, specialized models (e.g.,\nrealism experts, styliza tion specialists) are selectively activated for each\nspatial par tition via attention-based gating mechanisms. The architec ture\nsupports lightweight integration and replacement of ex pert models, providing\nstrong extensibility. Additionally, an interactive interface enables real-time\nspatial layout editing and per-region style selection from a portfolio of\nexperts. Ex periments show that MEPG significantly outperforms base line models\nwith the same backbone in both image quality\n  and style diversity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image diffusion models have achieved remarkable image quality, but\nthey still struggle with complex, multiele ment prompts, and limited stylistic\ndiversity. To address these limitations, we propose a Multi-Expert Planning and\nGen eration Framework (MEPG) that synergistically integrates position- and\nstyle-aware large language models (LLMs) with spatial-semantic expert modules.\nThe framework comprises two core components: (1) a Position-Style-Aware (PSA)\nmodule that utilizes a supervised fine-tuned LLM to decom pose input prompts\ninto precise spatial coordinates and style encoded semantic instructions; and\n(2) a Multi-Expert Dif fusion (MED) module that implements cross-region genera\ntion through dynamic expert routing across both local regions and global areas.\nDuring the generation process for each lo cal region, specialized models (e.g.,\nrealism experts, styliza tion specialists) are selectively activated for each\nspatial par tition via attention-based gating mechanisms. The architec ture\nsupports lightweight integration and replacement of ex pert models, providing\nstrong extensibility. Additionally, an interactive interface enables real-time\nspatial layout editing and per-region style selection from a portfolio of\nexperts. Ex periments show that MEPG significantly outperforms base line models\nwith the same backbone in both image quality\n  and style diversity."
                },
                "authors": [
                    {
                        "name": "Yuan Zhao"
                    },
                    {
                        "name": "Liu Lin"
                    }
                ],
                "author_detail": {
                    "name": "Liu Lin"
                },
                "author": "Liu Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05248v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05248v3",
                "updated": "2025-09-04T11:42:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    42,
                    4,
                    3,
                    247,
                    0
                ],
                "published": "2024-12-06T18:27:15Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    27,
                    15,
                    4,
                    341,
                    0
                ],
                "title": "Enhancing FKG.in: automating Indian food composition analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing FKG.in: automating Indian food composition analysis"
                },
                "summary": "This paper presents a novel approach to compute food composition data for\nIndian recipes using a knowledge graph for Indian food (FKG[.]in) and LLMs. The\nprimary focus is to provide a broad overview of an automated food composition\nanalysis workflow and describe its core functionalities: nutrition data\naggregation, food composition analysis, and LLM-augmented information\nresolution. This workflow aims to complement FKG[.]in and iteratively\nsupplement food composition data from verified knowledge bases. Additionally,\nthis paper highlights the challenges of representing Indian food and accessing\nfood composition data digitally. It also reviews three key sources of food\ncomposition data: the Indian Food Composition Tables, the Indian Nutrient\nDatabank, and the Nutritionix API. Furthermore, it briefly outlines how users\ncan interact with the workflow to obtain diet-based health recommendations and\ndetailed food composition information for numerous recipes. We then explore the\ncomplex challenges of analyzing Indian recipe information across dimensions\nsuch as structure, multilingualism, and uncertainty as well as present our\nongoing work on LLM-based solutions to address these issues. The methods\nproposed in this workshop paper for AI-driven knowledge curation and\ninformation resolution are application-agnostic, generalizable, and replicable\nfor any domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach to compute food composition data for\nIndian recipes using a knowledge graph for Indian food (FKG[.]in) and LLMs. The\nprimary focus is to provide a broad overview of an automated food composition\nanalysis workflow and describe its core functionalities: nutrition data\naggregation, food composition analysis, and LLM-augmented information\nresolution. This workflow aims to complement FKG[.]in and iteratively\nsupplement food composition data from verified knowledge bases. Additionally,\nthis paper highlights the challenges of representing Indian food and accessing\nfood composition data digitally. It also reviews three key sources of food\ncomposition data: the Indian Food Composition Tables, the Indian Nutrient\nDatabank, and the Nutritionix API. Furthermore, it briefly outlines how users\ncan interact with the workflow to obtain diet-based health recommendations and\ndetailed food composition information for numerous recipes. We then explore the\ncomplex challenges of analyzing Indian recipe information across dimensions\nsuch as structure, multilingualism, and uncertainty as well as present our\nongoing work on LLM-based solutions to address these issues. The methods\nproposed in this workshop paper for AI-driven knowledge curation and\ninformation resolution are application-agnostic, generalizable, and replicable\nfor any domain."
                },
                "authors": [
                    {
                        "name": "Saransh Kumar Gupta"
                    },
                    {
                        "name": "Lipika Dey"
                    },
                    {
                        "name": "Partha Pratim Das"
                    },
                    {
                        "name": "Geeta Trilok-Kumar"
                    },
                    {
                        "name": "Ramesh Jain"
                    }
                ],
                "author_detail": {
                    "name": "Ramesh Jain"
                },
                "author": "Ramesh Jain",
                "arxiv_doi": "10.1007/978-3-031-88217-3_6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-88217-3_6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.05248v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05248v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 5 figures, 30 references, International Conference on\n  Pattern Recognition 2024 - Multimedia Assisted Dietary Management Workshop",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04123v1",
                "updated": "2025-09-04T11:37:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    37,
                    6,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T11:37:06Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    37,
                    6,
                    3,
                    247,
                    0
                ],
                "title": "TaleDiffusion: Multi-Character Story Generation with Dialogue Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TaleDiffusion: Multi-Character Story Generation with Dialogue Rendering"
                },
                "summary": "Text-to-story visualization is challenging due to the need for consistent\ninteraction among multiple characters across frames. Existing methods struggle\nwith character consistency, leading to artifact generation and inaccurate\ndialogue rendering, which results in disjointed storytelling. In response, we\nintroduce TaleDiffusion, a novel framework for generating multi-character\nstories with an iterative process, maintaining character consistency, and\naccurate dialogue assignment via postprocessing. Given a story, we use a\npre-trained LLM to generate per-frame descriptions, character details, and\ndialogues via in-context learning, followed by a bounded attention-based\nper-box mask technique to control character interactions and minimize\nartifacts. We then apply an identity-consistent self-attention mechanism to\nensure character consistency across frames and region-aware cross-attention for\nprecise object placement. Dialogues are also rendered as bubbles and assigned\nto characters via CLIPSeg. Experimental results demonstrate that TaleDiffusion\noutperforms existing methods in consistency, noise reduction, and dialogue\nrendering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-story visualization is challenging due to the need for consistent\ninteraction among multiple characters across frames. Existing methods struggle\nwith character consistency, leading to artifact generation and inaccurate\ndialogue rendering, which results in disjointed storytelling. In response, we\nintroduce TaleDiffusion, a novel framework for generating multi-character\nstories with an iterative process, maintaining character consistency, and\naccurate dialogue assignment via postprocessing. Given a story, we use a\npre-trained LLM to generate per-frame descriptions, character details, and\ndialogues via in-context learning, followed by a bounded attention-based\nper-box mask technique to control character interactions and minimize\nartifacts. We then apply an identity-consistent self-attention mechanism to\nensure character consistency across frames and region-aware cross-attention for\nprecise object placement. Dialogues are also rendered as bubbles and assigned\nto characters via CLIPSeg. Experimental results demonstrate that TaleDiffusion\noutperforms existing methods in consistency, noise reduction, and dialogue\nrendering."
                },
                "authors": [
                    {
                        "name": "Ayan Banerjee"
                    },
                    {
                        "name": "Josep Llads"
                    },
                    {
                        "name": "Umapada Pal"
                    },
                    {
                        "name": "Anjan Dutta"
                    }
                ],
                "author_detail": {
                    "name": "Anjan Dutta"
                },
                "author": "Anjan Dutta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14585v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14585v2",
                "updated": "2025-09-04T11:31:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    31,
                    24,
                    3,
                    247,
                    0
                ],
                "published": "2025-05-20T16:40:09Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    40,
                    9,
                    1,
                    140,
                    0
                ],
                "title": "Context Reasoner: Incentivizing Reasoning Capability for Contextualized\n  Privacy and Safety Compliance via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Reasoner: Incentivizing Reasoning Capability for Contextualized\n  Privacy and Safety Compliance via Reinforcement Learning"
                },
                "summary": "While Large Language Models (LLMs) exhibit remarkable capabilities, they also\nintroduce significant safety and privacy risks. Current mitigation strategies\noften fail to preserve contextual reasoning capabilities in risky scenarios.\nInstead, they rely heavily on sensitive pattern matching to protect LLMs, which\nlimits the scope. Furthermore, they overlook established safety and privacy\nstandards, leading to systemic risks for legal compliance. To address these\ngaps, we formulate safety and privacy issues into contextualized compliance\nproblems following the Contextual Integrity (CI) theory. Under the CI\nframework, we align our model with three critical regulatory standards: GDPR,\nEU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with\na rule-based reward to incentivize contextual reasoning capabilities while\nenhancing compliance with safety and privacy norms. Through extensive\nexperiments, we demonstrate that our method not only significantly enhances\nlegal compliance (achieving a +8.58% accuracy improvement in safety/privacy\nbenchmarks) but also further improves general reasoning capability. For\nOpenThinker-7B, a strong reasoning model that significantly outperforms its\nbase model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its\ngeneral reasoning capabilities, with +2.05% and +8.98% accuracy improvement on\nthe MMLU and LegalBench benchmark, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) exhibit remarkable capabilities, they also\nintroduce significant safety and privacy risks. Current mitigation strategies\noften fail to preserve contextual reasoning capabilities in risky scenarios.\nInstead, they rely heavily on sensitive pattern matching to protect LLMs, which\nlimits the scope. Furthermore, they overlook established safety and privacy\nstandards, leading to systemic risks for legal compliance. To address these\ngaps, we formulate safety and privacy issues into contextualized compliance\nproblems following the Contextual Integrity (CI) theory. Under the CI\nframework, we align our model with three critical regulatory standards: GDPR,\nEU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with\na rule-based reward to incentivize contextual reasoning capabilities while\nenhancing compliance with safety and privacy norms. Through extensive\nexperiments, we demonstrate that our method not only significantly enhances\nlegal compliance (achieving a +8.58% accuracy improvement in safety/privacy\nbenchmarks) but also further improves general reasoning capability. For\nOpenThinker-7B, a strong reasoning model that significantly outperforms its\nbase model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its\ngeneral reasoning capabilities, with +2.05% and +8.98% accuracy improvement on\nthe MMLU and LegalBench benchmark, respectively."
                },
                "authors": [
                    {
                        "name": "Wenbin Hu"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Huihao Jing"
                    },
                    {
                        "name": "Qi Hu"
                    },
                    {
                        "name": "Ziqian Zeng"
                    },
                    {
                        "name": "Sirui Han"
                    },
                    {
                        "name": "Heli Xu"
                    },
                    {
                        "name": "Tianshu Chu"
                    },
                    {
                        "name": "Peizhao Hu"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "Accepted to EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14585v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14585v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04118v1",
                "updated": "2025-09-04T11:31:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    31,
                    12,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T11:31:12Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    31,
                    12,
                    3,
                    247,
                    0
                ],
                "title": "EHVC: Efficient Hierarchical Reference and Quality Structure for Neural\n  Video Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EHVC: Efficient Hierarchical Reference and Quality Structure for Neural\n  Video Coding"
                },
                "summary": "Neural video codecs (NVCs), leveraging the power of end-to-end learning, have\ndemonstrated remarkable coding efficiency improvements over traditional video\ncodecs. Recent research has begun to pay attention to the quality structures in\nNVCs, optimizing them by introducing explicit hierarchical designs. However,\nless attention has been paid to the reference structure design, which\nfundamentally should be aligned with the hierarchical quality structure. In\naddition, there is still significant room for further optimization of the\nhierarchical quality structure. To address these challenges in NVCs, we propose\nEHVC, an efficient hierarchical neural video codec featuring three key\ninnovations: (1) a hierarchical multi-reference scheme that draws on\ntraditional video codec design to align reference and quality structures,\nthereby addressing the reference-quality mismatch; (2) a lookahead strategy to\nutilize an encoder-side context from future frames to enhance the quality\nstructure; (3) a layer-wise quality scale with random quality training strategy\nto stabilize quality structures during inference. With these improvements, EHVC\nachieves significantly superior performance to the state-of-the-art NVCs. Code\nwill be released in: https://github.com/bytedance/NEVC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural video codecs (NVCs), leveraging the power of end-to-end learning, have\ndemonstrated remarkable coding efficiency improvements over traditional video\ncodecs. Recent research has begun to pay attention to the quality structures in\nNVCs, optimizing them by introducing explicit hierarchical designs. However,\nless attention has been paid to the reference structure design, which\nfundamentally should be aligned with the hierarchical quality structure. In\naddition, there is still significant room for further optimization of the\nhierarchical quality structure. To address these challenges in NVCs, we propose\nEHVC, an efficient hierarchical neural video codec featuring three key\ninnovations: (1) a hierarchical multi-reference scheme that draws on\ntraditional video codec design to align reference and quality structures,\nthereby addressing the reference-quality mismatch; (2) a lookahead strategy to\nutilize an encoder-side context from future frames to enhance the quality\nstructure; (3) a layer-wise quality scale with random quality training strategy\nto stabilize quality structures during inference. With these improvements, EHVC\nachieves significantly superior performance to the state-of-the-art NVCs. Code\nwill be released in: https://github.com/bytedance/NEVC."
                },
                "authors": [
                    {
                        "name": "Junqi Liao"
                    },
                    {
                        "name": "Yaojun Wu"
                    },
                    {
                        "name": "Chaoyi Lin"
                    },
                    {
                        "name": "Zhipin Deng"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Xiaoyan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyan Sun"
                },
                "author": "Xiaoyan Sun",
                "arxiv_comment": "9 pages, 8 figures, Accepted to ACMMM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13618v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13618v2",
                "updated": "2025-09-04T11:24:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    24,
                    58,
                    3,
                    247,
                    0
                ],
                "published": "2025-06-16T15:47:52Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    47,
                    52,
                    0,
                    167,
                    0
                ],
                "title": "Thermal electrons in the radio afterglow of relativistic tidal\n  disruption event ZTF22aaajecp/AT2022cmc",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thermal electrons in the radio afterglow of relativistic tidal\n  disruption event ZTF22aaajecp/AT2022cmc"
                },
                "summary": "A tidal disruption event (TDE) occurs when a star travels too close to a\nsupermassive black hole. In some cases, accretion of the disrupted material\nonto the black hole launches a relativistic jet. In this paper, we present a\nlong term observing campaign to study the radio and sub-millimeter emission\nassociated with the fifth jetted/relativistic TDE: AT2022cmc. Our campaign\nreveals a long lived counterpart. We fit three different models to our data: a\nnon-thermal jet, a spherical outflow consisting of both thermal and non-thermal\nelectrons, and a jet with thermal and non-thermal electrons. We find that the\ndata is best described by a relativistic spherical outflow propagating into an\nenvironment with a density profile following R^-1.8. Comparison of AT2022cmc to\nother TDEs finds agreement in the density profile of the environment but also\nthat AT2022cmc is twice as energetic as the other well-studied relativistic TDE\nSwift J1644. Our observations of AT2022cmc allow a thermal electron population\nto be inferred for the first time in a jetted transient providing, new insights\ninto the microphysics of relativistic transients jets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A tidal disruption event (TDE) occurs when a star travels too close to a\nsupermassive black hole. In some cases, accretion of the disrupted material\nonto the black hole launches a relativistic jet. In this paper, we present a\nlong term observing campaign to study the radio and sub-millimeter emission\nassociated with the fifth jetted/relativistic TDE: AT2022cmc. Our campaign\nreveals a long lived counterpart. We fit three different models to our data: a\nnon-thermal jet, a spherical outflow consisting of both thermal and non-thermal\nelectrons, and a jet with thermal and non-thermal electrons. We find that the\ndata is best described by a relativistic spherical outflow propagating into an\nenvironment with a density profile following R^-1.8. Comparison of AT2022cmc to\nother TDEs finds agreement in the density profile of the environment but also\nthat AT2022cmc is twice as energetic as the other well-studied relativistic TDE\nSwift J1644. Our observations of AT2022cmc allow a thermal electron population\nto be inferred for the first time in a jetted transient providing, new insights\ninto the microphysics of relativistic transients jets."
                },
                "authors": [
                    {
                        "name": "Lauren Rhodes"
                    },
                    {
                        "name": "Ben Margalit"
                    },
                    {
                        "name": "Joe S. Bright"
                    },
                    {
                        "name": "Hannah Dykaar"
                    },
                    {
                        "name": "Rob Fender"
                    },
                    {
                        "name": "David A. Green"
                    },
                    {
                        "name": "Daryl Haggard"
                    },
                    {
                        "name": "Assaf Horesh"
                    },
                    {
                        "name": "Alexander J. van der Horst"
                    },
                    {
                        "name": "Andrew Hughes"
                    },
                    {
                        "name": "Kunal Mooley"
                    },
                    {
                        "name": "Itai Sfaradi"
                    },
                    {
                        "name": "David Titterington"
                    },
                    {
                        "name": "David WIlliams-Baldwin"
                    }
                ],
                "author_detail": {
                    "name": "David WIlliams-Baldwin"
                },
                "author": "David WIlliams-Baldwin",
                "arxiv_comment": "18 pages, 8 figures, 1 table. Accepted by ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13618v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13618v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04112v1",
                "updated": "2025-09-04T11:22:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    22,
                    8,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T11:22:08Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    22,
                    8,
                    3,
                    247,
                    0
                ],
                "title": "Synthetic Counterfactual Labels for Efficient Conformal Counterfactual\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Counterfactual Labels for Efficient Conformal Counterfactual\n  Inference"
                },
                "summary": "This work addresses the problem of constructing reliable prediction intervals\nfor individual counterfactual outcomes. Existing conformal counterfactual\ninference (CCI) methods provide marginal coverage guarantees but often produce\noverly conservative intervals, particularly under treatment imbalance when\ncounterfactual samples are scarce. We introduce synthetic data-powered CCI\n(SP-CCI), a new framework that augments the calibration set with synthetic\ncounterfactual labels generated by a pre-trained counterfactual model. To\nensure validity, SP-CCI incorporates synthetic samples into a conformal\ncalibration procedure based on risk-controlling prediction sets (RCPS) with a\ndebiasing step informed by prediction-powered inference (PPI). We prove that\nSP-CCI achieves tighter prediction intervals while preserving marginal\ncoverage, with theoretical guarantees under both exact and approximate\nimportance weighting. Empirical results on different datasets confirm that\nSP-CCI consistently reduces interval width compared to standard CCI across all\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work addresses the problem of constructing reliable prediction intervals\nfor individual counterfactual outcomes. Existing conformal counterfactual\ninference (CCI) methods provide marginal coverage guarantees but often produce\noverly conservative intervals, particularly under treatment imbalance when\ncounterfactual samples are scarce. We introduce synthetic data-powered CCI\n(SP-CCI), a new framework that augments the calibration set with synthetic\ncounterfactual labels generated by a pre-trained counterfactual model. To\nensure validity, SP-CCI incorporates synthetic samples into a conformal\ncalibration procedure based on risk-controlling prediction sets (RCPS) with a\ndebiasing step informed by prediction-powered inference (PPI). We prove that\nSP-CCI achieves tighter prediction intervals while preserving marginal\ncoverage, with theoretical guarantees under both exact and approximate\nimportance weighting. Empirical results on different datasets confirm that\nSP-CCI consistently reduces interval width compared to standard CCI across all\nsettings."
                },
                "authors": [
                    {
                        "name": "Amirmohammad Farzaneh"
                    },
                    {
                        "name": "Matteo Zecchin"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04111v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04111v2",
                "updated": "2025-09-05T09:12:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    9,
                    12,
                    3,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-04T11:20:53Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    20,
                    53,
                    3,
                    247,
                    0
                ],
                "title": "MultiWikiQA: A Reading Comprehension Benchmark in 300+ Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiWikiQA: A Reading Comprehension Benchmark in 300+ Languages"
                },
                "summary": "We introduce a new reading comprehension dataset, dubbed MultiWikiQA, which\ncovers 306 languages. The context data comes from Wikipedia articles, with\nquestions generated by an LLM and the answers appearing verbatim in the\nWikipedia articles. We conduct a crowdsourced human evaluation of the fluency\nof the generated questions across 30 of the languages, providing evidence that\nthe questions are of good quality. We evaluate 6 different language models,\nboth decoder and encoder models of varying sizes, showing that the benchmark is\nsufficiently difficult and that there is a large performance discrepancy\namongst the languages. The dataset and survey evaluations are freely available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new reading comprehension dataset, dubbed MultiWikiQA, which\ncovers 306 languages. The context data comes from Wikipedia articles, with\nquestions generated by an LLM and the answers appearing verbatim in the\nWikipedia articles. We conduct a crowdsourced human evaluation of the fluency\nof the generated questions across 30 of the languages, providing evidence that\nthe questions are of good quality. We evaluate 6 different language models,\nboth decoder and encoder models of varying sizes, showing that the benchmark is\nsufficiently difficult and that there is a large performance discrepancy\namongst the languages. The dataset and survey evaluations are freely available."
                },
                "authors": [
                    {
                        "name": "Dan Saattrup Smart"
                    }
                ],
                "author_detail": {
                    "name": "Dan Saattrup Smart"
                },
                "author": "Dan Saattrup Smart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04111v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04111v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21687v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21687v2",
                "updated": "2025-09-04T11:17:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    17,
                    46,
                    3,
                    247,
                    0
                ],
                "published": "2025-03-27T16:54:22Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    16,
                    54,
                    22,
                    3,
                    86,
                    0
                ],
                "title": "Exploiting synergies between JWST and cosmic 21-cm observations to\n  uncover star formation in the early Universe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting synergies between JWST and cosmic 21-cm observations to\n  uncover star formation in the early Universe"
                },
                "summary": "In the current era of JWST, we continue to uncover a wealth of information\nabout the Universe deep into the Epoch of Reionization. In this work, we use a\nsuite of simulations with 21cmSPACE, to explore the astrophysical properties of\nearly galaxies and their imprint on high-redshift observables. Our analysis\nincorporates a range of multi-wavelength datasets including UV luminosity\nfunctions (UVLFs) from HST and JWST spanning $z=6-14.5$, the 21-cm global\nsignal and power spectrum limits from SARAS 3 and HERA respectively, as well as\npresent-day diffuse X-ray and radio backgrounds. We constrain a flexible\nhalo-mass and redshift dependent model of star-formation efficiency (SFE),\ndefined as the fraction of gas converted into stars, and find that it is best\ndescribed by minimal redshift evolution at $z\\approx 6-10$, followed by rapid\nevolution at $z\\approx10-15$. Using Bayesian inference, we derive functional\nposteriors of the SFE, inferring that halos of mass\n$M_h=10^{10}\\,\\mathrm{M}_\\odot$ have efficiencies of $\\approx 1 - 2\\%$ at\n$z\\lesssim10$, $\\approx8\\%$ at $z=12$ and $\\approx21\\%$ at $z=15$. We also\nhighlight the synergy between UVLFs and global 21-cm signal from SARAS 3 in\nconstraining the minimum virial conditions required for star-formation in\nhalos. In parallel, we find the X-ray and radio efficiencies of early galaxies\nto be $f_X = 0.8^{+9.7}_{-0.4}$ and $f_r \\lesssim 16.9$ respectively, improving\nupon previous works that exclude UVLF data. Our results underscore the critical\nrole of UVLFs in constraining early galaxy properties, and their synergy with\n21-cm and other multi-wavelength observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the current era of JWST, we continue to uncover a wealth of information\nabout the Universe deep into the Epoch of Reionization. In this work, we use a\nsuite of simulations with 21cmSPACE, to explore the astrophysical properties of\nearly galaxies and their imprint on high-redshift observables. Our analysis\nincorporates a range of multi-wavelength datasets including UV luminosity\nfunctions (UVLFs) from HST and JWST spanning $z=6-14.5$, the 21-cm global\nsignal and power spectrum limits from SARAS 3 and HERA respectively, as well as\npresent-day diffuse X-ray and radio backgrounds. We constrain a flexible\nhalo-mass and redshift dependent model of star-formation efficiency (SFE),\ndefined as the fraction of gas converted into stars, and find that it is best\ndescribed by minimal redshift evolution at $z\\approx 6-10$, followed by rapid\nevolution at $z\\approx10-15$. Using Bayesian inference, we derive functional\nposteriors of the SFE, inferring that halos of mass\n$M_h=10^{10}\\,\\mathrm{M}_\\odot$ have efficiencies of $\\approx 1 - 2\\%$ at\n$z\\lesssim10$, $\\approx8\\%$ at $z=12$ and $\\approx21\\%$ at $z=15$. We also\nhighlight the synergy between UVLFs and global 21-cm signal from SARAS 3 in\nconstraining the minimum virial conditions required for star-formation in\nhalos. In parallel, we find the X-ray and radio efficiencies of early galaxies\nto be $f_X = 0.8^{+9.7}_{-0.4}$ and $f_r \\lesssim 16.9$ respectively, improving\nupon previous works that exclude UVLF data. Our results underscore the critical\nrole of UVLFs in constraining early galaxy properties, and their synergy with\n21-cm and other multi-wavelength observations."
                },
                "authors": [
                    {
                        "name": "Jiten Dhandha"
                    },
                    {
                        "name": "Anastasia Fialkov"
                    },
                    {
                        "name": "Thomas Gessey-Jones"
                    },
                    {
                        "name": "Harry T. J. Bevins"
                    },
                    {
                        "name": "Sandro Tacchella"
                    },
                    {
                        "name": "Simon Pochinda"
                    },
                    {
                        "name": "Eloy de Lera Acedo"
                    },
                    {
                        "name": "Saurabh Singh"
                    },
                    {
                        "name": "Rennan Barkana"
                    }
                ],
                "author_detail": {
                    "name": "Rennan Barkana"
                },
                "author": "Rennan Barkana",
                "arxiv_doi": "10.1093/mnras/staf1359",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/staf1359",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.21687v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21687v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "30 pages, 14 figures, 6 tables. Accepted for publication in MNRAS,\n  updated to accepted version",
                "arxiv_journal_ref": "Monthly Notices of the Royal Astronomical Society, Volume 542,\n  Issue 3, September 2025, Pages 2292-2322",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04104v1",
                "updated": "2025-09-04T11:07:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    7,
                    27,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T11:07:27Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    7,
                    27,
                    3,
                    247,
                    0
                ],
                "title": "Towards Stable and Personalised Profiles for Lexical Alignment in Spoken\n  Human-Agent Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Stable and Personalised Profiles for Lexical Alignment in Spoken\n  Human-Agent Dialogue"
                },
                "summary": "Lexical alignment, where speakers start to use similar words across\nconversation, is known to contribute to successful communication. However, its\nimplementation in conversational agents remains underexplored, particularly\nconsidering the recent advancements in large language models (LLMs). As a first\nstep towards enabling lexical alignment in human-agent dialogue, this study\ndraws on strategies for personalising conversational agents and investigates\nthe construction of stable, personalised lexical profiles as a basis for\nlexical alignment. Specifically, we varied the amounts of transcribed spoken\ndata used for construction as well as the number of items included in the\nprofiles per part-of-speech (POS) category and evaluated profile performance\nacross time using recall, coverage, and cosine similarity metrics. It was shown\nthat smaller and more compact profiles, created after 10 min of transcribed\nspeech containing 5 items for adjectives, 5 items for conjunctions, and 10\nitems for adverbs, nouns, pronouns, and verbs each, offered the best balance in\nboth performance and data efficiency. In conclusion, this study offers\npractical insights into constructing stable, personalised lexical profiles,\ntaking into account minimal data requirements, serving as a foundational step\ntoward lexical alignment strategies in conversational agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lexical alignment, where speakers start to use similar words across\nconversation, is known to contribute to successful communication. However, its\nimplementation in conversational agents remains underexplored, particularly\nconsidering the recent advancements in large language models (LLMs). As a first\nstep towards enabling lexical alignment in human-agent dialogue, this study\ndraws on strategies for personalising conversational agents and investigates\nthe construction of stable, personalised lexical profiles as a basis for\nlexical alignment. Specifically, we varied the amounts of transcribed spoken\ndata used for construction as well as the number of items included in the\nprofiles per part-of-speech (POS) category and evaluated profile performance\nacross time using recall, coverage, and cosine similarity metrics. It was shown\nthat smaller and more compact profiles, created after 10 min of transcribed\nspeech containing 5 items for adjectives, 5 items for conjunctions, and 10\nitems for adverbs, nouns, pronouns, and verbs each, offered the best balance in\nboth performance and data efficiency. In conclusion, this study offers\npractical insights into constructing stable, personalised lexical profiles,\ntaking into account minimal data requirements, serving as a foundational step\ntoward lexical alignment strategies in conversational agents."
                },
                "authors": [
                    {
                        "name": "Keara Schaaij"
                    },
                    {
                        "name": "Roel Boumans"
                    },
                    {
                        "name": "Tibor Bosse"
                    },
                    {
                        "name": "Iris Hendrickx"
                    }
                ],
                "author_detail": {
                    "name": "Iris Hendrickx"
                },
                "author": "Iris Hendrickx",
                "arxiv_doi": "10.1007/978-3-032-02548-7_5",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-032-02548-7_5",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.04104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for TSD 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04092v1",
                "updated": "2025-09-04T10:48:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    10,
                    48,
                    25,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T10:48:25Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    10,
                    48,
                    25,
                    3,
                    247,
                    0
                ],
                "title": "TriLiteNet: Lightweight Model for Multi-Task Visual Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TriLiteNet: Lightweight Model for Multi-Task Visual Perception"
                },
                "summary": "Efficient perception models are essential for Advanced Driver Assistance\nSystems (ADAS), as these applications require rapid processing and response to\nensure safety and effectiveness in real-world environments. To address the\nreal-time execution needs of such perception models, this study introduces the\nTriLiteNet model. This model can simultaneously manage multiple tasks related\nto panoramic driving perception. TriLiteNet is designed to optimize performance\nwhile maintaining low computational costs. Experimental results on the BDD100k\ndataset demonstrate that the model achieves competitive performance across\nthree key tasks: vehicle detection, drivable area segmentation, and lane line\nsegmentation. Specifically, the TriLiteNet_{base} demonstrated a recall of\n85.6% for vehicle detection, a mean Intersection over Union (mIoU) of 92.4% for\ndrivable area segmentation, and an Acc of 82.3% for lane line segmentation with\nonly 2.35M parameters and a computational cost of 7.72 GFLOPs. Our proposed\nmodel includes a tiny configuration with just 0.14M parameters, which provides\na multi-task solution with minimal computational demand. Evaluated for latency\nand power consumption on embedded devices, TriLiteNet in both configurations\nshows low latency and reasonable power during inference. By balancing\nperformance, computational efficiency, and scalability, TriLiteNet offers a\npractical and deployable solution for real-world autonomous driving\napplications. Code is available at https://github.com/chequanghuy/TriLiteNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient perception models are essential for Advanced Driver Assistance\nSystems (ADAS), as these applications require rapid processing and response to\nensure safety and effectiveness in real-world environments. To address the\nreal-time execution needs of such perception models, this study introduces the\nTriLiteNet model. This model can simultaneously manage multiple tasks related\nto panoramic driving perception. TriLiteNet is designed to optimize performance\nwhile maintaining low computational costs. Experimental results on the BDD100k\ndataset demonstrate that the model achieves competitive performance across\nthree key tasks: vehicle detection, drivable area segmentation, and lane line\nsegmentation. Specifically, the TriLiteNet_{base} demonstrated a recall of\n85.6% for vehicle detection, a mean Intersection over Union (mIoU) of 92.4% for\ndrivable area segmentation, and an Acc of 82.3% for lane line segmentation with\nonly 2.35M parameters and a computational cost of 7.72 GFLOPs. Our proposed\nmodel includes a tiny configuration with just 0.14M parameters, which provides\na multi-task solution with minimal computational demand. Evaluated for latency\nand power consumption on embedded devices, TriLiteNet in both configurations\nshows low latency and reasonable power during inference. By balancing\nperformance, computational efficiency, and scalability, TriLiteNet offers a\npractical and deployable solution for real-world autonomous driving\napplications. Code is available at https://github.com/chequanghuy/TriLiteNet."
                },
                "authors": [
                    {
                        "name": "Quang-Huy Che"
                    },
                    {
                        "name": "Duc-Khai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Duc-Khai Lam"
                },
                "author": "Duc-Khai Lam",
                "arxiv_doi": "10.1109/ACCESS.2025.3552088",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2025.3552088",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.04092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Access 13 (2025) 50152-50166",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04091v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04091v2",
                "updated": "2025-09-05T10:34:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    34,
                    25,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-04T10:48:02Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    10,
                    48,
                    2,
                    3,
                    247,
                    0
                ],
                "title": "Revisiting Third-Party Library Detection: A Ground Truth Dataset and Its\n  Implications Across Security Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Third-Party Library Detection: A Ground Truth Dataset and Its\n  Implications Across Security Tasks"
                },
                "summary": "Accurate detection of third-party libraries (TPLs) is fundamental to Android\nsecurity, supporting vulnerability tracking, malware detection, and supply\nchain auditing. Despite many proposed tools, their real-world effectiveness\nremains unclear. We present the first large-scale empirical study of ten\nstate-of-the-art TPL detection techniques across over 6,000 apps, enabled by a\nnew ground truth dataset with precise version-level annotations for both remote\nand local dependencies. Our evaluation exposes tool fragility to R8-era\ntransformations, weak version discrimination, inaccurate correspondence of\ncandidate libraries, difficulty in generalizing similarity thresholds, and\nprohibitive runtime/memory overheads at scale. Beyond tool assessment, we\nfurther analyze how TPLs shape downstream tasks, including vulnerability\nanalysis, malware detection, secret leakage assessment, and LLM-based\nevaluation. From this perspective, our study provides concrete insights into\nhow TPL characteristics affect these tasks and informs future improvements in\nsecurity analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate detection of third-party libraries (TPLs) is fundamental to Android\nsecurity, supporting vulnerability tracking, malware detection, and supply\nchain auditing. Despite many proposed tools, their real-world effectiveness\nremains unclear. We present the first large-scale empirical study of ten\nstate-of-the-art TPL detection techniques across over 6,000 apps, enabled by a\nnew ground truth dataset with precise version-level annotations for both remote\nand local dependencies. Our evaluation exposes tool fragility to R8-era\ntransformations, weak version discrimination, inaccurate correspondence of\ncandidate libraries, difficulty in generalizing similarity thresholds, and\nprohibitive runtime/memory overheads at scale. Beyond tool assessment, we\nfurther analyze how TPLs shape downstream tasks, including vulnerability\nanalysis, malware detection, secret leakage assessment, and LLM-based\nevaluation. From this perspective, our study provides concrete insights into\nhow TPL characteristics affect these tasks and informs future improvements in\nsecurity analysis."
                },
                "authors": [
                    {
                        "name": "Jintao Gu"
                    },
                    {
                        "name": "Haolang Lu"
                    },
                    {
                        "name": "Guoshun Nan"
                    },
                    {
                        "name": "Yihan Lin"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Yuchun Guo"
                    },
                    {
                        "name": "Yigui Cao"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "20pages, 7figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04091v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04091v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5; D.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04083v1",
                "updated": "2025-09-04T10:25:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    10,
                    25,
                    50,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T10:25:50Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    10,
                    25,
                    50,
                    3,
                    247,
                    0
                ],
                "title": "Intermediate Languages Matter: Formal Languages and LLMs affect\n  Neurosymbolic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intermediate Languages Matter: Formal Languages and LLMs affect\n  Neurosymbolic Reasoning"
                },
                "summary": "Large language models (LLMs) achieve astonishing results on a wide range of\ntasks. However, their formal reasoning ability still lags behind. A promising\napproach is Neurosymbolic LLM reasoning. It works by using LLMs as translators\nfrom natural to formal languages and symbolic solvers for deriving correct\nresults. Still, the contributing factors to the success of Neurosymbolic LLM\nreasoning remain unclear. This paper demonstrates that one previously\noverlooked factor is the choice of the formal language. We introduce the\nintermediate language challenge: selecting a suitable formal language for\nneurosymbolic reasoning. By comparing four formal languages across three\ndatasets and seven LLMs, we show that the choice of formal language affects\nboth syntactic and semantic reasoning capabilities. We also discuss the varying\neffects across different LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve astonishing results on a wide range of\ntasks. However, their formal reasoning ability still lags behind. A promising\napproach is Neurosymbolic LLM reasoning. It works by using LLMs as translators\nfrom natural to formal languages and symbolic solvers for deriving correct\nresults. Still, the contributing factors to the success of Neurosymbolic LLM\nreasoning remain unclear. This paper demonstrates that one previously\noverlooked factor is the choice of the formal language. We introduce the\nintermediate language challenge: selecting a suitable formal language for\nneurosymbolic reasoning. By comparing four formal languages across three\ndatasets and seven LLMs, we show that the choice of formal language affects\nboth syntactic and semantic reasoning capabilities. We also discuss the varying\neffects across different LLMs."
                },
                "authors": [
                    {
                        "name": "Alexander Beiser"
                    },
                    {
                        "name": "David Penz"
                    },
                    {
                        "name": "Nysret Musliu"
                    }
                ],
                "author_detail": {
                    "name": "Nysret Musliu"
                },
                "author": "Nysret Musliu",
                "arxiv_comment": "To appear in the proceedings of The Second Workshop on Knowledge\n  Graphs and Neurosymbolic AI (KG-NeSy) Co-located with SEMANTiCS 2025\n  Conference, Vienna, Austria - September 3rd, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04078v1",
                "updated": "2025-09-04T10:13:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    10,
                    13,
                    21,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T10:13:21Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    10,
                    13,
                    21,
                    3,
                    247,
                    0
                ],
                "title": "RepoDebug: Repository-Level Multi-Task and Multi-Language Debugging\n  Evaluation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepoDebug: Repository-Level Multi-Task and Multi-Language Debugging\n  Evaluation of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have exhibited significant proficiency in code\ndebugging, especially in automatic program repair, which may substantially\nreduce the time consumption of developers and enhance their efficiency.\nSignificant advancements in debugging datasets have been made to promote the\ndevelopment of code debugging. However, these datasets primarily focus on\nassessing the LLM's function-level code repair capabilities, neglecting the\nmore complex and realistic repository-level scenarios, which leads to an\nincomplete understanding of the LLM's challenges in repository-level debugging.\nWhile several repository-level datasets have been proposed, they often suffer\nfrom limitations such as limited diversity of tasks, languages, and error\ntypes. To mitigate this challenge, this paper introduces RepoDebug, a\nmulti-task and multi-language repository-level code debugging dataset with 22\nsubtypes of errors that supports 8 commonly used programming languages and 3\ndebugging tasks. Furthermore, we conduct evaluation experiments on 10 LLMs,\nwhere Claude 3.5 Sonnect, the best-performing model, still cannot perform well\nin repository-level debugging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited significant proficiency in code\ndebugging, especially in automatic program repair, which may substantially\nreduce the time consumption of developers and enhance their efficiency.\nSignificant advancements in debugging datasets have been made to promote the\ndevelopment of code debugging. However, these datasets primarily focus on\nassessing the LLM's function-level code repair capabilities, neglecting the\nmore complex and realistic repository-level scenarios, which leads to an\nincomplete understanding of the LLM's challenges in repository-level debugging.\nWhile several repository-level datasets have been proposed, they often suffer\nfrom limitations such as limited diversity of tasks, languages, and error\ntypes. To mitigate this challenge, this paper introduces RepoDebug, a\nmulti-task and multi-language repository-level code debugging dataset with 22\nsubtypes of errors that supports 8 commonly used programming languages and 3\ndebugging tasks. Furthermore, we conduct evaluation experiments on 10 LLMs,\nwhere Claude 3.5 Sonnect, the best-performing model, still cannot perform well\nin repository-level debugging."
                },
                "authors": [
                    {
                        "name": "Jingjing Liu"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Zihao Cheng"
                    },
                    {
                        "name": "Mengliang He"
                    },
                    {
                        "name": "Xiaoming Shi"
                    },
                    {
                        "name": "Yuhang Guo"
                    },
                    {
                        "name": "Xiangrong Zhu"
                    },
                    {
                        "name": "Yuanfang Guo"
                    },
                    {
                        "name": "Yunhong Wang"
                    },
                    {
                        "name": "Haifeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haifeng Wang"
                },
                "author": "Haifeng Wang",
                "arxiv_comment": "30 pages, 12 figures, EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04073v1",
                "updated": "2025-09-04T10:08:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    10,
                    8,
                    36,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T10:08:36Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    10,
                    8,
                    36,
                    3,
                    247,
                    0
                ],
                "title": "ZTF SNe Ia DR2: Towards cosmology-grade ZTF supernova light curves using\n  scene modeling photometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZTF SNe Ia DR2: Towards cosmology-grade ZTF supernova light curves using\n  scene modeling photometry"
                },
                "summary": "The Zwicky Transient Facility (ZTF) is conducting a wide-field survey of the\nnorthern sky in three optical bands and the collaboration cosmology working\ngroup has released 3628 spectroscopically confirmed Type Ia supernovae (SNe Ia)\ndiscovered during its first 2.5 years of operation. This \"ZTF SN Ia DR2\" sample\nis the largest SN Ia dataset to date.\n  Fully exploiting this dataset to improve understanding of the properties of\ndark energy requires a photometric accuracy of O(0.1%). This can be achieved\nusing Scene Modeling Photometry (SMP), which is optimal to extract a transient\nsignal (SN) from a complex background (its host), while ensuring a common flux\nestimator with nearby stars used as calibration reference. In this paper, we\npresent the status of the SMP development and use it to assess the precision\nand accuracy of the ZTF SN Ia DR2 force photometry light curves.\n  We reach a repeatability of the star observations better than 1%. However, we\nhave identified a new sensor effect, dubbed \"pocket-effect\", which distorts the\nPoint Spread Function (PSF) in a flux-dependent manner leading to\nnon-linearities in the photometry of a few percent. Correcting for this effect\nrequires time- and sensor-dependent corrections to be applied at the pixel\nlevel, which is currently under development. This effects affects all light\ncurve releases to date -- both from forced photometry and scene modelling\npreventing ZTF SN Ia DR2 to be used for accurate cosmological inference.\n  Comparing the SMP and forced photometry measurements, we find that stretch\nand color estimated from both processings are consistent, aside from a 10 mmag\nshift in color. This assess the robustness of results presented as part of the\nthe ZTF SN Ia DR2 release. The absolute calibration however shifts by 90 mmag.\nA reprocessing of the full ZTF SN Ia DR2 dataset using the SMP method is\ncurrently in progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Zwicky Transient Facility (ZTF) is conducting a wide-field survey of the\nnorthern sky in three optical bands and the collaboration cosmology working\ngroup has released 3628 spectroscopically confirmed Type Ia supernovae (SNe Ia)\ndiscovered during its first 2.5 years of operation. This \"ZTF SN Ia DR2\" sample\nis the largest SN Ia dataset to date.\n  Fully exploiting this dataset to improve understanding of the properties of\ndark energy requires a photometric accuracy of O(0.1%). This can be achieved\nusing Scene Modeling Photometry (SMP), which is optimal to extract a transient\nsignal (SN) from a complex background (its host), while ensuring a common flux\nestimator with nearby stars used as calibration reference. In this paper, we\npresent the status of the SMP development and use it to assess the precision\nand accuracy of the ZTF SN Ia DR2 force photometry light curves.\n  We reach a repeatability of the star observations better than 1%. However, we\nhave identified a new sensor effect, dubbed \"pocket-effect\", which distorts the\nPoint Spread Function (PSF) in a flux-dependent manner leading to\nnon-linearities in the photometry of a few percent. Correcting for this effect\nrequires time- and sensor-dependent corrections to be applied at the pixel\nlevel, which is currently under development. This effects affects all light\ncurve releases to date -- both from forced photometry and scene modelling\npreventing ZTF SN Ia DR2 to be used for accurate cosmological inference.\n  Comparing the SMP and forced photometry measurements, we find that stretch\nand color estimated from both processings are consistent, aside from a 10 mmag\nshift in color. This assess the robustness of results presented as part of the\nthe ZTF SN Ia DR2 release. The absolute calibration however shifts by 90 mmag.\nA reprocessing of the full ZTF SN Ia DR2 dataset using the SMP method is\ncurrently in progress."
                },
                "authors": [
                    {
                        "name": "L. Lacroix"
                    },
                    {
                        "name": "N. Regnault"
                    },
                    {
                        "name": "T. de Jaeger"
                    },
                    {
                        "name": "M. Le Jeune"
                    },
                    {
                        "name": "M. Betoule"
                    },
                    {
                        "name": "J. -M. Colley"
                    },
                    {
                        "name": "M. Bernard"
                    },
                    {
                        "name": "M. Rigault"
                    },
                    {
                        "name": "M. Smith"
                    },
                    {
                        "name": "A. Goobar"
                    },
                    {
                        "name": "K. Maguire"
                    },
                    {
                        "name": "G. Dimitriadis"
                    },
                    {
                        "name": "J. Nordin"
                    },
                    {
                        "name": "J. Johansson"
                    },
                    {
                        "name": "M. Aubert"
                    },
                    {
                        "name": "C. Barjou"
                    },
                    {
                        "name": "E. C. Bellm"
                    },
                    {
                        "name": "S. Bongard"
                    },
                    {
                        "name": "U. Burgaz"
                    },
                    {
                        "name": "B. Carreres"
                    },
                    {
                        "name": "D. Fouchez"
                    },
                    {
                        "name": "F. Feinstein"
                    },
                    {
                        "name": "L. Galbany"
                    },
                    {
                        "name": "M. Ginolin"
                    },
                    {
                        "name": "M. Graham"
                    },
                    {
                        "name": "D. Kuhn"
                    },
                    {
                        "name": "R. R. Laher"
                    },
                    {
                        "name": "T. E. Mller-Bravo"
                    },
                    {
                        "name": "J. Neveu"
                    },
                    {
                        "name": "M. Osman"
                    },
                    {
                        "name": "B. Popovic"
                    },
                    {
                        "name": "B. Racine"
                    },
                    {
                        "name": "P. Rosnet"
                    },
                    {
                        "name": "D. Rosselli"
                    },
                    {
                        "name": "R. Smith"
                    },
                    {
                        "name": "J. Sollerman"
                    },
                    {
                        "name": "J. H. Terwel"
                    },
                    {
                        "name": "A. Townsend"
                    },
                    {
                        "name": "A. Wold"
                    }
                ],
                "author_detail": {
                    "name": "A. Wold"
                },
                "author": "A. Wold",
                "arxiv_comment": "ZTF SN Ia DR2: photometry paper, Submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04066v1",
                "updated": "2025-09-04T09:55:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    9,
                    55,
                    16,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T09:55:16Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    9,
                    55,
                    16,
                    3,
                    247,
                    0
                ],
                "title": "Arabic Chatbot Technologies in Education: An Overview",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arabic Chatbot Technologies in Education: An Overview"
                },
                "summary": "The recent advancements in Artificial Intelligence (AI) in general, and in\nNatural Language Processing (NLP) in particular, and some of its applications\nsuch as chatbots, have led to their implementation in different domains like\neducation, healthcare, tourism, and customer service. Since the COVID-19\npandemic, there has been an increasing interest in these digital technologies\nto allow and enhance remote access. In education, e-learning systems have been\nmassively adopted worldwide. The emergence of Large Language Models (LLM) such\nas BERT (Bidirectional Encoder Representations from Transformers) and GPT\n(Generative Pre-trained Transformers) made chatbots even more popular. In this\nstudy, we present a survey on existing Arabic chatbots in education and their\ndifferent characteristics such as the adopted approaches, language variety, and\nmetrics used to measure their performance. We were able to identified some\nresearch gaps when we discovered that, despite the success of chatbots in other\nlanguages such as English, only a few educational Arabic chatbots used modern\ntechniques. Finally, we discuss future directions of research in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advancements in Artificial Intelligence (AI) in general, and in\nNatural Language Processing (NLP) in particular, and some of its applications\nsuch as chatbots, have led to their implementation in different domains like\neducation, healthcare, tourism, and customer service. Since the COVID-19\npandemic, there has been an increasing interest in these digital technologies\nto allow and enhance remote access. In education, e-learning systems have been\nmassively adopted worldwide. The emergence of Large Language Models (LLM) such\nas BERT (Bidirectional Encoder Representations from Transformers) and GPT\n(Generative Pre-trained Transformers) made chatbots even more popular. In this\nstudy, we present a survey on existing Arabic chatbots in education and their\ndifferent characteristics such as the adopted approaches, language variety, and\nmetrics used to measure their performance. We were able to identified some\nresearch gaps when we discovered that, despite the success of chatbots in other\nlanguages such as English, only a few educational Arabic chatbots used modern\ntechniques. Finally, we discuss future directions of research in this field."
                },
                "authors": [
                    {
                        "name": "Hicham Bourhil"
                    },
                    {
                        "name": "Yacine El Younoussi"
                    }
                ],
                "author_detail": {
                    "name": "Yacine El Younoussi"
                },
                "author": "Yacine El Younoussi",
                "arxiv_doi": "10.54988/uaj.000027.001",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.54988/uaj.000027.001",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.04066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published as a book chapter in: Transformaci\\'on Digital en la\n  Educaci\\'on: Innovaciones y Desaf\\'ios desde los Campus Virtuales (UA\n  Journals, 2024), pp. 11-14",
                "arxiv_journal_ref": "In: Transformaci\\'on Digital en la Educaci\\'on: Innovaciones y\n  Desaf\\'ios desde los Campus Virtuales. UA Journals, 2024. pp. 11-14",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04059v1",
                "updated": "2025-09-04T09:42:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    9,
                    42,
                    17,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T09:42:17Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    9,
                    42,
                    17,
                    3,
                    247,
                    0
                ],
                "title": "Synthesizing Sheet Music Problems for Evaluation and Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing Sheet Music Problems for Evaluation and Reinforcement\n  Learning"
                },
                "summary": "Enhancing the ability of Large Language Models (LLMs) and Multimodal Large\nLanguage Models (MLLMs) to interpret sheet music is a crucial step toward\nbuilding AI musicians. However, current research lacks both evaluation\nbenchmarks and training data for sheet music reasoning. To address this, we\npropose the idea of synthesizing sheet music problems grounded in music theory,\nwhich can serve both as evaluation benchmarks and as training data for\nreinforcement learning with verifiable rewards (RLVR). We introduce a data\nsynthesis framework that generates verifiable sheet music questions in both\ntextual and visual modalities, leading to the Synthetic Sheet Music Reasoning\nBenchmark (SSMR-Bench) and a complementary training set. Evaluation results on\nSSMR-Bench show the importance of models' reasoning abilities in interpreting\nsheet music. At the same time, the poor performance of Gemini 2.5-Pro\nhighlights the challenges that MLLMs still face in interpreting sheet music in\na visual format. By leveraging synthetic data for RLVR, Qwen3-8B-Base and\nQwen2.5-VL-Instruct achieve improvements on the SSMR-Bench. Besides, the\ntrained Qwen3-8B-Base surpasses GPT-4 in overall performance on\nMusicTheoryBench and achieves reasoning performance comparable to GPT-4 with\nthe strategies of Role play and Chain-of-Thought. Notably, its performance on\nmath problems also improves relative to the original Qwen3-8B-Base.\nFurthermore, our results show that the enhanced reasoning ability can also\nfacilitate music composition. In conclusion, we are the first to propose the\nidea of synthesizing sheet music problems based on music theory rules, and\ndemonstrate its effectiveness not only in advancing model reasoning for sheet\nmusic understanding but also in unlocking new possibilities for AI-assisted\nmusic creation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing the ability of Large Language Models (LLMs) and Multimodal Large\nLanguage Models (MLLMs) to interpret sheet music is a crucial step toward\nbuilding AI musicians. However, current research lacks both evaluation\nbenchmarks and training data for sheet music reasoning. To address this, we\npropose the idea of synthesizing sheet music problems grounded in music theory,\nwhich can serve both as evaluation benchmarks and as training data for\nreinforcement learning with verifiable rewards (RLVR). We introduce a data\nsynthesis framework that generates verifiable sheet music questions in both\ntextual and visual modalities, leading to the Synthetic Sheet Music Reasoning\nBenchmark (SSMR-Bench) and a complementary training set. Evaluation results on\nSSMR-Bench show the importance of models' reasoning abilities in interpreting\nsheet music. At the same time, the poor performance of Gemini 2.5-Pro\nhighlights the challenges that MLLMs still face in interpreting sheet music in\na visual format. By leveraging synthetic data for RLVR, Qwen3-8B-Base and\nQwen2.5-VL-Instruct achieve improvements on the SSMR-Bench. Besides, the\ntrained Qwen3-8B-Base surpasses GPT-4 in overall performance on\nMusicTheoryBench and achieves reasoning performance comparable to GPT-4 with\nthe strategies of Role play and Chain-of-Thought. Notably, its performance on\nmath problems also improves relative to the original Qwen3-8B-Base.\nFurthermore, our results show that the enhanced reasoning ability can also\nfacilitate music composition. In conclusion, we are the first to propose the\nidea of synthesizing sheet music problems based on music theory rules, and\ndemonstrate its effectiveness not only in advancing model reasoning for sheet\nmusic understanding but also in unlocking new possibilities for AI-assisted\nmusic creation."
                },
                "authors": [
                    {
                        "name": "Zhilin Wang"
                    },
                    {
                        "name": "Zhe Yang"
                    },
                    {
                        "name": "Yun Luo"
                    },
                    {
                        "name": "Yafu Li"
                    },
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Runzhe Zhan"
                    },
                    {
                        "name": "Derek F. Wong"
                    },
                    {
                        "name": "Jizhe Zhou"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04058v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04058v1",
                "updated": "2025-09-04T09:41:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    9,
                    41,
                    18,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T09:41:18Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    9,
                    41,
                    18,
                    3,
                    247,
                    0
                ],
                "title": "SMooGPT: Stylized Motion Generation using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMooGPT: Stylized Motion Generation using Large Language Models"
                },
                "summary": "Stylized motion generation is actively studied in computer graphics,\nespecially benefiting from the rapid advances in diffusion models. The goal of\nthis task is to produce a novel motion respecting both the motion content and\nthe desired motion style, e.g., ``walking in a loop like a Monkey''. Existing\nresearch attempts to address this problem via motion style transfer or\nconditional motion generation. They typically embed the motion style into a\nlatent space and guide the motion implicitly in a latent space as well. Despite\nthe progress, their methods suffer from low interpretability and control,\nlimited generalization to new styles, and fail to produce motions other than\n``walking'' due to the strong bias in the public stylization dataset. In this\npaper, we propose to solve the stylized motion generation problem from a new\nperspective of reasoning-composition-generation, based on our observations: i)\nhuman motion can often be effectively described using natural language in a\nbody-part centric manner, ii) LLMs exhibit a strong ability to understand and\nreason about human motion, and iii) human motion has an inherently\ncompositional nature, facilitating the new motion content or style generation\nvia effective recomposing. We thus propose utilizing body-part text space as an\nintermediate representation, and present SMooGPT, a fine-tuned LLM, acting as a\nreasoner, composer, and generator when generating the desired stylized motion.\nOur method executes in the body-part text space with much higher\ninterpretability, enabling fine-grained motion control, effectively resolving\npotential conflicts between motion content and style, and generalizes well to\nnew styles thanks to the open-vocabulary ability of LLMs. Comprehensive\nexperiments and evaluations, and a user perceptual study, demonstrate the\neffectiveness of our approach, especially under the pure text-driven stylized\nmotion generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stylized motion generation is actively studied in computer graphics,\nespecially benefiting from the rapid advances in diffusion models. The goal of\nthis task is to produce a novel motion respecting both the motion content and\nthe desired motion style, e.g., ``walking in a loop like a Monkey''. Existing\nresearch attempts to address this problem via motion style transfer or\nconditional motion generation. They typically embed the motion style into a\nlatent space and guide the motion implicitly in a latent space as well. Despite\nthe progress, their methods suffer from low interpretability and control,\nlimited generalization to new styles, and fail to produce motions other than\n``walking'' due to the strong bias in the public stylization dataset. In this\npaper, we propose to solve the stylized motion generation problem from a new\nperspective of reasoning-composition-generation, based on our observations: i)\nhuman motion can often be effectively described using natural language in a\nbody-part centric manner, ii) LLMs exhibit a strong ability to understand and\nreason about human motion, and iii) human motion has an inherently\ncompositional nature, facilitating the new motion content or style generation\nvia effective recomposing. We thus propose utilizing body-part text space as an\nintermediate representation, and present SMooGPT, a fine-tuned LLM, acting as a\nreasoner, composer, and generator when generating the desired stylized motion.\nOur method executes in the body-part text space with much higher\ninterpretability, enabling fine-grained motion control, effectively resolving\npotential conflicts between motion content and style, and generalizes well to\nnew styles thanks to the open-vocabulary ability of LLMs. Comprehensive\nexperiments and evaluations, and a user perceptual study, demonstrate the\neffectiveness of our approach, especially under the pure text-driven stylized\nmotion generation."
                },
                "authors": [
                    {
                        "name": "Lei Zhong"
                    },
                    {
                        "name": "Yi Yang"
                    },
                    {
                        "name": "Changjian Li"
                    }
                ],
                "author_detail": {
                    "name": "Changjian Li"
                },
                "author": "Changjian Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04058v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04058v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01221v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01221v2",
                "updated": "2025-09-04T09:30:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    9,
                    30,
                    16,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-01T08:06:49Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    8,
                    6,
                    49,
                    0,
                    244,
                    0
                ],
                "title": "DaMoC: Efficiently Selecting the Optimal Large Language Model for\n  Fine-tuning Domain Tasks Based on Data and Model Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DaMoC: Efficiently Selecting the Optimal Large Language Model for\n  Fine-tuning Domain Tasks Based on Data and Model Compression"
                },
                "summary": "Large language models (LLMs) excel in general tasks but struggle with\ndomain-specific ones, requiring fine-tuning with specific data. With many\nopen-source LLMs available, selecting the best model for fine-tuning downstream\ntasks is challenging, primarily focusing on how to quickly identify the optimal\nLLM. We introduce a Data and Model Compression Framework (DaMoC) that addresses\nthis challenge by: 1) Data Level: A systematic categorization of data filtering\nmethodologies for LLMs is first established, classifying them into three\ndistinct paradigms: (1) distribution-aware methods, (2) quality-aware methods,\nand (3) hybrid approaches considering both dimensions. Further, we enhance the\ndensity of key tokens in the text achieving token compression. Subsequently, we\nuse an LLM to iterative rewrite the text to optimize its expression. 2) Model\nLevel: We use layer similarity scores to assess each layer's importance and\nremove those with lower importance. Then, we introduce a sparse merging\nparadigm to preserve as much of the original model's capability as possible.\nExtensive experiments on four datasets, medical Q&A, financial Q&A, general\nQ&A, and reading comprehension, show that we can select the optimal LLM while\nsaving approximately 20-fold in training time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel in general tasks but struggle with\ndomain-specific ones, requiring fine-tuning with specific data. With many\nopen-source LLMs available, selecting the best model for fine-tuning downstream\ntasks is challenging, primarily focusing on how to quickly identify the optimal\nLLM. We introduce a Data and Model Compression Framework (DaMoC) that addresses\nthis challenge by: 1) Data Level: A systematic categorization of data filtering\nmethodologies for LLMs is first established, classifying them into three\ndistinct paradigms: (1) distribution-aware methods, (2) quality-aware methods,\nand (3) hybrid approaches considering both dimensions. Further, we enhance the\ndensity of key tokens in the text achieving token compression. Subsequently, we\nuse an LLM to iterative rewrite the text to optimize its expression. 2) Model\nLevel: We use layer similarity scores to assess each layer's importance and\nremove those with lower importance. Then, we introduce a sparse merging\nparadigm to preserve as much of the original model's capability as possible.\nExtensive experiments on four datasets, medical Q&A, financial Q&A, general\nQ&A, and reading comprehension, show that we can select the optimal LLM while\nsaving approximately 20-fold in training time."
                },
                "authors": [
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Huang Wei"
                    },
                    {
                        "name": "Yinggui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yinggui Wang"
                },
                "author": "Yinggui Wang",
                "arxiv_comment": "Accepted by EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01221v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01221v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03335v2",
                "updated": "2025-09-04T09:25:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    9,
                    25,
                    5,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-03T14:10:56Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    10,
                    56,
                    2,
                    246,
                    0
                ],
                "title": "EvolveSignal: A Large Language Model Powered Coding Agent for\n  Discovering Traffic Signal Control Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvolveSignal: A Large Language Model Powered Coding Agent for\n  Discovering Traffic Signal Control Algorithms"
                },
                "summary": "In traffic engineering, the fixed-time traffic signal control remains widely\nused for its low cost, stability, and interpretability. However, its design\ndepends on hand-crafted formulas (e.g., Webster) and manual re-timing by\nengineers to adapt to demand changes, which is labor-intensive and often yields\nsuboptimal results under heterogeneous or congested conditions. This paper\nintroduces the EvolveSignal, a large language models (LLMs) powered coding\nagent to automatically discover new traffic signal control algorithms. We\nformulate the problem as program synthesis, where candidate algorithms are\nrepresented as Python functions with fixed input-output structures, and\niteratively optimized through external evaluations (e.g., a traffic simulator)\nand evolutionary search. Experiments on a signalized intersection demonstrate\nthat the discovered algorithms outperform Webster's baseline, reducing average\ndelay by 20.1% and average stops by 47.1%. Beyond performance, ablation and\nincremental analyses reveal that EvolveSignal modifications-such as adjusting\ncycle length bounds, incorporating right-turn demand, and rescaling green\nallocations-can offer practically meaningful insights for traffic engineers.\nThis work opens a new research direction by leveraging AI for algorithm design\nin traffic signal control, bridging program synthesis with transportation\nengineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In traffic engineering, the fixed-time traffic signal control remains widely\nused for its low cost, stability, and interpretability. However, its design\ndepends on hand-crafted formulas (e.g., Webster) and manual re-timing by\nengineers to adapt to demand changes, which is labor-intensive and often yields\nsuboptimal results under heterogeneous or congested conditions. This paper\nintroduces the EvolveSignal, a large language models (LLMs) powered coding\nagent to automatically discover new traffic signal control algorithms. We\nformulate the problem as program synthesis, where candidate algorithms are\nrepresented as Python functions with fixed input-output structures, and\niteratively optimized through external evaluations (e.g., a traffic simulator)\nand evolutionary search. Experiments on a signalized intersection demonstrate\nthat the discovered algorithms outperform Webster's baseline, reducing average\ndelay by 20.1% and average stops by 47.1%. Beyond performance, ablation and\nincremental analyses reveal that EvolveSignal modifications-such as adjusting\ncycle length bounds, incorporating right-turn demand, and rescaling green\nallocations-can offer practically meaningful insights for traffic engineers.\nThis work opens a new research direction by leveraging AI for algorithm design\nin traffic signal control, bridging program synthesis with transportation\nengineering."
                },
                "authors": [
                    {
                        "name": "Leizhen Wang"
                    },
                    {
                        "name": "Peibo Duan"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Jian Xu"
                    },
                    {
                        "name": "Nan Zheng"
                    },
                    {
                        "name": "Zhenliang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zhenliang Ma"
                },
                "author": "Zhenliang Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.04442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04442v1",
                "updated": "2025-09-04T17:59:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    59,
                    6,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T17:59:06Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    59,
                    6,
                    3,
                    247,
                    0
                ],
                "title": "Delta Activations: A Representation for Finetuned Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delta Activations: A Representation for Finetuned Large Language Models"
                },
                "summary": "The success of powerful open source Large Language Models (LLMs) has enabled\nthe community to create a vast collection of post-trained models adapted to\nspecific tasks and domains. However, navigating and understanding these models\nremains challenging due to inconsistent metadata and unstructured repositories.\nWe introduce Delta Activations, a method to represent finetuned models as\nvector embeddings by measuring shifts in their internal activations relative to\na base model. This representation allows for effective clustering by domain and\ntask, revealing structure in the model landscape. Delta Activations also\ndemonstrate desirable properties: it is robust across finetuning settings and\nexhibits an additive property when finetuning datasets are mixed. In addition,\nwe show that Delta Activations can embed tasks via few-shot finetuning, and\nfurther explore its use for model selection and merging. We hope Delta\nActivations can facilitate the practice of reusing publicly available models.\nCode is available at https://github.com/OscarXZQ/delta_activations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of powerful open source Large Language Models (LLMs) has enabled\nthe community to create a vast collection of post-trained models adapted to\nspecific tasks and domains. However, navigating and understanding these models\nremains challenging due to inconsistent metadata and unstructured repositories.\nWe introduce Delta Activations, a method to represent finetuned models as\nvector embeddings by measuring shifts in their internal activations relative to\na base model. This representation allows for effective clustering by domain and\ntask, revealing structure in the model landscape. Delta Activations also\ndemonstrate desirable properties: it is robust across finetuning settings and\nexhibits an additive property when finetuning datasets are mixed. In addition,\nwe show that Delta Activations can embed tasks via few-shot finetuning, and\nfurther explore its use for model selection and merging. We hope Delta\nActivations can facilitate the practice of reusing publicly available models.\nCode is available at https://github.com/OscarXZQ/delta_activations."
                },
                "authors": [
                    {
                        "name": "Zhiqiu Xu"
                    },
                    {
                        "name": "Amish Sethi"
                    },
                    {
                        "name": "Mayur Naik"
                    },
                    {
                        "name": "Ser-Nam Lim"
                    }
                ],
                "author_detail": {
                    "name": "Ser-Nam Lim"
                },
                "author": "Ser-Nam Lim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12736v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12736v2",
                "updated": "2025-09-04T17:56:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    56,
                    24,
                    3,
                    247,
                    0
                ],
                "published": "2024-11-19T18:58:03Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    58,
                    3,
                    1,
                    324,
                    0
                ],
                "title": "ACING: Actor-Critic for Instruction Learning in Black-Box LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACING: Actor-Critic for Instruction Learning in Black-Box LLMs"
                },
                "summary": "The effectiveness of Large Language Models (LLMs) in solving tasks depends\nsignificantly on the quality of their instructions, which often require\nsubstantial human effort to craft. This underscores the need for automated\ninstruction optimization. However, optimizing instructions is particularly\nchallenging when working with black-box LLMs, where model parameters and\ngradients are inaccessible. We introduce ACING, an actor-critic reinforcement\nlearning framework that formulates instruction optimization as a stateless,\ncontinuous-action problem, enabling exploration of infinite instruction spaces\nusing only black-box feedback. ACING automatically discovers prompts that\noutperform human-written prompts in 76% of instruction-induction tasks, with\ngains of up to 33 points and a 10-point median improvement over the best\nautomatic baseline in 33 tasks spanning instruction-induction, summarization,\nand chain-of-thought reasoning. Extensive ablations highlight its robustness\nand efficiency. An implementation of ACING is available at\nhttps://github.com/salmakh1/ACING.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effectiveness of Large Language Models (LLMs) in solving tasks depends\nsignificantly on the quality of their instructions, which often require\nsubstantial human effort to craft. This underscores the need for automated\ninstruction optimization. However, optimizing instructions is particularly\nchallenging when working with black-box LLMs, where model parameters and\ngradients are inaccessible. We introduce ACING, an actor-critic reinforcement\nlearning framework that formulates instruction optimization as a stateless,\ncontinuous-action problem, enabling exploration of infinite instruction spaces\nusing only black-box feedback. ACING automatically discovers prompts that\noutperform human-written prompts in 76% of instruction-induction tasks, with\ngains of up to 33 points and a 10-point median improvement over the best\nautomatic baseline in 33 tasks spanning instruction-induction, summarization,\nand chain-of-thought reasoning. Extensive ablations highlight its robustness\nand efficiency. An implementation of ACING is available at\nhttps://github.com/salmakh1/ACING."
                },
                "authors": [
                    {
                        "name": "Salma Kharrat"
                    },
                    {
                        "name": "Fares Fourati"
                    },
                    {
                        "name": "Marco Canini"
                    }
                ],
                "author_detail": {
                    "name": "Marco Canini"
                },
                "author": "Marco Canini",
                "arxiv_comment": "Accepted at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12736v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12736v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04439v1",
                "updated": "2025-09-04T17:54:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    54,
                    19,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T17:54:19Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    54,
                    19,
                    3,
                    247,
                    0
                ],
                "title": "ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory"
                },
                "summary": "While inference-time scaling enables LLMs to carry out increasingly long and\ncapable reasoning traces, the patterns and insights uncovered during these\ntraces are immediately discarded once the context window is reset for a new\nquery. External memory is a natural way to persist these discoveries, and\nrecent work has shown clear benefits for reasoning-intensive tasks. We see an\nopportunity to make such memories more broadly reusable and scalable by moving\nbeyond instance-based memory entries (e.g. exact query/response pairs, or\nsummaries tightly coupled with the original problem context) toward\nconcept-level memory: reusable, modular abstractions distilled from solution\ntraces and stored in natural language. For future queries, relevant concepts\nare selectively retrieved and integrated into the prompt, enabling test-time\ncontinual learning without weight updates. Our design introduces new strategies\nfor abstracting takeaways from rollouts and retrieving entries for new queries,\npromoting reuse and allowing memory to expand with additional experiences. On\nthe challenging ARC-AGI benchmark, our method yields a 7.5% relative gain over\na strong no-memory baseline with performance continuing to scale with inference\ncompute. We find abstract concepts to be the most consistent memory design,\noutscoring the baseline at all tested inference compute scales. Moreover, we\nconfirm that dynamically updating memory during test-time outperforms an\notherwise identical fixed memory setting with additional attempts, supporting\nthe hypothesis that solving more problems and abstracting more patterns to\nmemory enables further solutions in a form of self-improvement. Code available\nat https://github.com/matt-seb-ho/arc_memo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While inference-time scaling enables LLMs to carry out increasingly long and\ncapable reasoning traces, the patterns and insights uncovered during these\ntraces are immediately discarded once the context window is reset for a new\nquery. External memory is a natural way to persist these discoveries, and\nrecent work has shown clear benefits for reasoning-intensive tasks. We see an\nopportunity to make such memories more broadly reusable and scalable by moving\nbeyond instance-based memory entries (e.g. exact query/response pairs, or\nsummaries tightly coupled with the original problem context) toward\nconcept-level memory: reusable, modular abstractions distilled from solution\ntraces and stored in natural language. For future queries, relevant concepts\nare selectively retrieved and integrated into the prompt, enabling test-time\ncontinual learning without weight updates. Our design introduces new strategies\nfor abstracting takeaways from rollouts and retrieving entries for new queries,\npromoting reuse and allowing memory to expand with additional experiences. On\nthe challenging ARC-AGI benchmark, our method yields a 7.5% relative gain over\na strong no-memory baseline with performance continuing to scale with inference\ncompute. We find abstract concepts to be the most consistent memory design,\noutscoring the baseline at all tested inference compute scales. Moreover, we\nconfirm that dynamically updating memory during test-time outperforms an\notherwise identical fixed memory setting with additional attempts, supporting\nthe hypothesis that solving more problems and abstracting more patterns to\nmemory enables further solutions in a form of self-improvement. Code available\nat https://github.com/matt-seb-ho/arc_memo."
                },
                "authors": [
                    {
                        "name": "Matthew Ho"
                    },
                    {
                        "name": "Chen Si"
                    },
                    {
                        "name": "Zhaoxiang Feng"
                    },
                    {
                        "name": "Fangxu Yu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Zhiting Hu"
                    },
                    {
                        "name": "Lianhui Qin"
                    }
                ],
                "author_detail": {
                    "name": "Lianhui Qin"
                },
                "author": "Lianhui Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14326v2",
                "updated": "2025-09-04T17:49:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    49,
                    22,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-24T08:39:50Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    39,
                    50,
                    4,
                    24,
                    0
                ],
                "title": "Assessing Large Language Models in Comprehending and Verifying\n  Concurrent Programs across Memory Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Large Language Models in Comprehending and Verifying\n  Concurrent Programs across Memory Models"
                },
                "summary": "As concurrent programming becomes increasingly prevalent, effectively\nidentifying and addressing concurrency issues such as data races and deadlocks\nis critical. This study evaluates the performance of several leading large\nlanguage models (LLMs), including GPT-3.5-turbo, GPT-4, GPT-4o, GPT-4o-mini,\nand Mistral-AI's Large2, in understanding and analyzing concurrency issues\nwithin software programs. Given that relaxed memory models, such as Total Store\nOrder (TSO) and Partial Store Order (PSO), are widely implemented and adapted\nin modern systems, supported even by commodity architectures like ARM and x86,\nour evaluation focuses not only on sequentially consistent memory models but\nalso on these relaxed memory models. Specifically, we assess two main aspects:\nthe models' capacity to detect concurrency problems under a sequentially\nconsistent memory model and their ability to verify the correctness conditions\nof concurrent programs across both sequentially consistent and relaxed memory\nmodels. To do this, we leverage SV-COMP's pthread tests and 25 ARM Litmus tests\ndesigned to evaluate Total Store Order (TSO) and Partial Store Order (PSO)\nmemory models. The experimental results reveal that GPT-4, GPT-4o, and\nMistral-AI's Large2 demonstrate a robust understanding of concurrency issues,\neffectively identifying data races and deadlocks when assessed under a\nsequentially consistent memory model. However, despite its superior\nperformance, all selected LLMs face significant challenges verifying program\ncorrectness under relaxed memory models. These LLMs exhibit limitations in\naccurately capturing memory ordering constraints, and their current\ncapabilities fall short in verifying even small programs in these complex\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As concurrent programming becomes increasingly prevalent, effectively\nidentifying and addressing concurrency issues such as data races and deadlocks\nis critical. This study evaluates the performance of several leading large\nlanguage models (LLMs), including GPT-3.5-turbo, GPT-4, GPT-4o, GPT-4o-mini,\nand Mistral-AI's Large2, in understanding and analyzing concurrency issues\nwithin software programs. Given that relaxed memory models, such as Total Store\nOrder (TSO) and Partial Store Order (PSO), are widely implemented and adapted\nin modern systems, supported even by commodity architectures like ARM and x86,\nour evaluation focuses not only on sequentially consistent memory models but\nalso on these relaxed memory models. Specifically, we assess two main aspects:\nthe models' capacity to detect concurrency problems under a sequentially\nconsistent memory model and their ability to verify the correctness conditions\nof concurrent programs across both sequentially consistent and relaxed memory\nmodels. To do this, we leverage SV-COMP's pthread tests and 25 ARM Litmus tests\ndesigned to evaluate Total Store Order (TSO) and Partial Store Order (PSO)\nmemory models. The experimental results reveal that GPT-4, GPT-4o, and\nMistral-AI's Large2 demonstrate a robust understanding of concurrency issues,\neffectively identifying data races and deadlocks when assessed under a\nsequentially consistent memory model. However, despite its superior\nperformance, all selected LLMs face significant challenges verifying program\ncorrectness under relaxed memory models. These LLMs exhibit limitations in\naccurately capturing memory ordering constraints, and their current\ncapabilities fall short in verifying even small programs in these complex\nscenarios."
                },
                "authors": [
                    {
                        "name": "Ridhi Jain"
                    },
                    {
                        "name": "Rahul Purandare"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Purandare"
                },
                "author": "Rahul Purandare",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03312v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03312v2",
                "updated": "2025-09-04T17:49:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    49,
                    20,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-03T13:42:14Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    13,
                    42,
                    14,
                    2,
                    246,
                    0
                ],
                "title": "AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?"
                },
                "summary": "Large Language Model (LLM)-based agentic systems, often comprising multiple\nmodels, complex tool invocations, and orchestration protocols, substantially\noutperform monolithic agents. Yet this very sophistication amplifies their\nfragility, making them more prone to system failure. Pinpointing the specific\nagent or step responsible for an error within long execution traces defines the\ntask of agentic system failure attribution. Current state-of-the-art reasoning\nLLMs, however, remain strikingly inadequate for this challenge, with accuracy\ngenerally below 10%. To address this gap, we propose AgenTracer, the first\nautomated framework for annotating failed multi-agent trajectories via\ncounterfactual replay and programmed fault injection, producing the curated\ndataset TracerTraj. Leveraging this resource, we develop AgenTracer-8B, a\nlightweight failure tracer trained with multi-granular reinforcement learning,\ncapable of efficiently diagnosing errors in verbose multi-agent interactions.\nOn the Who&When benchmark, AgenTracer-8B outperforms giant proprietary LLMs\nlike Gemini-2.5-Pro and Claude-4-Sonnet by up to 18.18%, setting a new standard\nin LLM agentic failure attribution. More importantly, AgenTracer-8B delivers\nactionable feedback to off-the-shelf multi-agent systems like MetaGPT and MaAS\nwith 4.8-14.2% performance gains, empowering self-correcting and self-evolving\nagentic AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agentic systems, often comprising multiple\nmodels, complex tool invocations, and orchestration protocols, substantially\noutperform monolithic agents. Yet this very sophistication amplifies their\nfragility, making them more prone to system failure. Pinpointing the specific\nagent or step responsible for an error within long execution traces defines the\ntask of agentic system failure attribution. Current state-of-the-art reasoning\nLLMs, however, remain strikingly inadequate for this challenge, with accuracy\ngenerally below 10%. To address this gap, we propose AgenTracer, the first\nautomated framework for annotating failed multi-agent trajectories via\ncounterfactual replay and programmed fault injection, producing the curated\ndataset TracerTraj. Leveraging this resource, we develop AgenTracer-8B, a\nlightweight failure tracer trained with multi-granular reinforcement learning,\ncapable of efficiently diagnosing errors in verbose multi-agent interactions.\nOn the Who&When benchmark, AgenTracer-8B outperforms giant proprietary LLMs\nlike Gemini-2.5-Pro and Claude-4-Sonnet by up to 18.18%, setting a new standard\nin LLM agentic failure attribution. More importantly, AgenTracer-8B delivers\nactionable feedback to off-the-shelf multi-agent systems like MetaGPT and MaAS\nwith 4.8-14.2% performance gains, empowering self-correcting and self-evolving\nagentic AI."
                },
                "authors": [
                    {
                        "name": "Guibin Zhang"
                    },
                    {
                        "name": "Junhao Wang"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Shuicheng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Shuicheng Yan"
                },
                "author": "Shuicheng Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03312v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03312v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05122v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05122v2",
                "updated": "2025-09-04T17:41:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    41,
                    13,
                    3,
                    247,
                    0
                ],
                "published": "2025-05-08T10:51:13Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    10,
                    51,
                    13,
                    3,
                    128,
                    0
                ],
                "title": "Text2Cypher: Data Pruning using Hard Example Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text2Cypher: Data Pruning using Hard Example Selection"
                },
                "summary": "Database query languages such as SQL for relational databases and Cypher for\ngraph databases have been widely adopted. Recent advancements in large language\nmodels (LLMs) enable natural language interactions with databases through\nmodels like Text2SQL and Text2Cypher. Fine-tuning these models typically\nrequires large, diverse datasets containing non-trivial examples. However, as\ndataset size increases, the cost of fine-tuning also rises. This makes smaller,\nhigh-quality datasets essential for reducing costs for the same or better\nperformance. In this paper, we propose five hard-example selection techniques\nfor pruning the Text2Cypher dataset, aiming to preserve or improve performance\nwhile reducing resource usage. Our results show that these hard-example\nselection approaches can halve training time and costs with minimal impact on\nperformance, and demonstrates that hard-example selection provides a\ncost-effective solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Database query languages such as SQL for relational databases and Cypher for\ngraph databases have been widely adopted. Recent advancements in large language\nmodels (LLMs) enable natural language interactions with databases through\nmodels like Text2SQL and Text2Cypher. Fine-tuning these models typically\nrequires large, diverse datasets containing non-trivial examples. However, as\ndataset size increases, the cost of fine-tuning also rises. This makes smaller,\nhigh-quality datasets essential for reducing costs for the same or better\nperformance. In this paper, we propose five hard-example selection techniques\nfor pruning the Text2Cypher dataset, aiming to preserve or improve performance\nwhile reducing resource usage. Our results show that these hard-example\nselection approaches can halve training time and costs with minimal impact on\nperformance, and demonstrates that hard-example selection provides a\ncost-effective solution."
                },
                "authors": [
                    {
                        "name": "Makbule Gulcin Ozsoy"
                    }
                ],
                "author_detail": {
                    "name": "Makbule Gulcin Ozsoy"
                },
                "author": "Makbule Gulcin Ozsoy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05122v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05122v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01185v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01185v2",
                "updated": "2025-09-04T17:22:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    22,
                    16,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-01T07:08:45Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    7,
                    8,
                    45,
                    0,
                    244,
                    0
                ],
                "title": "Modular Techniques for Synthetic Long-Context Data Generation in\n  Language Model Training and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modular Techniques for Synthetic Long-Context Data Generation in\n  Language Model Training and Evaluation"
                },
                "summary": "The ability of large language models (LLMs) to process and reason over long\ntextual inputs is critical for a wide range of real-world applications.\nHowever, progress in this area is significantly constrained by the absence of\nhigh-quality, diverse, and verifiable long-context datasets suitable for both\ntraining and evaluation. This work introduces a modular, extensible framework\nfor synthetic long-context data generation via prompt-based interaction with\nLLMs. The framework supports multiple training and alignment objectives,\nincluding Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO),\nand Group Relative Policy Optimization (GRPO). It encompasses four core\ngeneration paradigms: multi-turn conversational dialogues, document-grounded\ninput-output pairs, verifiable instruction-response tasks, and long-context\nreasoning examples. Through templated prompting, a model-agnostic architecture,\nand metadata-enriched outputs, the proposed approach facilitates scalable,\ncontrollable, and purpose-aligned dataset creation for advancing long-context\ncapabilities in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability of large language models (LLMs) to process and reason over long\ntextual inputs is critical for a wide range of real-world applications.\nHowever, progress in this area is significantly constrained by the absence of\nhigh-quality, diverse, and verifiable long-context datasets suitable for both\ntraining and evaluation. This work introduces a modular, extensible framework\nfor synthetic long-context data generation via prompt-based interaction with\nLLMs. The framework supports multiple training and alignment objectives,\nincluding Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO),\nand Group Relative Policy Optimization (GRPO). It encompasses four core\ngeneration paradigms: multi-turn conversational dialogues, document-grounded\ninput-output pairs, verifiable instruction-response tasks, and long-context\nreasoning examples. Through templated prompting, a model-agnostic architecture,\nand metadata-enriched outputs, the proposed approach facilitates scalable,\ncontrollable, and purpose-aligned dataset creation for advancing long-context\ncapabilities in LLMs."
                },
                "authors": [
                    {
                        "name": "Seganrasan Subramanian"
                    },
                    {
                        "name": "Abhigya Verma"
                    }
                ],
                "author_detail": {
                    "name": "Abhigya Verma"
                },
                "author": "Abhigya Verma",
                "arxiv_comment": "26 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01185v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01185v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04404v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04404v1",
                "updated": "2025-09-04T17:16:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    16,
                    26,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T17:16:26Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    16,
                    26,
                    3,
                    247,
                    0
                ],
                "title": "No Thoughts Just AI: Biased LLM Recommendations Limit Human Agency in\n  Resume Screening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Thoughts Just AI: Biased LLM Recommendations Limit Human Agency in\n  Resume Screening"
                },
                "summary": "In this study, we conduct a resume-screening experiment (N=528) where people\ncollaborate with simulated AI models exhibiting race-based preferences (bias)\nto evaluate candidates for 16 high and low status occupations. Simulated AI\nbias approximates factual and counterfactual estimates of racial bias in\nreal-world AI systems. We investigate people's preferences for White, Black,\nHispanic, and Asian candidates (represented through names and affinity groups\non quality-controlled resumes) across 1,526 scenarios and measure their\nunconscious associations between race and status using implicit association\ntests (IATs), which predict discriminatory hiring decisions but have not been\ninvestigated in human-AI collaboration. When making decisions without AI or\nwith AI that exhibits no race-based preferences, people select all candidates\nat equal rates. However, when interacting with AI favoring a particular group,\npeople also favor those candidates up to 90% of the time, indicating a\nsignificant behavioral shift. The likelihood of selecting candidates whose\nidentities do not align with common race-status stereotypes can increase by 13%\nif people complete an IAT before conducting resume screening. Finally, even if\npeople think AI recommendations are low quality or not important, their\ndecisions are still vulnerable to AI bias under certain circumstances. This\nwork has implications for people's autonomy in AI-HITL scenarios, AI and work,\ndesign and evaluation of AI hiring systems, and strategies for mitigating bias\nin collaborative decision-making tasks. In particular, organizational and\nregulatory policy should acknowledge the complex nature of AI-HITL decision\nmaking when implementing these systems, educating people who use them, and\ndetermining which are subject to oversight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we conduct a resume-screening experiment (N=528) where people\ncollaborate with simulated AI models exhibiting race-based preferences (bias)\nto evaluate candidates for 16 high and low status occupations. Simulated AI\nbias approximates factual and counterfactual estimates of racial bias in\nreal-world AI systems. We investigate people's preferences for White, Black,\nHispanic, and Asian candidates (represented through names and affinity groups\non quality-controlled resumes) across 1,526 scenarios and measure their\nunconscious associations between race and status using implicit association\ntests (IATs), which predict discriminatory hiring decisions but have not been\ninvestigated in human-AI collaboration. When making decisions without AI or\nwith AI that exhibits no race-based preferences, people select all candidates\nat equal rates. However, when interacting with AI favoring a particular group,\npeople also favor those candidates up to 90% of the time, indicating a\nsignificant behavioral shift. The likelihood of selecting candidates whose\nidentities do not align with common race-status stereotypes can increase by 13%\nif people complete an IAT before conducting resume screening. Finally, even if\npeople think AI recommendations are low quality or not important, their\ndecisions are still vulnerable to AI bias under certain circumstances. This\nwork has implications for people's autonomy in AI-HITL scenarios, AI and work,\ndesign and evaluation of AI hiring systems, and strategies for mitigating bias\nin collaborative decision-making tasks. In particular, organizational and\nregulatory policy should acknowledge the complex nature of AI-HITL decision\nmaking when implementing these systems, educating people who use them, and\ndetermining which are subject to oversight."
                },
                "authors": [
                    {
                        "name": "Kyra Wilson"
                    },
                    {
                        "name": "Mattea Sim"
                    },
                    {
                        "name": "Anna-Maria Gueorguieva"
                    },
                    {
                        "name": "Aylin Caliskan"
                    }
                ],
                "author_detail": {
                    "name": "Aylin Caliskan"
                },
                "author": "Aylin Caliskan",
                "arxiv_comment": "Published in Proceedings of the 2025 AAAI/ACM Conference on AI,\n  Ethics, and Society; code available at\n  https://github.com/kyrawilson/No-Thoughts-Just-AI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04404v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04404v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.4.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04392v1",
                "updated": "2025-09-04T17:03:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    3,
                    58,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T17:03:58Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    3,
                    58,
                    3,
                    247,
                    0
                ],
                "title": "Denoising GER: A Noise-Robust Generative Error Correction with LLM for\n  Speech Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Denoising GER: A Noise-Robust Generative Error Correction with LLM for\n  Speech Recognition"
                },
                "summary": "In recent years, large language models (LLM) have made significant progress\nin the task of generation error correction (GER) for automatic speech\nrecognition (ASR) post-processing. However, in complex noisy environments, they\nstill face challenges such as poor adaptability and low information\nutilization, resulting in limited effectiveness of GER. To address these\nissues, this paper proposes a noise-robust multi-modal GER framework (Denoising\nGER). The framework enhances the model's adaptability to different noisy\nscenarios through a noise-adaptive acoustic encoder and optimizes the\nintegration of multi-modal information via a heterogeneous feature compensation\ndynamic fusion (HFCDF) mechanism, improving the LLM's utilization of\nmulti-modal information. Additionally, reinforcement learning (RL) training\nstrategies are introduced to enhance the model's predictive capabilities.\nExperimental results demonstrate that Denoising GER significantly improves\naccuracy and robustness in noisy environments and exhibits good generalization\nabilities in unseen noise scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLM) have made significant progress\nin the task of generation error correction (GER) for automatic speech\nrecognition (ASR) post-processing. However, in complex noisy environments, they\nstill face challenges such as poor adaptability and low information\nutilization, resulting in limited effectiveness of GER. To address these\nissues, this paper proposes a noise-robust multi-modal GER framework (Denoising\nGER). The framework enhances the model's adaptability to different noisy\nscenarios through a noise-adaptive acoustic encoder and optimizes the\nintegration of multi-modal information via a heterogeneous feature compensation\ndynamic fusion (HFCDF) mechanism, improving the LLM's utilization of\nmulti-modal information. Additionally, reinforcement learning (RL) training\nstrategies are introduced to enhance the model's predictive capabilities.\nExperimental results demonstrate that Denoising GER significantly improves\naccuracy and robustness in noisy environments and exhibits good generalization\nabilities in unseen noise scenarios."
                },
                "authors": [
                    {
                        "name": "Yanyan Liu"
                    },
                    {
                        "name": "Minqiang Xu"
                    },
                    {
                        "name": "Yihao Chen"
                    },
                    {
                        "name": "Liang He"
                    },
                    {
                        "name": "Lei Fang"
                    },
                    {
                        "name": "Sian Fang"
                    },
                    {
                        "name": "Lin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Lin Liu"
                },
                "author": "Lin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04377v1",
                "updated": "2025-09-04T16:40:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    40,
                    1,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T16:40:01Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    40,
                    1,
                    3,
                    247,
                    0
                ],
                "title": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient\n  Large Language Model Inference"
                },
                "summary": "KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks."
                },
                "authors": [
                    {
                        "name": "Krishna Teja Chitty-Venkata"
                    },
                    {
                        "name": "Jie Ye"
                    },
                    {
                        "name": "Xian-He Sun"
                    },
                    {
                        "name": "Anthony Kougkas"
                    },
                    {
                        "name": "Murali Emani"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    },
                    {
                        "name": "Bogdan Nicolae"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Nicolae"
                },
                "author": "Bogdan Nicolae",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04376v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04376v2",
                "updated": "2025-09-05T02:40:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    2,
                    40,
                    36,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-04T16:34:46Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    34,
                    46,
                    3,
                    247,
                    0
                ],
                "title": "AnomalyLMM: Bridging Generative Knowledge and Discriminative Retrieval\n  for Text-Based Person Anomaly Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnomalyLMM: Bridging Generative Knowledge and Discriminative Retrieval\n  for Text-Based Person Anomaly Search"
                },
                "summary": "With growing public safety demands, text-based person anomaly search has\nemerged as a critical task, aiming to retrieve individuals with abnormal\nbehaviors via natural language descriptions. Unlike conventional person search,\nthis task presents two unique challenges: (1) fine-grained cross-modal\nalignment between textual anomalies and visual behaviors, and (2) anomaly\nrecognition under sparse real-world samples. While Large Multi-modal Models\n(LMMs) excel in multi-modal understanding, their potential for fine-grained\nanomaly retrieval remains underexplored, hindered by: (1) a domain gap between\ngenerative knowledge and discriminative retrieval, and (2) the absence of\nefficient adaptation strategies for deployment. In this work, we propose\nAnomalyLMM, the first framework that harnesses LMMs for text-based person\nanomaly search. Our key contributions are: (1) A novel coarse-to-fine pipeline\nintegrating LMMs to bridge generative world knowledge with retrieval-centric\nanomaly detection; (2) A training-free adaptation cookbook featuring masked\ncross-modal prompting, behavioral saliency prediction, and knowledge-aware\nre-ranking, enabling zero-shot focus on subtle anomaly cues. As the first study\nto explore LMMs for this task, we conduct a rigorous evaluation on the PAB\ndataset, the only publicly available benchmark for text-based person anomaly\nsearch, with its curated real-world anomalies covering diverse scenarios (e.g.,\nfalling, collision, and being hit). Experiments show the effectiveness of the\nproposed method, surpassing the competitive baseline by +0.96% Recall@1\naccuracy. Notably, our method reveals interpretable alignment between textual\nanomalies and visual behaviors, validated via qualitative analysis. Our code\nand models will be released for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With growing public safety demands, text-based person anomaly search has\nemerged as a critical task, aiming to retrieve individuals with abnormal\nbehaviors via natural language descriptions. Unlike conventional person search,\nthis task presents two unique challenges: (1) fine-grained cross-modal\nalignment between textual anomalies and visual behaviors, and (2) anomaly\nrecognition under sparse real-world samples. While Large Multi-modal Models\n(LMMs) excel in multi-modal understanding, their potential for fine-grained\nanomaly retrieval remains underexplored, hindered by: (1) a domain gap between\ngenerative knowledge and discriminative retrieval, and (2) the absence of\nefficient adaptation strategies for deployment. In this work, we propose\nAnomalyLMM, the first framework that harnesses LMMs for text-based person\nanomaly search. Our key contributions are: (1) A novel coarse-to-fine pipeline\nintegrating LMMs to bridge generative world knowledge with retrieval-centric\nanomaly detection; (2) A training-free adaptation cookbook featuring masked\ncross-modal prompting, behavioral saliency prediction, and knowledge-aware\nre-ranking, enabling zero-shot focus on subtle anomaly cues. As the first study\nto explore LMMs for this task, we conduct a rigorous evaluation on the PAB\ndataset, the only publicly available benchmark for text-based person anomaly\nsearch, with its curated real-world anomalies covering diverse scenarios (e.g.,\nfalling, collision, and being hit). Experiments show the effectiveness of the\nproposed method, surpassing the competitive baseline by +0.96% Recall@1\naccuracy. Notably, our method reveals interpretable alignment between textual\nanomalies and visual behaviors, validated via qualitative analysis. Our code\nand models will be released for future research."
                },
                "authors": [
                    {
                        "name": "Hao Ju"
                    },
                    {
                        "name": "Hu Zhang"
                    },
                    {
                        "name": "Zhedong Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhedong Zheng"
                },
                "author": "Zhedong Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04376v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04376v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04373v1",
                "updated": "2025-09-04T16:32:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    32,
                    18,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T16:32:18Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    32,
                    18,
                    3,
                    247,
                    0
                ],
                "title": "Measuring Bias or Measuring the Task: Understanding the Brittle Nature\n  of LLM Gender Biases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Bias or Measuring the Task: Understanding the Brittle Nature\n  of LLM Gender Biases"
                },
                "summary": "As LLMs are increasingly applied in socially impactful settings, concerns\nabout gender bias have prompted growing efforts both to measure and mitigate\nsuch bias. These efforts often rely on evaluation tasks that differ from\nnatural language distributions, as they typically involve carefully constructed\ntask prompts that overtly or covertly signal the presence of gender\nbias-related content. In this paper, we examine how signaling the evaluative\npurpose of a task impacts measured gender bias in LLMs. Concretely, we test\nmodels under prompt conditions that (1) make the testing context salient, and\n(2) make gender-focused content salient. We then assess prompt sensitivity\nacross four task formats with both token-probability and discrete-choice\nmetrics. We find that even minor prompt changes can substantially alter bias\noutcomes, sometimes reversing their direction entirely. Discrete-choice metrics\nfurther tend to amplify bias relative to probabilistic measures. These findings\ndo not only highlight the brittleness of LLM gender bias evaluations but open a\nnew puzzle for the NLP benchmarking and development community: To what extent\ncan well-controlled testing designs trigger LLM ``testing mode'' performance,\nand what does this mean for the ecological validity of future benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs are increasingly applied in socially impactful settings, concerns\nabout gender bias have prompted growing efforts both to measure and mitigate\nsuch bias. These efforts often rely on evaluation tasks that differ from\nnatural language distributions, as they typically involve carefully constructed\ntask prompts that overtly or covertly signal the presence of gender\nbias-related content. In this paper, we examine how signaling the evaluative\npurpose of a task impacts measured gender bias in LLMs. Concretely, we test\nmodels under prompt conditions that (1) make the testing context salient, and\n(2) make gender-focused content salient. We then assess prompt sensitivity\nacross four task formats with both token-probability and discrete-choice\nmetrics. We find that even minor prompt changes can substantially alter bias\noutcomes, sometimes reversing their direction entirely. Discrete-choice metrics\nfurther tend to amplify bias relative to probabilistic measures. These findings\ndo not only highlight the brittleness of LLM gender bias evaluations but open a\nnew puzzle for the NLP benchmarking and development community: To what extent\ncan well-controlled testing designs trigger LLM ``testing mode'' performance,\nand what does this mean for the ecological validity of future benchmarks."
                },
                "authors": [
                    {
                        "name": "Bufan Gao"
                    },
                    {
                        "name": "Elisa Kreiss"
                    }
                ],
                "author_detail": {
                    "name": "Elisa Kreiss"
                },
                "author": "Elisa Kreiss",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07900v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07900v2",
                "updated": "2025-09-04T16:23:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    23,
                    2,
                    3,
                    247,
                    0
                ],
                "published": "2025-06-09T16:16:50Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    16,
                    16,
                    50,
                    0,
                    160,
                    0
                ],
                "title": "MiniCPM4: Ultra-Efficient LLMs on End Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniCPM4: Ultra-Efficient LLMs on End Devices"
                },
                "summary": "This paper introduces MiniCPM4, a highly efficient large language model (LLM)\ndesigned explicitly for end-side devices. We achieve this efficiency through\nsystematic innovation in four key dimensions: model architecture, training\ndata, training algorithms, and inference systems. Specifically, in terms of\nmodel architecture, we propose InfLLM v2, a trainable sparse attention\nmechanism that accelerates both prefilling and decoding phases for long-context\nprocessing. Regarding training data, we propose UltraClean, an efficient and\naccurate pre-training data filtering and generation strategy, and UltraChat v2,\na comprehensive supervised fine-tuning dataset. These datasets enable\nsatisfactory model performance to be achieved using just 8 trillion training\ntokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient\npre-training strategy search, and improve existing post-training methods by\nintroducing chunk-wise rollout for load-balanced reinforcement learning and\ndata-efficient tenary LLM, BitCPM. Regarding inference systems, we propose\nCPM.cu that integrates sparse attention, model quantization, and speculative\nsampling to achieve efficient prefilling and decoding. To meet diverse\non-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B\nparameters, respectively. Furthermore, we construct a hybrid reasoning model,\nMiniCPM4.1, which can be used in both deep reasoning mode and non-reasoning\nmode. Evaluation results demonstrate that MiniCPM4 and MiniCPM4.1 outperform\nsimilar-sized open-source models across benchmarks, with the 8B variants\nshowing significant speed improvements on long sequence understanding and\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces MiniCPM4, a highly efficient large language model (LLM)\ndesigned explicitly for end-side devices. We achieve this efficiency through\nsystematic innovation in four key dimensions: model architecture, training\ndata, training algorithms, and inference systems. Specifically, in terms of\nmodel architecture, we propose InfLLM v2, a trainable sparse attention\nmechanism that accelerates both prefilling and decoding phases for long-context\nprocessing. Regarding training data, we propose UltraClean, an efficient and\naccurate pre-training data filtering and generation strategy, and UltraChat v2,\na comprehensive supervised fine-tuning dataset. These datasets enable\nsatisfactory model performance to be achieved using just 8 trillion training\ntokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient\npre-training strategy search, and improve existing post-training methods by\nintroducing chunk-wise rollout for load-balanced reinforcement learning and\ndata-efficient tenary LLM, BitCPM. Regarding inference systems, we propose\nCPM.cu that integrates sparse attention, model quantization, and speculative\nsampling to achieve efficient prefilling and decoding. To meet diverse\non-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B\nparameters, respectively. Furthermore, we construct a hybrid reasoning model,\nMiniCPM4.1, which can be used in both deep reasoning mode and non-reasoning\nmode. Evaluation results demonstrate that MiniCPM4 and MiniCPM4.1 outperform\nsimilar-sized open-source models across benchmarks, with the 8B variants\nshowing significant speed improvements on long sequence understanding and\ngeneration."
                },
                "authors": [
                    {
                        "name": "MiniCPM Team"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Yuxuan Li"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Yuzhuo Bai"
                    },
                    {
                        "name": "Jie Cai"
                    },
                    {
                        "name": "Haotian Chen"
                    },
                    {
                        "name": "Wentong Chen"
                    },
                    {
                        "name": "Xin Cong"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Shengda Fan"
                    },
                    {
                        "name": "Yewei Fang"
                    },
                    {
                        "name": "Zixuan Fu"
                    },
                    {
                        "name": "Wenyu Guan"
                    },
                    {
                        "name": "Yitong Guan"
                    },
                    {
                        "name": "Junshao Guo"
                    },
                    {
                        "name": "Yufeng Han"
                    },
                    {
                        "name": "Bingxiang He"
                    },
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Baoxi Ji"
                    },
                    {
                        "name": "Cunliang Kong"
                    },
                    {
                        "name": "Qiuzuo Li"
                    },
                    {
                        "name": "Siyuan Li"
                    },
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Yishan Li"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Dan Liu"
                    },
                    {
                        "name": "Biyuan Lin"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Xiang Long"
                    },
                    {
                        "name": "Quanyu Lu"
                    },
                    {
                        "name": "Yaxi Lu"
                    },
                    {
                        "name": "Peiyan Luo"
                    },
                    {
                        "name": "Hongya Lyu"
                    },
                    {
                        "name": "Litu Ou"
                    },
                    {
                        "name": "Yinxu Pan"
                    },
                    {
                        "name": "Lushi Pu"
                    },
                    {
                        "name": "Zekai Qu"
                    },
                    {
                        "name": "Qundong Shi"
                    },
                    {
                        "name": "Zijun Song"
                    },
                    {
                        "name": "Jiayuan Su"
                    },
                    {
                        "name": "Zhou Su"
                    },
                    {
                        "name": "Ao Sun"
                    },
                    {
                        "name": "Xianghui Sun"
                    },
                    {
                        "name": "Peijun Tang"
                    },
                    {
                        "name": "Fangzheng Wang"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Yudong Wang"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Yesai Wu"
                    },
                    {
                        "name": "Zhenyu Xiao"
                    },
                    {
                        "name": "Jie Xie"
                    },
                    {
                        "name": "Zihao Xie"
                    },
                    {
                        "name": "Xiaoyue Xu"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Jiarui Yuan"
                    },
                    {
                        "name": "Jinqian Zhang"
                    },
                    {
                        "name": "Kaihuo Zhang"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Linyue Zhang"
                    },
                    {
                        "name": "Xueren Zhang"
                    },
                    {
                        "name": "Yudi Zhang"
                    },
                    {
                        "name": "Hengyu Zhao"
                    },
                    {
                        "name": "Weilin Zhao"
                    },
                    {
                        "name": "Weilun Zhao"
                    },
                    {
                        "name": "Yuanqian Zhao"
                    },
                    {
                        "name": "Zhi Zheng"
                    },
                    {
                        "name": "Chuyue Zhou"
                    },
                    {
                        "name": "Ge Zhou"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Yanghao Zhou"
                    },
                    {
                        "name": "Zihan Zhou"
                    },
                    {
                        "name": "Zixuan Zhou"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Guoyang Zeng"
                    },
                    {
                        "name": "Chao Jia"
                    },
                    {
                        "name": "Dahai Li"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "MiniCPM4 Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07900v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07900v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01747v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01747v3",
                "updated": "2025-09-04T16:22:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    22,
                    32,
                    3,
                    247,
                    0
                ],
                "published": "2024-11-04T02:08:59Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    8,
                    59,
                    0,
                    309,
                    0
                ],
                "title": "DynaSaur: Large Language Agents Beyond Predefined Actions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynaSaur: Large Language Agents Beyond Predefined Actions"
                },
                "summary": "Existing LLM agent systems typically select actions from a fixed and\npredefined set at every step. While this approach is effective in closed,\nnarrowly scoped environments, it presents two major challenges for real-world,\nopen-ended scenarios: (1) it significantly restricts the planning and acting\ncapabilities of LLM agents, and (2) it requires substantial human effort to\nenumerate and implement all possible actions, which is impractical in complex\nenvironments with a vast number of potential actions. To address these\nlimitations, we propose an LLM agent framework that can dynamically create and\ncompose actions as needed. In this framework, the agent interacts with its\nenvironment by generating and executing programs written in a general-purpose\nprogramming language. Moreover, generated actions are accumulated over time for\nfuture reuse. Our extensive experiments across multiple benchmarks show that\nthis framework significantly improves flexibility and outperforms prior methods\nthat rely on a fixed action set. Notably, it enables LLM agents to adapt and\nrecover in scenarios where predefined actions are insufficient or fail due to\nunforeseen edge cases. Our code can be found in\nhttps://github.com/adobe-research/dynasaur.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM agent systems typically select actions from a fixed and\npredefined set at every step. While this approach is effective in closed,\nnarrowly scoped environments, it presents two major challenges for real-world,\nopen-ended scenarios: (1) it significantly restricts the planning and acting\ncapabilities of LLM agents, and (2) it requires substantial human effort to\nenumerate and implement all possible actions, which is impractical in complex\nenvironments with a vast number of potential actions. To address these\nlimitations, we propose an LLM agent framework that can dynamically create and\ncompose actions as needed. In this framework, the agent interacts with its\nenvironment by generating and executing programs written in a general-purpose\nprogramming language. Moreover, generated actions are accumulated over time for\nfuture reuse. Our extensive experiments across multiple benchmarks show that\nthis framework significantly improves flexibility and outperforms prior methods\nthat rely on a fixed action set. Notably, it enables LLM agents to adapt and\nrecover in scenarios where predefined actions are insufficient or fail due to\nunforeseen edge cases. Our code can be found in\nhttps://github.com/adobe-research/dynasaur."
                },
                "authors": [
                    {
                        "name": "Dang Nguyen"
                    },
                    {
                        "name": "Viet Dac Lai"
                    },
                    {
                        "name": "Seunghyun Yoon"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Handong Zhao"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Puneet Mathur"
                    },
                    {
                        "name": "Nedim Lipka"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Tianyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhou"
                },
                "author": "Tianyi Zhou",
                "arxiv_comment": "Published as a conference paper at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01747v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01747v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04356v1",
                "updated": "2025-09-04T16:18:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    18,
                    4,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T16:18:04Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    18,
                    4,
                    3,
                    247,
                    0
                ],
                "title": "SRWToolkit: An Open Source Wizard of Oz Toolkit to Create Social Robotic\n  Avatars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SRWToolkit: An Open Source Wizard of Oz Toolkit to Create Social Robotic\n  Avatars"
                },
                "summary": "We present SRWToolkit, an open-source Wizard of Oz toolkit designed to\nfacilitate the rapid prototyping of social robotic avatars powered by local\nlarge language models (LLMs). Our web-based toolkit enables multimodal\ninteraction through text input, button-activated speech, and wake-word command.\nThe toolkit offers real-time configuration of avatar appearance, behavior,\nlanguage, and voice via an intuitive control panel. In contrast to prior works\nthat rely on cloud-based LLM services, SRWToolkit emphasizes modularity and\nensures on-device functionality through local LLM inference. In our small-scale\nuser study ($n=11$), participants created and interacted with diverse robotic\nroles (hospital receptionist, mathematics teacher, and driving assistant),\nwhich demonstrated positive outcomes in the toolkit's usability, trust, and\nuser experience. The toolkit enables rapid and efficient development of robot\ncharacters customized to researchers' needs, supporting scalable research in\nhuman-robot interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present SRWToolkit, an open-source Wizard of Oz toolkit designed to\nfacilitate the rapid prototyping of social robotic avatars powered by local\nlarge language models (LLMs). Our web-based toolkit enables multimodal\ninteraction through text input, button-activated speech, and wake-word command.\nThe toolkit offers real-time configuration of avatar appearance, behavior,\nlanguage, and voice via an intuitive control panel. In contrast to prior works\nthat rely on cloud-based LLM services, SRWToolkit emphasizes modularity and\nensures on-device functionality through local LLM inference. In our small-scale\nuser study ($n=11$), participants created and interacted with diverse robotic\nroles (hospital receptionist, mathematics teacher, and driving assistant),\nwhich demonstrated positive outcomes in the toolkit's usability, trust, and\nuser experience. The toolkit enables rapid and efficient development of robot\ncharacters customized to researchers' needs, supporting scalable research in\nhuman-robot interaction."
                },
                "authors": [
                    {
                        "name": "Atikkhan Faridkhan Nilgar"
                    },
                    {
                        "name": "Kristof Van Laerhoven"
                    },
                    {
                        "name": "Ayub Kinoti"
                    }
                ],
                "author_detail": {
                    "name": "Ayub Kinoti"
                },
                "author": "Ayub Kinoti",
                "arxiv_journal_ref": "2025 International Conference on Social Robotics (ICSR)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04343v1",
                "updated": "2025-09-04T16:03:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    3,
                    3,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T16:03:03Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    3,
                    3,
                    3,
                    247,
                    0
                ],
                "title": "Psychologically Enhanced AI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Psychologically Enhanced AI Agents"
                },
                "summary": "We introduce MBTI-in-Thoughts, a framework for enhancing the effectiveness of\nLarge Language Model (LLM) agents through psychologically grounded personality\nconditioning. Drawing on the Myers-Briggs Type Indicator (MBTI), our method\nprimes agents with distinct personality archetypes via prompt engineering,\nenabling control over behavior along two foundational axes of human psychology,\ncognition and affect. We show that such personality priming yields consistent,\ninterpretable behavioral biases across diverse tasks: emotionally expressive\nagents excel in narrative generation, while analytically primed agents adopt\nmore stable strategies in game-theoretic settings. Our framework supports\nexperimenting with structured multi-agent communication protocols and reveals\nthat self-reflection prior to interaction improves cooperation and reasoning\nquality. To ensure trait persistence, we integrate the official 16Personalities\ntest for automated verification. While our focus is on MBTI, we show that our\napproach generalizes seamlessly to other psychological frameworks such as Big\nFive, HEXACO, or Enneagram. By bridging psychological theory and LLM behavior\ndesign, we establish a foundation for psychologically enhanced AI agents\nwithout any fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MBTI-in-Thoughts, a framework for enhancing the effectiveness of\nLarge Language Model (LLM) agents through psychologically grounded personality\nconditioning. Drawing on the Myers-Briggs Type Indicator (MBTI), our method\nprimes agents with distinct personality archetypes via prompt engineering,\nenabling control over behavior along two foundational axes of human psychology,\ncognition and affect. We show that such personality priming yields consistent,\ninterpretable behavioral biases across diverse tasks: emotionally expressive\nagents excel in narrative generation, while analytically primed agents adopt\nmore stable strategies in game-theoretic settings. Our framework supports\nexperimenting with structured multi-agent communication protocols and reveals\nthat self-reflection prior to interaction improves cooperation and reasoning\nquality. To ensure trait persistence, we integrate the official 16Personalities\ntest for automated verification. While our focus is on MBTI, we show that our\napproach generalizes seamlessly to other psychological frameworks such as Big\nFive, HEXACO, or Enneagram. By bridging psychological theory and LLM behavior\ndesign, we establish a foundation for psychologically enhanced AI agents\nwithout any fine-tuning."
                },
                "authors": [
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Shriram Chandran"
                    },
                    {
                        "name": "Robert Gerstenberger"
                    },
                    {
                        "name": "Mathis Lindner"
                    },
                    {
                        "name": "Marcin Chrapek"
                    },
                    {
                        "name": "Sebastian Hermann Martschat"
                    },
                    {
                        "name": "Taraneh Ghandi"
                    },
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Hubert Niewiadomski"
                    },
                    {
                        "name": "Piotr Nyczyk"
                    },
                    {
                        "name": "Jrgen Mller"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04340v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04340v1",
                "updated": "2025-09-04T15:59:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    59,
                    45,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T15:59:45Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    59,
                    45,
                    3,
                    247,
                    0
                ],
                "title": "Write on Paper, Wrong in Practice: Why LLMs Still Struggle with Writing\n  Clinical Notes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Write on Paper, Wrong in Practice: Why LLMs Still Struggle with Writing\n  Clinical Notes"
                },
                "summary": "Large Language Models (LLMs) are often proposed as tools to streamline\nclinical documentation, a task viewed as both high-volume and low-risk.\nHowever, even seemingly straightforward applications of LLMs raise complex\nsociotechnical considerations to translate into practice. This case study,\nconducted at KidsAbility, a pediatric rehabilitation facility in Ontario,\nCanada examined the use of LLMs to support occupational therapists in reducing\ndocumentation burden.We conducted a qualitative study involving 20 clinicians\nwho participated in pilot programs using two AI technologies: a general-purpose\nproprietary LLM and a bespoke model fine-tuned on proprietary historical\ndocumentation.\n  Our findings reveal that documentation challenges are sociotechnical in\nnature, shaped by clinical workflows, organizational policies, and system\nconstraints. Four key themes emerged: (1) the heterogeneity of workflows, (2)\nthe documentation burden is systemic and not directly linked to the creation of\nany single type of documentation, (3) the need for flexible tools and clinician\nautonomy, and (4) effective implementation requires mutual learning between\nclinicians and AI systems.\n  While LLMs show promise in easing documentation tasks, their success will\ndepend on flexible, adaptive integration that supports clinician autonomy.\nBeyond technical performance, sustained adoption will require training programs\nand implementation strategies that reflect the complexity of clinical\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are often proposed as tools to streamline\nclinical documentation, a task viewed as both high-volume and low-risk.\nHowever, even seemingly straightforward applications of LLMs raise complex\nsociotechnical considerations to translate into practice. This case study,\nconducted at KidsAbility, a pediatric rehabilitation facility in Ontario,\nCanada examined the use of LLMs to support occupational therapists in reducing\ndocumentation burden.We conducted a qualitative study involving 20 clinicians\nwho participated in pilot programs using two AI technologies: a general-purpose\nproprietary LLM and a bespoke model fine-tuned on proprietary historical\ndocumentation.\n  Our findings reveal that documentation challenges are sociotechnical in\nnature, shaped by clinical workflows, organizational policies, and system\nconstraints. Four key themes emerged: (1) the heterogeneity of workflows, (2)\nthe documentation burden is systemic and not directly linked to the creation of\nany single type of documentation, (3) the need for flexible tools and clinician\nautonomy, and (4) effective implementation requires mutual learning between\nclinicians and AI systems.\n  While LLMs show promise in easing documentation tasks, their success will\ndepend on flexible, adaptive integration that supports clinician autonomy.\nBeyond technical performance, sustained adoption will require training programs\nand implementation strategies that reflect the complexity of clinical\nenvironments."
                },
                "authors": [
                    {
                        "name": "Kristina L. Kupferschmidt"
                    },
                    {
                        "name": "Kieran O'Doherty"
                    },
                    {
                        "name": "Joshua A. Skorburg"
                    }
                ],
                "author_detail": {
                    "name": "Joshua A. Skorburg"
                },
                "author": "Joshua A. Skorburg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04340v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04340v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14723v2",
                "updated": "2025-09-04T15:58:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    58,
                    17,
                    3,
                    247,
                    0
                ],
                "published": "2025-08-20T14:05:18Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    5,
                    18,
                    2,
                    232,
                    0
                ],
                "title": "Transplant Then Regenerate: A New Paradigm for Text Data Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transplant Then Regenerate: A New Paradigm for Text Data Augmentation"
                },
                "summary": "Data augmentation is a critical technique in deep learning. Traditional\nmethods like Back-translation typically focus on lexical-level rephrasing,\nwhich primarily produces variations with the same semantics. While large\nlanguage models (LLMs) have enhanced text augmentation by their \"knowledge\nemergence\" capability, controlling the style and structure of these outputs\nremains challenging and requires meticulous prompt engineering. In this paper,\nwe propose LMTransplant, a novel text augmentation paradigm leveraging LLMs.\nThe core idea of LMTransplant is transplant-then-regenerate: incorporating seed\ntext into a context expanded by LLM, and asking the LLM to regenerate a variant\nbased on the expanded context. This strategy allows the model to create more\ndiverse and creative content-level variants by fully leveraging the knowledge\nembedded in LLMs, while preserving the core attributes of the original text. We\nevaluate LMTransplant across various text-related tasks, demonstrating its\nsuperior performance over existing text augmentation methods. Moreover,\nLMTransplant demonstrates exceptional scalability as the size of augmented data\ngrows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data augmentation is a critical technique in deep learning. Traditional\nmethods like Back-translation typically focus on lexical-level rephrasing,\nwhich primarily produces variations with the same semantics. While large\nlanguage models (LLMs) have enhanced text augmentation by their \"knowledge\nemergence\" capability, controlling the style and structure of these outputs\nremains challenging and requires meticulous prompt engineering. In this paper,\nwe propose LMTransplant, a novel text augmentation paradigm leveraging LLMs.\nThe core idea of LMTransplant is transplant-then-regenerate: incorporating seed\ntext into a context expanded by LLM, and asking the LLM to regenerate a variant\nbased on the expanded context. This strategy allows the model to create more\ndiverse and creative content-level variants by fully leveraging the knowledge\nembedded in LLMs, while preserving the core attributes of the original text. We\nevaluate LMTransplant across various text-related tasks, demonstrating its\nsuperior performance over existing text augmentation methods. Moreover,\nLMTransplant demonstrates exceptional scalability as the size of augmented data\ngrows."
                },
                "authors": [
                    {
                        "name": "Guangzhan Wang"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Beijun Shen"
                    },
                    {
                        "name": "Xiaodong Gu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodong Gu"
                },
                "author": "Xiaodong Gu",
                "arxiv_comment": "Accepted by EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04316v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04316v2",
                "updated": "2025-09-04T15:53:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    53,
                    10,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-08T07:28:10Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    28,
                    10,
                    2,
                    8,
                    0
                ],
                "title": "Small Changes, Large Consequences: Analyzing the Allocational Fairness\n  of LLMs in Hiring Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Changes, Large Consequences: Analyzing the Allocational Fairness\n  of LLMs in Hiring Contexts"
                },
                "summary": "Large language models (LLMs) are increasingly being deployed in high-stakes\napplications like hiring, yet their potential for unfair decision-making\nremains understudied in generative and retrieval settings. In this work, we\nexamine the allocational fairness of LLM-based hiring systems through two tasks\nthat reflect actual HR usage: resume summarization and applicant ranking. By\nconstructing a synthetic resume dataset with controlled perturbations and\ncurating job postings, we investigate whether model behavior differs across\ndemographic groups. Our findings reveal that generated summaries exhibit\nmeaningful differences more frequently for race than for gender perturbations.\nModels also display non-uniform retrieval selection patterns across demographic\ngroups and exhibit high ranking sensitivity to both gender and race\nperturbations. Surprisingly, retrieval models can show comparable sensitivity\nto both demographic and non-demographic changes, suggesting that fairness\nissues may stem from broader model brittleness. Overall, our results indicate\nthat LLM-based hiring systems, especially in the retrieval stage, can exhibit\nnotable biases that lead to discriminatory outcomes in real-world contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being deployed in high-stakes\napplications like hiring, yet their potential for unfair decision-making\nremains understudied in generative and retrieval settings. In this work, we\nexamine the allocational fairness of LLM-based hiring systems through two tasks\nthat reflect actual HR usage: resume summarization and applicant ranking. By\nconstructing a synthetic resume dataset with controlled perturbations and\ncurating job postings, we investigate whether model behavior differs across\ndemographic groups. Our findings reveal that generated summaries exhibit\nmeaningful differences more frequently for race than for gender perturbations.\nModels also display non-uniform retrieval selection patterns across demographic\ngroups and exhibit high ranking sensitivity to both gender and race\nperturbations. Surprisingly, retrieval models can show comparable sensitivity\nto both demographic and non-demographic changes, suggesting that fairness\nissues may stem from broader model brittleness. Overall, our results indicate\nthat LLM-based hiring systems, especially in the retrieval stage, can exhibit\nnotable biases that lead to discriminatory outcomes in real-world contexts."
                },
                "authors": [
                    {
                        "name": "Preethi Seshadri"
                    },
                    {
                        "name": "Hongyu Chen"
                    },
                    {
                        "name": "Sameer Singh"
                    },
                    {
                        "name": "Seraphina Goldfarb-Tarrant"
                    }
                ],
                "author_detail": {
                    "name": "Seraphina Goldfarb-Tarrant"
                },
                "author": "Seraphina Goldfarb-Tarrant",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04316v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04316v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04328v1",
                "updated": "2025-09-04T15:48:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    48,
                    13,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T15:48:13Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    48,
                    13,
                    3,
                    247,
                    0
                ],
                "title": "FaaSGuard: Secure CI/CD for Serverless Applications -- An OpenFaaS Case\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaaSGuard: Secure CI/CD for Serverless Applications -- An OpenFaaS Case\n  Study"
                },
                "summary": "Serverless computing significantly alters software development by abstracting\ninfrastructure management and enabling rapid, modular, event-driven\ndeployments. Despite its benefits, the distinct characteristics of serverless\nfunctions, such as ephemeral execution and fine-grained scalability, pose\nunique security challenges, particularly in open-source platforms like\nOpenFaaS. Existing approaches typically address isolated phases of the\nDevSecOps lifecycle, lacking an integrated and comprehensive security strategy.\nTo bridge this gap, we propose FaaSGuard, a unified DevSecOps pipeline\nexplicitly designed for open-source serverless environments. FaaSGuard\nsystematically embeds lightweight, fail-closed security checks into every stage\nof the development lifecycle-planning, coding, building, deployment, and\nmonitoring-effectively addressing threats such as injection attacks, hard-coded\nsecrets, and resource exhaustion. We validate our approach empirically through\na case study involving 20 real-world serverless functions from public GitHub\nrepositories. Results indicate that FaaSGuard effectively detects and prevents\ncritical vulnerabilities, demonstrating high precision (95%) and recall (91%)\nwithout significant disruption to established CI/CD practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing significantly alters software development by abstracting\ninfrastructure management and enabling rapid, modular, event-driven\ndeployments. Despite its benefits, the distinct characteristics of serverless\nfunctions, such as ephemeral execution and fine-grained scalability, pose\nunique security challenges, particularly in open-source platforms like\nOpenFaaS. Existing approaches typically address isolated phases of the\nDevSecOps lifecycle, lacking an integrated and comprehensive security strategy.\nTo bridge this gap, we propose FaaSGuard, a unified DevSecOps pipeline\nexplicitly designed for open-source serverless environments. FaaSGuard\nsystematically embeds lightweight, fail-closed security checks into every stage\nof the development lifecycle-planning, coding, building, deployment, and\nmonitoring-effectively addressing threats such as injection attacks, hard-coded\nsecrets, and resource exhaustion. We validate our approach empirically through\na case study involving 20 real-world serverless functions from public GitHub\nrepositories. Results indicate that FaaSGuard effectively detects and prevents\ncritical vulnerabilities, demonstrating high precision (95%) and recall (91%)\nwithout significant disruption to established CI/CD practices."
                },
                "authors": [
                    {
                        "name": "Amine Barrak"
                    },
                    {
                        "name": "Emna Ksontini"
                    },
                    {
                        "name": "Ridouane Atike"
                    },
                    {
                        "name": "Fehmi Jaafar"
                    }
                ],
                "author_detail": {
                    "name": "Fehmi Jaafar"
                },
                "author": "Fehmi Jaafar",
                "arxiv_comment": "IEEE International Conference on Source Code Analysis & Manipulation\n  (SCAM 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07809v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07809v2",
                "updated": "2025-09-04T15:41:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    41,
                    36,
                    3,
                    247,
                    0
                ],
                "published": "2025-08-11T09:49:01Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    49,
                    1,
                    0,
                    223,
                    0
                ],
                "title": "EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning"
                },
                "summary": "Reinforcement learning with verifiable reward (RLVR) has become a promising\nparadigm for post-training large language models (LLMs) to improve their\nreasoning capability. However, when the rollout accuracy is low on hard\nproblems, the reward becomes sparse, limiting learning efficiency and causing\nexploration bottlenecks. Existing approaches either rely on stronger LLMs for\ndistillation or filter out difficult problems, which limits scalability or\nrestricts reasoning improvement through exploration.\n  We propose EvoCoT, a self-evolving curriculum learning framework based on\ntwo-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the\nexploration space by self-generating and verifying CoT trajectories, then\ngradually shortens them to expand the space in a controlled way. This enables\nLLMs to stably learn from initially unsolved hard problems under sparse\nrewards. We apply EvoCoT to multiple LLM families, including Qwen, DeepSeek,\nand Llama. Experiments show that EvoCoT enables LLMs to solve previously\nunsolved problems, improves reasoning capability without external CoT\nsupervision, and is compatible with various RL fine-tuning methods. We release\nthe source code to support future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable reward (RLVR) has become a promising\nparadigm for post-training large language models (LLMs) to improve their\nreasoning capability. However, when the rollout accuracy is low on hard\nproblems, the reward becomes sparse, limiting learning efficiency and causing\nexploration bottlenecks. Existing approaches either rely on stronger LLMs for\ndistillation or filter out difficult problems, which limits scalability or\nrestricts reasoning improvement through exploration.\n  We propose EvoCoT, a self-evolving curriculum learning framework based on\ntwo-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the\nexploration space by self-generating and verifying CoT trajectories, then\ngradually shortens them to expand the space in a controlled way. This enables\nLLMs to stably learn from initially unsolved hard problems under sparse\nrewards. We apply EvoCoT to multiple LLM families, including Qwen, DeepSeek,\nand Llama. Experiments show that EvoCoT enables LLMs to solve previously\nunsolved problems, improves reasoning capability without external CoT\nsupervision, and is compatible with various RL fine-tuning methods. We release\nthe source code to support future research."
                },
                "authors": [
                    {
                        "name": "Huanyu Liu"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Chang Yu"
                    },
                    {
                        "name": "Taozhi Chen"
                    },
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Lecheng Wang"
                    },
                    {
                        "name": "XiaoLong Hu"
                    },
                    {
                        "name": "Ge Li"
                    }
                ],
                "author_detail": {
                    "name": "Ge Li"
                },
                "author": "Ge Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07809v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07809v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02846v2",
                "updated": "2025-09-04T15:40:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    40,
                    37,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-02T21:31:32Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    21,
                    31,
                    32,
                    1,
                    245,
                    0
                ],
                "title": "Towards Reasoning for PDE Foundation Models: A Reward-Model-Driven\n  Inference-Time-Scaling Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Reasoning for PDE Foundation Models: A Reward-Model-Driven\n  Inference-Time-Scaling Algorithm"
                },
                "summary": "Partial Differential Equations (PDEs) are the bedrock for modern\ncomputational sciences and engineering, and inherently computationally\nexpensive. While PDE foundation models have shown much promise for simulating\nsuch complex spatio-temporal phenomena, existing models remain constrained by\nthe pretraining datasets and struggle with auto-regressive rollout performance,\nespecially in out-of-distribution (OOD) cases. Furthermore, they have\nsignificant compute and training data requirements which hamper their use in\nmany critical applications. Inspired by recent advances in ``thinking\"\nstrategies used in large language models (LLMs), we introduce the first\ntest-time computing (TTC) strategy for PDEs that utilizes computational\nresources during inference to achieve more accurate predictions with fewer\ntraining samples and smaller models. We accomplish this with two types of\nreward models that evaluate predictions of a stochastic based model for\nspatio-temporal consistency. We demonstrate this method on compressible\nEuler-equation simulations from the PDEGym benchmark and show that TTC captures\nimproved predictions relative to standard non-adaptive auto-regressive\ninference. This TTC framework marks a foundational step towards more advanced\nreasoning algorithms or PDE modeling, inluding building\nreinforcement-learning-based approaches, potentially transforming computational\nworkflows in physics and engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partial Differential Equations (PDEs) are the bedrock for modern\ncomputational sciences and engineering, and inherently computationally\nexpensive. While PDE foundation models have shown much promise for simulating\nsuch complex spatio-temporal phenomena, existing models remain constrained by\nthe pretraining datasets and struggle with auto-regressive rollout performance,\nespecially in out-of-distribution (OOD) cases. Furthermore, they have\nsignificant compute and training data requirements which hamper their use in\nmany critical applications. Inspired by recent advances in ``thinking\"\nstrategies used in large language models (LLMs), we introduce the first\ntest-time computing (TTC) strategy for PDEs that utilizes computational\nresources during inference to achieve more accurate predictions with fewer\ntraining samples and smaller models. We accomplish this with two types of\nreward models that evaluate predictions of a stochastic based model for\nspatio-temporal consistency. We demonstrate this method on compressible\nEuler-equation simulations from the PDEGym benchmark and show that TTC captures\nimproved predictions relative to standard non-adaptive auto-regressive\ninference. This TTC framework marks a foundational step towards more advanced\nreasoning algorithms or PDE modeling, inluding building\nreinforcement-learning-based approaches, potentially transforming computational\nworkflows in physics and engineering."
                },
                "authors": [
                    {
                        "name": "Siddharth Mansingh"
                    },
                    {
                        "name": "James Amarel"
                    },
                    {
                        "name": "Ragib Arnab"
                    },
                    {
                        "name": "Arvind Mohan"
                    },
                    {
                        "name": "Kamaljeet Singh"
                    },
                    {
                        "name": "Gerd J. Kunde"
                    },
                    {
                        "name": "Nicolas Hengartner"
                    },
                    {
                        "name": "Benjamin Migliori"
                    },
                    {
                        "name": "Emily Casleton"
                    },
                    {
                        "name": "Nathan A. Debardeleben"
                    },
                    {
                        "name": "Ayan Biswas"
                    },
                    {
                        "name": "Diane Oyen"
                    },
                    {
                        "name": "Earl Lawrence"
                    }
                ],
                "author_detail": {
                    "name": "Earl Lawrence"
                },
                "author": "Earl Lawrence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02761v2",
                "updated": "2025-09-04T15:30:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    30,
                    53,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-02T19:06:56Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    19,
                    6,
                    56,
                    1,
                    245,
                    0
                ],
                "title": "Plan Verification for LLM-Based Embodied Task Completion Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plan Verification for LLM-Based Embodied Task Completion Agents"
                },
                "summary": "Large language model (LLM) based task plans and corresponding human\ndemonstrations for embodied AI may be noisy, with unnecessary actions,\nredundant navigation, and logical errors that reduce policy quality. We propose\nan iterative verification framework in which a Judge LLM critiques action\nsequences and a Planner LLM applies the revisions, yielding progressively\ncleaner and more spatially coherent trajectories. Unlike rule-based approaches,\nour method relies on natural language prompting, enabling broad generalization\nacross error types including irrelevant actions, contradictions, and missing\nsteps. On a set of manually annotated actions from the TEACh embodied AI\ndataset, our framework achieves up to 90% recall and 100% precision across four\nstate-of-the-art LLMs (GPT o4-mini, DeepSeek-R1, Gemini 2.5, LLaMA 4 Scout).\nThe refinement loop converges quickly, with 96.5% of sequences requiring at\nmost three iterations, while improving both temporal efficiency and spatial\naction organization. Crucially, the method preserves human error-recovery\npatterns rather than collapsing them, supporting future work on robust\ncorrective behavior. By establishing plan verification as a reliable LLM\ncapability for spatial planning and action refinement, we provide a scalable\npath to higher-quality training data for imitation learning in embodied AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) based task plans and corresponding human\ndemonstrations for embodied AI may be noisy, with unnecessary actions,\nredundant navigation, and logical errors that reduce policy quality. We propose\nan iterative verification framework in which a Judge LLM critiques action\nsequences and a Planner LLM applies the revisions, yielding progressively\ncleaner and more spatially coherent trajectories. Unlike rule-based approaches,\nour method relies on natural language prompting, enabling broad generalization\nacross error types including irrelevant actions, contradictions, and missing\nsteps. On a set of manually annotated actions from the TEACh embodied AI\ndataset, our framework achieves up to 90% recall and 100% precision across four\nstate-of-the-art LLMs (GPT o4-mini, DeepSeek-R1, Gemini 2.5, LLaMA 4 Scout).\nThe refinement loop converges quickly, with 96.5% of sequences requiring at\nmost three iterations, while improving both temporal efficiency and spatial\naction organization. Crucially, the method preserves human error-recovery\npatterns rather than collapsing them, supporting future work on robust\ncorrective behavior. By establishing plan verification as a reliable LLM\ncapability for spatial planning and action refinement, we provide a scalable\npath to higher-quality training data for imitation learning in embodied AI."
                },
                "authors": [
                    {
                        "name": "Ananth Hariharan"
                    },
                    {
                        "name": "Vardhan Dongre"
                    },
                    {
                        "name": "Dilek Hakkani-Tr"
                    },
                    {
                        "name": "Gokhan Tur"
                    }
                ],
                "author_detail": {
                    "name": "Gokhan Tur"
                },
                "author": "Gokhan Tur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04310v1",
                "updated": "2025-09-04T15:23:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    23,
                    58,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T15:23:58Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    23,
                    58,
                    3,
                    247,
                    0
                ],
                "title": "EvoEmo: Towards Evolved Emotional Policies for LLM Agents in Multi-Turn\n  Negotiation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvoEmo: Towards Evolved Emotional Policies for LLM Agents in Multi-Turn\n  Negotiation"
                },
                "summary": "Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models\n(LLMs) has demonstrated that agents can engage in \\textit{complex},\n\\textit{multi-turn} negotiations, opening new avenues for agentic AI. However,\nexisting LLM agents largely overlook the functional role of emotions in such\nnegotiations, instead generating passive, preference-driven emotional responses\nthat make them vulnerable to manipulation and strategic exploitation by\nadversarial counterparts. To address this gap, we present EvoEmo, an\nevolutionary reinforcement learning framework that optimizes dynamic emotional\nexpression in negotiations. EvoEmo models emotional state transitions as a\nMarkov Decision Process and employs population-based genetic optimization to\nevolve high-reward emotion policies across diverse negotiation scenarios. We\nfurther propose an evaluation framework with two baselines -- vanilla\nstrategies and fixed-emotion strategies -- for benchmarking emotion-aware\nnegotiation. Extensive experiments and ablation studies show that EvoEmo\nconsistently outperforms both baselines, achieving higher success rates, higher\nefficiency, and increased buyer savings. This findings highlight the importance\nof adaptive emotional expression in enabling more effective LLM agents for\nmulti-turn negotiation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models\n(LLMs) has demonstrated that agents can engage in \\textit{complex},\n\\textit{multi-turn} negotiations, opening new avenues for agentic AI. However,\nexisting LLM agents largely overlook the functional role of emotions in such\nnegotiations, instead generating passive, preference-driven emotional responses\nthat make them vulnerable to manipulation and strategic exploitation by\nadversarial counterparts. To address this gap, we present EvoEmo, an\nevolutionary reinforcement learning framework that optimizes dynamic emotional\nexpression in negotiations. EvoEmo models emotional state transitions as a\nMarkov Decision Process and employs population-based genetic optimization to\nevolve high-reward emotion policies across diverse negotiation scenarios. We\nfurther propose an evaluation framework with two baselines -- vanilla\nstrategies and fixed-emotion strategies -- for benchmarking emotion-aware\nnegotiation. Extensive experiments and ablation studies show that EvoEmo\nconsistently outperforms both baselines, achieving higher success rates, higher\nefficiency, and increased buyer savings. This findings highlight the importance\nof adaptive emotional expression in enabling more effective LLM agents for\nmulti-turn negotiation."
                },
                "authors": [
                    {
                        "name": "Yunbo Long"
                    },
                    {
                        "name": "Liming Xu"
                    },
                    {
                        "name": "Lukas Beckenbauer"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Alexandra Brintrup"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Brintrup"
                },
                "author": "Alexandra Brintrup",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04308v1",
                "updated": "2025-09-04T15:21:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    21,
                    54,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T15:21:54Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    21,
                    54,
                    3,
                    247,
                    0
                ],
                "title": "Learning Optimal Crew Dispatch for Grid Restoration Following an\n  Earthquake",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Optimal Crew Dispatch for Grid Restoration Following an\n  Earthquake"
                },
                "summary": "Post-disaster crew dispatch is a critical but computationally intensive task.\nTraditional mixed-integer linear programming methods often require minutes to\nseveral hours to compute solutions, leading to delays that hinder timely\ndecision-making in highly dynamic restoration environments. To address this\nchallenge, we propose a novel learning-based framework that integrates\ntransformer architectures with deep reinforcement learning (DRL) to deliver\nnear real-time decision support without compromising solution quality. Crew\ndispatch is formulated as a sequential decision-making problem under\nuncertainty, where transformers capture high-dimensional system states and\ntemporal dependencies, while DRL enables adaptive and scalable decision-making.\nEarthquake-induced distribution network damage is first characterized using\nestablished seismic standards, followed by a scenario generation and reduction\npipeline that aggregates probable outcomes into a single geospatial impact map.\nConditioned on this map, the proposed framework generates second-level dispatch\nstrategies, trained offline on simulated and historical events and deployed\nonline for rapid response. In addition to substantial runtime improvements, the\nproposed method enhances system resilience by enabling faster and more\neffective recovery and restoration. Case studies, particularly on the 2869-bus\nEuropean gas and power network, demonstrate that the method substantially\naccelerates restoration while maintaining high-quality solutions, underscoring\nits potential for practical deployment in large-scale disaster response.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-disaster crew dispatch is a critical but computationally intensive task.\nTraditional mixed-integer linear programming methods often require minutes to\nseveral hours to compute solutions, leading to delays that hinder timely\ndecision-making in highly dynamic restoration environments. To address this\nchallenge, we propose a novel learning-based framework that integrates\ntransformer architectures with deep reinforcement learning (DRL) to deliver\nnear real-time decision support without compromising solution quality. Crew\ndispatch is formulated as a sequential decision-making problem under\nuncertainty, where transformers capture high-dimensional system states and\ntemporal dependencies, while DRL enables adaptive and scalable decision-making.\nEarthquake-induced distribution network damage is first characterized using\nestablished seismic standards, followed by a scenario generation and reduction\npipeline that aggregates probable outcomes into a single geospatial impact map.\nConditioned on this map, the proposed framework generates second-level dispatch\nstrategies, trained offline on simulated and historical events and deployed\nonline for rapid response. In addition to substantial runtime improvements, the\nproposed method enhances system resilience by enabling faster and more\neffective recovery and restoration. Case studies, particularly on the 2869-bus\nEuropean gas and power network, demonstrate that the method substantially\naccelerates restoration while maintaining high-quality solutions, underscoring\nits potential for practical deployment in large-scale disaster response."
                },
                "authors": [
                    {
                        "name": "Farshad Amani"
                    },
                    {
                        "name": "Faezeh Ardali"
                    },
                    {
                        "name": "Amin Kargarian"
                    }
                ],
                "author_detail": {
                    "name": "Amin Kargarian"
                },
                "author": "Amin Kargarian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13755v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13755v2",
                "updated": "2025-09-04T15:21:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    21,
                    27,
                    3,
                    247,
                    0
                ],
                "published": "2025-08-19T11:51:40Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    51,
                    40,
                    1,
                    231,
                    0
                ],
                "title": "Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with\n  Adaptive Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with\n  Adaptive Exploration"
                },
                "summary": "Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a\npowerful paradigm for unlocking reasoning capabilities in large language\nmodels, yet its full potential is hindered by two under-explored dimensions:\nDepth-the hardest problem a model can sample; Breadth-the number of instances\nconsumed in a single iteration. We dissect the popular GRPO algorithm and\nreveal a systematic bias: the cumulative-advantage disproportionately weights\nsamples with medium accuracy, while down-weighting the low-accuracy instances\nthat are crucial for pushing reasoning boundaries. To rectify the depth\nneglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which\nre-weights hard problems through targeted multi-stage rollouts, thereby\nincreasing the number of positive rollouts for hard problems. Empirically,\nnaively enlarging rollout size only accelerates convergence and even hurts\nPass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra\ninference cost at convergence. Just as we adaptively expanded the depth of\nexploration, we now ask whether aggressively scaling the breadth of training\ndata can further amplify reasoning gains. To this end, we intensely scale batch\nsize and replace PPO's mini-batch iterations with full-batch updates over\nmultiple epochs. Increasing breadth significantly enhances Pass@1 performance.\nLarge-breadth training sustains high token-level entropy, indicating continued\nexploration and reduced gradient noise. We further present DARS-B, which\naugments DARS with large breadth, and demonstrate simultaneous gains in Pass@K\nand Pass@1. The results confirm that breadth and adaptive exploration across\ndepth operate as orthogonal dimensions in RLVR, which are key to unleashing the\nreasoning power of RLVR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a\npowerful paradigm for unlocking reasoning capabilities in large language\nmodels, yet its full potential is hindered by two under-explored dimensions:\nDepth-the hardest problem a model can sample; Breadth-the number of instances\nconsumed in a single iteration. We dissect the popular GRPO algorithm and\nreveal a systematic bias: the cumulative-advantage disproportionately weights\nsamples with medium accuracy, while down-weighting the low-accuracy instances\nthat are crucial for pushing reasoning boundaries. To rectify the depth\nneglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which\nre-weights hard problems through targeted multi-stage rollouts, thereby\nincreasing the number of positive rollouts for hard problems. Empirically,\nnaively enlarging rollout size only accelerates convergence and even hurts\nPass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra\ninference cost at convergence. Just as we adaptively expanded the depth of\nexploration, we now ask whether aggressively scaling the breadth of training\ndata can further amplify reasoning gains. To this end, we intensely scale batch\nsize and replace PPO's mini-batch iterations with full-batch updates over\nmultiple epochs. Increasing breadth significantly enhances Pass@1 performance.\nLarge-breadth training sustains high token-level entropy, indicating continued\nexploration and reduced gradient noise. We further present DARS-B, which\naugments DARS with large breadth, and demonstrate simultaneous gains in Pass@K\nand Pass@1. The results confirm that breadth and adaptive exploration across\ndepth operate as orthogonal dimensions in RLVR, which are key to unleashing the\nreasoning power of RLVR."
                },
                "authors": [
                    {
                        "name": "Zhicheng Yang"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Yinya Huang"
                    },
                    {
                        "name": "Yongxin Wang"
                    },
                    {
                        "name": "Dongchun Xie"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Xiaodan Liang"
                    },
                    {
                        "name": "Jing Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Tang"
                },
                "author": "Jing Tang",
                "arxiv_comment": "16 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13755v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13755v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04304v1",
                "updated": "2025-09-04T15:17:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    17,
                    50,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T15:17:50Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    17,
                    50,
                    3,
                    247,
                    0
                ],
                "title": "Facts Fade Fast: Evaluating Memorization of Outdated Medical Knowledge\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facts Fade Fast: Evaluating Memorization of Outdated Medical Knowledge\n  in Large Language Models"
                },
                "summary": "The growing capabilities of Large Language Models (LLMs) show significant\npotential to enhance healthcare by assisting medical researchers and\nphysicians. However, their reliance on static training data is a major risk\nwhen medical recommendations evolve with new research and developments. When\nLLMs memorize outdated medical knowledge, they can provide harmful advice or\nfail at clinical reasoning tasks. To investigate this problem, we introduce two\nnovel question-answering (QA) datasets derived from systematic reviews:\nMedRevQA (16,501 QA pairs covering general biomedical knowledge) and\nMedChangeQA (a subset of 512 QA pairs where medical consensus has changed over\ntime). Our evaluation of eight prominent LLMs on the datasets reveals\nconsistent reliance on outdated knowledge across all models. We additionally\nanalyze the influence of obsolete pre-training data and training strategies to\nexplain this phenomenon and propose future directions for mitigation, laying\nthe groundwork for developing more current and reliable medical AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing capabilities of Large Language Models (LLMs) show significant\npotential to enhance healthcare by assisting medical researchers and\nphysicians. However, their reliance on static training data is a major risk\nwhen medical recommendations evolve with new research and developments. When\nLLMs memorize outdated medical knowledge, they can provide harmful advice or\nfail at clinical reasoning tasks. To investigate this problem, we introduce two\nnovel question-answering (QA) datasets derived from systematic reviews:\nMedRevQA (16,501 QA pairs covering general biomedical knowledge) and\nMedChangeQA (a subset of 512 QA pairs where medical consensus has changed over\ntime). Our evaluation of eight prominent LLMs on the datasets reveals\nconsistent reliance on outdated knowledge across all models. We additionally\nanalyze the influence of obsolete pre-training data and training strategies to\nexplain this phenomenon and propose future directions for mitigation, laying\nthe groundwork for developing more current and reliable medical AI systems."
                },
                "authors": [
                    {
                        "name": "Juraj Vladika"
                    },
                    {
                        "name": "Mahdi Dhaini"
                    },
                    {
                        "name": "Florian Matthes"
                    }
                ],
                "author_detail": {
                    "name": "Florian Matthes"
                },
                "author": "Florian Matthes",
                "arxiv_comment": "Accepted to Findings of EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14701v2",
                "updated": "2025-09-04T15:11:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    11,
                    24,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-24T18:24:16Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    24,
                    16,
                    4,
                    24,
                    0
                ],
                "title": "An Unsupervised Natural Language Processing Pipeline for Assessing\n  Referral Appropriateness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Unsupervised Natural Language Processing Pipeline for Assessing\n  Referral Appropriateness"
                },
                "summary": "Objective: Assessing the appropriateness of diagnostic referrals is critical\nfor improving healthcare efficiency and reducing unnecessary procedures.\nHowever, this task becomes challenging when referral reasons are recorded only\nas free text rather than structured codes, like in the Italian NHS. To address\nthis gap, we propose a fully unsupervised Natural Language Processing (NLP)\npipeline capable of extracting and evaluating referral reasons without relying\non labelled datasets.\n  Methods: Our pipeline leverages Transformer-based embeddings pre-trained on\nItalian medical texts to cluster referral reasons and assess their alignment\nwith appropriateness guidelines. It operates in an unsupervised setting and is\ndesigned to generalize across different examination types. We analyzed two\ncomplete regional datasets from the Lombardy Region (Italy), covering all\nreferrals between 2019 and 2021 for venous echocolordoppler of the lower limbs\n(ECD;n=496,971; development) and flexible endoscope colonoscopy (FEC;\nn=407,949; testing only). For both, a random sample of 1,000 referrals was\nmanually annotated to measure performance.\n  Results: The pipeline achieved high performance in identifying referral\nreasons (Prec=92.43% (ECD), 93.59% (FEC); Rec=83.28% (ECD), 92.70% (FEC)) and\nappropriateness (Prec=93.58% (ECD), 94.66% (FEC); Rec=91.52% (ECD), 93.96%\n(FEC)). At the regional level, the analysis identified relevant inappropriate\nreferral groups and variation across contexts, findings that informed a new\nLombardy Region resolution to reinforce guideline adherence.\n  Conclusions: This study presents a robust, scalable, unsupervised NLP\npipeline for assessing referral appropriateness in large, real-world datasets.\nIt demonstrates how such data can be effectively leveraged, providing public\nhealth authorities with a deployable AI tool to monitor practices and support\nevidence-based policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objective: Assessing the appropriateness of diagnostic referrals is critical\nfor improving healthcare efficiency and reducing unnecessary procedures.\nHowever, this task becomes challenging when referral reasons are recorded only\nas free text rather than structured codes, like in the Italian NHS. To address\nthis gap, we propose a fully unsupervised Natural Language Processing (NLP)\npipeline capable of extracting and evaluating referral reasons without relying\non labelled datasets.\n  Methods: Our pipeline leverages Transformer-based embeddings pre-trained on\nItalian medical texts to cluster referral reasons and assess their alignment\nwith appropriateness guidelines. It operates in an unsupervised setting and is\ndesigned to generalize across different examination types. We analyzed two\ncomplete regional datasets from the Lombardy Region (Italy), covering all\nreferrals between 2019 and 2021 for venous echocolordoppler of the lower limbs\n(ECD;n=496,971; development) and flexible endoscope colonoscopy (FEC;\nn=407,949; testing only). For both, a random sample of 1,000 referrals was\nmanually annotated to measure performance.\n  Results: The pipeline achieved high performance in identifying referral\nreasons (Prec=92.43% (ECD), 93.59% (FEC); Rec=83.28% (ECD), 92.70% (FEC)) and\nappropriateness (Prec=93.58% (ECD), 94.66% (FEC); Rec=91.52% (ECD), 93.96%\n(FEC)). At the regional level, the analysis identified relevant inappropriate\nreferral groups and variation across contexts, findings that informed a new\nLombardy Region resolution to reinforce guideline adherence.\n  Conclusions: This study presents a robust, scalable, unsupervised NLP\npipeline for assessing referral appropriateness in large, real-world datasets.\nIt demonstrates how such data can be effectively leveraged, providing public\nhealth authorities with a deployable AI tool to monitor practices and support\nevidence-based policy."
                },
                "authors": [
                    {
                        "name": "Vittorio Torri"
                    },
                    {
                        "name": "Annamaria Bottelli"
                    },
                    {
                        "name": "Michele Ercolanoni"
                    },
                    {
                        "name": "Olivia Leoni"
                    },
                    {
                        "name": "Francesca Ieva"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Ieva"
                },
                "author": "Francesca Ieva",
                "arxiv_comment": "49 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.1; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05138v2",
                "updated": "2025-09-04T15:08:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    8,
                    1,
                    3,
                    247,
                    0
                ],
                "published": "2025-06-05T15:22:04Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    22,
                    4,
                    3,
                    156,
                    0
                ],
                "title": "Federated Isolation Forest for Efficient Anomaly Detection on Edge IoT\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Isolation Forest for Efficient Anomaly Detection on Edge IoT\n  Systems"
                },
                "summary": "Recently, federated learning frameworks such as Python TestBed for Federated\nLearning Algorithms and MicroPython TestBed for Federated Learning Algorithms\nhave emerged to tackle user privacy concerns and efficiency in embedded\nsystems. Even more recently, an efficient federated anomaly detection\nalgorithm, FLiForest, based on Isolation Forests has been developed, offering a\nlow-resource, unsupervised method well-suited for edge deployment and\ncontinuous learning. In this paper, we present an application of Isolation\nForest-based temperature anomaly detection, developed using the previously\nmentioned federated learning frameworks, aimed at small edge devices and IoT\nsystems running MicroPython. The system has been experimentally evaluated,\nachieving over 96% accuracy in distinguishing normal from abnormal readings and\nabove 78% precision in detecting anomalies across all tested configurations,\nwhile maintaining a memory usage below 160 KB during model training. These\nresults highlight its suitability for resource-constrained environments and\nedge systems, while upholding federated learning principles of data privacy and\ncollaborative learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, federated learning frameworks such as Python TestBed for Federated\nLearning Algorithms and MicroPython TestBed for Federated Learning Algorithms\nhave emerged to tackle user privacy concerns and efficiency in embedded\nsystems. Even more recently, an efficient federated anomaly detection\nalgorithm, FLiForest, based on Isolation Forests has been developed, offering a\nlow-resource, unsupervised method well-suited for edge deployment and\ncontinuous learning. In this paper, we present an application of Isolation\nForest-based temperature anomaly detection, developed using the previously\nmentioned federated learning frameworks, aimed at small edge devices and IoT\nsystems running MicroPython. The system has been experimentally evaluated,\nachieving over 96% accuracy in distinguishing normal from abnormal readings and\nabove 78% precision in detecting anomalies across all tested configurations,\nwhile maintaining a memory usage below 160 KB during model training. These\nresults highlight its suitability for resource-constrained environments and\nedge systems, while upholding federated learning principles of data privacy and\ncollaborative learning."
                },
                "authors": [
                    {
                        "name": "Pavle Vasiljevic"
                    },
                    {
                        "name": "Milica Matic"
                    },
                    {
                        "name": "Miroslav Popovic"
                    }
                ],
                "author_detail": {
                    "name": "Miroslav Popovic"
                },
                "author": "Miroslav Popovic",
                "arxiv_doi": "10.1109/ZINC65316.2025.11103552",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ZINC65316.2025.11103552",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.05138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages, 4 algorithms, 5 figures, 2 tables",
                "arxiv_journal_ref": "Published by IEEE Xplore",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21080v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21080v4",
                "updated": "2025-09-04T15:06:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    6,
                    27,
                    3,
                    247,
                    0
                ],
                "published": "2025-03-27T01:41:34Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    1,
                    41,
                    34,
                    3,
                    86,
                    0
                ],
                "title": "EQ-Knight: A Memory-Augmented LLM Agent for Strategic Affective Gaming\n  in Debt Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EQ-Knight: A Memory-Augmented LLM Agent for Strategic Affective Gaming\n  in Debt Recovery"
                },
                "summary": "Large language model-based chatbots have enhanced engagement in financial\nnegotiations, but their overreliance on passive empathy introduces critical\nrisks in credit collection. While empathy-driven approaches preserve client\nsatisfaction in benign cases, they fail catastrophically against dishonest\ndebtors--individuals who exploit conciliatory tactics to manipulate terms or\nevade repayment. Blindly prioritizing \"customer experience\" in such scenarios\nleads to creditor vulnerabilities: revenue leakage, moral hazard, and systemic\nexploitation. To address this, we propose EQ-Knight, an LLM agent that\ndynamically optimizes emotional strategy to defend creditor interests. Unlike\nnaive empathy-centric bots, EQ-Knight integrates emotion memory and\ngame-theoretic reasoning, powered by a Hidden Markov Model (HMM) to track and\npredict debtor emotional states. By analyzing both real-time and historical\nemotional cues, EQ-Knight strategically counters negative emotions (e.g.,\naggression, feigned distress) while preserving productive debtor relationships.\nExperiments demonstrate EQ-Knight's superiority over conventional LLM\nnegotiators: it achieves a 32\\% reduction in concession losses without\ncompromising recovery rates, particularly in adversarial cases where debtors\nweaponize negative emotions (e.g., intimidation, guilt-tripping) to coerce\nconcessions. For credit agencies, EQ-Knight transforms LLMs from high-risk\n\"people-pleasers\" into strategic emotion-defenders--balancing emotional\nintelligence with tactical rigor to enforce accountability and deter\nexploitation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model-based chatbots have enhanced engagement in financial\nnegotiations, but their overreliance on passive empathy introduces critical\nrisks in credit collection. While empathy-driven approaches preserve client\nsatisfaction in benign cases, they fail catastrophically against dishonest\ndebtors--individuals who exploit conciliatory tactics to manipulate terms or\nevade repayment. Blindly prioritizing \"customer experience\" in such scenarios\nleads to creditor vulnerabilities: revenue leakage, moral hazard, and systemic\nexploitation. To address this, we propose EQ-Knight, an LLM agent that\ndynamically optimizes emotional strategy to defend creditor interests. Unlike\nnaive empathy-centric bots, EQ-Knight integrates emotion memory and\ngame-theoretic reasoning, powered by a Hidden Markov Model (HMM) to track and\npredict debtor emotional states. By analyzing both real-time and historical\nemotional cues, EQ-Knight strategically counters negative emotions (e.g.,\naggression, feigned distress) while preserving productive debtor relationships.\nExperiments demonstrate EQ-Knight's superiority over conventional LLM\nnegotiators: it achieves a 32\\% reduction in concession losses without\ncompromising recovery rates, particularly in adversarial cases where debtors\nweaponize negative emotions (e.g., intimidation, guilt-tripping) to coerce\nconcessions. For credit agencies, EQ-Knight transforms LLMs from high-risk\n\"people-pleasers\" into strategic emotion-defenders--balancing emotional\nintelligence with tactical rigor to enforce accountability and deter\nexploitation."
                },
                "authors": [
                    {
                        "name": "Yunbo Long"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Liming Xu"
                    },
                    {
                        "name": "Alexandra Brintrup"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Brintrup"
                },
                "author": "Alexandra Brintrup",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21080v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21080v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04292v1",
                "updated": "2025-09-04T15:03:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    3,
                    2,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T15:03:02Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    3,
                    2,
                    3,
                    247,
                    0
                ],
                "title": "Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow\n  Real Instructions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow\n  Real Instructions?"
                },
                "summary": "Large Language Models (LLMs) achieve strong performance on diverse tasks but\noften exhibit cognitive inertia, struggling to follow instructions that\nconflict with the standardized patterns learned during supervised fine-tuning\n(SFT). To evaluate this limitation, we propose Inverse IFEval, a benchmark that\nmeasures models Counter-intuitive Abilitytheir capacity to override\ntraining-induced biases and comply with adversarial instructions. Inverse\nIFEval introduces eight types of such challenges, including Question\nCorrection, Intentional Textual Flaws, Code without Comments, and\nCounterfactual Answering. Using a human-in-the-loop pipeline, we construct a\ndataset of 1012 high-quality Chinese and English questions across 23 domains,\nevaluated under an optimized LLM-as-a-Judge framework. Experiments on existing\nleading LLMs demonstrate the necessity of our proposed Inverse IFEval\nbenchmark. Our findings emphasize that future alignment efforts should not only\npursue fluency and factual correctness but also account for adaptability under\nunconventional contexts. We hope that Inverse IFEval serves as both a\ndiagnostic tool and a foundation for developing methods that mitigate cognitive\ninertia, reduce overfitting to narrow patterns, and ultimately enhance the\ninstruction-following reliability of LLMs in diverse and unpredictable\nreal-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) achieve strong performance on diverse tasks but\noften exhibit cognitive inertia, struggling to follow instructions that\nconflict with the standardized patterns learned during supervised fine-tuning\n(SFT). To evaluate this limitation, we propose Inverse IFEval, a benchmark that\nmeasures models Counter-intuitive Abilitytheir capacity to override\ntraining-induced biases and comply with adversarial instructions. Inverse\nIFEval introduces eight types of such challenges, including Question\nCorrection, Intentional Textual Flaws, Code without Comments, and\nCounterfactual Answering. Using a human-in-the-loop pipeline, we construct a\ndataset of 1012 high-quality Chinese and English questions across 23 domains,\nevaluated under an optimized LLM-as-a-Judge framework. Experiments on existing\nleading LLMs demonstrate the necessity of our proposed Inverse IFEval\nbenchmark. Our findings emphasize that future alignment efforts should not only\npursue fluency and factual correctness but also account for adaptability under\nunconventional contexts. We hope that Inverse IFEval serves as both a\ndiagnostic tool and a foundation for developing methods that mitigate cognitive\ninertia, reduce overfitting to narrow patterns, and ultimately enhance the\ninstruction-following reliability of LLMs in diverse and unpredictable\nreal-world scenarios."
                },
                "authors": [
                    {
                        "name": "Qinyan Zhang"
                    },
                    {
                        "name": "Xinping Lei"
                    },
                    {
                        "name": "Ruijie Miao"
                    },
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Haojie Fan"
                    },
                    {
                        "name": "Le Chang"
                    },
                    {
                        "name": "Jiafan Hou"
                    },
                    {
                        "name": "Dingling Zhang"
                    },
                    {
                        "name": "Zhongfei Hou"
                    },
                    {
                        "name": "Ziqiang Yang"
                    },
                    {
                        "name": "Changxin Pu"
                    },
                    {
                        "name": "Fei Hu"
                    },
                    {
                        "name": "Jingkai Liu"
                    },
                    {
                        "name": "Mengyun Liu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xiang Gao"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "Zaiyuan Wang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wenhao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhao Huang"
                },
                "author": "Wenhao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14791v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14791v4",
                "updated": "2025-09-04T14:58:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    14,
                    58,
                    9,
                    3,
                    247,
                    0
                ],
                "published": "2025-02-20T18:11:38Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    11,
                    38,
                    3,
                    51,
                    0
                ],
                "title": "Rapid Word Learning Through Meta In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid Word Learning Through Meta In-Context Learning"
                },
                "summary": "Humans can quickly learn a new word from a few illustrative examples, and\nthen systematically and flexibly use it in novel contexts. Yet the abilities of\ncurrent language models for few-shot word learning, and methods for improving\nthese abilities, are underexplored. In this study, we introduce a novel method,\nMeta-training for IN-context learNing Of Words (Minnow). This method trains\nlanguage models to generate new examples of a word's usage given a few\nin-context examples, using a special placeholder token to represent the new\nword. This training is repeated on many new words to develop a general\nword-learning ability. We find that training models from scratch with Minnow on\nhuman-scale child-directed language enables strong few-shot word learning,\ncomparable to a large language model (LLM) pre-trained on orders of magnitude\nmore data. Furthermore, through discriminative and generative evaluations, we\ndemonstrate that finetuning pre-trained LLMs with Minnow improves their ability\nto discriminate between new words, identify syntactic categories of new words,\nand generate reasonable new usages and definitions for new words, based on one\nor a few in-context examples. These findings highlight the data efficiency of\nMinnow and its potential to improve language model performance in word learning\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans can quickly learn a new word from a few illustrative examples, and\nthen systematically and flexibly use it in novel contexts. Yet the abilities of\ncurrent language models for few-shot word learning, and methods for improving\nthese abilities, are underexplored. In this study, we introduce a novel method,\nMeta-training for IN-context learNing Of Words (Minnow). This method trains\nlanguage models to generate new examples of a word's usage given a few\nin-context examples, using a special placeholder token to represent the new\nword. This training is repeated on many new words to develop a general\nword-learning ability. We find that training models from scratch with Minnow on\nhuman-scale child-directed language enables strong few-shot word learning,\ncomparable to a large language model (LLM) pre-trained on orders of magnitude\nmore data. Furthermore, through discriminative and generative evaluations, we\ndemonstrate that finetuning pre-trained LLMs with Minnow improves their ability\nto discriminate between new words, identify syntactic categories of new words,\nand generate reasonable new usages and definitions for new words, based on one\nor a few in-context examples. These findings highlight the data efficiency of\nMinnow and its potential to improve language model performance in word learning\ntasks."
                },
                "authors": [
                    {
                        "name": "Wentao Wang"
                    },
                    {
                        "name": "Guangyuan Jiang"
                    },
                    {
                        "name": "Tal Linzen"
                    },
                    {
                        "name": "Brenden M. Lake"
                    }
                ],
                "author_detail": {
                    "name": "Brenden M. Lake"
                },
                "author": "Brenden M. Lake",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14791v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14791v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08193v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08193v2",
                "updated": "2025-09-04T14:42:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    14,
                    42,
                    6,
                    3,
                    247,
                    0
                ],
                "published": "2025-08-11T17:12:55Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    12,
                    55,
                    0,
                    223,
                    0
                ],
                "title": "Street-Level AI: Are Large Language Models Ready for Real-World\n  Judgments?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Street-Level AI: Are Large Language Models Ready for Real-World\n  Judgments?"
                },
                "summary": "A surge of recent work explores the ethical and societal implications of\nlarge-scale AI models that make \"moral\" judgments. Much of this literature\nfocuses either on alignment with human judgments through various thought\nexperiments or on the group fairness implications of AI judgments. However, the\nmost immediate and likely use of AI is to help or fully replace the so-called\nstreet-level bureaucrats, the individuals deciding to allocate scarce social\nresources or approve benefits. There is a rich history underlying how\nprinciples of local justice determine how society decides on prioritization\nmechanisms in such domains. In this paper, we examine how well LLM judgments\nalign with human judgments, as well as with socially and politically determined\nvulnerability scoring systems currently used in the domain of homelessness\nresource allocation. Crucially, we use real data on those needing services\n(maintaining strict confidentiality by only using local large models) to\nperform our analyses. We find that LLM prioritizations are extremely\ninconsistent in several ways: internally on different runs, between different\nLLMs, and between LLMs and the vulnerability scoring systems. At the same time,\nLLMs demonstrate qualitative consistency with lay human judgments in pairwise\ntesting. Findings call into question the readiness of current generation AI\nsystems for naive integration in high-stakes societal decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A surge of recent work explores the ethical and societal implications of\nlarge-scale AI models that make \"moral\" judgments. Much of this literature\nfocuses either on alignment with human judgments through various thought\nexperiments or on the group fairness implications of AI judgments. However, the\nmost immediate and likely use of AI is to help or fully replace the so-called\nstreet-level bureaucrats, the individuals deciding to allocate scarce social\nresources or approve benefits. There is a rich history underlying how\nprinciples of local justice determine how society decides on prioritization\nmechanisms in such domains. In this paper, we examine how well LLM judgments\nalign with human judgments, as well as with socially and politically determined\nvulnerability scoring systems currently used in the domain of homelessness\nresource allocation. Crucially, we use real data on those needing services\n(maintaining strict confidentiality by only using local large models) to\nperform our analyses. We find that LLM prioritizations are extremely\ninconsistent in several ways: internally on different runs, between different\nLLMs, and between LLMs and the vulnerability scoring systems. At the same time,\nLLMs demonstrate qualitative consistency with lay human judgments in pairwise\ntesting. Findings call into question the readiness of current generation AI\nsystems for naive integration in high-stakes societal decision-making."
                },
                "authors": [
                    {
                        "name": "Gaurab Pokharel"
                    },
                    {
                        "name": "Shafkat Farabi"
                    },
                    {
                        "name": "Patrick J. Fowler"
                    },
                    {
                        "name": "Sanmay Das"
                    }
                ],
                "author_detail": {
                    "name": "Sanmay Das"
                },
                "author": "Sanmay Das",
                "arxiv_comment": "This work has been accepted for publication as a full paper at the\n  AAAI/ACM Conference on AI, Ethics, and Society (AIES 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08193v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08193v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04260v1",
                "updated": "2025-09-04T14:38:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    14,
                    38,
                    28,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T14:38:28Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    14,
                    38,
                    28,
                    3,
                    247,
                    0
                ],
                "title": "An Empirical Study of Vulnerabilities in Python Packages and Their\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study of Vulnerabilities in Python Packages and Their\n  Detection"
                },
                "summary": "In the rapidly evolving software development landscape, Python stands out for\nits simplicity, versatility, and extensive ecosystem. Python packages, as units\nof organization, reusability, and distribution, have become a pressing concern,\nhighlighted by the considerable number of vulnerability reports. As a scripting\nlanguage, Python often cooperates with other languages for performance or\ninteroperability. This adds complexity to the vulnerabilities inherent to\nPython packages, and the effectiveness of current vulnerability detection tools\nremains underexplored. This paper addresses these gaps by introducing PyVul,\nthe first comprehensive benchmark suite of Python-package vulnerabilities.\nPyVul includes 1,157 publicly reported, developer-verified vulnerabilities,\neach linked to its affected packages. To accommodate diverse detection\ntechniques, it provides annotations at both commit and function levels. An\nLLM-assisted data cleansing method is incorporated to improve label accuracy,\nachieving 100% commit-level and 94% function-level accuracy, establishing PyVul\nas the most precise large-scale Python vulnerability benchmark. We further\ncarry out a distribution analysis of PyVul, which demonstrates that\nvulnerabilities in Python packages involve multiple programming languages and\nexhibit a wide variety of types. Moreover, our analysis reveals that\nmulti-lingual Python packages are potentially more susceptible to\nvulnerabilities. Evaluation of state-of-the-art detectors using this benchmark\nreveals a significant discrepancy between the capabilities of existing tools\nand the demands of effectively identifying real-world security issues in Python\npackages. Additionally, we conduct an empirical review of the top-ranked CWEs\nobserved in Python packages, to diagnose the fine-grained limitations of\ncurrent detection tools and highlight the necessity for future advancements in\nthe field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving software development landscape, Python stands out for\nits simplicity, versatility, and extensive ecosystem. Python packages, as units\nof organization, reusability, and distribution, have become a pressing concern,\nhighlighted by the considerable number of vulnerability reports. As a scripting\nlanguage, Python often cooperates with other languages for performance or\ninteroperability. This adds complexity to the vulnerabilities inherent to\nPython packages, and the effectiveness of current vulnerability detection tools\nremains underexplored. This paper addresses these gaps by introducing PyVul,\nthe first comprehensive benchmark suite of Python-package vulnerabilities.\nPyVul includes 1,157 publicly reported, developer-verified vulnerabilities,\neach linked to its affected packages. To accommodate diverse detection\ntechniques, it provides annotations at both commit and function levels. An\nLLM-assisted data cleansing method is incorporated to improve label accuracy,\nachieving 100% commit-level and 94% function-level accuracy, establishing PyVul\nas the most precise large-scale Python vulnerability benchmark. We further\ncarry out a distribution analysis of PyVul, which demonstrates that\nvulnerabilities in Python packages involve multiple programming languages and\nexhibit a wide variety of types. Moreover, our analysis reveals that\nmulti-lingual Python packages are potentially more susceptible to\nvulnerabilities. Evaluation of state-of-the-art detectors using this benchmark\nreveals a significant discrepancy between the capabilities of existing tools\nand the demands of effectively identifying real-world security issues in Python\npackages. Additionally, we conduct an empirical review of the top-ranked CWEs\nobserved in Python packages, to diagnose the fine-grained limitations of\ncurrent detection tools and highlight the necessity for future advancements in\nthe field."
                },
                "authors": [
                    {
                        "name": "Haowei Quan"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Xinzhe Li"
                    },
                    {
                        "name": "Terry Yue Zhuo"
                    },
                    {
                        "name": "Xiao Chen"
                    },
                    {
                        "name": "Xiaoning Du"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoning Du"
                },
                "author": "Xiaoning Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05997v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05997v2",
                "updated": "2025-09-04T14:30:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    14,
                    30,
                    12,
                    3,
                    247,
                    0
                ],
                "published": "2025-06-06T11:35:48Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    11,
                    35,
                    48,
                    4,
                    157,
                    0
                ],
                "title": "Spatially-Enhanced Recurrent Memory for Long-Range Mapless Navigation\n  via End-to-End Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatially-Enhanced Recurrent Memory for Long-Range Mapless Navigation\n  via End-to-End Reinforcement Learning"
                },
                "summary": "Recent advancements in robot navigation, particularly with end-to-end\nlearning approaches such as reinforcement learning (RL), have demonstrated\nstrong performance. However, successful navigation still depends on two key\ncapabilities: mapping and planning (explicitly or implicitly). Classical\napproaches rely on explicit mapping pipelines to register egocentric\nobservations into a coherent map. In contrast, end-to-end learning often\nachieves this implicitly -- through recurrent neural networks (RNNs) that fuse\ncurrent and historical observations into a latent space for planning. While\nexisting architectures, such as LSTM and GRU, can capture temporal\ndependencies, our findings reveal a critical limitation: their inability to\neffectively perform spatial memorization. This capability is essential for\nintegrating sequential observations from varying perspectives to build spatial\nrepresentations that support planning. To address this, we propose\nSpatially-Enhanced Recurrent Units (SRUs) -- a simple yet effective\nmodification to existing RNNs -- that enhance spatial memorization. We further\nintroduce an attention-based network architecture integrated with SRUs,\nenabling long-range mapless navigation using a single forward-facing stereo\ncamera. We also employ regularization techniques to facilitate robust\nend-to-end recurrent training via RL. Experimental results show 23.5% overall\nimprovement in long-range navigation compared to existing RNNs. With SRU\nmemory, our method outperforms RL baselines -- one relying on explicit mapping\nand the other on stacked historical observations -- by 29.6% and 105.0%,\nrespectively, across diverse environments requiring long-horizon mapping and\nmemorization. Finally, we address the sim-to-real gap by leveraging large-scale\npretraining on synthetic depth data, enabling zero-shot transfer for deployment\nacross diverse and complex real-world environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in robot navigation, particularly with end-to-end\nlearning approaches such as reinforcement learning (RL), have demonstrated\nstrong performance. However, successful navigation still depends on two key\ncapabilities: mapping and planning (explicitly or implicitly). Classical\napproaches rely on explicit mapping pipelines to register egocentric\nobservations into a coherent map. In contrast, end-to-end learning often\nachieves this implicitly -- through recurrent neural networks (RNNs) that fuse\ncurrent and historical observations into a latent space for planning. While\nexisting architectures, such as LSTM and GRU, can capture temporal\ndependencies, our findings reveal a critical limitation: their inability to\neffectively perform spatial memorization. This capability is essential for\nintegrating sequential observations from varying perspectives to build spatial\nrepresentations that support planning. To address this, we propose\nSpatially-Enhanced Recurrent Units (SRUs) -- a simple yet effective\nmodification to existing RNNs -- that enhance spatial memorization. We further\nintroduce an attention-based network architecture integrated with SRUs,\nenabling long-range mapless navigation using a single forward-facing stereo\ncamera. We also employ regularization techniques to facilitate robust\nend-to-end recurrent training via RL. Experimental results show 23.5% overall\nimprovement in long-range navigation compared to existing RNNs. With SRU\nmemory, our method outperforms RL baselines -- one relying on explicit mapping\nand the other on stacked historical observations -- by 29.6% and 105.0%,\nrespectively, across diverse environments requiring long-horizon mapping and\nmemorization. Finally, we address the sim-to-real gap by leveraging large-scale\npretraining on synthetic depth data, enabling zero-shot transfer for deployment\nacross diverse and complex real-world environments."
                },
                "authors": [
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Per Frivik"
                    },
                    {
                        "name": "David Hoeller"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Cesar Cadena"
                    },
                    {
                        "name": "Marco Hutter"
                    }
                ],
                "author_detail": {
                    "name": "Marco Hutter"
                },
                "author": "Marco Hutter",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05997v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05997v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01030v2",
                "updated": "2025-09-04T14:24:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    14,
                    24,
                    40,
                    3,
                    247,
                    0
                ],
                "published": "2025-08-01T19:14:14Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    19,
                    14,
                    14,
                    4,
                    213,
                    0
                ],
                "title": "Telecommunications fiber-optic and free-space quantum local area\n  networks at the Air Force Research Laboratory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Telecommunications fiber-optic and free-space quantum local area\n  networks at the Air Force Research Laboratory"
                },
                "summary": "As quantum computing, sensing, timing, and networking technologies mature,\nquantum network testbeds are being deployed across the United States and around\nthe world. To support the Air Force Research Laboratory (AFRL)'s mission of\nbuilding heterogeneous quantum networks, we report on the development of\nQuantum Local Area Networks (QLANs) operating at telecommunications-band\nfrequencies. The multi-node, reconfigurable QLANs include deployed optical\nfiber and free-space links connected to pristine laboratory environments and\nrugged outdoor test facilities. Each QLAN is tailored to distinct operating\nconditions and use cases, with unique environmental characteristics and\ncapabilities. We present network topologies and in-depth link characterization\ndata for three such networks. Using photonic integrated circuit-based sources\nof entangled photons, we demonstrate entanglement distribution of time-energy\nBell states across deployed fiber in a wooded environment. The high quality of\nthe entanglement is confirmed by a Clauser-Horne-Shimony-Holt inequality\nviolation of $S=2.717$, approaching the theoretical maximum of $S=2.828$. We\nconclude with a discussion of future work aimed at expanding QLAN functionality\nand enabling entanglement distribution between heterogeneous matter-based\nquantum systems, including superconducting qubits and trapped ions. These\nresults underscore the practical viability of field-deployable, qubit-agnostic\nquantum network infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As quantum computing, sensing, timing, and networking technologies mature,\nquantum network testbeds are being deployed across the United States and around\nthe world. To support the Air Force Research Laboratory (AFRL)'s mission of\nbuilding heterogeneous quantum networks, we report on the development of\nQuantum Local Area Networks (QLANs) operating at telecommunications-band\nfrequencies. The multi-node, reconfigurable QLANs include deployed optical\nfiber and free-space links connected to pristine laboratory environments and\nrugged outdoor test facilities. Each QLAN is tailored to distinct operating\nconditions and use cases, with unique environmental characteristics and\ncapabilities. We present network topologies and in-depth link characterization\ndata for three such networks. Using photonic integrated circuit-based sources\nof entangled photons, we demonstrate entanglement distribution of time-energy\nBell states across deployed fiber in a wooded environment. The high quality of\nthe entanglement is confirmed by a Clauser-Horne-Shimony-Holt inequality\nviolation of $S=2.717$, approaching the theoretical maximum of $S=2.828$. We\nconclude with a discussion of future work aimed at expanding QLAN functionality\nand enabling entanglement distribution between heterogeneous matter-based\nquantum systems, including superconducting qubits and trapped ions. These\nresults underscore the practical viability of field-deployable, qubit-agnostic\nquantum network infrastructure."
                },
                "authors": [
                    {
                        "name": "Erin Sheridan"
                    },
                    {
                        "name": "Nicholas J. Barton"
                    },
                    {
                        "name": "Richard Birrittella"
                    },
                    {
                        "name": "Vedansh Nehra"
                    },
                    {
                        "name": "Zachary Smith"
                    },
                    {
                        "name": "Christopher Tison"
                    },
                    {
                        "name": "Amos Matthew Smith"
                    },
                    {
                        "name": "Shashank Dharanibalan"
                    },
                    {
                        "name": "Vijit Bedi"
                    },
                    {
                        "name": "David Hucul"
                    },
                    {
                        "name": "Benjamin Kyle"
                    },
                    {
                        "name": "Christopher Nadeau"
                    },
                    {
                        "name": "Mary Draper"
                    },
                    {
                        "name": "John Heinig"
                    },
                    {
                        "name": "Scott Faulkner"
                    },
                    {
                        "name": "Randal Scales"
                    },
                    {
                        "name": "Andrew M. Brownell"
                    },
                    {
                        "name": "Stefan Preble"
                    },
                    {
                        "name": "James Schneeloch"
                    },
                    {
                        "name": "Samuel Schwab"
                    },
                    {
                        "name": "Daniel Campbell"
                    },
                    {
                        "name": "Derrick Sica"
                    },
                    {
                        "name": "Peter Ricci"
                    },
                    {
                        "name": "Vladimir Nikulin"
                    },
                    {
                        "name": "John Malowicki"
                    },
                    {
                        "name": "Jacob Hall"
                    },
                    {
                        "name": "Michael Fanto"
                    },
                    {
                        "name": "Matthew D. LaHaye"
                    },
                    {
                        "name": "Laura Wessing"
                    },
                    {
                        "name": "Paul M. Alsing"
                    },
                    {
                        "name": "Kathy-Anne Soderberg"
                    },
                    {
                        "name": "Donald Telesca"
                    }
                ],
                "author_detail": {
                    "name": "Donald Telesca"
                },
                "author": "Donald Telesca",
                "arxiv_comment": "51 pages, 45 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13958v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13958v2",
                "updated": "2025-09-04T14:24:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    14,
                    24,
                    19,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-21T06:25:21Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    6,
                    25,
                    21,
                    1,
                    21,
                    0
                ],
                "title": "A Survey of Graph Retrieval-Augmented Generation for Customized Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Graph Retrieval-Augmented Generation for Customized Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in a\nwide range of tasks, yet their application to specialized domains remains\nchallenging due to the need for deep expertise. Retrieval-Augmented generation\n(RAG) has emerged as a promising solution to customize LLMs for professional\nfields by seamlessly integrating external knowledge bases, enabling real-time\naccess to domain-specific expertise during inference. Despite its potential,\ntraditional RAG systems, based on flat text retrieval, face three critical\nchallenges: (i) complex query understanding in professional contexts, (ii)\ndifficulties in knowledge integration across distributed sources, and (iii)\nsystem efficiency bottlenecks at scale. This survey presents a systematic\nanalysis of Graph-based Retrieval-Augmented Generation (GraphRAG), a new\nparadigm that revolutionizes domain-specific LLM applications. GraphRAG\naddresses traditional RAG limitations through three key innovations: (i)\ngraph-structured knowledge representation that explicitly captures entity\nrelationships and domain hierarchies, (ii) efficient graph-based retrieval\ntechniques that enable context-preserving knowledge retrieval with multihop\nreasoning ability, and (iii) structure-aware knowledge integration algorithms\nthat leverage retrieved knowledge for accurate and logical coherent generation\nof LLMs. In this survey, we systematically analyze the technical foundations of\nGraphRAG and examine current implementations across various professional\ndomains, identifying key technical challenges and promising research\ndirections. All the related resources of GraphRAG, including research papers,\nopen-source data, and projects, are collected for the community in\nhttps://github.com/DEEP-PolyU/Awesome-GraphRAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in a\nwide range of tasks, yet their application to specialized domains remains\nchallenging due to the need for deep expertise. Retrieval-Augmented generation\n(RAG) has emerged as a promising solution to customize LLMs for professional\nfields by seamlessly integrating external knowledge bases, enabling real-time\naccess to domain-specific expertise during inference. Despite its potential,\ntraditional RAG systems, based on flat text retrieval, face three critical\nchallenges: (i) complex query understanding in professional contexts, (ii)\ndifficulties in knowledge integration across distributed sources, and (iii)\nsystem efficiency bottlenecks at scale. This survey presents a systematic\nanalysis of Graph-based Retrieval-Augmented Generation (GraphRAG), a new\nparadigm that revolutionizes domain-specific LLM applications. GraphRAG\naddresses traditional RAG limitations through three key innovations: (i)\ngraph-structured knowledge representation that explicitly captures entity\nrelationships and domain hierarchies, (ii) efficient graph-based retrieval\ntechniques that enable context-preserving knowledge retrieval with multihop\nreasoning ability, and (iii) structure-aware knowledge integration algorithms\nthat leverage retrieved knowledge for accurate and logical coherent generation\nof LLMs. In this survey, we systematically analyze the technical foundations of\nGraphRAG and examine current implementations across various professional\ndomains, identifying key technical challenges and promising research\ndirections. All the related resources of GraphRAG, including research papers,\nopen-source data, and projects, are collected for the community in\nhttps://github.com/DEEP-PolyU/Awesome-GraphRAG."
                },
                "authors": [
                    {
                        "name": "Qinggang Zhang"
                    },
                    {
                        "name": "Shengyuan Chen"
                    },
                    {
                        "name": "Yuanchen Bei"
                    },
                    {
                        "name": "Zheng Yuan"
                    },
                    {
                        "name": "Huachi Zhou"
                    },
                    {
                        "name": "Zijin Hong"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Yilin Xiao"
                    },
                    {
                        "name": "Chuang Zhou"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Xiao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Huang"
                },
                "author": "Xiao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13958v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13958v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04250v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04250v1",
                "updated": "2025-09-04T14:23:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    14,
                    23,
                    35,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T14:23:35Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    14,
                    23,
                    35,
                    3,
                    247,
                    0
                ],
                "title": "How many patients could we save with LLM priors?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How many patients could we save with LLM priors?"
                },
                "summary": "Imagine a world where clinical trials need far fewer patients to achieve the\nsame statistical power, thanks to the knowledge encoded in large language\nmodels (LLMs). We present a novel framework for hierarchical Bayesian modeling\nof adverse events in multi-center clinical trials, leveraging LLM-informed\nprior distributions. Unlike data augmentation approaches that generate\nsynthetic data points, our methodology directly obtains parametric priors from\nthe model. Our approach systematically elicits informative priors for\nhyperparameters in hierarchical Bayesian models using a pre-trained LLM,\nenabling the incorporation of external clinical expertise directly into\nBayesian safety modeling. Through comprehensive temperature sensitivity\nanalysis and rigorous cross-validation on real-world clinical trial data, we\ndemonstrate that LLM-derived priors consistently improve predictive performance\ncompared to traditional meta-analytical approaches. This methodology paves the\nway for more efficient and expert-informed clinical trial design, enabling\nsubstantial reductions in the number of patients required to achieve robust\nsafety assessment and with the potential to transform drug safety monitoring\nand regulatory decision making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imagine a world where clinical trials need far fewer patients to achieve the\nsame statistical power, thanks to the knowledge encoded in large language\nmodels (LLMs). We present a novel framework for hierarchical Bayesian modeling\nof adverse events in multi-center clinical trials, leveraging LLM-informed\nprior distributions. Unlike data augmentation approaches that generate\nsynthetic data points, our methodology directly obtains parametric priors from\nthe model. Our approach systematically elicits informative priors for\nhyperparameters in hierarchical Bayesian models using a pre-trained LLM,\nenabling the incorporation of external clinical expertise directly into\nBayesian safety modeling. Through comprehensive temperature sensitivity\nanalysis and rigorous cross-validation on real-world clinical trial data, we\ndemonstrate that LLM-derived priors consistently improve predictive performance\ncompared to traditional meta-analytical approaches. This methodology paves the\nway for more efficient and expert-informed clinical trial design, enabling\nsubstantial reductions in the number of patients required to achieve robust\nsafety assessment and with the potential to transform drug safety monitoring\nand regulatory decision making."
                },
                "authors": [
                    {
                        "name": "Shota Arai"
                    },
                    {
                        "name": "David Selby"
                    },
                    {
                        "name": "Andrew Vargo"
                    },
                    {
                        "name": "Sebastian Vollmer"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Vollmer"
                },
                "author": "Sebastian Vollmer",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04250v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04244v1",
                "updated": "2025-09-04T14:17:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    14,
                    17,
                    28,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T14:17:28Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    14,
                    17,
                    28,
                    3,
                    247,
                    0
                ],
                "title": "Integrating Pruning with Quantization for Efficient Deep Neural Networks\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Pruning with Quantization for Efficient Deep Neural Networks\n  Compression"
                },
                "summary": "Deep Neural Networks (DNNs) have achieved significant advances in a wide\nrange of applications. However, their deployment on resource-constrained\ndevices remains a challenge due to the large number of layers and parameters,\nwhich result in considerable computational and memory demands. To address this\nissue, pruning and quantization are two widely used compression techniques,\ncommonly applied individually in most studies to reduce model size and enhance\nprocessing speed. Nevertheless, combining these two techniques can yield even\ngreater compression benefits. Effectively integrating pruning and quantization\nto harness their complementary advantages poses a challenging task, primarily\ndue to their potential impact on model accuracy and the complexity of jointly\noptimizing both processes. In this paper, we propose two approaches that\nintegrate similarity-based filter pruning with Adaptive Power-of-Two (APoT)\nquantization to achieve higher compression efficiency while preserving model\naccuracy. In the first approach, pruning and quantization are applied\nsimultaneously during training. In the second approach, pruning is performed\nfirst to remove less important parameters, followed by quantization of the\npruned model using low-bit representations. Experimental results demonstrate\nthat our proposed approaches achieve effective model compression with minimal\naccuracy degradation, making them well-suited for deployment on devices with\nlimited computational resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Neural Networks (DNNs) have achieved significant advances in a wide\nrange of applications. However, their deployment on resource-constrained\ndevices remains a challenge due to the large number of layers and parameters,\nwhich result in considerable computational and memory demands. To address this\nissue, pruning and quantization are two widely used compression techniques,\ncommonly applied individually in most studies to reduce model size and enhance\nprocessing speed. Nevertheless, combining these two techniques can yield even\ngreater compression benefits. Effectively integrating pruning and quantization\nto harness their complementary advantages poses a challenging task, primarily\ndue to their potential impact on model accuracy and the complexity of jointly\noptimizing both processes. In this paper, we propose two approaches that\nintegrate similarity-based filter pruning with Adaptive Power-of-Two (APoT)\nquantization to achieve higher compression efficiency while preserving model\naccuracy. In the first approach, pruning and quantization are applied\nsimultaneously during training. In the second approach, pruning is performed\nfirst to remove less important parameters, followed by quantization of the\npruned model using low-bit representations. Experimental results demonstrate\nthat our proposed approaches achieve effective model compression with minimal\naccuracy degradation, making them well-suited for deployment on devices with\nlimited computational resources."
                },
                "authors": [
                    {
                        "name": "Sara Makenali"
                    },
                    {
                        "name": "Babak Rokh"
                    },
                    {
                        "name": "Ali Azarpeyvand"
                    }
                ],
                "author_detail": {
                    "name": "Ali Azarpeyvand"
                },
                "author": "Ali Azarpeyvand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11701v2",
                "updated": "2025-09-04T14:12:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    14,
                    12,
                    36,
                    3,
                    247,
                    0
                ],
                "published": "2025-05-16T21:09:36Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    21,
                    9,
                    36,
                    4,
                    136,
                    0
                ],
                "title": "DMN-Guided Prompting: A Framework for Controlling LLM Behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DMN-Guided Prompting: A Framework for Controlling LLM Behavior"
                },
                "summary": "Large Language Models (LLMs) have shown considerable potential in automating\ndecision logic within knowledge-intensive processes. However, their\neffectiveness largely depends on the strategy and quality of prompting. Since\ndecision logic is typically embedded in prompts, it becomes challenging for end\nusers to modify or refine it. Decision Model and Notation (DMN) offers a\nstandardized graphical approach for defining decision logic in a structured,\nuser-friendly manner. This paper introduces a DMN-guided prompting framework\nthat breaks down complex decision logic into smaller, manageable components,\nguiding LLMs through structured decision pathways. We implemented the framework\nin a graduate-level course where students submitted assignments. The\nassignments and DMN models representing feedback instructions served as inputs\nto our framework. The instructor evaluated the generated feedback and labeled\nit for performance assessment. Our approach demonstrated promising results,\noutperforming chain-of-thought (CoT) prompting in our case study. Students also\nresponded positively to the generated feedback, reporting high levels of\nperceived usefulness in a survey based on the Technology Acceptance Model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown considerable potential in automating\ndecision logic within knowledge-intensive processes. However, their\neffectiveness largely depends on the strategy and quality of prompting. Since\ndecision logic is typically embedded in prompts, it becomes challenging for end\nusers to modify or refine it. Decision Model and Notation (DMN) offers a\nstandardized graphical approach for defining decision logic in a structured,\nuser-friendly manner. This paper introduces a DMN-guided prompting framework\nthat breaks down complex decision logic into smaller, manageable components,\nguiding LLMs through structured decision pathways. We implemented the framework\nin a graduate-level course where students submitted assignments. The\nassignments and DMN models representing feedback instructions served as inputs\nto our framework. The instructor evaluated the generated feedback and labeled\nit for performance assessment. Our approach demonstrated promising results,\noutperforming chain-of-thought (CoT) prompting in our case study. Students also\nresponded positively to the generated feedback, reporting high levels of\nperceived usefulness in a survey based on the Technology Acceptance Model."
                },
                "authors": [
                    {
                        "name": "Shaghayegh Abedi"
                    },
                    {
                        "name": "Amin Jalali"
                    }
                ],
                "author_detail": {
                    "name": "Amin Jalali"
                },
                "author": "Amin Jalali",
                "arxiv_comment": "Large Language Models, Decision Model and Notation, Automated\n  Feedback, Prompt Engineering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03562v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03562v2",
                "updated": "2025-09-04T14:07:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    14,
                    7,
                    26,
                    3,
                    247,
                    0
                ],
                "published": "2024-11-05T23:55:23Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    23,
                    55,
                    23,
                    1,
                    310,
                    0
                ],
                "title": "Kolb-Based Experiential Learning for Generalist Agents with Human-Level\n  Kaggle Data Science Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kolb-Based Experiential Learning for Generalist Agents with Human-Level\n  Kaggle Data Science Performance"
                },
                "summary": "Human expertise emerges through iterative cycles of interaction, reflection,\nand internal model updating, which are central to cognitive theories such as\nKolb's experiential learning and Vygotsky's zone of proximal development. In\ncontrast, current AI systems, particularly LLM agents, rely on static\npre-training or rigid workflows, lacking mechanisms for continual adaptation.\nRecent studies identified early cognitive traits in LLM agents (reflection,\nrevision, and self-correction) suggesting foundational elements of human-like\nexperiential learning. Thus the key question: Can we design LLM agents capable\nof structured, cognitively grounded learning similar to human processes? In\nresponse, we propose a computational framework of Kolb's learning cycle with\nVygotsky's ZPD for autonomous agents. Our architecture separates extrinsic\n(environment interaction) and intrinsic (internal reflection/abstraction)\nfunctions, enabling cognitively grounded scaffolded learning, where the agent\ninitially learns within structured environments, followed by open-ended\ngeneralisation. This approach empowers agents to master complex tasks ; domains\nthat traditional fine-tuning or simple reflective methods could not tackle\neffectively. Its potential is powerfully demonstrated via direct comparison\nwith humans in real-world Kaggle data science competitions. Learning fully\nautomated data science code generation across 81 tasks, our system, Agent K,\ndemonstrated the ability to perform the entire workflow autonomously, achieving\nan Elo-MMR score of 1694, beyond median score of the Kaggle Masters (the top 2%\namong 200,000 users) of our study. With 9 gold, 8 silver, and 12 bronze medals\nlevel performance - including 4 gold and 4 silver on prize-awarding\ncompetitions - Agent K is the 1st AI system to successfully integrate Kolb- and\nVygotsky-inspired human cognitive learning, marking a major step toward\ngeneralist AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human expertise emerges through iterative cycles of interaction, reflection,\nand internal model updating, which are central to cognitive theories such as\nKolb's experiential learning and Vygotsky's zone of proximal development. In\ncontrast, current AI systems, particularly LLM agents, rely on static\npre-training or rigid workflows, lacking mechanisms for continual adaptation.\nRecent studies identified early cognitive traits in LLM agents (reflection,\nrevision, and self-correction) suggesting foundational elements of human-like\nexperiential learning. Thus the key question: Can we design LLM agents capable\nof structured, cognitively grounded learning similar to human processes? In\nresponse, we propose a computational framework of Kolb's learning cycle with\nVygotsky's ZPD for autonomous agents. Our architecture separates extrinsic\n(environment interaction) and intrinsic (internal reflection/abstraction)\nfunctions, enabling cognitively grounded scaffolded learning, where the agent\ninitially learns within structured environments, followed by open-ended\ngeneralisation. This approach empowers agents to master complex tasks ; domains\nthat traditional fine-tuning or simple reflective methods could not tackle\neffectively. Its potential is powerfully demonstrated via direct comparison\nwith humans in real-world Kaggle data science competitions. Learning fully\nautomated data science code generation across 81 tasks, our system, Agent K,\ndemonstrated the ability to perform the entire workflow autonomously, achieving\nan Elo-MMR score of 1694, beyond median score of the Kaggle Masters (the top 2%\namong 200,000 users) of our study. With 9 gold, 8 silver, and 12 bronze medals\nlevel performance - including 4 gold and 4 silver on prize-awarding\ncompetitions - Agent K is the 1st AI system to successfully integrate Kolb- and\nVygotsky-inspired human cognitive learning, marking a major step toward\ngeneralist AI."
                },
                "authors": [
                    {
                        "name": "Antoine Grosnit"
                    },
                    {
                        "name": "Alexandre Maraval"
                    },
                    {
                        "name": "Refinath S N"
                    },
                    {
                        "name": "Zichao Zhao"
                    },
                    {
                        "name": "James Dora"
                    },
                    {
                        "name": "Giuseppe Paolo"
                    },
                    {
                        "name": "Albert Thomas"
                    },
                    {
                        "name": "Jonas Gonzalez"
                    },
                    {
                        "name": "Abhineet Kumar"
                    },
                    {
                        "name": "Khyati Khandelwal"
                    },
                    {
                        "name": "Abdelhakim Benechehab"
                    },
                    {
                        "name": "Hamza Cherkaoui"
                    },
                    {
                        "name": "Youssef Attia El-Hili"
                    },
                    {
                        "name": "Kun Shao"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Jun Yao"
                    },
                    {
                        "name": "Balzs Kgl"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03562v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03562v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11110v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11110v4",
                "updated": "2025-09-04T14:01:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    14,
                    1,
                    21,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-19T16:53:26Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    16,
                    53,
                    26,
                    6,
                    19,
                    0
                ],
                "title": "Chain-of-Reasoning: Towards Unified Mathematical Reasoning in Large\n  Language Models via a Multi-Paradigm Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Reasoning: Towards Unified Mathematical Reasoning in Large\n  Language Models via a Multi-Paradigm Perspective"
                },
                "summary": "Large Language Models (LLMs) have made notable progress in mathematical\nreasoning, yet often rely on single-paradigm reasoning, limiting their\neffectiveness across diverse tasks. We introduce Chain-of-Reasoning (CoR), a\nnovel unified framework integrating multiple reasoning paradigms--Natural\nLanguage Reasoning (NLR), Algorithmic Reasoning (AR), and Symbolic Reasoning\n(SR)--to enable synergistic collaboration. CoR generates multiple potential\nanswers via different reasoning paradigms and synthesizes them into a coherent\nfinal solution. We propose a Progressive Paradigm Training (PPT) strategy for\nmodels to progressively master these paradigms, leading to CoR-Math-7B.\nExperimental results demonstrate that CoR-Math-7B significantly outperforms\ncurrent SOTA models, achieving up to a 41.0% absolute improvement over GPT-4o\nin theorem proving and a 15.0% improvement over RL-based methods on the MATH\nbenchmark in arithmetic tasks. These results show the enhanced mathematical\ncomprehension ability of our model, enabling zero-shot generalization across\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made notable progress in mathematical\nreasoning, yet often rely on single-paradigm reasoning, limiting their\neffectiveness across diverse tasks. We introduce Chain-of-Reasoning (CoR), a\nnovel unified framework integrating multiple reasoning paradigms--Natural\nLanguage Reasoning (NLR), Algorithmic Reasoning (AR), and Symbolic Reasoning\n(SR)--to enable synergistic collaboration. CoR generates multiple potential\nanswers via different reasoning paradigms and synthesizes them into a coherent\nfinal solution. We propose a Progressive Paradigm Training (PPT) strategy for\nmodels to progressively master these paradigms, leading to CoR-Math-7B.\nExperimental results demonstrate that CoR-Math-7B significantly outperforms\ncurrent SOTA models, achieving up to a 41.0% absolute improvement over GPT-4o\nin theorem proving and a 15.0% improvement over RL-based methods on the MATH\nbenchmark in arithmetic tasks. These results show the enhanced mathematical\ncomprehension ability of our model, enabling zero-shot generalization across\ntasks."
                },
                "authors": [
                    {
                        "name": "Yiyao Yu"
                    },
                    {
                        "name": "Yuxiang Zhang"
                    },
                    {
                        "name": "Dongdong Zhang"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Hengyuan Zhang"
                    },
                    {
                        "name": "Xingxing Zhang"
                    },
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Mahmoud Khademi"
                    },
                    {
                        "name": "Hany Awadalla"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "Accepted to ACL 2025 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11110v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11110v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13287v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13287v6",
                "updated": "2025-09-04T13:51:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    51,
                    56,
                    3,
                    247,
                    0
                ],
                "published": "2024-10-17T07:33:35Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    7,
                    33,
                    35,
                    3,
                    291,
                    0
                ],
                "title": "PAK-UCB Contextual Bandit: An Online Learning Approach to Prompt-Aware\n  Selection of Generative Models and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAK-UCB Contextual Bandit: An Online Learning Approach to Prompt-Aware\n  Selection of Generative Models and LLMs"
                },
                "summary": "Selecting a sample generation scheme from multiple prompt-based generative\nmodels, including large language models (LLMs) and prompt-guided image and\nvideo generation models, is typically addressed by choosing the model that\nmaximizes an averaged evaluation score. However, this score-based selection\noverlooks the possibility that different models achieve the best generation\nperformance for different types of text prompts. An online identification of\nthe best generation model for various input prompts can reduce the costs\nassociated with querying sub-optimal models. In this work, we explore the\npossibility of varying rankings of text-based generative models for different\ntext prompts and propose an online learning framework to predict the best data\ngeneration model for a given input prompt. The proposed PAK-UCB algorithm\naddresses a contextual bandit (CB) setting with shared context variables across\nthe arms, utilizing the generated data to update kernel-based functions that\npredict the score of each model available for unseen text prompts.\nAdditionally, we leverage random Fourier features (RFF) to accelerate the\nonline learning process of PAK-UCB. Our numerical experiments on real and\nsimulated text-to-image and image-to-text generative models show that RFF-UCB\nperforms successfully in identifying the best generation model across different\nsample types. The code is available at:\ngithub.com/yannxiaoyanhu/dgm-online-select.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selecting a sample generation scheme from multiple prompt-based generative\nmodels, including large language models (LLMs) and prompt-guided image and\nvideo generation models, is typically addressed by choosing the model that\nmaximizes an averaged evaluation score. However, this score-based selection\noverlooks the possibility that different models achieve the best generation\nperformance for different types of text prompts. An online identification of\nthe best generation model for various input prompts can reduce the costs\nassociated with querying sub-optimal models. In this work, we explore the\npossibility of varying rankings of text-based generative models for different\ntext prompts and propose an online learning framework to predict the best data\ngeneration model for a given input prompt. The proposed PAK-UCB algorithm\naddresses a contextual bandit (CB) setting with shared context variables across\nthe arms, utilizing the generated data to update kernel-based functions that\npredict the score of each model available for unseen text prompts.\nAdditionally, we leverage random Fourier features (RFF) to accelerate the\nonline learning process of PAK-UCB. Our numerical experiments on real and\nsimulated text-to-image and image-to-text generative models show that RFF-UCB\nperforms successfully in identifying the best generation model across different\nsample types. The code is available at:\ngithub.com/yannxiaoyanhu/dgm-online-select."
                },
                "authors": [
                    {
                        "name": "Xiaoyan Hu"
                    },
                    {
                        "name": "Ho-fung Leung"
                    },
                    {
                        "name": "Farzan Farnia"
                    }
                ],
                "author_detail": {
                    "name": "Farzan Farnia"
                },
                "author": "Farzan Farnia",
                "arxiv_comment": "accepted to ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13287v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13287v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04214v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04214v1",
                "updated": "2025-09-04T13:39:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    39,
                    37,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T13:39:37Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    39,
                    37,
                    3,
                    247,
                    0
                ],
                "title": "An Automated, Scalable Machine Learning Model Inversion Assessment\n  Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Automated, Scalable Machine Learning Model Inversion Assessment\n  Pipeline"
                },
                "summary": "Machine learning (ML) models have the potential to transform military\nbattlefields, presenting a large external pressure to rapidly incorporate them\ninto operational settings. However, it is well-established that these ML models\nare vulnerable to a number of adversarial attacks throughout the model\ndeployment pipeline that threaten to negate battlefield advantage. One broad\ncategory is privacy attacks (such as model inversion) where an adversary can\nreverse engineer information from the model, such as the sensitive data used in\nits training. The ability to quantify the risk of model inversion attacks\n(MIAs) is not well studied, and there is a lack of automated developmental test\nand evaluation (DT&E) tools and metrics to quantify the effectiveness of\nprivacy loss of the MIA. The current DT&E process is difficult because ML model\ninversions can be hard for a human to interpret, subjective when they are\ninterpretable, and difficult to quantify in terms of inversion quality.\nAdditionally, scaling the DT&E process is challenging due to many ML model\narchitectures and data modalities that need to be assessed. In this work, we\npresent a novel DT&E tool that quantifies the risk of data privacy loss from\nMIAs and introduces four adversarial risk dimensions to quantify privacy loss.\nOur DT&E pipeline combines inversion with vision language models (VLMs) to\nimprove effectiveness while enabling scalable analysis. We demonstrate\neffectiveness using multiple MIA techniques and VLMs configured for zero-shot\nclassification and image captioning. We benchmark the pipeline using several\nstate-of-the-art MIAs in the computer vision domain with an image\nclassification task that is typical in military applications. In general, our\ninnovative pipeline extends the current model inversion DT&E capabilities by\nimproving the effectiveness and scalability of the privacy loss analysis in an\nautomated fashion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) models have the potential to transform military\nbattlefields, presenting a large external pressure to rapidly incorporate them\ninto operational settings. However, it is well-established that these ML models\nare vulnerable to a number of adversarial attacks throughout the model\ndeployment pipeline that threaten to negate battlefield advantage. One broad\ncategory is privacy attacks (such as model inversion) where an adversary can\nreverse engineer information from the model, such as the sensitive data used in\nits training. The ability to quantify the risk of model inversion attacks\n(MIAs) is not well studied, and there is a lack of automated developmental test\nand evaluation (DT&E) tools and metrics to quantify the effectiveness of\nprivacy loss of the MIA. The current DT&E process is difficult because ML model\ninversions can be hard for a human to interpret, subjective when they are\ninterpretable, and difficult to quantify in terms of inversion quality.\nAdditionally, scaling the DT&E process is challenging due to many ML model\narchitectures and data modalities that need to be assessed. In this work, we\npresent a novel DT&E tool that quantifies the risk of data privacy loss from\nMIAs and introduces four adversarial risk dimensions to quantify privacy loss.\nOur DT&E pipeline combines inversion with vision language models (VLMs) to\nimprove effectiveness while enabling scalable analysis. We demonstrate\neffectiveness using multiple MIA techniques and VLMs configured for zero-shot\nclassification and image captioning. We benchmark the pipeline using several\nstate-of-the-art MIAs in the computer vision domain with an image\nclassification task that is typical in military applications. In general, our\ninnovative pipeline extends the current model inversion DT&E capabilities by\nimproving the effectiveness and scalability of the privacy loss analysis in an\nautomated fashion."
                },
                "authors": [
                    {
                        "name": "Tyler Shumaker"
                    },
                    {
                        "name": "Jessica Carpenter"
                    },
                    {
                        "name": "David Saranchak"
                    },
                    {
                        "name": "Nathaniel D. Bastian"
                    }
                ],
                "author_detail": {
                    "name": "Nathaniel D. Bastian"
                },
                "author": "Nathaniel D. Bastian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04214v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04214v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04213v1",
                "updated": "2025-09-04T13:38:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    38,
                    54,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T13:38:54Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    38,
                    54,
                    3,
                    247,
                    0
                ],
                "title": "Sailing Towards Zero-Shot State Estimation using Foundation Models\n  Combined with a UKF",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sailing Towards Zero-Shot State Estimation using Foundation Models\n  Combined with a UKF"
                },
                "summary": "State estimation in control and systems engineering traditionally requires\nextensive manual system identification or data-collection effort. However,\ntransformer-based foundation models in other domains have reduced data\nrequirements by leveraging pre-trained generalist models. Ultimately,\ndeveloping zero-shot foundation models of system dynamics could drastically\nreduce manual deployment effort. While recent work shows that transformer-based\nend-to-end approaches can achieve zero-shot performance on unseen systems, they\nare limited to sensor models seen during training. We introduce the foundation\nmodel unscented Kalman filter (FM-UKF), which combines a transformer-based\nmodel of system dynamics with analytically known sensor models via an UKF,\nenabling generalization across varying dynamics without retraining for new\nsensor configurations. We evaluate FM-UKF on a new benchmark of container ship\nmodels with complex dynamics, demonstrating a competitive accuracy, effort, and\nrobustness trade-off compared to classical methods with approximate system\nknowledge and to an end-to-end approach. The benchmark and dataset are open\nsourced to further support future research in zero-shot state estimation via\nfoundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State estimation in control and systems engineering traditionally requires\nextensive manual system identification or data-collection effort. However,\ntransformer-based foundation models in other domains have reduced data\nrequirements by leveraging pre-trained generalist models. Ultimately,\ndeveloping zero-shot foundation models of system dynamics could drastically\nreduce manual deployment effort. While recent work shows that transformer-based\nend-to-end approaches can achieve zero-shot performance on unseen systems, they\nare limited to sensor models seen during training. We introduce the foundation\nmodel unscented Kalman filter (FM-UKF), which combines a transformer-based\nmodel of system dynamics with analytically known sensor models via an UKF,\nenabling generalization across varying dynamics without retraining for new\nsensor configurations. We evaluate FM-UKF on a new benchmark of container ship\nmodels with complex dynamics, demonstrating a competitive accuracy, effort, and\nrobustness trade-off compared to classical methods with approximate system\nknowledge and to an end-to-end approach. The benchmark and dataset are open\nsourced to further support future research in zero-shot state estimation via\nfoundation models."
                },
                "authors": [
                    {
                        "name": "Tobin Holtmann"
                    },
                    {
                        "name": "David Stenger"
                    },
                    {
                        "name": "Andres Posada-Moreno"
                    },
                    {
                        "name": "Friedrich Solowjow"
                    },
                    {
                        "name": "Sebastian Trimpe"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Trimpe"
                },
                "author": "Sebastian Trimpe",
                "arxiv_comment": "Accepted for publication at CDC2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04210v1",
                "updated": "2025-09-04T13:35:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    35,
                    49,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T13:35:49Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    35,
                    49,
                    3,
                    247,
                    0
                ],
                "title": "COBRA: Multimodal Sensing Deep Learning Framework for Remote Chronic\n  Obesity Management via Wrist-Worn Activity Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COBRA: Multimodal Sensing Deep Learning Framework for Remote Chronic\n  Obesity Management via Wrist-Worn Activity Monitoring"
                },
                "summary": "Chronic obesity management requires continuous monitoring of energy balance\nbehaviors, yet traditional self-reported methods suffer from significant\nunderreporting and recall bias, and difficulty in integration with modern\ndigital health systems. This study presents COBRA (Chronic Obesity Behavioral\nRecognition Architecture), a novel deep learning framework for objective\nbehavioral monitoring using wrist-worn multimodal sensors. COBRA integrates a\nhybrid D-Net architecture combining U-Net spatial modeling, multi-head\nself-attention mechanisms, and BiLSTM temporal processing to classify daily\nactivities into four obesity-relevant categories: Food Intake, Physical\nActivity, Sedentary Behavior, and Daily Living. Validated on the WISDM-Smart\ndataset with 51 subjects performing 18 activities, COBRA's optimal\npreprocessing strategy combines spectral-temporal feature extraction, achieving\nhigh performance across multiple architectures. D-Net demonstrates 96.86%\noverall accuracy with category-specific F1-scores of 98.55% (Physical\nActivity), 95.53% (Food Intake), 94.63% (Sedentary Behavior), and 98.68% (Daily\nLiving), outperforming state-of-the-art baselines by 1.18% in accuracy. The\nframework shows robust generalizability with low demographic variance (<3%),\nenabling scalable deployment for personalized obesity interventions and\ncontinuous lifestyle monitoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chronic obesity management requires continuous monitoring of energy balance\nbehaviors, yet traditional self-reported methods suffer from significant\nunderreporting and recall bias, and difficulty in integration with modern\ndigital health systems. This study presents COBRA (Chronic Obesity Behavioral\nRecognition Architecture), a novel deep learning framework for objective\nbehavioral monitoring using wrist-worn multimodal sensors. COBRA integrates a\nhybrid D-Net architecture combining U-Net spatial modeling, multi-head\nself-attention mechanisms, and BiLSTM temporal processing to classify daily\nactivities into four obesity-relevant categories: Food Intake, Physical\nActivity, Sedentary Behavior, and Daily Living. Validated on the WISDM-Smart\ndataset with 51 subjects performing 18 activities, COBRA's optimal\npreprocessing strategy combines spectral-temporal feature extraction, achieving\nhigh performance across multiple architectures. D-Net demonstrates 96.86%\noverall accuracy with category-specific F1-scores of 98.55% (Physical\nActivity), 95.53% (Food Intake), 94.63% (Sedentary Behavior), and 98.68% (Daily\nLiving), outperforming state-of-the-art baselines by 1.18% in accuracy. The\nframework shows robust generalizability with low demographic variance (<3%),\nenabling scalable deployment for personalized obesity interventions and\ncontinuous lifestyle monitoring."
                },
                "authors": [
                    {
                        "name": "Zhengyang Shen"
                    },
                    {
                        "name": "Bo Gao"
                    },
                    {
                        "name": "Mayue Shi"
                    }
                ],
                "author_detail": {
                    "name": "Mayue Shi"
                },
                "arxiv_affiliation": "Institute of Biomedical Engineering, University of Oxford, UK",
                "author": "Mayue Shi",
                "arxiv_comment": "19 pages, 4 figures. *Correspondence: m.shi16@imperial.ac.uk.\n  Accepted by the IUPESM World Congress on Medical Physics and Biomedical\n  Engineering 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04198v1",
                "updated": "2025-09-04T13:22:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    22,
                    44,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T13:22:44Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    22,
                    44,
                    3,
                    247,
                    0
                ],
                "title": "Are LLM Agents the New RPA? A Comparative Study with RPA Across\n  Enterprise Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLM Agents the New RPA? A Comparative Study with RPA Across\n  Enterprise Workflows"
                },
                "summary": "The emergence of large language models (LLMs) has introduced a new paradigm\nin automation: LLM agents or Agentic Automation with Computer Use (AACU).\nUnlike traditional Robotic Process Automation (RPA), which relies on rule-based\nworkflows and scripting, AACU enables intelligent agents to perform tasks\nthrough natural language instructions and autonomous interaction with user\ninterfaces. This study investigates whether AACU can serve as a viable\nalternative to RPA in enterprise workflow automation. We conducted controlled\nexperiments across three standard RPA challenges data entry, monitoring, and\ndocument extraction comparing RPA (via UiPath) and AACU (via Anthropic's\nComputer Use Agent) in terms of speed, reliability, and development effort.\nResults indicate that RPA outperforms AACU in execution speed and reliability,\nparticularly in repetitive, stable environments. However, AACU significantly\nreduces development time and adapts more flexibly to dynamic interfaces. While\ncurrent AACU implementations are not yet production-ready, their promise in\nrapid prototyping and lightweight automation is evident. Future research should\nexplore multi-agent orchestration, hybrid RPA-AACU architectures, and more\nrobust evaluation across industries and platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) has introduced a new paradigm\nin automation: LLM agents or Agentic Automation with Computer Use (AACU).\nUnlike traditional Robotic Process Automation (RPA), which relies on rule-based\nworkflows and scripting, AACU enables intelligent agents to perform tasks\nthrough natural language instructions and autonomous interaction with user\ninterfaces. This study investigates whether AACU can serve as a viable\nalternative to RPA in enterprise workflow automation. We conducted controlled\nexperiments across three standard RPA challenges data entry, monitoring, and\ndocument extraction comparing RPA (via UiPath) and AACU (via Anthropic's\nComputer Use Agent) in terms of speed, reliability, and development effort.\nResults indicate that RPA outperforms AACU in execution speed and reliability,\nparticularly in repetitive, stable environments. However, AACU significantly\nreduces development time and adapts more flexibly to dynamic interfaces. While\ncurrent AACU implementations are not yet production-ready, their promise in\nrapid prototyping and lightweight automation is evident. Future research should\nexplore multi-agent orchestration, hybrid RPA-AACU architectures, and more\nrobust evaluation across industries and platforms."
                },
                "authors": [
                    {
                        "name": "Petr Prcha"
                    },
                    {
                        "name": "Michaela Matoukov"
                    },
                    {
                        "name": "Jan Strnad"
                    }
                ],
                "author_detail": {
                    "name": "Jan Strnad"
                },
                "author": "Jan Strnad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v2",
                "updated": "2025-09-04T13:14:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    14,
                    33,
                    3,
                    247,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04191v1",
                "updated": "2025-09-04T13:13:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    13,
                    57,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T13:13:57Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    13,
                    57,
                    3,
                    247,
                    0
                ],
                "title": "KubeGuard: LLM-Assisted Kubernetes Hardening via Configuration Files and\n  Runtime Logs Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KubeGuard: LLM-Assisted Kubernetes Hardening via Configuration Files and\n  Runtime Logs Analysis"
                },
                "summary": "The widespread adoption of Kubernetes (K8s) for orchestrating cloud-native\napplications has introduced significant security challenges, such as\nmisconfigured resources and overly permissive configurations. Failing to\naddress these issues can result in unauthorized access, privilege escalation,\nand lateral movement within clusters. Most existing K8s security solutions\nfocus on detecting misconfigurations, typically through static analysis or\nanomaly detection. In contrast, this paper presents KubeGuard, a novel runtime\nlog-driven recommender framework aimed at mitigating risks by addressing overly\npermissive configurations. KubeGuard is designed to harden K8s environments\nthrough two complementary tasks: Resource Creation and Resource Refinement. It\nleverages large language models (LLMs) to analyze manifests and runtime logs\nreflecting actual system behavior, using modular prompt-chaining workflows.\nThis approach enables KubeGuard to create least-privilege configurations for\nnew resources and refine existing manifests to reduce the attack surface.\nKubeGuard's output manifests are presented as recommendations that users (e.g.,\ndevelopers and operators) can review and adopt to enhance cluster security. Our\nevaluation demonstrates that KubeGuard effectively generates and refines K8s\nmanifests for Roles, NetworkPolicies, and Deployments, leveraging both\nproprietary and open-source LLMs. The high precision, recall, and F1-scores\naffirm KubeGuard's practicality as a framework that translates runtime\nobservability into actionable, least-privilege configuration guidance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of Kubernetes (K8s) for orchestrating cloud-native\napplications has introduced significant security challenges, such as\nmisconfigured resources and overly permissive configurations. Failing to\naddress these issues can result in unauthorized access, privilege escalation,\nand lateral movement within clusters. Most existing K8s security solutions\nfocus on detecting misconfigurations, typically through static analysis or\nanomaly detection. In contrast, this paper presents KubeGuard, a novel runtime\nlog-driven recommender framework aimed at mitigating risks by addressing overly\npermissive configurations. KubeGuard is designed to harden K8s environments\nthrough two complementary tasks: Resource Creation and Resource Refinement. It\nleverages large language models (LLMs) to analyze manifests and runtime logs\nreflecting actual system behavior, using modular prompt-chaining workflows.\nThis approach enables KubeGuard to create least-privilege configurations for\nnew resources and refine existing manifests to reduce the attack surface.\nKubeGuard's output manifests are presented as recommendations that users (e.g.,\ndevelopers and operators) can review and adopt to enhance cluster security. Our\nevaluation demonstrates that KubeGuard effectively generates and refines K8s\nmanifests for Roles, NetworkPolicies, and Deployments, leveraging both\nproprietary and open-source LLMs. The high precision, recall, and F1-scores\naffirm KubeGuard's practicality as a framework that translates runtime\nobservability into actionable, least-privilege configuration guidance."
                },
                "authors": [
                    {
                        "name": "Omri Sgan Cohen"
                    },
                    {
                        "name": "Ehud Malul"
                    },
                    {
                        "name": "Yair Meidan"
                    },
                    {
                        "name": "Dudu Mimran"
                    },
                    {
                        "name": "Yuval Elovici"
                    },
                    {
                        "name": "Asaf Shabtai"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Shabtai"
                },
                "author": "Asaf Shabtai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04185v1",
                "updated": "2025-09-04T13:02:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    2,
                    39,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T13:02:39Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    2,
                    39,
                    3,
                    247,
                    0
                ],
                "title": "Set Block Decoding is a Language Model Inference Accelerator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Set Block Decoding is a Language Model Inference Accelerator"
                },
                "summary": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training."
                },
                "authors": [
                    {
                        "name": "Itai Gat"
                    },
                    {
                        "name": "Heli Ben-Hamu"
                    },
                    {
                        "name": "Marton Havasi"
                    },
                    {
                        "name": "Daniel Haziza"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Gabriel Synnaeve"
                    },
                    {
                        "name": "David Lopez-Paz"
                    },
                    {
                        "name": "Brian Karrer"
                    },
                    {
                        "name": "Yaron Lipman"
                    }
                ],
                "author_detail": {
                    "name": "Yaron Lipman"
                },
                "author": "Yaron Lipman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04183v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04183v1",
                "updated": "2025-09-04T12:59:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    59,
                    24,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T12:59:24Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    59,
                    24,
                    3,
                    247,
                    0
                ],
                "title": "MAGneT: Coordinated Multi-Agent Generation of Synthetic Multi-Turn\n  Mental Health Counseling Sessions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGneT: Coordinated Multi-Agent Generation of Synthetic Multi-Turn\n  Mental Health Counseling Sessions"
                },
                "summary": "The growing demand for scalable psychological counseling highlights the need\nfor fine-tuning open-source Large Language Models (LLMs) with high-quality,\nprivacy-compliant data, yet such data remains scarce. Here we introduce MAGneT,\na novel multi-agent framework for synthetic psychological counseling session\ngeneration that decomposes counselor response generation into coordinated\nsub-tasks handled by specialized LLM agents, each modeling a key psychological\ntechnique. Unlike prior single-agent approaches, MAGneT better captures the\nstructure and nuance of real counseling. In addition, we address\ninconsistencies in prior evaluation protocols by proposing a unified evaluation\nframework integrating diverse automatic and expert metrics. Furthermore, we\nexpand the expert evaluations from four aspects of counseling in previous works\nto nine aspects, enabling a more thorough and robust assessment of data\nquality. Empirical results show that MAGneT significantly outperforms existing\nmethods in quality, diversity, and therapeutic alignment of the generated\ncounseling sessions, improving general counseling skills by 3.2% and\nCBT-specific skills by 4.3% on average on cognitive therapy rating scale\n(CTRS). Crucially, experts prefer MAGneT-generated sessions in 77.2% of cases\non average across all aspects. Moreover, fine-tuning an open-source model on\nMAGneT-generated sessions shows better performance, with improvements of 6.3%\non general counseling skills and 7.3% on CBT-specific skills on average on CTRS\nover those fine-tuned with sessions generated by baseline methods. We also make\nour code and data public.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for scalable psychological counseling highlights the need\nfor fine-tuning open-source Large Language Models (LLMs) with high-quality,\nprivacy-compliant data, yet such data remains scarce. Here we introduce MAGneT,\na novel multi-agent framework for synthetic psychological counseling session\ngeneration that decomposes counselor response generation into coordinated\nsub-tasks handled by specialized LLM agents, each modeling a key psychological\ntechnique. Unlike prior single-agent approaches, MAGneT better captures the\nstructure and nuance of real counseling. In addition, we address\ninconsistencies in prior evaluation protocols by proposing a unified evaluation\nframework integrating diverse automatic and expert metrics. Furthermore, we\nexpand the expert evaluations from four aspects of counseling in previous works\nto nine aspects, enabling a more thorough and robust assessment of data\nquality. Empirical results show that MAGneT significantly outperforms existing\nmethods in quality, diversity, and therapeutic alignment of the generated\ncounseling sessions, improving general counseling skills by 3.2% and\nCBT-specific skills by 4.3% on average on cognitive therapy rating scale\n(CTRS). Crucially, experts prefer MAGneT-generated sessions in 77.2% of cases\non average across all aspects. Moreover, fine-tuning an open-source model on\nMAGneT-generated sessions shows better performance, with improvements of 6.3%\non general counseling skills and 7.3% on CBT-specific skills on average on CTRS\nover those fine-tuned with sessions generated by baseline methods. We also make\nour code and data public."
                },
                "authors": [
                    {
                        "name": "Aishik Mandal"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "arxiv_comment": "25 pages, 29 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04183v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04183v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04173v1",
                "updated": "2025-09-04T12:45:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    45,
                    49,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T12:45:49Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    45,
                    49,
                    3,
                    247,
                    0
                ],
                "title": "Real-time Object Detection and Associated Hardware Accelerators\n  Targeting Autonomous Vehicles: A Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Object Detection and Associated Hardware Accelerators\n  Targeting Autonomous Vehicles: A Review"
                },
                "summary": "The efficiency of object detectors depends on factors like detection\naccuracy, processing time, and computational resources. Processing time is\ncrucial for real-time applications, particularly for autonomous vehicles (AVs),\nwhere instantaneous responses are vital for safety. This review paper provides\na concise yet comprehensive survey of real-time object detection (OD)\nalgorithms for autonomous cars delving into their hardware accelerators (HAs).\nNon-neural network-based algorithms, which use statistical image processing,\nhave been entirely substituted by AI algorithms, such as different models of\nconvolutional neural networks (CNNs). Their intrinsically parallel features led\nthem to be deployable into edge-based HAs of various types, where GPUs and, to\na lesser extent, ASIC (application-specific integrated circuit) remain the most\nwidely used. Throughputs of hundreds of frames/s (fps) could be reached;\nhowever, handling object detection for all the cameras available in a typical\nAV requires further hardware and algorithmic improvements. The intensive\ncompetition between AV providers has limited the disclosure of algorithms,\nfirmware, and even hardware platform details. This remains a hurdle for\nresearchers, as commercial systems provide valuable insights while academics\nundergo lengthy training and testing on restricted datasets and road scenarios.\nConsequently, many AV research papers may not be reflected in end products,\nbeing developed under limited conditions. This paper surveys state-of-the-art\nOD algorithms and aims to bridge the gap with technologies in commercial AVs.\nTo our knowledge, this aspect has not been addressed in earlier surveys. Hence,\nthe paper serves as a tangible reference for researchers designing future\ngenerations of vehicles, expected to be fully autonomous for comfort and\nsafety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of object detectors depends on factors like detection\naccuracy, processing time, and computational resources. Processing time is\ncrucial for real-time applications, particularly for autonomous vehicles (AVs),\nwhere instantaneous responses are vital for safety. This review paper provides\na concise yet comprehensive survey of real-time object detection (OD)\nalgorithms for autonomous cars delving into their hardware accelerators (HAs).\nNon-neural network-based algorithms, which use statistical image processing,\nhave been entirely substituted by AI algorithms, such as different models of\nconvolutional neural networks (CNNs). Their intrinsically parallel features led\nthem to be deployable into edge-based HAs of various types, where GPUs and, to\na lesser extent, ASIC (application-specific integrated circuit) remain the most\nwidely used. Throughputs of hundreds of frames/s (fps) could be reached;\nhowever, handling object detection for all the cameras available in a typical\nAV requires further hardware and algorithmic improvements. The intensive\ncompetition between AV providers has limited the disclosure of algorithms,\nfirmware, and even hardware platform details. This remains a hurdle for\nresearchers, as commercial systems provide valuable insights while academics\nundergo lengthy training and testing on restricted datasets and road scenarios.\nConsequently, many AV research papers may not be reflected in end products,\nbeing developed under limited conditions. This paper surveys state-of-the-art\nOD algorithms and aims to bridge the gap with technologies in commercial AVs.\nTo our knowledge, this aspect has not been addressed in earlier surveys. Hence,\nthe paper serves as a tangible reference for researchers designing future\ngenerations of vehicles, expected to be fully autonomous for comfort and\nsafety."
                },
                "authors": [
                    {
                        "name": "Safa Sali"
                    },
                    {
                        "name": "Anis Meribout"
                    },
                    {
                        "name": "Ashiyana Majeed"
                    },
                    {
                        "name": "Mahmoud Meribout"
                    },
                    {
                        "name": "Juan Pablo"
                    },
                    {
                        "name": "Varun Tiwari"
                    },
                    {
                        "name": "Asma Baobaid"
                    }
                ],
                "author_detail": {
                    "name": "Asma Baobaid"
                },
                "author": "Asma Baobaid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12065v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12065v3",
                "updated": "2025-09-04T12:43:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    43,
                    14,
                    3,
                    247,
                    0
                ],
                "published": "2025-02-17T17:34:48Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    17,
                    34,
                    48,
                    0,
                    48,
                    0
                ],
                "title": "Autoformalization in the Wild: Assessing LLMs on Real-World Mathematical\n  Definitions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoformalization in the Wild: Assessing LLMs on Real-World Mathematical\n  Definitions"
                },
                "summary": "Thanks to their linguistic capabilities, LLMs offer an opportunity to bridge\nthe gap between informal mathematics and formal languages through\nautoformalization. However, it is still unclear how well LLMs generalize to\nsophisticated and naturally occurring mathematical statements. To address this\ngap, we investigate the task of autoformalizing real-world mathematical\ndefinitions: a critical component of mathematical discourse. Specifically, we\nintroduce two novel resources for autoformalization, collecting definitions\nfrom Wikipedia (Def_Wiki) and arXiv papers (Def_ArXiv). We then systematically\nevaluate a range of LLMs, analyzing their ability to formalize definitions into\nIsabelle/HOL. Furthermore, we investigate strategies to enhance LLMs'\nperformance including refinement through external feedback from Proof\nAssistants, and formal definition grounding, where we augment LLMs'\nformalizations through relevant contextual elements from formal mathematical\nlibraries. Our findings reveal that definitions present a greater challenge\ncompared to existing benchmarks, such as miniF2F. In particular, we found that\nLLMs still struggle with self-correction, and aligning with relevant\nmathematical libraries. At the same time, structured refinement methods and\ndefinition grounding strategies yield notable improvements of up to 16% on\nself-correction capabilities and 43% on the reduction of undefined errors,\nhighlighting promising directions for enhancing LLM-based autoformalization in\nreal-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thanks to their linguistic capabilities, LLMs offer an opportunity to bridge\nthe gap between informal mathematics and formal languages through\nautoformalization. However, it is still unclear how well LLMs generalize to\nsophisticated and naturally occurring mathematical statements. To address this\ngap, we investigate the task of autoformalizing real-world mathematical\ndefinitions: a critical component of mathematical discourse. Specifically, we\nintroduce two novel resources for autoformalization, collecting definitions\nfrom Wikipedia (Def_Wiki) and arXiv papers (Def_ArXiv). We then systematically\nevaluate a range of LLMs, analyzing their ability to formalize definitions into\nIsabelle/HOL. Furthermore, we investigate strategies to enhance LLMs'\nperformance including refinement through external feedback from Proof\nAssistants, and formal definition grounding, where we augment LLMs'\nformalizations through relevant contextual elements from formal mathematical\nlibraries. Our findings reveal that definitions present a greater challenge\ncompared to existing benchmarks, such as miniF2F. In particular, we found that\nLLMs still struggle with self-correction, and aligning with relevant\nmathematical libraries. At the same time, structured refinement methods and\ndefinition grounding strategies yield notable improvements of up to 16% on\nself-correction capabilities and 43% on the reduction of undefined errors,\nhighlighting promising directions for enhancing LLM-based autoformalization in\nreal-world scenarios."
                },
                "authors": [
                    {
                        "name": "Lan Zhang"
                    },
                    {
                        "name": "Marco Valentino"
                    },
                    {
                        "name": "Andre Freitas"
                    }
                ],
                "author_detail": {
                    "name": "Andre Freitas"
                },
                "author": "Andre Freitas",
                "arxiv_comment": "EMNLP 2025 Camera-Ready Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12065v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12065v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04162v1",
                "updated": "2025-09-04T12:35:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    35,
                    26,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T12:35:26Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    35,
                    26,
                    3,
                    247,
                    0
                ],
                "title": "Real Time FPGA Based Transformers & VLMs for Vision Tasks: SOTA Designs\n  and Optimizations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real Time FPGA Based Transformers & VLMs for Vision Tasks: SOTA Designs\n  and Optimizations"
                },
                "summary": "Transformers and vision-language models (VLMs) have emerged as dominant\narchitectures in computer vision and multimodal AI, offering state-of-the-art\nperformance in tasks such as image classification, object detection, visual\nquestion answering, and caption generation. However, their high computational\ncomplexity, large memory footprints, and irregular data access patterns present\nsignificant challenges for deployment in latency- and power-constrained\nenvironments. Field-programmable gate arrays (FPGAs) provide an attractive\nhardware platform for such workloads due to their reconfigurability,\nfine-grained parallelism, and potential for energy-efficient acceleration. This\npaper presents a comprehensive review of design trade-offs, optimization\nstrategies, and implementation challenges for FPGA-based inference of\ntransformers and VLMs. We examine critical factors such as device-class\nselection, memory subsystem constraints, dataflow orchestration, quantization\nstrategies, sparsity exploitation, and toolchain choices, alongside\nmodality-specific issues unique to VLMs, including heterogeneous compute\nbalancing and cross-attention memory management. Additionally, we discuss\nemerging trends in hardware-algorithm co-design, highlighting innovations in\nattention mechanisms, compression, and modular overlays to improve efficiency\nand adaptability. Practical issues such as runtime flexibility, verification\noverhead, and the absence of standardized FPGA multimodal benchmarks are also\nconsidered. Finally, we outline future directions toward scalable, portable,\nand reconfigurable FPGA solutions that adapt to evolving model architectures\nwhile sustaining high utilization and predictable performance. This synthesis\noffers both a technical foundation and a forward-looking perspective to help\nbridge the gap between advanced multimodal AI models and efficient FPGA\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers and vision-language models (VLMs) have emerged as dominant\narchitectures in computer vision and multimodal AI, offering state-of-the-art\nperformance in tasks such as image classification, object detection, visual\nquestion answering, and caption generation. However, their high computational\ncomplexity, large memory footprints, and irregular data access patterns present\nsignificant challenges for deployment in latency- and power-constrained\nenvironments. Field-programmable gate arrays (FPGAs) provide an attractive\nhardware platform for such workloads due to their reconfigurability,\nfine-grained parallelism, and potential for energy-efficient acceleration. This\npaper presents a comprehensive review of design trade-offs, optimization\nstrategies, and implementation challenges for FPGA-based inference of\ntransformers and VLMs. We examine critical factors such as device-class\nselection, memory subsystem constraints, dataflow orchestration, quantization\nstrategies, sparsity exploitation, and toolchain choices, alongside\nmodality-specific issues unique to VLMs, including heterogeneous compute\nbalancing and cross-attention memory management. Additionally, we discuss\nemerging trends in hardware-algorithm co-design, highlighting innovations in\nattention mechanisms, compression, and modular overlays to improve efficiency\nand adaptability. Practical issues such as runtime flexibility, verification\noverhead, and the absence of standardized FPGA multimodal benchmarks are also\nconsidered. Finally, we outline future directions toward scalable, portable,\nand reconfigurable FPGA solutions that adapt to evolving model architectures\nwhile sustaining high utilization and predictable performance. This synthesis\noffers both a technical foundation and a forward-looking perspective to help\nbridge the gap between advanced multimodal AI models and efficient FPGA\ndeployment."
                },
                "authors": [
                    {
                        "name": "Safa Mohammed Sali"
                    },
                    {
                        "name": "Mahmoud Meribout"
                    },
                    {
                        "name": "Ashiyana Abdul Majeed"
                    }
                ],
                "author_detail": {
                    "name": "Ashiyana Abdul Majeed"
                },
                "author": "Ashiyana Abdul Majeed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04153v1",
                "updated": "2025-09-04T12:28:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    28,
                    36,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T12:28:36Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    28,
                    36,
                    3,
                    247,
                    0
                ],
                "title": "Real Time FPGA Based CNNs for Detection, Classification, and Tracking in\n  Autonomous Systems: State of the Art Designs and Optimizations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real Time FPGA Based CNNs for Detection, Classification, and Tracking in\n  Autonomous Systems: State of the Art Designs and Optimizations"
                },
                "summary": "This paper presents a comprehensive review of recent advances in deploying\nconvolutional neural networks (CNNs) for object detection, classification, and\ntracking on Field Programmable Gate Arrays (FPGAs). With the increasing demand\nfor real-time computer vision applications in domains such as autonomous\nvehicles, robotics, and surveillance, FPGAs have emerged as a powerful\nalternative to GPUs and ASICs due to their reconfigurability, low power\nconsumption, and deterministic latency. We critically examine state-of-the-art\nFPGA implementations of CNN-based vision tasks, covering algorithmic\ninnovations, hardware acceleration techniques, and the integration of\noptimization strategies like pruning, quantization, and sparsity-aware methods\nto maximize performance within hardware constraints. This survey also explores\nthe landscape of modern FPGA platforms, including classical LUT-DSP based\narchitectures, System-on-Chip (SoC) FPGAs, and Adaptive Compute Acceleration\nPlatforms (ACAPs), comparing their capabilities in handling deep learning\nworkloads. Furthermore, we review available software development tools such as\nVitis AI, FINN, and Intel FPGA AI Suite, which significantly streamline the\ndesign and deployment of AI models on FPGAs. The paper uniquely discusses\nhybrid architecture that combine GPUs and FPGAs for collaborative acceleration\nof AI inference, addressing challenges related to energy efficiency and\nthroughput. Additionally, we highlight hardware-software co-design practices,\ndataflow optimizations, and pipelined processing techniques essential for\nreal-time inference on resource-constrained devices. Through this survey,\nresearchers and engineers are equipped with insights to develop\nnext-generation, power-efficient, and high-performance vision systems optimized\nfor FPGA deployment in edge and embedded applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive review of recent advances in deploying\nconvolutional neural networks (CNNs) for object detection, classification, and\ntracking on Field Programmable Gate Arrays (FPGAs). With the increasing demand\nfor real-time computer vision applications in domains such as autonomous\nvehicles, robotics, and surveillance, FPGAs have emerged as a powerful\nalternative to GPUs and ASICs due to their reconfigurability, low power\nconsumption, and deterministic latency. We critically examine state-of-the-art\nFPGA implementations of CNN-based vision tasks, covering algorithmic\ninnovations, hardware acceleration techniques, and the integration of\noptimization strategies like pruning, quantization, and sparsity-aware methods\nto maximize performance within hardware constraints. This survey also explores\nthe landscape of modern FPGA platforms, including classical LUT-DSP based\narchitectures, System-on-Chip (SoC) FPGAs, and Adaptive Compute Acceleration\nPlatforms (ACAPs), comparing their capabilities in handling deep learning\nworkloads. Furthermore, we review available software development tools such as\nVitis AI, FINN, and Intel FPGA AI Suite, which significantly streamline the\ndesign and deployment of AI models on FPGAs. The paper uniquely discusses\nhybrid architecture that combine GPUs and FPGAs for collaborative acceleration\nof AI inference, addressing challenges related to energy efficiency and\nthroughput. Additionally, we highlight hardware-software co-design practices,\ndataflow optimizations, and pipelined processing techniques essential for\nreal-time inference on resource-constrained devices. Through this survey,\nresearchers and engineers are equipped with insights to develop\nnext-generation, power-efficient, and high-performance vision systems optimized\nfor FPGA deployment in edge and embedded applications."
                },
                "authors": [
                    {
                        "name": "Safa Mohammed Sali"
                    },
                    {
                        "name": "Mahmoud Meribout"
                    },
                    {
                        "name": "Ashiyana Abdul Majeed"
                    }
                ],
                "author_detail": {
                    "name": "Ashiyana Abdul Majeed"
                },
                "author": "Ashiyana Abdul Majeed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04152v1",
                "updated": "2025-09-04T12:25:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    25,
                    14,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T12:25:14Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    25,
                    14,
                    3,
                    247,
                    0
                ],
                "title": "TAGAL: Tabular Data Generation using Agentic LLM Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAGAL: Tabular Data Generation using Agentic LLM Methods"
                },
                "summary": "The generation of data is a common approach to improve the performance of\nmachine learning tasks, among which is the training of models for\nclassification. In this paper, we present TAGAL, a collection of methods able\nto generate synthetic tabular data using an agentic workflow. The methods\nleverage Large Language Models (LLMs) for an automatic and iterative process\nthat uses feedback to improve the generated data without any further LLM\ntraining. The use of LLMs also allows for the addition of external knowledge in\nthe generation process. We evaluate TAGAL across diverse datasets and different\naspects of quality for the generated data. We look at the utility of downstream\nML models, both by training classifiers on synthetic data only and by combining\nreal and synthetic data. Moreover, we compare the similarities between the real\nand the generated data. We show that TAGAL is able to perform on par with\nstate-of-the-art approaches that require LLM training and generally outperforms\nother training-free approaches. These findings highlight the potential of\nagentic workflow and open new directions for LLM-based data generation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation of data is a common approach to improve the performance of\nmachine learning tasks, among which is the training of models for\nclassification. In this paper, we present TAGAL, a collection of methods able\nto generate synthetic tabular data using an agentic workflow. The methods\nleverage Large Language Models (LLMs) for an automatic and iterative process\nthat uses feedback to improve the generated data without any further LLM\ntraining. The use of LLMs also allows for the addition of external knowledge in\nthe generation process. We evaluate TAGAL across diverse datasets and different\naspects of quality for the generated data. We look at the utility of downstream\nML models, both by training classifiers on synthetic data only and by combining\nreal and synthetic data. Moreover, we compare the similarities between the real\nand the generated data. We show that TAGAL is able to perform on par with\nstate-of-the-art approaches that require LLM training and generally outperforms\nother training-free approaches. These findings highlight the potential of\nagentic workflow and open new directions for LLM-based data generation methods."
                },
                "authors": [
                    {
                        "name": "Benot Ronval"
                    },
                    {
                        "name": "Pierre Dupont"
                    },
                    {
                        "name": "Siegfried Nijssen"
                    }
                ],
                "author_detail": {
                    "name": "Siegfried Nijssen"
                },
                "author": "Siegfried Nijssen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09454v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09454v4",
                "updated": "2025-09-04T12:20:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    20,
                    43,
                    3,
                    247,
                    0
                ],
                "published": "2025-03-12T14:57:08Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    57,
                    8,
                    2,
                    71,
                    0
                ],
                "title": "Explicit Learning and the LLM in Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explicit Learning and the LLM in Machine Translation"
                },
                "summary": "This study explores an LLM's ability to learn new languages using\nexplanations found in a grammar book, a process we term \"explicit learning.\" To\nrigorously assess this ability, we design controlled translation experiments\nbetween English and constructed languages generated, through specific\ncryptographic means, from Latin or French. Contrary to previous studies, our\nresults demonstrate that LLMs do possess a measurable capacity for explicit\nlearning. This ability, however, diminishes as the complexity of the linguistic\nphenomena to be learned increases. Supervised fine-tuning on ad hoc chains of\nthought significantly enhances LLM performance but struggles to generalize to\ntypologically novel or more complex linguistic features. These findings point\nto the need for more diverse training sets and alternative fine-tuning\nstrategies to further improve explicit learning by LLMs, benefiting\nlow-resource languages typically described in grammar books but lacking\nextensive corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores an LLM's ability to learn new languages using\nexplanations found in a grammar book, a process we term \"explicit learning.\" To\nrigorously assess this ability, we design controlled translation experiments\nbetween English and constructed languages generated, through specific\ncryptographic means, from Latin or French. Contrary to previous studies, our\nresults demonstrate that LLMs do possess a measurable capacity for explicit\nlearning. This ability, however, diminishes as the complexity of the linguistic\nphenomena to be learned increases. Supervised fine-tuning on ad hoc chains of\nthought significantly enhances LLM performance but struggles to generalize to\ntypologically novel or more complex linguistic features. These findings point\nto the need for more diverse training sets and alternative fine-tuning\nstrategies to further improve explicit learning by LLMs, benefiting\nlow-resource languages typically described in grammar books but lacking\nextensive corpora."
                },
                "authors": [
                    {
                        "name": "Malik Marmonier"
                    },
                    {
                        "name": "Rachel Bawden"
                    },
                    {
                        "name": "Benot Sagot"
                    }
                ],
                "author_detail": {
                    "name": "Benot Sagot"
                },
                "author": "Benot Sagot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09454v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09454v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00265v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00265v3",
                "updated": "2025-09-04T12:15:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    15,
                    53,
                    3,
                    247,
                    0
                ],
                "published": "2024-10-31T23:54:21Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    23,
                    54,
                    21,
                    3,
                    305,
                    0
                ],
                "title": "Quantifying Calibration Error in Neural Networks Through Evidence-Based\n  Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Calibration Error in Neural Networks Through Evidence-Based\n  Theory"
                },
                "summary": "Trustworthiness in neural networks is crucial for their deployment in\ncritical applications, where reliability, confidence, and uncertainty play\npivotal roles in decision-making. Traditional performance metrics such as\naccuracy and precision fail to capture these aspects, particularly in cases\nwhere models exhibit overconfidence. To address these limitations, this paper\nintroduces a novel framework for quantifying the trustworthiness of neural\nnetworks by incorporating subjective logic into the evaluation of Expected\nCalibration Error (ECE). This method provides a comprehensive measure of trust,\ndisbelief, and uncertainty by clustering predicted probabilities and fusing\nopinions using appropriate fusion operators. We demonstrate the effectiveness\nof this approach through experiments on MNIST and CIFAR-10 datasets, where\npost-calibration results indicate improved trustworthiness. The proposed\nframework offers a more interpretable and nuanced assessment of AI models, with\npotential applications in sensitive domains such as healthcare and autonomous\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthiness in neural networks is crucial for their deployment in\ncritical applications, where reliability, confidence, and uncertainty play\npivotal roles in decision-making. Traditional performance metrics such as\naccuracy and precision fail to capture these aspects, particularly in cases\nwhere models exhibit overconfidence. To address these limitations, this paper\nintroduces a novel framework for quantifying the trustworthiness of neural\nnetworks by incorporating subjective logic into the evaluation of Expected\nCalibration Error (ECE). This method provides a comprehensive measure of trust,\ndisbelief, and uncertainty by clustering predicted probabilities and fusing\nopinions using appropriate fusion operators. We demonstrate the effectiveness\nof this approach through experiments on MNIST and CIFAR-10 datasets, where\npost-calibration results indicate improved trustworthiness. The proposed\nframework offers a more interpretable and nuanced assessment of AI models, with\npotential applications in sensitive domains such as healthcare and autonomous\nsystems."
                },
                "authors": [
                    {
                        "name": "Koffi Ismael Ouattara"
                    },
                    {
                        "name": "Ioannis Krontiris"
                    },
                    {
                        "name": "Theo Dimitrakos"
                    },
                    {
                        "name": "Frank Kargl"
                    }
                ],
                "author_detail": {
                    "name": "Frank Kargl"
                },
                "author": "Frank Kargl",
                "arxiv_doi": "10.23919/FUSION65864.2025.11124121",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.23919/FUSION65864.2025.11124121",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.00265v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00265v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This is the preprint of the paper accepted to Fusion 2025 (28th\n  International Conference on Information Fusion, Rio de Janeiro, Brazil, July\n  7-10, 2025). The published version is available at\n  https://doi.org/10.23919/FUSION65864.2025.11124121",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04139v1",
                "updated": "2025-09-04T12:11:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    11,
                    3,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T12:11:03Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    11,
                    3,
                    3,
                    247,
                    0
                ],
                "title": "Enhancing Technical Documents Retrieval for RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Technical Documents Retrieval for RAG"
                },
                "summary": "In this paper, we introduce Technical-Embeddings, a novel framework designed\nto optimize semantic retrieval in technical documentation, with applications in\nboth hardware and software development. Our approach addresses the challenges\nof understanding and retrieving complex technical content by leveraging the\ncapabilities of Large Language Models (LLMs). First, we enhance user queries by\ngenerating expanded representations that better capture user intent and improve\ndataset diversity, thereby enriching the fine-tuning process for embedding\nmodels. Second, we apply summary extraction techniques to encode essential\ncontextual information, refining the representation of technical documents. To\nfurther enhance retrieval performance, we fine-tune a bi-encoder BERT model\nusing soft prompting, incorporating separate learning parameters for queries\nand document context to capture fine-grained semantic nuances. We evaluate our\napproach on two public datasets, RAG-EDA and Rust-Docs-QA, demonstrating that\nTechnical-Embeddings significantly outperforms baseline models in both\nprecision and recall. Our findings highlight the effectiveness of integrating\nquery expansion and contextual summarization to enhance information access and\ncomprehension in technical domains. This work advances the state of\nRetrieval-Augmented Generation (RAG) systems, offering new avenues for\nefficient and accurate technical document retrieval in engineering and product\ndevelopment workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Technical-Embeddings, a novel framework designed\nto optimize semantic retrieval in technical documentation, with applications in\nboth hardware and software development. Our approach addresses the challenges\nof understanding and retrieving complex technical content by leveraging the\ncapabilities of Large Language Models (LLMs). First, we enhance user queries by\ngenerating expanded representations that better capture user intent and improve\ndataset diversity, thereby enriching the fine-tuning process for embedding\nmodels. Second, we apply summary extraction techniques to encode essential\ncontextual information, refining the representation of technical documents. To\nfurther enhance retrieval performance, we fine-tune a bi-encoder BERT model\nusing soft prompting, incorporating separate learning parameters for queries\nand document context to capture fine-grained semantic nuances. We evaluate our\napproach on two public datasets, RAG-EDA and Rust-Docs-QA, demonstrating that\nTechnical-Embeddings significantly outperforms baseline models in both\nprecision and recall. Our findings highlight the effectiveness of integrating\nquery expansion and contextual summarization to enhance information access and\ncomprehension in technical domains. This work advances the state of\nRetrieval-Augmented Generation (RAG) systems, offering new avenues for\nefficient and accurate technical document retrieval in engineering and product\ndevelopment workflows."
                },
                "authors": [
                    {
                        "name": "Songjiang Lai"
                    },
                    {
                        "name": "Tsun-Hin Cheung"
                    },
                    {
                        "name": "Ka-Chun Fung"
                    },
                    {
                        "name": "Kaiwen Xue"
                    },
                    {
                        "name": "Kwan-Ho Lin"
                    },
                    {
                        "name": "Yan-Ming Choi"
                    },
                    {
                        "name": "Vincent Ng"
                    },
                    {
                        "name": "Kin-Man Lam"
                    }
                ],
                "author_detail": {
                    "name": "Kin-Man Lam"
                },
                "author": "Kin-Man Lam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00454v2",
                "updated": "2025-09-04T11:57:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    57,
                    46,
                    3,
                    247,
                    0
                ],
                "published": "2025-08-01T09:26:01Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    9,
                    26,
                    1,
                    4,
                    213,
                    0
                ],
                "title": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges"
                },
                "summary": "Evaluating the conversational abilities of large language models (LLMs)\nremains a challenging task. Current mainstream approaches primarily rely on the\n\"LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator to\nassess dialogue quality. However, such methods often suffer from various\nbiases, which undermine the reliability and consistency of the evaluation\nresults. To mitigate these biases, recent methods employ multiple LLMs as\njudges and aggregate their judgments to select the optimal assessment. Although\neffective, this multi-judge approach incurs significant computational overhead\nduring inference. In this paper, we propose an efficient multi-turn dialogue\nevaluator that captures the collective wisdom of multiple LLM judges by\naggregating their preference knowledge into a single model. Our approach\npreserves the advantages of diverse multi-judge feedback while drastically\nreducing the evaluation cost, enabling fast and flexible dialogue quality\nassessment. Extensive experiments on seven single rating and pairwise\ncomparison dialogue evaluation benchmarks demonstrate that our method\noutperforms existing baselines across diverse scenarios, showcasing its\nefficiency and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the conversational abilities of large language models (LLMs)\nremains a challenging task. Current mainstream approaches primarily rely on the\n\"LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator to\nassess dialogue quality. However, such methods often suffer from various\nbiases, which undermine the reliability and consistency of the evaluation\nresults. To mitigate these biases, recent methods employ multiple LLMs as\njudges and aggregate their judgments to select the optimal assessment. Although\neffective, this multi-judge approach incurs significant computational overhead\nduring inference. In this paper, we propose an efficient multi-turn dialogue\nevaluator that captures the collective wisdom of multiple LLM judges by\naggregating their preference knowledge into a single model. Our approach\npreserves the advantages of diverse multi-judge feedback while drastically\nreducing the evaluation cost, enabling fast and flexible dialogue quality\nassessment. Extensive experiments on seven single rating and pairwise\ncomparison dialogue evaluation benchmarks demonstrate that our method\noutperforms existing baselines across diverse scenarios, showcasing its\nefficiency and robustness."
                },
                "authors": [
                    {
                        "name": "Yuqi Tang"
                    },
                    {
                        "name": "Kehua Feng"
                    },
                    {
                        "name": "Yunfeng Wang"
                    },
                    {
                        "name": "Zhiwen Chen"
                    },
                    {
                        "name": "Chengfei Lv"
                    },
                    {
                        "name": "Gang Yu"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Keyan Ding"
                    }
                ],
                "author_detail": {
                    "name": "Keyan Ding"
                },
                "author": "Keyan Ding",
                "arxiv_comment": "15 pages, 2 pages, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01909v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01909v2",
                "updated": "2025-09-04T11:54:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    54,
                    6,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-02T03:04:27Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    3,
                    4,
                    27,
                    1,
                    245,
                    0
                ],
                "title": "Oyster-I: Beyond Refusal -- Constructive Safety Alignment for\n  Responsible Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oyster-I: Beyond Refusal -- Constructive Safety Alignment for\n  Responsible Language Models"
                },
                "summary": "Large language models (LLMs) typically deploy safety mechanisms to prevent\nharmful content generation. Most current approaches focus narrowly on risks\nposed by malicious actors, often framing risks as adversarial events and\nrelying on defensive refusals. However, in real-world settings, risks also come\nfrom non-malicious users seeking help while under psychological distress (e.g.,\nself-harm intentions). In such cases, the model's response can strongly\ninfluence the user's next actions. Simple refusals may lead them to repeat,\nescalate, or move to unsafe platforms, creating worse outcomes. We introduce\nConstructive Safety Alignment (CSA), a human-centric paradigm that protects\nagainst malicious misuse while actively guiding vulnerable users toward safe\nand helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic\nanticipation of user reactions, fine-grained risk boundary discovery, and\ninterpretable reasoning control, turning safety into a trust-building process.\nOy1 achieves state-of-the-art safety among open models while retaining high\ngeneral capabilities. On our Constructive Benchmark, it shows strong\nconstructive engagement, close to GPT-5, and unmatched robustness on the\nStrata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from\nrefusal-first to guidance-first safety, CSA redefines the model-user\nrelationship, aiming for systems that are not just safe, but meaningfully\nhelpful. We release Oy1, code, and the benchmark to support responsible,\nuser-centered AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) typically deploy safety mechanisms to prevent\nharmful content generation. Most current approaches focus narrowly on risks\nposed by malicious actors, often framing risks as adversarial events and\nrelying on defensive refusals. However, in real-world settings, risks also come\nfrom non-malicious users seeking help while under psychological distress (e.g.,\nself-harm intentions). In such cases, the model's response can strongly\ninfluence the user's next actions. Simple refusals may lead them to repeat,\nescalate, or move to unsafe platforms, creating worse outcomes. We introduce\nConstructive Safety Alignment (CSA), a human-centric paradigm that protects\nagainst malicious misuse while actively guiding vulnerable users toward safe\nand helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic\nanticipation of user reactions, fine-grained risk boundary discovery, and\ninterpretable reasoning control, turning safety into a trust-building process.\nOy1 achieves state-of-the-art safety among open models while retaining high\ngeneral capabilities. On our Constructive Benchmark, it shows strong\nconstructive engagement, close to GPT-5, and unmatched robustness on the\nStrata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from\nrefusal-first to guidance-first safety, CSA redefines the model-user\nrelationship, aiming for systems that are not just safe, but meaningfully\nhelpful. We release Oy1, code, and the benchmark to support responsible,\nuser-centered AI."
                },
                "authors": [
                    {
                        "name": "Ranjie Duan"
                    },
                    {
                        "name": "Jiexi Liu"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Shiji Zhao"
                    },
                    {
                        "name": "Ruoxi Cheng"
                    },
                    {
                        "name": "Fengxiang Wang"
                    },
                    {
                        "name": "Cheng Wei"
                    },
                    {
                        "name": "Yong Xie"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Defeng Li"
                    },
                    {
                        "name": "Yinpeng Dong"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Yuefeng Chen"
                    },
                    {
                        "name": "Chongwen Wang"
                    },
                    {
                        "name": "Xingjun Ma"
                    },
                    {
                        "name": "Xingxing Wei"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Jun Zhu"
                    },
                    {
                        "name": "Xinfeng Li"
                    },
                    {
                        "name": "Yitong Sun"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Jinzhao Hu"
                    },
                    {
                        "name": "Sha Xu"
                    },
                    {
                        "name": "Yitong Yang"
                    },
                    {
                        "name": "Jialing Tao"
                    },
                    {
                        "name": "Hui Xue"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xue"
                },
                "author": "Hui Xue",
                "arxiv_comment": "Technical Report Code & Model weights available:\n  https://github.com/Alibaba-AAIG/Oyster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01909v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01909v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04126v1",
                "updated": "2025-09-04T11:44:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    44,
                    28,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T11:44:28Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    44,
                    28,
                    3,
                    247,
                    0
                ],
                "title": "MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image\n  Generation"
                },
                "summary": "Text-to-image diffusion models have achieved remarkable image quality, but\nthey still struggle with complex, multiele ment prompts, and limited stylistic\ndiversity. To address these limitations, we propose a Multi-Expert Planning and\nGen eration Framework (MEPG) that synergistically integrates position- and\nstyle-aware large language models (LLMs) with spatial-semantic expert modules.\nThe framework comprises two core components: (1) a Position-Style-Aware (PSA)\nmodule that utilizes a supervised fine-tuned LLM to decom pose input prompts\ninto precise spatial coordinates and style encoded semantic instructions; and\n(2) a Multi-Expert Dif fusion (MED) module that implements cross-region genera\ntion through dynamic expert routing across both local regions and global areas.\nDuring the generation process for each lo cal region, specialized models (e.g.,\nrealism experts, styliza tion specialists) are selectively activated for each\nspatial par tition via attention-based gating mechanisms. The architec ture\nsupports lightweight integration and replacement of ex pert models, providing\nstrong extensibility. Additionally, an interactive interface enables real-time\nspatial layout editing and per-region style selection from a portfolio of\nexperts. Ex periments show that MEPG significantly outperforms base line models\nwith the same backbone in both image quality\n  and style diversity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image diffusion models have achieved remarkable image quality, but\nthey still struggle with complex, multiele ment prompts, and limited stylistic\ndiversity. To address these limitations, we propose a Multi-Expert Planning and\nGen eration Framework (MEPG) that synergistically integrates position- and\nstyle-aware large language models (LLMs) with spatial-semantic expert modules.\nThe framework comprises two core components: (1) a Position-Style-Aware (PSA)\nmodule that utilizes a supervised fine-tuned LLM to decom pose input prompts\ninto precise spatial coordinates and style encoded semantic instructions; and\n(2) a Multi-Expert Dif fusion (MED) module that implements cross-region genera\ntion through dynamic expert routing across both local regions and global areas.\nDuring the generation process for each lo cal region, specialized models (e.g.,\nrealism experts, styliza tion specialists) are selectively activated for each\nspatial par tition via attention-based gating mechanisms. The architec ture\nsupports lightweight integration and replacement of ex pert models, providing\nstrong extensibility. Additionally, an interactive interface enables real-time\nspatial layout editing and per-region style selection from a portfolio of\nexperts. Ex periments show that MEPG significantly outperforms base line models\nwith the same backbone in both image quality\n  and style diversity."
                },
                "authors": [
                    {
                        "name": "Yuan Zhao"
                    },
                    {
                        "name": "Liu Lin"
                    }
                ],
                "author_detail": {
                    "name": "Liu Lin"
                },
                "author": "Liu Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05248v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05248v3",
                "updated": "2025-09-04T11:42:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    42,
                    4,
                    3,
                    247,
                    0
                ],
                "published": "2024-12-06T18:27:15Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    27,
                    15,
                    4,
                    341,
                    0
                ],
                "title": "Enhancing FKG.in: automating Indian food composition analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing FKG.in: automating Indian food composition analysis"
                },
                "summary": "This paper presents a novel approach to compute food composition data for\nIndian recipes using a knowledge graph for Indian food (FKG[.]in) and LLMs. The\nprimary focus is to provide a broad overview of an automated food composition\nanalysis workflow and describe its core functionalities: nutrition data\naggregation, food composition analysis, and LLM-augmented information\nresolution. This workflow aims to complement FKG[.]in and iteratively\nsupplement food composition data from verified knowledge bases. Additionally,\nthis paper highlights the challenges of representing Indian food and accessing\nfood composition data digitally. It also reviews three key sources of food\ncomposition data: the Indian Food Composition Tables, the Indian Nutrient\nDatabank, and the Nutritionix API. Furthermore, it briefly outlines how users\ncan interact with the workflow to obtain diet-based health recommendations and\ndetailed food composition information for numerous recipes. We then explore the\ncomplex challenges of analyzing Indian recipe information across dimensions\nsuch as structure, multilingualism, and uncertainty as well as present our\nongoing work on LLM-based solutions to address these issues. The methods\nproposed in this workshop paper for AI-driven knowledge curation and\ninformation resolution are application-agnostic, generalizable, and replicable\nfor any domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach to compute food composition data for\nIndian recipes using a knowledge graph for Indian food (FKG[.]in) and LLMs. The\nprimary focus is to provide a broad overview of an automated food composition\nanalysis workflow and describe its core functionalities: nutrition data\naggregation, food composition analysis, and LLM-augmented information\nresolution. This workflow aims to complement FKG[.]in and iteratively\nsupplement food composition data from verified knowledge bases. Additionally,\nthis paper highlights the challenges of representing Indian food and accessing\nfood composition data digitally. It also reviews three key sources of food\ncomposition data: the Indian Food Composition Tables, the Indian Nutrient\nDatabank, and the Nutritionix API. Furthermore, it briefly outlines how users\ncan interact with the workflow to obtain diet-based health recommendations and\ndetailed food composition information for numerous recipes. We then explore the\ncomplex challenges of analyzing Indian recipe information across dimensions\nsuch as structure, multilingualism, and uncertainty as well as present our\nongoing work on LLM-based solutions to address these issues. The methods\nproposed in this workshop paper for AI-driven knowledge curation and\ninformation resolution are application-agnostic, generalizable, and replicable\nfor any domain."
                },
                "authors": [
                    {
                        "name": "Saransh Kumar Gupta"
                    },
                    {
                        "name": "Lipika Dey"
                    },
                    {
                        "name": "Partha Pratim Das"
                    },
                    {
                        "name": "Geeta Trilok-Kumar"
                    },
                    {
                        "name": "Ramesh Jain"
                    }
                ],
                "author_detail": {
                    "name": "Ramesh Jain"
                },
                "author": "Ramesh Jain",
                "arxiv_doi": "10.1007/978-3-031-88217-3_6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-88217-3_6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.05248v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05248v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 5 figures, 30 references, International Conference on\n  Pattern Recognition 2024 - Multimedia Assisted Dietary Management Workshop",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04123v1",
                "updated": "2025-09-04T11:37:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    37,
                    6,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T11:37:06Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    37,
                    6,
                    3,
                    247,
                    0
                ],
                "title": "TaleDiffusion: Multi-Character Story Generation with Dialogue Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TaleDiffusion: Multi-Character Story Generation with Dialogue Rendering"
                },
                "summary": "Text-to-story visualization is challenging due to the need for consistent\ninteraction among multiple characters across frames. Existing methods struggle\nwith character consistency, leading to artifact generation and inaccurate\ndialogue rendering, which results in disjointed storytelling. In response, we\nintroduce TaleDiffusion, a novel framework for generating multi-character\nstories with an iterative process, maintaining character consistency, and\naccurate dialogue assignment via postprocessing. Given a story, we use a\npre-trained LLM to generate per-frame descriptions, character details, and\ndialogues via in-context learning, followed by a bounded attention-based\nper-box mask technique to control character interactions and minimize\nartifacts. We then apply an identity-consistent self-attention mechanism to\nensure character consistency across frames and region-aware cross-attention for\nprecise object placement. Dialogues are also rendered as bubbles and assigned\nto characters via CLIPSeg. Experimental results demonstrate that TaleDiffusion\noutperforms existing methods in consistency, noise reduction, and dialogue\nrendering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-story visualization is challenging due to the need for consistent\ninteraction among multiple characters across frames. Existing methods struggle\nwith character consistency, leading to artifact generation and inaccurate\ndialogue rendering, which results in disjointed storytelling. In response, we\nintroduce TaleDiffusion, a novel framework for generating multi-character\nstories with an iterative process, maintaining character consistency, and\naccurate dialogue assignment via postprocessing. Given a story, we use a\npre-trained LLM to generate per-frame descriptions, character details, and\ndialogues via in-context learning, followed by a bounded attention-based\nper-box mask technique to control character interactions and minimize\nartifacts. We then apply an identity-consistent self-attention mechanism to\nensure character consistency across frames and region-aware cross-attention for\nprecise object placement. Dialogues are also rendered as bubbles and assigned\nto characters via CLIPSeg. Experimental results demonstrate that TaleDiffusion\noutperforms existing methods in consistency, noise reduction, and dialogue\nrendering."
                },
                "authors": [
                    {
                        "name": "Ayan Banerjee"
                    },
                    {
                        "name": "Josep Llads"
                    },
                    {
                        "name": "Umapada Pal"
                    },
                    {
                        "name": "Anjan Dutta"
                    }
                ],
                "author_detail": {
                    "name": "Anjan Dutta"
                },
                "author": "Anjan Dutta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14585v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14585v2",
                "updated": "2025-09-04T11:31:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    31,
                    24,
                    3,
                    247,
                    0
                ],
                "published": "2025-05-20T16:40:09Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    40,
                    9,
                    1,
                    140,
                    0
                ],
                "title": "Context Reasoner: Incentivizing Reasoning Capability for Contextualized\n  Privacy and Safety Compliance via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Reasoner: Incentivizing Reasoning Capability for Contextualized\n  Privacy and Safety Compliance via Reinforcement Learning"
                },
                "summary": "While Large Language Models (LLMs) exhibit remarkable capabilities, they also\nintroduce significant safety and privacy risks. Current mitigation strategies\noften fail to preserve contextual reasoning capabilities in risky scenarios.\nInstead, they rely heavily on sensitive pattern matching to protect LLMs, which\nlimits the scope. Furthermore, they overlook established safety and privacy\nstandards, leading to systemic risks for legal compliance. To address these\ngaps, we formulate safety and privacy issues into contextualized compliance\nproblems following the Contextual Integrity (CI) theory. Under the CI\nframework, we align our model with three critical regulatory standards: GDPR,\nEU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with\na rule-based reward to incentivize contextual reasoning capabilities while\nenhancing compliance with safety and privacy norms. Through extensive\nexperiments, we demonstrate that our method not only significantly enhances\nlegal compliance (achieving a +8.58% accuracy improvement in safety/privacy\nbenchmarks) but also further improves general reasoning capability. For\nOpenThinker-7B, a strong reasoning model that significantly outperforms its\nbase model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its\ngeneral reasoning capabilities, with +2.05% and +8.98% accuracy improvement on\nthe MMLU and LegalBench benchmark, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) exhibit remarkable capabilities, they also\nintroduce significant safety and privacy risks. Current mitigation strategies\noften fail to preserve contextual reasoning capabilities in risky scenarios.\nInstead, they rely heavily on sensitive pattern matching to protect LLMs, which\nlimits the scope. Furthermore, they overlook established safety and privacy\nstandards, leading to systemic risks for legal compliance. To address these\ngaps, we formulate safety and privacy issues into contextualized compliance\nproblems following the Contextual Integrity (CI) theory. Under the CI\nframework, we align our model with three critical regulatory standards: GDPR,\nEU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with\na rule-based reward to incentivize contextual reasoning capabilities while\nenhancing compliance with safety and privacy norms. Through extensive\nexperiments, we demonstrate that our method not only significantly enhances\nlegal compliance (achieving a +8.58% accuracy improvement in safety/privacy\nbenchmarks) but also further improves general reasoning capability. For\nOpenThinker-7B, a strong reasoning model that significantly outperforms its\nbase model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its\ngeneral reasoning capabilities, with +2.05% and +8.98% accuracy improvement on\nthe MMLU and LegalBench benchmark, respectively."
                },
                "authors": [
                    {
                        "name": "Wenbin Hu"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Huihao Jing"
                    },
                    {
                        "name": "Qi Hu"
                    },
                    {
                        "name": "Ziqian Zeng"
                    },
                    {
                        "name": "Sirui Han"
                    },
                    {
                        "name": "Heli Xu"
                    },
                    {
                        "name": "Tianshu Chu"
                    },
                    {
                        "name": "Peizhao Hu"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "Accepted to EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14585v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14585v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04111v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04111v2",
                "updated": "2025-09-05T09:12:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    9,
                    12,
                    3,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-04T11:20:53Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    20,
                    53,
                    3,
                    247,
                    0
                ],
                "title": "MultiWikiQA: A Reading Comprehension Benchmark in 300+ Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiWikiQA: A Reading Comprehension Benchmark in 300+ Languages"
                },
                "summary": "We introduce a new reading comprehension dataset, dubbed MultiWikiQA, which\ncovers 306 languages. The context data comes from Wikipedia articles, with\nquestions generated by an LLM and the answers appearing verbatim in the\nWikipedia articles. We conduct a crowdsourced human evaluation of the fluency\nof the generated questions across 30 of the languages, providing evidence that\nthe questions are of good quality. We evaluate 6 different language models,\nboth decoder and encoder models of varying sizes, showing that the benchmark is\nsufficiently difficult and that there is a large performance discrepancy\namongst the languages. The dataset and survey evaluations are freely available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new reading comprehension dataset, dubbed MultiWikiQA, which\ncovers 306 languages. The context data comes from Wikipedia articles, with\nquestions generated by an LLM and the answers appearing verbatim in the\nWikipedia articles. We conduct a crowdsourced human evaluation of the fluency\nof the generated questions across 30 of the languages, providing evidence that\nthe questions are of good quality. We evaluate 6 different language models,\nboth decoder and encoder models of varying sizes, showing that the benchmark is\nsufficiently difficult and that there is a large performance discrepancy\namongst the languages. The dataset and survey evaluations are freely available."
                },
                "authors": [
                    {
                        "name": "Dan Saattrup Smart"
                    }
                ],
                "author_detail": {
                    "name": "Dan Saattrup Smart"
                },
                "author": "Dan Saattrup Smart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04111v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04111v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04104v1",
                "updated": "2025-09-04T11:07:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    7,
                    27,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T11:07:27Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    7,
                    27,
                    3,
                    247,
                    0
                ],
                "title": "Towards Stable and Personalised Profiles for Lexical Alignment in Spoken\n  Human-Agent Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Stable and Personalised Profiles for Lexical Alignment in Spoken\n  Human-Agent Dialogue"
                },
                "summary": "Lexical alignment, where speakers start to use similar words across\nconversation, is known to contribute to successful communication. However, its\nimplementation in conversational agents remains underexplored, particularly\nconsidering the recent advancements in large language models (LLMs). As a first\nstep towards enabling lexical alignment in human-agent dialogue, this study\ndraws on strategies for personalising conversational agents and investigates\nthe construction of stable, personalised lexical profiles as a basis for\nlexical alignment. Specifically, we varied the amounts of transcribed spoken\ndata used for construction as well as the number of items included in the\nprofiles per part-of-speech (POS) category and evaluated profile performance\nacross time using recall, coverage, and cosine similarity metrics. It was shown\nthat smaller and more compact profiles, created after 10 min of transcribed\nspeech containing 5 items for adjectives, 5 items for conjunctions, and 10\nitems for adverbs, nouns, pronouns, and verbs each, offered the best balance in\nboth performance and data efficiency. In conclusion, this study offers\npractical insights into constructing stable, personalised lexical profiles,\ntaking into account minimal data requirements, serving as a foundational step\ntoward lexical alignment strategies in conversational agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lexical alignment, where speakers start to use similar words across\nconversation, is known to contribute to successful communication. However, its\nimplementation in conversational agents remains underexplored, particularly\nconsidering the recent advancements in large language models (LLMs). As a first\nstep towards enabling lexical alignment in human-agent dialogue, this study\ndraws on strategies for personalising conversational agents and investigates\nthe construction of stable, personalised lexical profiles as a basis for\nlexical alignment. Specifically, we varied the amounts of transcribed spoken\ndata used for construction as well as the number of items included in the\nprofiles per part-of-speech (POS) category and evaluated profile performance\nacross time using recall, coverage, and cosine similarity metrics. It was shown\nthat smaller and more compact profiles, created after 10 min of transcribed\nspeech containing 5 items for adjectives, 5 items for conjunctions, and 10\nitems for adverbs, nouns, pronouns, and verbs each, offered the best balance in\nboth performance and data efficiency. In conclusion, this study offers\npractical insights into constructing stable, personalised lexical profiles,\ntaking into account minimal data requirements, serving as a foundational step\ntoward lexical alignment strategies in conversational agents."
                },
                "authors": [
                    {
                        "name": "Keara Schaaij"
                    },
                    {
                        "name": "Roel Boumans"
                    },
                    {
                        "name": "Tibor Bosse"
                    },
                    {
                        "name": "Iris Hendrickx"
                    }
                ],
                "author_detail": {
                    "name": "Iris Hendrickx"
                },
                "author": "Iris Hendrickx",
                "arxiv_doi": "10.1007/978-3-032-02548-7_5",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-032-02548-7_5",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.04104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for TSD 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04095v1",
                "updated": "2025-09-04T10:53:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    10,
                    53,
                    27,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T10:53:27Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    10,
                    53,
                    27,
                    3,
                    247,
                    0
                ],
                "title": "Cloud-Assisted Remote Control for Aerial Robots: From Theory to\n  Proof-of-Concept Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud-Assisted Remote Control for Aerial Robots: From Theory to\n  Proof-of-Concept Implementation"
                },
                "summary": "Cloud robotics has emerged as a promising technology for robotics\napplications due to its advantages of offloading computationally intensive\ntasks, facilitating data sharing, and enhancing robot coordination. However,\nintegrating cloud computing with robotics remains a complex challenge due to\nnetwork latency, security concerns, and the need for efficient resource\nmanagement. In this work, we present a scalable and intuitive framework for\ntesting cloud and edge robotic systems. The framework consists of two main\ncomponents enabled by containerized technology: (a) a containerized cloud\ncluster and (b) the containerized robot simulation environment. The system\nincorporates two endpoints of a User Datagram Protocol (UDP) tunnel, enabling\nbidirectional communication between the cloud cluster container and the robot\nsimulation environment, while simulating realistic network conditions. To\nachieve this, we consider the use case of cloud-assisted remote control for\naerial robots, while utilizing Linux-based traffic control to introduce\nartificial delay and jitter, replicating variable network conditions\nencountered in practical cloud-robot deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud robotics has emerged as a promising technology for robotics\napplications due to its advantages of offloading computationally intensive\ntasks, facilitating data sharing, and enhancing robot coordination. However,\nintegrating cloud computing with robotics remains a complex challenge due to\nnetwork latency, security concerns, and the need for efficient resource\nmanagement. In this work, we present a scalable and intuitive framework for\ntesting cloud and edge robotic systems. The framework consists of two main\ncomponents enabled by containerized technology: (a) a containerized cloud\ncluster and (b) the containerized robot simulation environment. The system\nincorporates two endpoints of a User Datagram Protocol (UDP) tunnel, enabling\nbidirectional communication between the cloud cluster container and the robot\nsimulation environment, while simulating realistic network conditions. To\nachieve this, we consider the use case of cloud-assisted remote control for\naerial robots, while utilizing Linux-based traffic control to introduce\nartificial delay and jitter, replicating variable network conditions\nencountered in practical cloud-robot deployments."
                },
                "authors": [
                    {
                        "name": "Achilleas Santi Seisa"
                    },
                    {
                        "name": "Viswa Narayanan Sankaranarayanan"
                    },
                    {
                        "name": "Gerasimos Damigos"
                    },
                    {
                        "name": "Sumeet Gajanan Satpute"
                    },
                    {
                        "name": "George Nikolakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "George Nikolakopoulos"
                },
                "author": "George Nikolakopoulos",
                "arxiv_doi": "10.1109/CCGridW65158.2025.00032",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/CCGridW65158.2025.00032",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.04095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages, 7 figures, CCGridW 2025",
                "arxiv_journal_ref": "2025 IEEE 25th International Symposium on Cluster, Cloud and\n  Internet Computing Workshops (CCGridW)",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04092v1",
                "updated": "2025-09-04T10:48:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    10,
                    48,
                    25,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T10:48:25Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    10,
                    48,
                    25,
                    3,
                    247,
                    0
                ],
                "title": "TriLiteNet: Lightweight Model for Multi-Task Visual Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TriLiteNet: Lightweight Model for Multi-Task Visual Perception"
                },
                "summary": "Efficient perception models are essential for Advanced Driver Assistance\nSystems (ADAS), as these applications require rapid processing and response to\nensure safety and effectiveness in real-world environments. To address the\nreal-time execution needs of such perception models, this study introduces the\nTriLiteNet model. This model can simultaneously manage multiple tasks related\nto panoramic driving perception. TriLiteNet is designed to optimize performance\nwhile maintaining low computational costs. Experimental results on the BDD100k\ndataset demonstrate that the model achieves competitive performance across\nthree key tasks: vehicle detection, drivable area segmentation, and lane line\nsegmentation. Specifically, the TriLiteNet_{base} demonstrated a recall of\n85.6% for vehicle detection, a mean Intersection over Union (mIoU) of 92.4% for\ndrivable area segmentation, and an Acc of 82.3% for lane line segmentation with\nonly 2.35M parameters and a computational cost of 7.72 GFLOPs. Our proposed\nmodel includes a tiny configuration with just 0.14M parameters, which provides\na multi-task solution with minimal computational demand. Evaluated for latency\nand power consumption on embedded devices, TriLiteNet in both configurations\nshows low latency and reasonable power during inference. By balancing\nperformance, computational efficiency, and scalability, TriLiteNet offers a\npractical and deployable solution for real-world autonomous driving\napplications. Code is available at https://github.com/chequanghuy/TriLiteNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient perception models are essential for Advanced Driver Assistance\nSystems (ADAS), as these applications require rapid processing and response to\nensure safety and effectiveness in real-world environments. To address the\nreal-time execution needs of such perception models, this study introduces the\nTriLiteNet model. This model can simultaneously manage multiple tasks related\nto panoramic driving perception. TriLiteNet is designed to optimize performance\nwhile maintaining low computational costs. Experimental results on the BDD100k\ndataset demonstrate that the model achieves competitive performance across\nthree key tasks: vehicle detection, drivable area segmentation, and lane line\nsegmentation. Specifically, the TriLiteNet_{base} demonstrated a recall of\n85.6% for vehicle detection, a mean Intersection over Union (mIoU) of 92.4% for\ndrivable area segmentation, and an Acc of 82.3% for lane line segmentation with\nonly 2.35M parameters and a computational cost of 7.72 GFLOPs. Our proposed\nmodel includes a tiny configuration with just 0.14M parameters, which provides\na multi-task solution with minimal computational demand. Evaluated for latency\nand power consumption on embedded devices, TriLiteNet in both configurations\nshows low latency and reasonable power during inference. By balancing\nperformance, computational efficiency, and scalability, TriLiteNet offers a\npractical and deployable solution for real-world autonomous driving\napplications. Code is available at https://github.com/chequanghuy/TriLiteNet."
                },
                "authors": [
                    {
                        "name": "Quang-Huy Che"
                    },
                    {
                        "name": "Duc-Khai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Duc-Khai Lam"
                },
                "author": "Duc-Khai Lam",
                "arxiv_doi": "10.1109/ACCESS.2025.3552088",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2025.3552088",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.04092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Access 13 (2025) 50152-50166",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04091v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04091v2",
                "updated": "2025-09-05T10:34:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    34,
                    25,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-04T10:48:02Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    10,
                    48,
                    2,
                    3,
                    247,
                    0
                ],
                "title": "Revisiting Third-Party Library Detection: A Ground Truth Dataset and Its\n  Implications Across Security Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Third-Party Library Detection: A Ground Truth Dataset and Its\n  Implications Across Security Tasks"
                },
                "summary": "Accurate detection of third-party libraries (TPLs) is fundamental to Android\nsecurity, supporting vulnerability tracking, malware detection, and supply\nchain auditing. Despite many proposed tools, their real-world effectiveness\nremains unclear. We present the first large-scale empirical study of ten\nstate-of-the-art TPL detection techniques across over 6,000 apps, enabled by a\nnew ground truth dataset with precise version-level annotations for both remote\nand local dependencies. Our evaluation exposes tool fragility to R8-era\ntransformations, weak version discrimination, inaccurate correspondence of\ncandidate libraries, difficulty in generalizing similarity thresholds, and\nprohibitive runtime/memory overheads at scale. Beyond tool assessment, we\nfurther analyze how TPLs shape downstream tasks, including vulnerability\nanalysis, malware detection, secret leakage assessment, and LLM-based\nevaluation. From this perspective, our study provides concrete insights into\nhow TPL characteristics affect these tasks and informs future improvements in\nsecurity analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate detection of third-party libraries (TPLs) is fundamental to Android\nsecurity, supporting vulnerability tracking, malware detection, and supply\nchain auditing. Despite many proposed tools, their real-world effectiveness\nremains unclear. We present the first large-scale empirical study of ten\nstate-of-the-art TPL detection techniques across over 6,000 apps, enabled by a\nnew ground truth dataset with precise version-level annotations for both remote\nand local dependencies. Our evaluation exposes tool fragility to R8-era\ntransformations, weak version discrimination, inaccurate correspondence of\ncandidate libraries, difficulty in generalizing similarity thresholds, and\nprohibitive runtime/memory overheads at scale. Beyond tool assessment, we\nfurther analyze how TPLs shape downstream tasks, including vulnerability\nanalysis, malware detection, secret leakage assessment, and LLM-based\nevaluation. From this perspective, our study provides concrete insights into\nhow TPL characteristics affect these tasks and informs future improvements in\nsecurity analysis."
                },
                "authors": [
                    {
                        "name": "Jintao Gu"
                    },
                    {
                        "name": "Haolang Lu"
                    },
                    {
                        "name": "Guoshun Nan"
                    },
                    {
                        "name": "Yihan Lin"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Yuchun Guo"
                    },
                    {
                        "name": "Yigui Cao"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "20pages, 7figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04091v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04091v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5; D.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04083v1",
                "updated": "2025-09-04T10:25:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    10,
                    25,
                    50,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T10:25:50Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    10,
                    25,
                    50,
                    3,
                    247,
                    0
                ],
                "title": "Intermediate Languages Matter: Formal Languages and LLMs affect\n  Neurosymbolic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intermediate Languages Matter: Formal Languages and LLMs affect\n  Neurosymbolic Reasoning"
                },
                "summary": "Large language models (LLMs) achieve astonishing results on a wide range of\ntasks. However, their formal reasoning ability still lags behind. A promising\napproach is Neurosymbolic LLM reasoning. It works by using LLMs as translators\nfrom natural to formal languages and symbolic solvers for deriving correct\nresults. Still, the contributing factors to the success of Neurosymbolic LLM\nreasoning remain unclear. This paper demonstrates that one previously\noverlooked factor is the choice of the formal language. We introduce the\nintermediate language challenge: selecting a suitable formal language for\nneurosymbolic reasoning. By comparing four formal languages across three\ndatasets and seven LLMs, we show that the choice of formal language affects\nboth syntactic and semantic reasoning capabilities. We also discuss the varying\neffects across different LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve astonishing results on a wide range of\ntasks. However, their formal reasoning ability still lags behind. A promising\napproach is Neurosymbolic LLM reasoning. It works by using LLMs as translators\nfrom natural to formal languages and symbolic solvers for deriving correct\nresults. Still, the contributing factors to the success of Neurosymbolic LLM\nreasoning remain unclear. This paper demonstrates that one previously\noverlooked factor is the choice of the formal language. We introduce the\nintermediate language challenge: selecting a suitable formal language for\nneurosymbolic reasoning. By comparing four formal languages across three\ndatasets and seven LLMs, we show that the choice of formal language affects\nboth syntactic and semantic reasoning capabilities. We also discuss the varying\neffects across different LLMs."
                },
                "authors": [
                    {
                        "name": "Alexander Beiser"
                    },
                    {
                        "name": "David Penz"
                    },
                    {
                        "name": "Nysret Musliu"
                    }
                ],
                "author_detail": {
                    "name": "Nysret Musliu"
                },
                "author": "Nysret Musliu",
                "arxiv_comment": "To appear in the proceedings of The Second Workshop on Knowledge\n  Graphs and Neurosymbolic AI (KG-NeSy) Co-located with SEMANTiCS 2025\n  Conference, Vienna, Austria - September 3rd, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00515v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00515v2",
                "updated": "2025-09-04T10:16:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    10,
                    16,
                    0,
                    3,
                    247,
                    0
                ],
                "published": "2024-11-01T11:20:05Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    11,
                    20,
                    5,
                    4,
                    306,
                    0
                ],
                "title": "Zero-shot Generalization in Inventory Management: Train, then Estimate\n  and Decide",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot Generalization in Inventory Management: Train, then Estimate\n  and Decide"
                },
                "summary": "Deploying deep reinforcement learning (DRL) in real-world inventory\nmanagement presents challenges, including dynamic environments and uncertain\nproblem parameters, e.g. demand and lead time distributions. These challenges\nhighlight a research gap, suggesting a need for a unifying framework to model\nand solve sequential decision-making under parameter uncertainty. We address\nthis by exploring an underexplored area of DRL for inventory management:\ntraining generally capable agents (GCAs) under zero-shot generalization (ZSG).\nHere, GCAs are advanced DRL policies designed to handle a broad range of\nsampled problem instances with diverse inventory challenges. ZSG refers to the\nability to successfully apply learned policies to unseen instances with unknown\nparameters without retraining.\n  We propose a unifying Super-Markov Decision Process formulation and the\nTrain, then Estimate and Decide (TED) framework to train and deploy a GCA\ntailored to inventory management applications. The TED framework consists of\nthree phases: training a GCA on varied problem instances, continuously\nestimating problem parameters during deployment, and making decisions based on\nthese estimates. Applied to periodic review inventory problems with lost sales,\ncyclic demand patterns, and stochastic lead times, our trained agent, the\nGenerally Capable Lost Sales Network (GC-LSN) consistently outperforms\nwell-known traditional policies when problem parameters are known. Moreover,\nunder conditions where demand and/or lead time distributions are initially\nunknown and must be estimated, we benchmark against online learning methods\nthat provide worst-case performance guarantees. Our GC-LSN policy, paired with\nthe Kaplan-Meier estimator, is demonstrated to complement these methods by\nproviding superior empirical performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying deep reinforcement learning (DRL) in real-world inventory\nmanagement presents challenges, including dynamic environments and uncertain\nproblem parameters, e.g. demand and lead time distributions. These challenges\nhighlight a research gap, suggesting a need for a unifying framework to model\nand solve sequential decision-making under parameter uncertainty. We address\nthis by exploring an underexplored area of DRL for inventory management:\ntraining generally capable agents (GCAs) under zero-shot generalization (ZSG).\nHere, GCAs are advanced DRL policies designed to handle a broad range of\nsampled problem instances with diverse inventory challenges. ZSG refers to the\nability to successfully apply learned policies to unseen instances with unknown\nparameters without retraining.\n  We propose a unifying Super-Markov Decision Process formulation and the\nTrain, then Estimate and Decide (TED) framework to train and deploy a GCA\ntailored to inventory management applications. The TED framework consists of\nthree phases: training a GCA on varied problem instances, continuously\nestimating problem parameters during deployment, and making decisions based on\nthese estimates. Applied to periodic review inventory problems with lost sales,\ncyclic demand patterns, and stochastic lead times, our trained agent, the\nGenerally Capable Lost Sales Network (GC-LSN) consistently outperforms\nwell-known traditional policies when problem parameters are known. Moreover,\nunder conditions where demand and/or lead time distributions are initially\nunknown and must be estimated, we benchmark against online learning methods\nthat provide worst-case performance guarantees. Our GC-LSN policy, paired with\nthe Kaplan-Meier estimator, is demonstrated to complement these methods by\nproviding superior empirical performance."
                },
                "authors": [
                    {
                        "name": "Tarkan Temizz"
                    },
                    {
                        "name": "Christina Imdahl"
                    },
                    {
                        "name": "Remco Dijkman"
                    },
                    {
                        "name": "Douniel Lamghari-Idrissi"
                    },
                    {
                        "name": "Willem van Jaarsveld"
                    }
                ],
                "author_detail": {
                    "name": "Willem van Jaarsveld"
                },
                "author": "Willem van Jaarsveld",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00515v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00515v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04078v1",
                "updated": "2025-09-04T10:13:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    10,
                    13,
                    21,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T10:13:21Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    10,
                    13,
                    21,
                    3,
                    247,
                    0
                ],
                "title": "RepoDebug: Repository-Level Multi-Task and Multi-Language Debugging\n  Evaluation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepoDebug: Repository-Level Multi-Task and Multi-Language Debugging\n  Evaluation of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have exhibited significant proficiency in code\ndebugging, especially in automatic program repair, which may substantially\nreduce the time consumption of developers and enhance their efficiency.\nSignificant advancements in debugging datasets have been made to promote the\ndevelopment of code debugging. However, these datasets primarily focus on\nassessing the LLM's function-level code repair capabilities, neglecting the\nmore complex and realistic repository-level scenarios, which leads to an\nincomplete understanding of the LLM's challenges in repository-level debugging.\nWhile several repository-level datasets have been proposed, they often suffer\nfrom limitations such as limited diversity of tasks, languages, and error\ntypes. To mitigate this challenge, this paper introduces RepoDebug, a\nmulti-task and multi-language repository-level code debugging dataset with 22\nsubtypes of errors that supports 8 commonly used programming languages and 3\ndebugging tasks. Furthermore, we conduct evaluation experiments on 10 LLMs,\nwhere Claude 3.5 Sonnect, the best-performing model, still cannot perform well\nin repository-level debugging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited significant proficiency in code\ndebugging, especially in automatic program repair, which may substantially\nreduce the time consumption of developers and enhance their efficiency.\nSignificant advancements in debugging datasets have been made to promote the\ndevelopment of code debugging. However, these datasets primarily focus on\nassessing the LLM's function-level code repair capabilities, neglecting the\nmore complex and realistic repository-level scenarios, which leads to an\nincomplete understanding of the LLM's challenges in repository-level debugging.\nWhile several repository-level datasets have been proposed, they often suffer\nfrom limitations such as limited diversity of tasks, languages, and error\ntypes. To mitigate this challenge, this paper introduces RepoDebug, a\nmulti-task and multi-language repository-level code debugging dataset with 22\nsubtypes of errors that supports 8 commonly used programming languages and 3\ndebugging tasks. Furthermore, we conduct evaluation experiments on 10 LLMs,\nwhere Claude 3.5 Sonnect, the best-performing model, still cannot perform well\nin repository-level debugging."
                },
                "authors": [
                    {
                        "name": "Jingjing Liu"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Zihao Cheng"
                    },
                    {
                        "name": "Mengliang He"
                    },
                    {
                        "name": "Xiaoming Shi"
                    },
                    {
                        "name": "Yuhang Guo"
                    },
                    {
                        "name": "Xiangrong Zhu"
                    },
                    {
                        "name": "Yuanfang Guo"
                    },
                    {
                        "name": "Yunhong Wang"
                    },
                    {
                        "name": "Haifeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haifeng Wang"
                },
                "author": "Haifeng Wang",
                "arxiv_comment": "30 pages, 12 figures, EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04069v1",
                "updated": "2025-09-04T10:02:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    10,
                    2,
                    32,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T10:02:32Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    10,
                    2,
                    32,
                    3,
                    247,
                    0
                ],
                "title": "Solving Robotics Tasks with Prior Demonstration via\n  Exploration-Efficient Deep Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving Robotics Tasks with Prior Demonstration via\n  Exploration-Efficient Deep Reinforcement Learning"
                },
                "summary": "This paper proposes an exploration-efficient Deep Reinforcement Learning with\nReference policy (DRLR) framework for learning robotics tasks that incorporates\ndemonstrations. The DRLR framework is developed based on an algorithm called\nImitation Bootstrapped Reinforcement Learning (IBRL). We propose to improve\nIBRL by modifying the action selection module. The proposed action selection\nmodule provides a calibrated Q-value, which mitigates the bootstrapping error\nthat otherwise leads to inefficient exploration. Furthermore, to prevent the RL\npolicy from converging to a sub-optimal policy, SAC is used as the RL policy\ninstead of TD3. The effectiveness of our method in mitigating bootstrapping\nerror and preventing overfitting is empirically validated by learning two\nrobotics tasks: bucket loading and open drawer, which require extensive\ninteractions with the environment. Simulation results also demonstrate the\nrobustness of the DRLR framework across tasks with both low and high\nstate-action dimensions, and varying demonstration qualities. To evaluate the\ndeveloped framework on a real-world industrial robotics task, the bucket\nloading task is deployed on a real wheel loader. The sim2real results validate\nthe successful deployment of the DRLR framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes an exploration-efficient Deep Reinforcement Learning with\nReference policy (DRLR) framework for learning robotics tasks that incorporates\ndemonstrations. The DRLR framework is developed based on an algorithm called\nImitation Bootstrapped Reinforcement Learning (IBRL). We propose to improve\nIBRL by modifying the action selection module. The proposed action selection\nmodule provides a calibrated Q-value, which mitigates the bootstrapping error\nthat otherwise leads to inefficient exploration. Furthermore, to prevent the RL\npolicy from converging to a sub-optimal policy, SAC is used as the RL policy\ninstead of TD3. The effectiveness of our method in mitigating bootstrapping\nerror and preventing overfitting is empirically validated by learning two\nrobotics tasks: bucket loading and open drawer, which require extensive\ninteractions with the environment. Simulation results also demonstrate the\nrobustness of the DRLR framework across tasks with both low and high\nstate-action dimensions, and varying demonstration qualities. To evaluate the\ndeveloped framework on a real-world industrial robotics task, the bucket\nloading task is deployed on a real wheel loader. The sim2real results validate\nthe successful deployment of the DRLR framework."
                },
                "authors": [
                    {
                        "name": "Chengyandan Shen"
                    },
                    {
                        "name": "Christoffer Sloth"
                    }
                ],
                "author_detail": {
                    "name": "Christoffer Sloth"
                },
                "author": "Christoffer Sloth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04066v1",
                "updated": "2025-09-04T09:55:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    9,
                    55,
                    16,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T09:55:16Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    9,
                    55,
                    16,
                    3,
                    247,
                    0
                ],
                "title": "Arabic Chatbot Technologies in Education: An Overview",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arabic Chatbot Technologies in Education: An Overview"
                },
                "summary": "The recent advancements in Artificial Intelligence (AI) in general, and in\nNatural Language Processing (NLP) in particular, and some of its applications\nsuch as chatbots, have led to their implementation in different domains like\neducation, healthcare, tourism, and customer service. Since the COVID-19\npandemic, there has been an increasing interest in these digital technologies\nto allow and enhance remote access. In education, e-learning systems have been\nmassively adopted worldwide. The emergence of Large Language Models (LLM) such\nas BERT (Bidirectional Encoder Representations from Transformers) and GPT\n(Generative Pre-trained Transformers) made chatbots even more popular. In this\nstudy, we present a survey on existing Arabic chatbots in education and their\ndifferent characteristics such as the adopted approaches, language variety, and\nmetrics used to measure their performance. We were able to identified some\nresearch gaps when we discovered that, despite the success of chatbots in other\nlanguages such as English, only a few educational Arabic chatbots used modern\ntechniques. Finally, we discuss future directions of research in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advancements in Artificial Intelligence (AI) in general, and in\nNatural Language Processing (NLP) in particular, and some of its applications\nsuch as chatbots, have led to their implementation in different domains like\neducation, healthcare, tourism, and customer service. Since the COVID-19\npandemic, there has been an increasing interest in these digital technologies\nto allow and enhance remote access. In education, e-learning systems have been\nmassively adopted worldwide. The emergence of Large Language Models (LLM) such\nas BERT (Bidirectional Encoder Representations from Transformers) and GPT\n(Generative Pre-trained Transformers) made chatbots even more popular. In this\nstudy, we present a survey on existing Arabic chatbots in education and their\ndifferent characteristics such as the adopted approaches, language variety, and\nmetrics used to measure their performance. We were able to identified some\nresearch gaps when we discovered that, despite the success of chatbots in other\nlanguages such as English, only a few educational Arabic chatbots used modern\ntechniques. Finally, we discuss future directions of research in this field."
                },
                "authors": [
                    {
                        "name": "Hicham Bourhil"
                    },
                    {
                        "name": "Yacine El Younoussi"
                    }
                ],
                "author_detail": {
                    "name": "Yacine El Younoussi"
                },
                "author": "Yacine El Younoussi",
                "arxiv_doi": "10.54988/uaj.000027.001",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.54988/uaj.000027.001",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.04066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published as a book chapter in: Transformaci\\'on Digital en la\n  Educaci\\'on: Innovaciones y Desaf\\'ios desde los Campus Virtuales (UA\n  Journals, 2024), pp. 11-14",
                "arxiv_journal_ref": "In: Transformaci\\'on Digital en la Educaci\\'on: Innovaciones y\n  Desaf\\'ios desde los Campus Virtuales. UA Journals, 2024. pp. 11-14",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04059v1",
                "updated": "2025-09-04T09:42:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    9,
                    42,
                    17,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T09:42:17Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    9,
                    42,
                    17,
                    3,
                    247,
                    0
                ],
                "title": "Synthesizing Sheet Music Problems for Evaluation and Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing Sheet Music Problems for Evaluation and Reinforcement\n  Learning"
                },
                "summary": "Enhancing the ability of Large Language Models (LLMs) and Multimodal Large\nLanguage Models (MLLMs) to interpret sheet music is a crucial step toward\nbuilding AI musicians. However, current research lacks both evaluation\nbenchmarks and training data for sheet music reasoning. To address this, we\npropose the idea of synthesizing sheet music problems grounded in music theory,\nwhich can serve both as evaluation benchmarks and as training data for\nreinforcement learning with verifiable rewards (RLVR). We introduce a data\nsynthesis framework that generates verifiable sheet music questions in both\ntextual and visual modalities, leading to the Synthetic Sheet Music Reasoning\nBenchmark (SSMR-Bench) and a complementary training set. Evaluation results on\nSSMR-Bench show the importance of models' reasoning abilities in interpreting\nsheet music. At the same time, the poor performance of Gemini 2.5-Pro\nhighlights the challenges that MLLMs still face in interpreting sheet music in\na visual format. By leveraging synthetic data for RLVR, Qwen3-8B-Base and\nQwen2.5-VL-Instruct achieve improvements on the SSMR-Bench. Besides, the\ntrained Qwen3-8B-Base surpasses GPT-4 in overall performance on\nMusicTheoryBench and achieves reasoning performance comparable to GPT-4 with\nthe strategies of Role play and Chain-of-Thought. Notably, its performance on\nmath problems also improves relative to the original Qwen3-8B-Base.\nFurthermore, our results show that the enhanced reasoning ability can also\nfacilitate music composition. In conclusion, we are the first to propose the\nidea of synthesizing sheet music problems based on music theory rules, and\ndemonstrate its effectiveness not only in advancing model reasoning for sheet\nmusic understanding but also in unlocking new possibilities for AI-assisted\nmusic creation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing the ability of Large Language Models (LLMs) and Multimodal Large\nLanguage Models (MLLMs) to interpret sheet music is a crucial step toward\nbuilding AI musicians. However, current research lacks both evaluation\nbenchmarks and training data for sheet music reasoning. To address this, we\npropose the idea of synthesizing sheet music problems grounded in music theory,\nwhich can serve both as evaluation benchmarks and as training data for\nreinforcement learning with verifiable rewards (RLVR). We introduce a data\nsynthesis framework that generates verifiable sheet music questions in both\ntextual and visual modalities, leading to the Synthetic Sheet Music Reasoning\nBenchmark (SSMR-Bench) and a complementary training set. Evaluation results on\nSSMR-Bench show the importance of models' reasoning abilities in interpreting\nsheet music. At the same time, the poor performance of Gemini 2.5-Pro\nhighlights the challenges that MLLMs still face in interpreting sheet music in\na visual format. By leveraging synthetic data for RLVR, Qwen3-8B-Base and\nQwen2.5-VL-Instruct achieve improvements on the SSMR-Bench. Besides, the\ntrained Qwen3-8B-Base surpasses GPT-4 in overall performance on\nMusicTheoryBench and achieves reasoning performance comparable to GPT-4 with\nthe strategies of Role play and Chain-of-Thought. Notably, its performance on\nmath problems also improves relative to the original Qwen3-8B-Base.\nFurthermore, our results show that the enhanced reasoning ability can also\nfacilitate music composition. In conclusion, we are the first to propose the\nidea of synthesizing sheet music problems based on music theory rules, and\ndemonstrate its effectiveness not only in advancing model reasoning for sheet\nmusic understanding but also in unlocking new possibilities for AI-assisted\nmusic creation."
                },
                "authors": [
                    {
                        "name": "Zhilin Wang"
                    },
                    {
                        "name": "Zhe Yang"
                    },
                    {
                        "name": "Yun Luo"
                    },
                    {
                        "name": "Yafu Li"
                    },
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Runzhe Zhan"
                    },
                    {
                        "name": "Derek F. Wong"
                    },
                    {
                        "name": "Jizhe Zhou"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04058v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04058v1",
                "updated": "2025-09-04T09:41:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    9,
                    41,
                    18,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T09:41:18Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    9,
                    41,
                    18,
                    3,
                    247,
                    0
                ],
                "title": "SMooGPT: Stylized Motion Generation using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMooGPT: Stylized Motion Generation using Large Language Models"
                },
                "summary": "Stylized motion generation is actively studied in computer graphics,\nespecially benefiting from the rapid advances in diffusion models. The goal of\nthis task is to produce a novel motion respecting both the motion content and\nthe desired motion style, e.g., ``walking in a loop like a Monkey''. Existing\nresearch attempts to address this problem via motion style transfer or\nconditional motion generation. They typically embed the motion style into a\nlatent space and guide the motion implicitly in a latent space as well. Despite\nthe progress, their methods suffer from low interpretability and control,\nlimited generalization to new styles, and fail to produce motions other than\n``walking'' due to the strong bias in the public stylization dataset. In this\npaper, we propose to solve the stylized motion generation problem from a new\nperspective of reasoning-composition-generation, based on our observations: i)\nhuman motion can often be effectively described using natural language in a\nbody-part centric manner, ii) LLMs exhibit a strong ability to understand and\nreason about human motion, and iii) human motion has an inherently\ncompositional nature, facilitating the new motion content or style generation\nvia effective recomposing. We thus propose utilizing body-part text space as an\nintermediate representation, and present SMooGPT, a fine-tuned LLM, acting as a\nreasoner, composer, and generator when generating the desired stylized motion.\nOur method executes in the body-part text space with much higher\ninterpretability, enabling fine-grained motion control, effectively resolving\npotential conflicts between motion content and style, and generalizes well to\nnew styles thanks to the open-vocabulary ability of LLMs. Comprehensive\nexperiments and evaluations, and a user perceptual study, demonstrate the\neffectiveness of our approach, especially under the pure text-driven stylized\nmotion generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stylized motion generation is actively studied in computer graphics,\nespecially benefiting from the rapid advances in diffusion models. The goal of\nthis task is to produce a novel motion respecting both the motion content and\nthe desired motion style, e.g., ``walking in a loop like a Monkey''. Existing\nresearch attempts to address this problem via motion style transfer or\nconditional motion generation. They typically embed the motion style into a\nlatent space and guide the motion implicitly in a latent space as well. Despite\nthe progress, their methods suffer from low interpretability and control,\nlimited generalization to new styles, and fail to produce motions other than\n``walking'' due to the strong bias in the public stylization dataset. In this\npaper, we propose to solve the stylized motion generation problem from a new\nperspective of reasoning-composition-generation, based on our observations: i)\nhuman motion can often be effectively described using natural language in a\nbody-part centric manner, ii) LLMs exhibit a strong ability to understand and\nreason about human motion, and iii) human motion has an inherently\ncompositional nature, facilitating the new motion content or style generation\nvia effective recomposing. We thus propose utilizing body-part text space as an\nintermediate representation, and present SMooGPT, a fine-tuned LLM, acting as a\nreasoner, composer, and generator when generating the desired stylized motion.\nOur method executes in the body-part text space with much higher\ninterpretability, enabling fine-grained motion control, effectively resolving\npotential conflicts between motion content and style, and generalizes well to\nnew styles thanks to the open-vocabulary ability of LLMs. Comprehensive\nexperiments and evaluations, and a user perceptual study, demonstrate the\neffectiveness of our approach, especially under the pure text-driven stylized\nmotion generation."
                },
                "authors": [
                    {
                        "name": "Lei Zhong"
                    },
                    {
                        "name": "Yi Yang"
                    },
                    {
                        "name": "Changjian Li"
                    }
                ],
                "author_detail": {
                    "name": "Changjian Li"
                },
                "author": "Changjian Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04058v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04058v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01221v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01221v2",
                "updated": "2025-09-04T09:30:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    9,
                    30,
                    16,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-01T08:06:49Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    8,
                    6,
                    49,
                    0,
                    244,
                    0
                ],
                "title": "DaMoC: Efficiently Selecting the Optimal Large Language Model for\n  Fine-tuning Domain Tasks Based on Data and Model Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DaMoC: Efficiently Selecting the Optimal Large Language Model for\n  Fine-tuning Domain Tasks Based on Data and Model Compression"
                },
                "summary": "Large language models (LLMs) excel in general tasks but struggle with\ndomain-specific ones, requiring fine-tuning with specific data. With many\nopen-source LLMs available, selecting the best model for fine-tuning downstream\ntasks is challenging, primarily focusing on how to quickly identify the optimal\nLLM. We introduce a Data and Model Compression Framework (DaMoC) that addresses\nthis challenge by: 1) Data Level: A systematic categorization of data filtering\nmethodologies for LLMs is first established, classifying them into three\ndistinct paradigms: (1) distribution-aware methods, (2) quality-aware methods,\nand (3) hybrid approaches considering both dimensions. Further, we enhance the\ndensity of key tokens in the text achieving token compression. Subsequently, we\nuse an LLM to iterative rewrite the text to optimize its expression. 2) Model\nLevel: We use layer similarity scores to assess each layer's importance and\nremove those with lower importance. Then, we introduce a sparse merging\nparadigm to preserve as much of the original model's capability as possible.\nExtensive experiments on four datasets, medical Q&A, financial Q&A, general\nQ&A, and reading comprehension, show that we can select the optimal LLM while\nsaving approximately 20-fold in training time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel in general tasks but struggle with\ndomain-specific ones, requiring fine-tuning with specific data. With many\nopen-source LLMs available, selecting the best model for fine-tuning downstream\ntasks is challenging, primarily focusing on how to quickly identify the optimal\nLLM. We introduce a Data and Model Compression Framework (DaMoC) that addresses\nthis challenge by: 1) Data Level: A systematic categorization of data filtering\nmethodologies for LLMs is first established, classifying them into three\ndistinct paradigms: (1) distribution-aware methods, (2) quality-aware methods,\nand (3) hybrid approaches considering both dimensions. Further, we enhance the\ndensity of key tokens in the text achieving token compression. Subsequently, we\nuse an LLM to iterative rewrite the text to optimize its expression. 2) Model\nLevel: We use layer similarity scores to assess each layer's importance and\nremove those with lower importance. Then, we introduce a sparse merging\nparadigm to preserve as much of the original model's capability as possible.\nExtensive experiments on four datasets, medical Q&A, financial Q&A, general\nQ&A, and reading comprehension, show that we can select the optimal LLM while\nsaving approximately 20-fold in training time."
                },
                "authors": [
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Huang Wei"
                    },
                    {
                        "name": "Yinggui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yinggui Wang"
                },
                "author": "Yinggui Wang",
                "arxiv_comment": "Accepted by EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01221v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01221v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03335v2",
                "updated": "2025-09-04T09:25:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    9,
                    25,
                    5,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-03T14:10:56Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    10,
                    56,
                    2,
                    246,
                    0
                ],
                "title": "EvolveSignal: A Large Language Model Powered Coding Agent for\n  Discovering Traffic Signal Control Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvolveSignal: A Large Language Model Powered Coding Agent for\n  Discovering Traffic Signal Control Algorithms"
                },
                "summary": "In traffic engineering, the fixed-time traffic signal control remains widely\nused for its low cost, stability, and interpretability. However, its design\ndepends on hand-crafted formulas (e.g., Webster) and manual re-timing by\nengineers to adapt to demand changes, which is labor-intensive and often yields\nsuboptimal results under heterogeneous or congested conditions. This paper\nintroduces the EvolveSignal, a large language models (LLMs) powered coding\nagent to automatically discover new traffic signal control algorithms. We\nformulate the problem as program synthesis, where candidate algorithms are\nrepresented as Python functions with fixed input-output structures, and\niteratively optimized through external evaluations (e.g., a traffic simulator)\nand evolutionary search. Experiments on a signalized intersection demonstrate\nthat the discovered algorithms outperform Webster's baseline, reducing average\ndelay by 20.1% and average stops by 47.1%. Beyond performance, ablation and\nincremental analyses reveal that EvolveSignal modifications-such as adjusting\ncycle length bounds, incorporating right-turn demand, and rescaling green\nallocations-can offer practically meaningful insights for traffic engineers.\nThis work opens a new research direction by leveraging AI for algorithm design\nin traffic signal control, bridging program synthesis with transportation\nengineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In traffic engineering, the fixed-time traffic signal control remains widely\nused for its low cost, stability, and interpretability. However, its design\ndepends on hand-crafted formulas (e.g., Webster) and manual re-timing by\nengineers to adapt to demand changes, which is labor-intensive and often yields\nsuboptimal results under heterogeneous or congested conditions. This paper\nintroduces the EvolveSignal, a large language models (LLMs) powered coding\nagent to automatically discover new traffic signal control algorithms. We\nformulate the problem as program synthesis, where candidate algorithms are\nrepresented as Python functions with fixed input-output structures, and\niteratively optimized through external evaluations (e.g., a traffic simulator)\nand evolutionary search. Experiments on a signalized intersection demonstrate\nthat the discovered algorithms outperform Webster's baseline, reducing average\ndelay by 20.1% and average stops by 47.1%. Beyond performance, ablation and\nincremental analyses reveal that EvolveSignal modifications-such as adjusting\ncycle length bounds, incorporating right-turn demand, and rescaling green\nallocations-can offer practically meaningful insights for traffic engineers.\nThis work opens a new research direction by leveraging AI for algorithm design\nin traffic signal control, bridging program synthesis with transportation\nengineering."
                },
                "authors": [
                    {
                        "name": "Leizhen Wang"
                    },
                    {
                        "name": "Peibo Duan"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Jian Xu"
                    },
                    {
                        "name": "Nan Zheng"
                    },
                    {
                        "name": "Zhenliang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zhenliang Ma"
                },
                "author": "Zhenliang Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20038v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20038v3",
                "updated": "2025-09-04T09:23:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    9,
                    23,
                    46,
                    3,
                    247,
                    0
                ],
                "published": "2025-08-27T16:44:03Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    16,
                    44,
                    3,
                    2,
                    239,
                    0
                ],
                "title": "Forewarned is Forearmed: Pre-Synthesizing Jailbreak-like Instructions to\n  Enhance LLM Safety Guardrail to Potential Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forewarned is Forearmed: Pre-Synthesizing Jailbreak-like Instructions to\n  Enhance LLM Safety Guardrail to Potential Attacks"
                },
                "summary": "Despite advances in improving large language model (LLM) to refuse to answer\nmalicious instructions, widely used LLMs remain vulnerable to jailbreak attacks\nwhere attackers generate instructions with distributions differing from safety\nalignment corpora. New attacks expose LLMs' inability to recognize unseen\nmalicious instructions, highlighting a critical distributional mismatch between\ntraining data and real-world attacks that forces developers into reactive\npatching cycles. To tackle this challenge, we propose IMAGINE, a synthesis\nframework that leverages embedding space distribution analysis to generate\njailbreak-like instructions. This approach effectively fills the distributional\ngap between authentic jailbreak patterns and safety alignment corpora. IMAGINE\nfollows an iterative optimization process that dynamically evolves text\ngeneration distributions across iterations, thereby augmenting the coverage of\nsafety alignment data distributions through synthesized data examples. Based on\nthe safety-aligned corpus enhanced through IMAGINE, our framework demonstrates\nsignificant decreases in attack success rate on Qwen2.5, Llama3.1, and Llama3.2\nwithout compromising their utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advances in improving large language model (LLM) to refuse to answer\nmalicious instructions, widely used LLMs remain vulnerable to jailbreak attacks\nwhere attackers generate instructions with distributions differing from safety\nalignment corpora. New attacks expose LLMs' inability to recognize unseen\nmalicious instructions, highlighting a critical distributional mismatch between\ntraining data and real-world attacks that forces developers into reactive\npatching cycles. To tackle this challenge, we propose IMAGINE, a synthesis\nframework that leverages embedding space distribution analysis to generate\njailbreak-like instructions. This approach effectively fills the distributional\ngap between authentic jailbreak patterns and safety alignment corpora. IMAGINE\nfollows an iterative optimization process that dynamically evolves text\ngeneration distributions across iterations, thereby augmenting the coverage of\nsafety alignment data distributions through synthesized data examples. Based on\nthe safety-aligned corpus enhanced through IMAGINE, our framework demonstrates\nsignificant decreases in attack success rate on Qwen2.5, Llama3.1, and Llama3.2\nwithout compromising their utility."
                },
                "authors": [
                    {
                        "name": "Sheng Liu"
                    },
                    {
                        "name": "Qiang Sheng"
                    },
                    {
                        "name": "Danding Wang"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Juan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Juan Cao"
                },
                "author": "Juan Cao",
                "arxiv_comment": "EMNLP 2025 findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20038v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20038v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06052v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06052v2",
                "updated": "2025-09-04T09:10:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    9,
                    10,
                    5,
                    3,
                    247,
                    0
                ],
                "published": "2025-06-06T12:56:02Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    12,
                    56,
                    2,
                    4,
                    157,
                    0
                ],
                "title": "CP-Bench: Evaluating Large Language Models for Constraint Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CP-Bench: Evaluating Large Language Models for Constraint Modelling"
                },
                "summary": "Constraint Programming (CP) is widely used to solve combinatorial problems,\nbut its core process, namely constraint modelling, requires significant\nexpertise and is considered to be a bottleneck for wider adoption. Aiming to\nalleviate this bottleneck, recent studies have explored using Large Language\nModels (LLMs) to transform combinatorial problem descriptions into executable\nconstraint models. However, the existing evaluation datasets for constraint\nmodelling are often limited to small, homogeneous, or domain-specific\ninstances, which do not capture the diversity of real-world scenarios. This\nwork addresses this gap by introducing CP-Bench, a novel benchmark that\nincludes a diverse set of well-known combinatorial problems sourced from the CP\ncommunity, structured explicitly for evaluating LLM-driven CP modelling. With\nthis dataset, and given the variety of constraint modelling frameworks, we\ncompare and evaluate the modelling capabilities of LLMs for three distinct\nconstraint modelling systems, which vary in abstraction level and underlying\nsyntax. Notably, the results show higher performance when modelling with a\nhigh-level Python-based framework. Additionally, we systematically evaluate the\nuse of prompt-based and inference-time compute methods across different LLMs,\nwhich further increase accuracy, reaching up to 70% on this highly challenging\nbenchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraint Programming (CP) is widely used to solve combinatorial problems,\nbut its core process, namely constraint modelling, requires significant\nexpertise and is considered to be a bottleneck for wider adoption. Aiming to\nalleviate this bottleneck, recent studies have explored using Large Language\nModels (LLMs) to transform combinatorial problem descriptions into executable\nconstraint models. However, the existing evaluation datasets for constraint\nmodelling are often limited to small, homogeneous, or domain-specific\ninstances, which do not capture the diversity of real-world scenarios. This\nwork addresses this gap by introducing CP-Bench, a novel benchmark that\nincludes a diverse set of well-known combinatorial problems sourced from the CP\ncommunity, structured explicitly for evaluating LLM-driven CP modelling. With\nthis dataset, and given the variety of constraint modelling frameworks, we\ncompare and evaluate the modelling capabilities of LLMs for three distinct\nconstraint modelling systems, which vary in abstraction level and underlying\nsyntax. Notably, the results show higher performance when modelling with a\nhigh-level Python-based framework. Additionally, we systematically evaluate the\nuse of prompt-based and inference-time compute methods across different LLMs,\nwhich further increase accuracy, reaching up to 70% on this highly challenging\nbenchmark."
                },
                "authors": [
                    {
                        "name": "Kostis Michailidis"
                    },
                    {
                        "name": "Dimos Tsouros"
                    },
                    {
                        "name": "Tias Guns"
                    }
                ],
                "author_detail": {
                    "name": "Tias Guns"
                },
                "author": "Tias Guns",
                "arxiv_comment": "ECAI 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06052v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06052v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19740v2",
                "updated": "2025-09-04T09:08:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    9,
                    8,
                    29,
                    3,
                    247,
                    0
                ],
                "published": "2025-08-27T10:11:27Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    11,
                    27,
                    2,
                    239,
                    0
                ],
                "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval"
                },
                "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04027v1",
                "updated": "2025-09-04T09:02:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    9,
                    2,
                    16,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T09:02:16Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    9,
                    2,
                    16,
                    3,
                    247,
                    0
                ],
                "title": "CoT-Space: A Theoretical Framework for Internal Slow-Thinking via\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoT-Space: A Theoretical Framework for Internal Slow-Thinking via\n  Reinforcement Learning"
                },
                "summary": "Reinforcement Learning (RL) has become a pivotal approach for enhancing the\nreasoning capabilities of Large Language Models (LLMs). However, a significant\ntheoretical gap persists, as traditional token-level RL frameworks fail to\nalign with the reasoning-level nature of complex, multi-step thought processes\nlike Chain-of-Thought (CoT). To address this challenge, we introduce CoT-Space,\na novel theoretical framework that recasts LLM reasoning from a discrete\ntoken-prediction task to an optimization process within a continuous,\nreasoning-level semantic space. By analyzing this process from both a noise\nperspective and a risk perspective, we demonstrate that the convergence to an\noptimal CoT length is a natural consequence of the fundamental trade-off\nbetween underfitting and overfitting. Furthermore, extensive experiments\nprovide strong empirical validation for our theoretical findings. Our framework\nnot only provides a coherent explanation for empirical phenomena such as\noverthinking but also offers a solid theoretical foundation to guide the future\ndevelopment of more effective and generalizable reasoning agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) has become a pivotal approach for enhancing the\nreasoning capabilities of Large Language Models (LLMs). However, a significant\ntheoretical gap persists, as traditional token-level RL frameworks fail to\nalign with the reasoning-level nature of complex, multi-step thought processes\nlike Chain-of-Thought (CoT). To address this challenge, we introduce CoT-Space,\na novel theoretical framework that recasts LLM reasoning from a discrete\ntoken-prediction task to an optimization process within a continuous,\nreasoning-level semantic space. By analyzing this process from both a noise\nperspective and a risk perspective, we demonstrate that the convergence to an\noptimal CoT length is a natural consequence of the fundamental trade-off\nbetween underfitting and overfitting. Furthermore, extensive experiments\nprovide strong empirical validation for our theoretical findings. Our framework\nnot only provides a coherent explanation for empirical phenomena such as\noverthinking but also offers a solid theoretical foundation to guide the future\ndevelopment of more effective and generalizable reasoning agents."
                },
                "authors": [
                    {
                        "name": "Zeyu Gan"
                    },
                    {
                        "name": "Hao Yi"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "Preprint Edition",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04018v1",
                "updated": "2025-09-04T08:47:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    47,
                    26,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T08:47:26Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    47,
                    26,
                    3,
                    247,
                    0
                ],
                "title": "FPC-VLA: A Vision-Language-Action Framework with a Supervisor for\n  Failure Prediction and Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FPC-VLA: A Vision-Language-Action Framework with a Supervisor for\n  Failure Prediction and Correction"
                },
                "summary": "Robotic manipulation is a fundamental component of automation. However,\ntraditional perception-planning pipelines often fall short in open-ended tasks\ndue to limited flexibility, while the architecture of a single end-to-end\nVision-Language-Action (VLA) offers promising capabilities but lacks crucial\nmechanisms for anticipating and recovering from failure. To address these\nchallenges, we propose FPC-VLA, a dual-model framework that integrates VLA with\na supervisor for failure prediction and correction. The supervisor evaluates\naction viability through vision-language queries and generates corrective\nstrategies when risks arise, trained efficiently without manual labeling. A\nsimilarity-guided fusion module further refines actions by leveraging past\npredictions. Evaluation results on multiple simulation platforms (SIMPLER and\nLIBERO) and robot embodiments (WidowX, Google Robot, Franka) show that FPC-VLA\noutperforms state-of-the-art models in both zero-shot and fine-tuned settings.\nBy activating the supervisor only at keyframes, our approach significantly\nincreases task success rates with minimal impact on execution time. Successful\nreal-world deployments on diverse, long-horizon tasks confirm FPC-VLA's strong\ngeneralization and practical utility for building more reliable autonomous\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic manipulation is a fundamental component of automation. However,\ntraditional perception-planning pipelines often fall short in open-ended tasks\ndue to limited flexibility, while the architecture of a single end-to-end\nVision-Language-Action (VLA) offers promising capabilities but lacks crucial\nmechanisms for anticipating and recovering from failure. To address these\nchallenges, we propose FPC-VLA, a dual-model framework that integrates VLA with\na supervisor for failure prediction and correction. The supervisor evaluates\naction viability through vision-language queries and generates corrective\nstrategies when risks arise, trained efficiently without manual labeling. A\nsimilarity-guided fusion module further refines actions by leveraging past\npredictions. Evaluation results on multiple simulation platforms (SIMPLER and\nLIBERO) and robot embodiments (WidowX, Google Robot, Franka) show that FPC-VLA\noutperforms state-of-the-art models in both zero-shot and fine-tuned settings.\nBy activating the supervisor only at keyframes, our approach significantly\nincreases task success rates with minimal impact on execution time. Successful\nreal-world deployments on diverse, long-horizon tasks confirm FPC-VLA's strong\ngeneralization and practical utility for building more reliable autonomous\nsystems."
                },
                "authors": [
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Zhixiang Duan"
                    },
                    {
                        "name": "Tianshi Xie"
                    },
                    {
                        "name": "Fuyu Cao"
                    },
                    {
                        "name": "Pinxi Shen"
                    },
                    {
                        "name": "Peili Song"
                    },
                    {
                        "name": "Piaopiao Jin"
                    },
                    {
                        "name": "Guokang Sun"
                    },
                    {
                        "name": "Shaoqing Xu"
                    },
                    {
                        "name": "Yangwei You"
                    },
                    {
                        "name": "Jingtai Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jingtai Liu"
                },
                "author": "Jingtai Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04013v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04013v1",
                "updated": "2025-09-04T08:43:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    43,
                    27,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T08:43:27Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    43,
                    27,
                    3,
                    247,
                    0
                ],
                "title": "On Robustness and Reliability of Benchmark-Based Evaluation of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Robustness and Reliability of Benchmark-Based Evaluation of LLMs"
                },
                "summary": "Large Language Models (LLMs) effectiveness is usually evaluated by means of\nbenchmarks such as MMLU, ARC-C, or HellaSwag, where questions are presented in\ntheir original wording, thus in a fixed, standardized format. However,\nreal-world applications involve linguistic variability, requiring models to\nmaintain their effectiveness across diverse rewordings of the same question or\nquery. In this study, we systematically assess the robustness of LLMs to\nparaphrased benchmark questions and investigate whether benchmark-based\nevaluations provide a reliable measure of model capabilities. We systematically\ngenerate various paraphrases of all the questions across six different common\nbenchmarks, and measure the resulting variations in effectiveness of 34\nstate-of-the-art LLMs, of different size and effectiveness. Our findings reveal\nthat while LLM rankings remain relatively stable across paraphrased inputs,\nabsolute effectiveness scores change, and decline significantly. This suggests\nthat LLMs struggle with linguistic variability, raising concerns about their\ngeneralization abilities and evaluation methodologies. Furthermore, the\nobserved performance drop challenges the reliability of benchmark-based\nevaluations, indicating that high benchmark scores may not fully capture a\nmodel's robustness to real-world input variations. We discuss the implications\nof these findings for LLM evaluation methodologies, emphasizing the need for\nrobustness-aware benchmarks that better reflect practical deployment scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) effectiveness is usually evaluated by means of\nbenchmarks such as MMLU, ARC-C, or HellaSwag, where questions are presented in\ntheir original wording, thus in a fixed, standardized format. However,\nreal-world applications involve linguistic variability, requiring models to\nmaintain their effectiveness across diverse rewordings of the same question or\nquery. In this study, we systematically assess the robustness of LLMs to\nparaphrased benchmark questions and investigate whether benchmark-based\nevaluations provide a reliable measure of model capabilities. We systematically\ngenerate various paraphrases of all the questions across six different common\nbenchmarks, and measure the resulting variations in effectiveness of 34\nstate-of-the-art LLMs, of different size and effectiveness. Our findings reveal\nthat while LLM rankings remain relatively stable across paraphrased inputs,\nabsolute effectiveness scores change, and decline significantly. This suggests\nthat LLMs struggle with linguistic variability, raising concerns about their\ngeneralization abilities and evaluation methodologies. Furthermore, the\nobserved performance drop challenges the reliability of benchmark-based\nevaluations, indicating that high benchmark scores may not fully capture a\nmodel's robustness to real-world input variations. We discuss the implications\nof these findings for LLM evaluation methodologies, emphasizing the need for\nrobustness-aware benchmarks that better reflect practical deployment scenarios."
                },
                "authors": [
                    {
                        "name": "Riccardo Lunardi"
                    },
                    {
                        "name": "Vincenzo Della Mea"
                    },
                    {
                        "name": "Stefano Mizzaro"
                    },
                    {
                        "name": "Kevin Roitero"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Roitero"
                },
                "author": "Kevin Roitero",
                "arxiv_comment": "Accepted at ECAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04013v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04013v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04011v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04011v1",
                "updated": "2025-09-04T08:42:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    42,
                    23,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T08:42:23Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    42,
                    23,
                    3,
                    247,
                    0
                ],
                "title": "NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware\n  Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware\n  Embeddings"
                },
                "summary": "We present NER Retriever, a zero-shot retrieval framework for ad-hoc Named\nEntity Retrieval, a variant of Named Entity Recognition (NER), where the types\nof interest are not provided in advance, and a user-defined type description is\nused to retrieve documents mentioning entities of that type. Instead of relying\non fixed schemas or fine-tuned models, our method builds on internal\nrepresentations of large language models (LLMs) to embed both entity mentions\nand user-provided open-ended type descriptions into a shared semantic space. We\nshow that internal representations, specifically the value vectors from\nmid-layer transformer blocks, encode fine-grained type information more\neffectively than commonly used top-layer embeddings. To refine these\nrepresentations, we train a lightweight contrastive projection network that\naligns type-compatible entities while separating unrelated types. The resulting\nentity embeddings are compact, type-aware, and well-suited for nearest-neighbor\nsearch. Evaluated on three benchmarks, NER Retriever significantly outperforms\nboth lexical and dense sentence-level retrieval baselines. Our findings provide\nempirical support for representation selection within LLMs and demonstrate a\npractical solution for scalable, schema-free entity retrieval. The NER\nRetriever Codebase is publicly available at\nhttps://github.com/ShacharOr100/ner_retriever",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present NER Retriever, a zero-shot retrieval framework for ad-hoc Named\nEntity Retrieval, a variant of Named Entity Recognition (NER), where the types\nof interest are not provided in advance, and a user-defined type description is\nused to retrieve documents mentioning entities of that type. Instead of relying\non fixed schemas or fine-tuned models, our method builds on internal\nrepresentations of large language models (LLMs) to embed both entity mentions\nand user-provided open-ended type descriptions into a shared semantic space. We\nshow that internal representations, specifically the value vectors from\nmid-layer transformer blocks, encode fine-grained type information more\neffectively than commonly used top-layer embeddings. To refine these\nrepresentations, we train a lightweight contrastive projection network that\naligns type-compatible entities while separating unrelated types. The resulting\nentity embeddings are compact, type-aware, and well-suited for nearest-neighbor\nsearch. Evaluated on three benchmarks, NER Retriever significantly outperforms\nboth lexical and dense sentence-level retrieval baselines. Our findings provide\nempirical support for representation selection within LLMs and demonstrate a\npractical solution for scalable, schema-free entity retrieval. The NER\nRetriever Codebase is publicly available at\nhttps://github.com/ShacharOr100/ner_retriever"
                },
                "authors": [
                    {
                        "name": "Or Shachar"
                    },
                    {
                        "name": "Uri Katz"
                    },
                    {
                        "name": "Yoav Goldberg"
                    },
                    {
                        "name": "Oren Glickman"
                    }
                ],
                "author_detail": {
                    "name": "Oren Glickman"
                },
                "author": "Oren Glickman",
                "arxiv_comment": "Findings of EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04011v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04007v1",
                "updated": "2025-09-04T08:38:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    38,
                    42,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T08:38:42Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    38,
                    42,
                    3,
                    247,
                    0
                ],
                "title": "AutoPBO: LLM-powered Optimization for Local Search PBO Solvers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoPBO: LLM-powered Optimization for Local Search PBO Solvers"
                },
                "summary": "Pseudo-Boolean Optimization (PBO) provides a powerful framework for modeling\ncombinatorial problems through pseudo-Boolean (PB) constraints. Local search\nsolvers have shown excellent performance in PBO solving, and their efficiency\nis highly dependent on their internal heuristics to guide the search. Still,\ntheir design often requires significant expert effort and manual tuning in\npractice. While Large Language Models (LLMs) have demonstrated potential in\nautomating algorithm design, their application to optimizing PBO solvers\nremains unexplored. In this work, we introduce AutoPBO, a novel LLM-powered\nframework to automatically enhance PBO local search solvers. We conduct\nexperiments on a broad range of four public benchmarks, including one\nreal-world benchmark, a benchmark from PB competition, an integer linear\nprogramming optimization benchmark, and a crafted combinatorial benchmark, to\nevaluate the performance improvement achieved by AutoPBO and compare it with\nsix state-of-the-art competitors, including two local search PBO solvers NuPBO\nand OraSLS, two complete PB solvers PBO-IHS and RoundingSat, and two mixed\ninteger programming (MIP) solvers Gurobi and SCIP. AutoPBO demonstrates\nsignificant improvements over previous local search approaches, while\nmaintaining competitive performance compared to state-of-the-art competitors.\nThe results suggest that AutoPBO offers a promising approach to automating\nlocal search solver design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pseudo-Boolean Optimization (PBO) provides a powerful framework for modeling\ncombinatorial problems through pseudo-Boolean (PB) constraints. Local search\nsolvers have shown excellent performance in PBO solving, and their efficiency\nis highly dependent on their internal heuristics to guide the search. Still,\ntheir design often requires significant expert effort and manual tuning in\npractice. While Large Language Models (LLMs) have demonstrated potential in\nautomating algorithm design, their application to optimizing PBO solvers\nremains unexplored. In this work, we introduce AutoPBO, a novel LLM-powered\nframework to automatically enhance PBO local search solvers. We conduct\nexperiments on a broad range of four public benchmarks, including one\nreal-world benchmark, a benchmark from PB competition, an integer linear\nprogramming optimization benchmark, and a crafted combinatorial benchmark, to\nevaluate the performance improvement achieved by AutoPBO and compare it with\nsix state-of-the-art competitors, including two local search PBO solvers NuPBO\nand OraSLS, two complete PB solvers PBO-IHS and RoundingSat, and two mixed\ninteger programming (MIP) solvers Gurobi and SCIP. AutoPBO demonstrates\nsignificant improvements over previous local search approaches, while\nmaintaining competitive performance compared to state-of-the-art competitors.\nThe results suggest that AutoPBO offers a promising approach to automating\nlocal search solver design."
                },
                "authors": [
                    {
                        "name": "Jinyuan Li"
                    },
                    {
                        "name": "Yi Chu"
                    },
                    {
                        "name": "Yiwen Sun"
                    },
                    {
                        "name": "Mengchuan Zou"
                    },
                    {
                        "name": "Shaowei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Shaowei Cai"
                },
                "author": "Shaowei Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02245v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02245v2",
                "updated": "2025-09-04T08:26:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    26,
                    18,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-02T12:14:59Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    12,
                    14,
                    59,
                    1,
                    245,
                    0
                ],
                "title": "Spectral characterization and performance of SPT-SLIM on-chip filterbank\n  spectrometers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectral characterization and performance of SPT-SLIM on-chip filterbank\n  spectrometers"
                },
                "summary": "The South Pole Telescope Shirokoff Line Intensity Mapper (SPT-SLIM)\nexperiment is a pathfinder for demonstrating the use of on-chip spectrometers\nfor millimeter Line Intensity Mapping. We present spectral bandpass\nmeasurements of the SLIM spectrometer channels made on site using a Fourier\nTransform Spectrometer during SPT-SLIMs first deployment the 2024-2025 austral\nsummer observing season. Through this we demonstrate a technique for measuring\nthe narrow band passes of the SPT-SLIM filterbanks that improves beyond the\nintrinsic resolution of a Fourier Transform Spectrometer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The South Pole Telescope Shirokoff Line Intensity Mapper (SPT-SLIM)\nexperiment is a pathfinder for demonstrating the use of on-chip spectrometers\nfor millimeter Line Intensity Mapping. We present spectral bandpass\nmeasurements of the SLIM spectrometer channels made on site using a Fourier\nTransform Spectrometer during SPT-SLIMs first deployment the 2024-2025 austral\nsummer observing season. Through this we demonstrate a technique for measuring\nthe narrow band passes of the SPT-SLIM filterbanks that improves beyond the\nintrinsic resolution of a Fourier Transform Spectrometer."
                },
                "authors": [
                    {
                        "name": "C. S. Benson"
                    },
                    {
                        "name": "K. Fichman"
                    },
                    {
                        "name": "M. Adamic"
                    },
                    {
                        "name": "A. J. Anderson"
                    },
                    {
                        "name": "P. Barry"
                    },
                    {
                        "name": "B. A. Benson"
                    },
                    {
                        "name": "E. Brooks"
                    },
                    {
                        "name": "J. E. Carlstrom"
                    },
                    {
                        "name": "T. Cecil"
                    },
                    {
                        "name": "C. L. Chang"
                    },
                    {
                        "name": "K. R. Dibert"
                    },
                    {
                        "name": "M. Dobbs"
                    },
                    {
                        "name": "K. S. Karkare"
                    },
                    {
                        "name": "G. K. Keating"
                    },
                    {
                        "name": "A. M. Lapuente"
                    },
                    {
                        "name": "M. Lisovenko"
                    },
                    {
                        "name": "D. P. Marrone"
                    },
                    {
                        "name": "J. Montgomery"
                    },
                    {
                        "name": "T. Natoli"
                    },
                    {
                        "name": "Z. Pan"
                    },
                    {
                        "name": "A. Rahlin"
                    },
                    {
                        "name": "G. Robson"
                    },
                    {
                        "name": "M. Rouble"
                    },
                    {
                        "name": "G. Smecher"
                    },
                    {
                        "name": "V. Yefremenko"
                    },
                    {
                        "name": "M. R. Young"
                    },
                    {
                        "name": "C. Yu"
                    },
                    {
                        "name": "J. A. Zebrowski"
                    },
                    {
                        "name": "C. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "C. Zhang"
                },
                "author": "C. Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02245v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02245v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03995v1",
                "updated": "2025-09-04T08:25:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    25,
                    1,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T08:25:01Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    25,
                    1,
                    3,
                    247,
                    0
                ],
                "title": "RTQA : Recursive Thinking for Complex Temporal Knowledge Graph Question\n  Answering with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RTQA : Recursive Thinking for Complex Temporal Knowledge Graph Question\n  Answering with Large Language Models"
                },
                "summary": "Current temporal knowledge graph question answering (TKGQA) methods primarily\nfocus on implicit temporal constraints, lacking the capability of handling more\ncomplex temporal queries, and struggle with limited reasoning abilities and\nerror propagation in decomposition frameworks. We propose RTQA, a novel\nframework to address these challenges by enhancing reasoning over TKGs without\nrequiring training. Following recursive thinking, RTQA recursively decomposes\nquestions into sub-problems, solves them bottom-up using LLMs and TKG\nknowledge, and employs multi-path answer aggregation to improve fault\ntolerance. RTQA consists of three core components: the Temporal Question\nDecomposer, the Recursive Solver, and the Answer Aggregator. Experiments on\nMultiTQ and TimelineKGQA benchmarks demonstrate significant Hits@1 improvements\nin \"Multiple\" and \"Complex\" categories, outperforming state-of-the-art methods.\nOur code and data are available at https://github.com/zjukg/RTQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current temporal knowledge graph question answering (TKGQA) methods primarily\nfocus on implicit temporal constraints, lacking the capability of handling more\ncomplex temporal queries, and struggle with limited reasoning abilities and\nerror propagation in decomposition frameworks. We propose RTQA, a novel\nframework to address these challenges by enhancing reasoning over TKGs without\nrequiring training. Following recursive thinking, RTQA recursively decomposes\nquestions into sub-problems, solves them bottom-up using LLMs and TKG\nknowledge, and employs multi-path answer aggregation to improve fault\ntolerance. RTQA consists of three core components: the Temporal Question\nDecomposer, the Recursive Solver, and the Answer Aggregator. Experiments on\nMultiTQ and TimelineKGQA benchmarks demonstrate significant Hits@1 improvements\nin \"Multiple\" and \"Complex\" categories, outperforming state-of-the-art methods.\nOur code and data are available at https://github.com/zjukg/RTQA."
                },
                "authors": [
                    {
                        "name": "Zhaoyan Gong"
                    },
                    {
                        "name": "Juan Li"
                    },
                    {
                        "name": "Zhiqiang Liu"
                    },
                    {
                        "name": "Lei Liang"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Wen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wen Zhang"
                },
                "author": "Wen Zhang",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20321v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20321v2",
                "updated": "2025-09-04T08:23:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    23,
                    37,
                    3,
                    247,
                    0
                ],
                "published": "2024-05-30T17:56:54Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    17,
                    56,
                    54,
                    3,
                    151,
                    0
                ],
                "title": "Vision-based Manipulation from Single Human Video with Open-World Object\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-based Manipulation from Single Human Video with Open-World Object\n  Graphs"
                },
                "summary": "This work presents an object-centric approach to learning vision-based\nmanipulation skills from human videos. We investigate the problem of robot\nmanipulation via imitation in the open-world setting, where a robot learns to\nmanipulate novel objects from a single video demonstration. We introduce ORION,\nan algorithm that tackles the problem by extracting an object-centric\nmanipulation plan from a single RGB or RGB-D video and deriving a policy that\nconditions on the extracted plan. Our method enables the robot to learn from\nvideos captured by daily mobile devices and to generalize the policies to\ndeployment environments with varying visual backgrounds, camera angles, spatial\nlayouts, and novel object instances. We systematically evaluate our method on\nboth short-horizon and long-horizon tasks, using RGB-D and RGB-only\ndemonstration videos. Across varied tasks and demonstration types (RGB-D /\nRGB), we observe an average success rate of 74.4%, demonstrating the efficacy\nof ORION in learning from a single human video in the open world. Additional\nmaterials can be found on our project website:\nhttps://ut-austin-rpl.github.io/ORION-release.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents an object-centric approach to learning vision-based\nmanipulation skills from human videos. We investigate the problem of robot\nmanipulation via imitation in the open-world setting, where a robot learns to\nmanipulate novel objects from a single video demonstration. We introduce ORION,\nan algorithm that tackles the problem by extracting an object-centric\nmanipulation plan from a single RGB or RGB-D video and deriving a policy that\nconditions on the extracted plan. Our method enables the robot to learn from\nvideos captured by daily mobile devices and to generalize the policies to\ndeployment environments with varying visual backgrounds, camera angles, spatial\nlayouts, and novel object instances. We systematically evaluate our method on\nboth short-horizon and long-horizon tasks, using RGB-D and RGB-only\ndemonstration videos. Across varied tasks and demonstration types (RGB-D /\nRGB), we observe an average success rate of 74.4%, demonstrating the efficacy\nof ORION in learning from a single human video in the open world. Additional\nmaterials can be found on our project website:\nhttps://ut-austin-rpl.github.io/ORION-release."
                },
                "authors": [
                    {
                        "name": "Yifeng Zhu"
                    },
                    {
                        "name": "Arisrei Lim"
                    },
                    {
                        "name": "Peter Stone"
                    },
                    {
                        "name": "Yuke Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Zhu"
                },
                "author": "Yuke Zhu",
                "arxiv_comment": "Extended version of paper adding results with RGB-only demonstration\n  videos uploaded on 09/04/2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20321v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20321v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03990v1",
                "updated": "2025-09-04T08:18:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    18,
                    39,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T08:18:39Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    18,
                    39,
                    3,
                    247,
                    0
                ],
                "title": "Meta-Policy Reflexion: Reusable Reflective Memory and Rule Admissibility\n  for Resource-Efficient LLM Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-Policy Reflexion: Reusable Reflective Memory and Rule Admissibility\n  for Resource-Efficient LLM Agent"
                },
                "summary": "Large language model (LLM) agents achieve impressive single-task performance\nbut commonly exhibit repeated failures, inefficient exploration, and limited\ncross-task adaptability. Existing reflective strategies (e.g., Reflexion,\nReAct) improve per-episode behavior but typically produce ephemeral,\ntask-specific traces that are not reused across tasks. Reinforcement-learning\nbased alternatives can produce transferable policies but require substantial\nparameter updates and compute. In this work we introduce Meta-Policy Reflexion\n(MPR): a hybrid framework that consolidates LLM-generated reflections into a\nstructured, predicate-like Meta-Policy Memory (MPM) and applies that memory at\ninference time through two complementary mechanisms soft memory-guided decoding\nand hard rule admissibility checks(HAC). MPR (i) externalizes reusable\ncorrective knowledge without model weight updates, (ii) enforces domain\nconstraints to reduce unsafe or invalid actions, and (iii) retains the\nadaptability of language-based reflection. We formalize the MPM representation,\npresent algorithms for update and decoding, and validate the approach in a\ntext-based agent environment following the experimental protocol described in\nthe provided implementation (AlfWorld-based). Empirical results reported in the\nsupplied material indicate consistent gains in execution accuracy and\nrobustness when compared to Reflexion baselines; rule admissibility further\nimproves stability. We analyze mechanisms that explain these gains, discuss\nscalability and failure modes, and outline future directions for multimodal and\nmulti?agent extensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) agents achieve impressive single-task performance\nbut commonly exhibit repeated failures, inefficient exploration, and limited\ncross-task adaptability. Existing reflective strategies (e.g., Reflexion,\nReAct) improve per-episode behavior but typically produce ephemeral,\ntask-specific traces that are not reused across tasks. Reinforcement-learning\nbased alternatives can produce transferable policies but require substantial\nparameter updates and compute. In this work we introduce Meta-Policy Reflexion\n(MPR): a hybrid framework that consolidates LLM-generated reflections into a\nstructured, predicate-like Meta-Policy Memory (MPM) and applies that memory at\ninference time through two complementary mechanisms soft memory-guided decoding\nand hard rule admissibility checks(HAC). MPR (i) externalizes reusable\ncorrective knowledge without model weight updates, (ii) enforces domain\nconstraints to reduce unsafe or invalid actions, and (iii) retains the\nadaptability of language-based reflection. We formalize the MPM representation,\npresent algorithms for update and decoding, and validate the approach in a\ntext-based agent environment following the experimental protocol described in\nthe provided implementation (AlfWorld-based). Empirical results reported in the\nsupplied material indicate consistent gains in execution accuracy and\nrobustness when compared to Reflexion baselines; rule admissibility further\nimproves stability. We analyze mechanisms that explain these gains, discuss\nscalability and failure modes, and outline future directions for multimodal and\nmulti?agent extensions."
                },
                "authors": [
                    {
                        "name": "Chunlong Wu"
                    },
                    {
                        "name": "Zhibo Qu"
                    }
                ],
                "author_detail": {
                    "name": "Zhibo Qu"
                },
                "author": "Zhibo Qu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16561v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16561v3",
                "updated": "2025-09-04T08:12:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    12,
                    41,
                    3,
                    247,
                    0
                ],
                "published": "2025-03-20T06:14:02Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    6,
                    14,
                    2,
                    3,
                    79,
                    0
                ],
                "title": "FutureGen: A RAG-based Approach to Generate the Future Work of\n  Scientific Article",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureGen: A RAG-based Approach to Generate the Future Work of\n  Scientific Article"
                },
                "summary": "The Future Work section of a scientific article outlines potential research\ndirections by identifying gaps and limitations of a current study. This section\nserves as a valuable resource for early-career researchers seeking unexplored\nareas and experienced researchers looking for new projects or collaborations.\nIn this study, we generate future work suggestions from a scientific article.\nTo enrich the generation process with broader insights and reduce the chance of\nmissing important research directions, we use context from related papers using\nRAG. We experimented with various Large Language Models (LLMs) integrated into\nRetrieval-Augmented Generation (RAG). We incorporate an LLM feedback mechanism\nto enhance the quality of the generated content and introduce an LLM-as-a-judge\nframework for robust evaluation, assessing key aspects such as novelty,\nhallucination, and feasibility. Our results demonstrate that the RAG-based\napproach using GPT-4o mini, combined with an LLM feedback mechanism,\noutperforms other methods based on both qualitative and quantitative\nevaluations. Moreover, we conduct a human evaluation to assess the LLM as an\nextractor, generator, and feedback provider.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Future Work section of a scientific article outlines potential research\ndirections by identifying gaps and limitations of a current study. This section\nserves as a valuable resource for early-career researchers seeking unexplored\nareas and experienced researchers looking for new projects or collaborations.\nIn this study, we generate future work suggestions from a scientific article.\nTo enrich the generation process with broader insights and reduce the chance of\nmissing important research directions, we use context from related papers using\nRAG. We experimented with various Large Language Models (LLMs) integrated into\nRetrieval-Augmented Generation (RAG). We incorporate an LLM feedback mechanism\nto enhance the quality of the generated content and introduce an LLM-as-a-judge\nframework for robust evaluation, assessing key aspects such as novelty,\nhallucination, and feasibility. Our results demonstrate that the RAG-based\napproach using GPT-4o mini, combined with an LLM feedback mechanism,\noutperforms other methods based on both qualitative and quantitative\nevaluations. Moreover, we conduct a human evaluation to assess the LLM as an\nextractor, generator, and feedback provider."
                },
                "authors": [
                    {
                        "name": "Ibrahim Al Azher"
                    },
                    {
                        "name": "Miftahul Jannat Mokarrama"
                    },
                    {
                        "name": "Zhishuai Guo"
                    },
                    {
                        "name": "Sagnik Ray Choudhury"
                    },
                    {
                        "name": "Hamed Alhoori"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Alhoori"
                },
                "author": "Hamed Alhoori",
                "arxiv_comment": "12 pages, 6 figures, Accepted for publication at the Workshop on AI\n  Principles in Science Communication (Ai4SC'25), held in conjunction with the\n  IEEE eScience Conference 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16561v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16561v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03985v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03985v1",
                "updated": "2025-09-04T08:12:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    12,
                    6,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T08:12:06Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    12,
                    6,
                    3,
                    247,
                    0
                ],
                "title": "NeuroBreak: Unveil Internal Jailbreak Mechanisms in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuroBreak: Unveil Internal Jailbreak Mechanisms in Large Language\n  Models"
                },
                "summary": "In deployment and application, large language models (LLMs) typically undergo\nsafety alignment to prevent illegal and unethical outputs. However, the\ncontinuous advancement of jailbreak attack techniques, designed to bypass\nsafety mechanisms with adversarial prompts, has placed increasing pressure on\nthe security defenses of LLMs. Strengthening resistance to jailbreak attacks\nrequires an in-depth understanding of the security mechanisms and\nvulnerabilities of LLMs. However, the vast number of parameters and complex\nstructure of LLMs make analyzing security weaknesses from an internal\nperspective a challenging task. This paper presents NeuroBreak, a top-down\njailbreak analysis system designed to analyze neuron-level safety mechanisms\nand mitigate vulnerabilities. We carefully design system requirements through\ncollaboration with three experts in the field of AI security. The system\nprovides a comprehensive analysis of various jailbreak attack methods. By\nincorporating layer-wise representation probing analysis, NeuroBreak offers a\nnovel perspective on the model's decision-making process throughout its\ngeneration steps. Furthermore, the system supports the analysis of critical\nneurons from both semantic and functional perspectives, facilitating a deeper\nexploration of security mechanisms. We conduct quantitative evaluations and\ncase studies to verify the effectiveness of our system, offering mechanistic\ninsights for developing next-generation defense strategies against evolving\njailbreak attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In deployment and application, large language models (LLMs) typically undergo\nsafety alignment to prevent illegal and unethical outputs. However, the\ncontinuous advancement of jailbreak attack techniques, designed to bypass\nsafety mechanisms with adversarial prompts, has placed increasing pressure on\nthe security defenses of LLMs. Strengthening resistance to jailbreak attacks\nrequires an in-depth understanding of the security mechanisms and\nvulnerabilities of LLMs. However, the vast number of parameters and complex\nstructure of LLMs make analyzing security weaknesses from an internal\nperspective a challenging task. This paper presents NeuroBreak, a top-down\njailbreak analysis system designed to analyze neuron-level safety mechanisms\nand mitigate vulnerabilities. We carefully design system requirements through\ncollaboration with three experts in the field of AI security. The system\nprovides a comprehensive analysis of various jailbreak attack methods. By\nincorporating layer-wise representation probing analysis, NeuroBreak offers a\nnovel perspective on the model's decision-making process throughout its\ngeneration steps. Furthermore, the system supports the analysis of critical\nneurons from both semantic and functional perspectives, facilitating a deeper\nexploration of security mechanisms. We conduct quantitative evaluations and\ncase studies to verify the effectiveness of our system, offering mechanistic\ninsights for developing next-generation defense strategies against evolving\njailbreak attacks."
                },
                "authors": [
                    {
                        "name": "Chuhan Zhang"
                    },
                    {
                        "name": "Ye Zhang"
                    },
                    {
                        "name": "Bowen Shi"
                    },
                    {
                        "name": "Yuyou Gan"
                    },
                    {
                        "name": "Tianyu Du"
                    },
                    {
                        "name": "Shouling Ji"
                    },
                    {
                        "name": "Dazhan Deng"
                    },
                    {
                        "name": "Yingcai Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yingcai Wu"
                },
                "author": "Yingcai Wu",
                "arxiv_comment": "12 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03985v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13238v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13238v2",
                "updated": "2025-09-04T08:05:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    5,
                    29,
                    3,
                    247,
                    0
                ],
                "published": "2025-08-18T03:28:57Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    3,
                    28,
                    57,
                    0,
                    230,
                    0
                ],
                "title": "DianJin-OCR-R1: Enhancing OCR Capabilities via a Reasoning-and-Tool\n  Interleaved Vision-Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DianJin-OCR-R1: Enhancing OCR Capabilities via a Reasoning-and-Tool\n  Interleaved Vision-Language Model"
                },
                "summary": "Recent advances in large vision-language models (LVLMs) have enabled a new\nparadigm of end-to-end document image parsing, excelling in Optical Character\nRecognition (OCR) tasks such as text, table, and formula recognition. However,\ngenerative LVLMs, similarly to large language models (LLMs), are prone to\nhallucinations--generating words that do not exist in input images.\nFurthermore, LVLMs are designed for general purposes and tend to be less\neffective on OCR tasks compared to expert models that are trained on\ndomain-specific datasets. In this paper, we propose DianJin-OCR-R1, a\nreasoning-enhanced framework designed to address these limitations through\ntraining reasoning-and-tool interleaved VLMs. Given a recognition instruction,\nour DianJin-OCR-R1 model first recognizes the content in the input image by its\nown OCR capabilities, and then calls other tools (i.e., other expert models) to\nobtain their results as references, finally \"looks again\" the image and\nrethinks about the reasoning process to provide the final recognized content.\nSince architectures of expert models are tailored for specific OCR tasks, which\nmakes them less prone to hallucinations, their results can help VLMs mitigate\nhallucinations. We evaluate our model on ReST and OmniDocBench, and\nexperimental results show that our DianJin-OCR-R1 models consistently\noutperform their non-reasoning counterparts and expert OCR models, which proves\nthe effectiveness of our method. Additionally, the results indicate that\nenhancing expert models, which are typically small and easy to iterate, enable\nperformance improvements for VLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large vision-language models (LVLMs) have enabled a new\nparadigm of end-to-end document image parsing, excelling in Optical Character\nRecognition (OCR) tasks such as text, table, and formula recognition. However,\ngenerative LVLMs, similarly to large language models (LLMs), are prone to\nhallucinations--generating words that do not exist in input images.\nFurthermore, LVLMs are designed for general purposes and tend to be less\neffective on OCR tasks compared to expert models that are trained on\ndomain-specific datasets. In this paper, we propose DianJin-OCR-R1, a\nreasoning-enhanced framework designed to address these limitations through\ntraining reasoning-and-tool interleaved VLMs. Given a recognition instruction,\nour DianJin-OCR-R1 model first recognizes the content in the input image by its\nown OCR capabilities, and then calls other tools (i.e., other expert models) to\nobtain their results as references, finally \"looks again\" the image and\nrethinks about the reasoning process to provide the final recognized content.\nSince architectures of expert models are tailored for specific OCR tasks, which\nmakes them less prone to hallucinations, their results can help VLMs mitigate\nhallucinations. We evaluate our model on ReST and OmniDocBench, and\nexperimental results show that our DianJin-OCR-R1 models consistently\noutperform their non-reasoning counterparts and expert OCR models, which proves\nthe effectiveness of our method. Additionally, the results indicate that\nenhancing expert models, which are typically small and easy to iterate, enable\nperformance improvements for VLMs."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Xianyin Zhang"
                    },
                    {
                        "name": "Lifan Guo"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Chi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chi Zhang"
                },
                "author": "Chi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13238v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03976v1",
                "updated": "2025-09-04T08:02:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    2,
                    38,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T08:02:38Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    2,
                    38,
                    3,
                    247,
                    0
                ],
                "title": "Harnessing modal fields retrieved from speckle for multi-dimensional\n  metrology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing modal fields retrieved from speckle for multi-dimensional\n  metrology"
                },
                "summary": "Although speckle is a powerful tool for high-precision metrology, large\ndatasets and cumbersome training are always required to learn from the encoded\nspeckle patterns, which is unfavorable for rapid deployment and\nmulti-dimensional metrology. To enable high accuracy and fast training,\nphysics-informed machine learning enforces physical laws to address\nhigh-dimensional problems. Here, we harness the modal fields in a few-mode\nfiber, which follow the law of beam propagation, to enable high-accuracy and\nfast-training parameter estimation. Anti-noise fast mode decomposition is\nimplemented to retrieve the modal fields from the speckles. The accuracy is\nenhanced since the modal fields enable parameter estimation at random points in\nthe continuous space-time domain. Artificial tactile perception and\nmulti-dimensional metrology are achieved with high accuracy because the modal\nfields respond diversely to different parameters. Meanwhile, the number of\nspecklegrams for training is reduced by around 5 times. The training time of\nmachine learning is significantly reduced by 800 times, from 9 hours and 45\nminutes to 40 seconds. Therefore, harnessing the modal fields paves a new way\nfor the speckle-based metrology to develop efficient, low-cost,\nmulti-dimensional sensors, making it suitable for intelligent wearable devices,\nindustrial robots and healthcare applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although speckle is a powerful tool for high-precision metrology, large\ndatasets and cumbersome training are always required to learn from the encoded\nspeckle patterns, which is unfavorable for rapid deployment and\nmulti-dimensional metrology. To enable high accuracy and fast training,\nphysics-informed machine learning enforces physical laws to address\nhigh-dimensional problems. Here, we harness the modal fields in a few-mode\nfiber, which follow the law of beam propagation, to enable high-accuracy and\nfast-training parameter estimation. Anti-noise fast mode decomposition is\nimplemented to retrieve the modal fields from the speckles. The accuracy is\nenhanced since the modal fields enable parameter estimation at random points in\nthe continuous space-time domain. Artificial tactile perception and\nmulti-dimensional metrology are achieved with high accuracy because the modal\nfields respond diversely to different parameters. Meanwhile, the number of\nspecklegrams for training is reduced by around 5 times. The training time of\nmachine learning is significantly reduced by 800 times, from 9 hours and 45\nminutes to 40 seconds. Therefore, harnessing the modal fields paves a new way\nfor the speckle-based metrology to develop efficient, low-cost,\nmulti-dimensional sensors, making it suitable for intelligent wearable devices,\nindustrial robots and healthcare applications."
                },
                "authors": [
                    {
                        "name": "Qingbo Liu"
                    },
                    {
                        "name": "Zhongyang Xu"
                    },
                    {
                        "name": "Guangkui Tao"
                    },
                    {
                        "name": "Xiuyuan Sun"
                    },
                    {
                        "name": "Min Xue"
                    },
                    {
                        "name": "Weihao Yuan"
                    },
                    {
                        "name": "Shilong Pan"
                    }
                ],
                "author_detail": {
                    "name": "Shilong Pan"
                },
                "author": "Shilong Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03020v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03020v2",
                "updated": "2025-09-04T08:02:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    2,
                    20,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-03T04:55:26Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    4,
                    55,
                    26,
                    2,
                    246,
                    0
                ],
                "title": "Training LLMs to be Better Text Embedders through Bidirectional\n  Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training LLMs to be Better Text Embedders through Bidirectional\n  Reconstruction"
                },
                "summary": "Large language models (LLMs) have increasingly been explored as powerful text\nembedders. Existing LLM-based text embedding approaches often leverage the\nembedding of the final token, typically a reserved special token such as [EOS].\nHowever, these tokens have not been intentionally trained to capture the\nsemantics of the whole context, limiting their capacity as text embeddings,\nespecially for retrieval and re-ranking tasks. We propose to add a new training\nstage before contrastive learning to enrich the semantics of the final token\nembedding. This stage employs bidirectional generative reconstruction tasks,\nnamely EBQ2D (Embedding-Based Query-to-Document) and EBD2Q (Embedding-Based\nDocument-to-Query), which interleave to anchor the [EOS] embedding and\nreconstruct either side of Query-Document pairs. Experimental results\ndemonstrate that our additional training stage significantly improves LLM\nperformance on the Massive Text Embedding Benchmark (MTEB), achieving new\nstate-of-the-art results across different LLM base models and scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have increasingly been explored as powerful text\nembedders. Existing LLM-based text embedding approaches often leverage the\nembedding of the final token, typically a reserved special token such as [EOS].\nHowever, these tokens have not been intentionally trained to capture the\nsemantics of the whole context, limiting their capacity as text embeddings,\nespecially for retrieval and re-ranking tasks. We propose to add a new training\nstage before contrastive learning to enrich the semantics of the final token\nembedding. This stage employs bidirectional generative reconstruction tasks,\nnamely EBQ2D (Embedding-Based Query-to-Document) and EBD2Q (Embedding-Based\nDocument-to-Query), which interleave to anchor the [EOS] embedding and\nreconstruct either side of Query-Document pairs. Experimental results\ndemonstrate that our additional training stage significantly improves LLM\nperformance on the Massive Text Embedding Benchmark (MTEB), achieving new\nstate-of-the-art results across different LLM base models and scales."
                },
                "authors": [
                    {
                        "name": "Chang Su"
                    },
                    {
                        "name": "Dengliang Shi"
                    },
                    {
                        "name": "Siyuan Huang"
                    },
                    {
                        "name": "Jintao Du"
                    },
                    {
                        "name": "Changhua Meng"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Weiqiang Wang"
                    },
                    {
                        "name": "Zhouhan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouhan Lin"
                },
                "author": "Zhouhan Lin",
                "arxiv_comment": "accepted by EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03020v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03020v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03972v1",
                "updated": "2025-09-04T07:56:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    7,
                    56,
                    24,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T07:56:24Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    7,
                    56,
                    24,
                    3,
                    247,
                    0
                ],
                "title": "Expanding Foundational Language Capabilities in Open-Source LLMs through\n  a Korean Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expanding Foundational Language Capabilities in Open-Source LLMs through\n  a Korean Case Study"
                },
                "summary": "We introduce Llama-3-Motif, a language model consisting of 102 billion\nparameters, specifically designed to enhance Korean capabilities while\nretaining strong performance in English. Developed on the Llama 3 architecture,\nLlama-3-Motif employs advanced training techniques, including LlamaPro and\nMasked Structure Growth, to effectively scale the model without altering its\ncore Transformer architecture. Using the MoAI platform for efficient training\nacross hyperscale GPU clusters, we optimized Llama-3-Motif using a carefully\ncurated dataset that maintains a balanced ratio of Korean and English data.\nLlama-3-Motif shows decent performance on Korean-specific benchmarks,\noutperforming existing models and achieving results comparable to GPT-4.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Llama-3-Motif, a language model consisting of 102 billion\nparameters, specifically designed to enhance Korean capabilities while\nretaining strong performance in English. Developed on the Llama 3 architecture,\nLlama-3-Motif employs advanced training techniques, including LlamaPro and\nMasked Structure Growth, to effectively scale the model without altering its\ncore Transformer architecture. Using the MoAI platform for efficient training\nacross hyperscale GPU clusters, we optimized Llama-3-Motif using a carefully\ncurated dataset that maintains a balanced ratio of Korean and English data.\nLlama-3-Motif shows decent performance on Korean-specific benchmarks,\noutperforming existing models and achieving results comparable to GPT-4."
                },
                "authors": [
                    {
                        "name": "Junghwan Lim"
                    },
                    {
                        "name": "Gangwon Jo"
                    },
                    {
                        "name": "Sungmin Lee"
                    },
                    {
                        "name": "Jiyoung Park"
                    },
                    {
                        "name": "Dongseok Kim"
                    },
                    {
                        "name": "Jihwan Kim"
                    },
                    {
                        "name": "Junhyeok Lee"
                    },
                    {
                        "name": "Wai Ting Cheung"
                    },
                    {
                        "name": "Dahye Choi"
                    },
                    {
                        "name": "Kibong Choi"
                    },
                    {
                        "name": "Jaeyeon Huh"
                    },
                    {
                        "name": "Beomgyu Kim"
                    },
                    {
                        "name": "Jangwoong Kim"
                    },
                    {
                        "name": "Taehyun Kim"
                    },
                    {
                        "name": "Haesol Lee"
                    },
                    {
                        "name": "Jeesoo Lee"
                    },
                    {
                        "name": "Dongpin Oh"
                    },
                    {
                        "name": "Changseok Song"
                    },
                    {
                        "name": "Daewon Suh"
                    }
                ],
                "author_detail": {
                    "name": "Daewon Suh"
                },
                "author": "Daewon Suh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08715v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08715v3",
                "updated": "2025-09-04T07:56:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    7,
                    56,
                    0,
                    3,
                    247,
                    0
                ],
                "published": "2025-08-12T07:58:48Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    7,
                    58,
                    48,
                    1,
                    224,
                    0
                ],
                "title": "MultiGen: Child-Friendly Multilingual Speech Generator with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiGen: Child-Friendly Multilingual Speech Generator with LLMs"
                },
                "summary": "Generative speech models have demonstrated significant potential in improving\nhuman-machine interactions, offering valuable real-world applications such as\nlanguage learning for children. However, achieving high-quality, child-friendly\nspeech generation remains challenging, particularly for low-resource languages\nacross diverse languages and cultural contexts. In this paper, we propose\nMultiGen, a multilingual speech generation model with child-friendly\ninteraction, leveraging LLM architecture for speech generation tailored for\nlow-resource languages. We propose to integrate age-appropriate multilingual\nspeech generation using LLM architectures, which can be used to facilitate\nyoung children's communication with AI systems through culturally relevant\ncontext in three low-resource languages: Singaporean accent Mandarin, Malay,\nand Tamil. Experimental results from both objective metrics and subjective\nevaluations demonstrate the superior performance of the proposed MultiGen\ncompared to baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative speech models have demonstrated significant potential in improving\nhuman-machine interactions, offering valuable real-world applications such as\nlanguage learning for children. However, achieving high-quality, child-friendly\nspeech generation remains challenging, particularly for low-resource languages\nacross diverse languages and cultural contexts. In this paper, we propose\nMultiGen, a multilingual speech generation model with child-friendly\ninteraction, leveraging LLM architecture for speech generation tailored for\nlow-resource languages. We propose to integrate age-appropriate multilingual\nspeech generation using LLM architectures, which can be used to facilitate\nyoung children's communication with AI systems through culturally relevant\ncontext in three low-resource languages: Singaporean accent Mandarin, Malay,\nand Tamil. Experimental results from both objective metrics and subjective\nevaluations demonstrate the superior performance of the proposed MultiGen\ncompared to baseline methods."
                },
                "authors": [
                    {
                        "name": "Xiaoxue Gao"
                    },
                    {
                        "name": "Huayun Zhang"
                    },
                    {
                        "name": "Nancy F. Chen"
                    }
                ],
                "author_detail": {
                    "name": "Nancy F. Chen"
                },
                "author": "Nancy F. Chen",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08715v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08715v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18158v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18158v3",
                "updated": "2025-09-04T07:46:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    7,
                    46,
                    37,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-30T05:48:13Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    5,
                    48,
                    13,
                    3,
                    30,
                    0
                ],
                "title": "Large Language Models for Cryptocurrency Transaction Analysis: A Bitcoin\n  Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Cryptocurrency Transaction Analysis: A Bitcoin\n  Case Study"
                },
                "summary": "Cryptocurrencies are widely used, yet current methods for analyzing\ntransactions often rely on opaque, black-box models. While these models may\nachieve high performance, their outputs are usually difficult to interpret and\nadapt, making it challenging to capture nuanced behavioral patterns. Large\nlanguage models (LLMs) have the potential to address these gaps, but their\ncapabilities in this area remain largely unexplored, particularly in cybercrime\ndetection. In this paper, we test this hypothesis by applying LLMs to\nreal-world cryptocurrency transaction graphs, with a focus on Bitcoin, one of\nthe most studied and widely adopted blockchain networks. We introduce a\nthree-tiered framework to assess LLM capabilities: foundational metrics,\ncharacteristic overview, and contextual interpretation. This includes a new,\nhuman-readable graph representation format, LLM4TG, and a connectivity-enhanced\ntransaction graph sampling algorithm, CETraS. Together, they significantly\nreduce token requirements, transforming the analysis of multiple moderately\nlarge-scale transaction graphs with LLMs from nearly impossible to feasible\nunder strict token limits. Experimental results demonstrate that LLMs have\noutstanding performance on foundational metrics and characteristic overview,\nwhere the accuracy of recognizing most basic information at the node level\nexceeds 98.50% and the proportion of obtaining meaningful characteristics\nreaches 95.00%. Regarding contextual interpretation, LLMs also demonstrate\nstrong performance in classification tasks, even with very limited labeled\ndata, where top-3 accuracy reaches 72.43% with explanations. While the\nexplanations are not always fully accurate, they highlight the strong potential\nof LLMs in this domain. At the same time, several limitations persist, which we\ndiscuss along with directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cryptocurrencies are widely used, yet current methods for analyzing\ntransactions often rely on opaque, black-box models. While these models may\nachieve high performance, their outputs are usually difficult to interpret and\nadapt, making it challenging to capture nuanced behavioral patterns. Large\nlanguage models (LLMs) have the potential to address these gaps, but their\ncapabilities in this area remain largely unexplored, particularly in cybercrime\ndetection. In this paper, we test this hypothesis by applying LLMs to\nreal-world cryptocurrency transaction graphs, with a focus on Bitcoin, one of\nthe most studied and widely adopted blockchain networks. We introduce a\nthree-tiered framework to assess LLM capabilities: foundational metrics,\ncharacteristic overview, and contextual interpretation. This includes a new,\nhuman-readable graph representation format, LLM4TG, and a connectivity-enhanced\ntransaction graph sampling algorithm, CETraS. Together, they significantly\nreduce token requirements, transforming the analysis of multiple moderately\nlarge-scale transaction graphs with LLMs from nearly impossible to feasible\nunder strict token limits. Experimental results demonstrate that LLMs have\noutstanding performance on foundational metrics and characteristic overview,\nwhere the accuracy of recognizing most basic information at the node level\nexceeds 98.50% and the proportion of obtaining meaningful characteristics\nreaches 95.00%. Regarding contextual interpretation, LLMs also demonstrate\nstrong performance in classification tasks, even with very limited labeled\ndata, where top-3 accuracy reaches 72.43% with explanations. While the\nexplanations are not always fully accurate, they highlight the strong potential\nof LLMs in this domain. At the same time, several limitations persist, which we\ndiscuss along with directions for future research."
                },
                "authors": [
                    {
                        "name": "Yuchen Lei"
                    },
                    {
                        "name": "Yuexin Xiang"
                    },
                    {
                        "name": "Qin Wang"
                    },
                    {
                        "name": "Rafael Dowsley"
                    },
                    {
                        "name": "Tsz Hon Yuen"
                    },
                    {
                        "name": "Kim-Kwang Raymond Choo"
                    },
                    {
                        "name": "Jiangshan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jiangshan Yu"
                },
                "author": "Jiangshan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18158v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18158v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03962v1",
                "updated": "2025-09-04T07:41:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    7,
                    41,
                    23,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T07:41:23Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    7,
                    41,
                    23,
                    3,
                    247,
                    0
                ],
                "title": "Exploring NLP Benchmarks in an Extremely Low-Resource Setting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring NLP Benchmarks in an Extremely Low-Resource Setting"
                },
                "summary": "The effectiveness of Large Language Models (LLMs) diminishes for extremely\nlow-resource languages, such as indigenous languages, primarily due to the lack\nof labeled data. Despite growing interest, the availability of high-quality\nnatural language processing (NLP) datasets for these languages remains limited,\nmaking it difficult to develop robust language technologies. This paper\naddresses such gap by focusing on Ladin, an endangered Romance language,\nspecifically targeting the Val Badia variant. Leveraging a small set of\nparallel Ladin-Italian sentence pairs, we create synthetic datasets for\nsentiment analysis and multiple-choice question answering (MCQA) by translating\nmonolingual Italian data. To ensure linguistic quality and reliability, we\napply rigorous filtering and back-translation procedures in our method. We\nfurther demonstrate that incorporating these synthetic datasets into machine\ntranslation training leads to substantial improvements over existing\nItalian-Ladin translation baselines. Our contributions include the first\npublicly available sentiment analysis and MCQA datasets for Ladin, establishing\nfoundational resources that can support broader NLP research and downstream\napplications for this underrepresented language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effectiveness of Large Language Models (LLMs) diminishes for extremely\nlow-resource languages, such as indigenous languages, primarily due to the lack\nof labeled data. Despite growing interest, the availability of high-quality\nnatural language processing (NLP) datasets for these languages remains limited,\nmaking it difficult to develop robust language technologies. This paper\naddresses such gap by focusing on Ladin, an endangered Romance language,\nspecifically targeting the Val Badia variant. Leveraging a small set of\nparallel Ladin-Italian sentence pairs, we create synthetic datasets for\nsentiment analysis and multiple-choice question answering (MCQA) by translating\nmonolingual Italian data. To ensure linguistic quality and reliability, we\napply rigorous filtering and back-translation procedures in our method. We\nfurther demonstrate that incorporating these synthetic datasets into machine\ntranslation training leads to substantial improvements over existing\nItalian-Ladin translation baselines. Our contributions include the first\npublicly available sentiment analysis and MCQA datasets for Ladin, establishing\nfoundational resources that can support broader NLP research and downstream\napplications for this underrepresented language."
                },
                "authors": [
                    {
                        "name": "Ulin Nuha"
                    },
                    {
                        "name": "Adam Jatowt"
                    }
                ],
                "author_detail": {
                    "name": "Adam Jatowt"
                },
                "author": "Adam Jatowt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]