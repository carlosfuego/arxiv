[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2511.17202v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17202v1",
                "title": "Ferroelectric Nematic Liquid Crystals as Charge Boosters for Triboelectric Nanogenerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ferroelectric Nematic Liquid Crystals as Charge Boosters for Triboelectric Nanogenerators"
                },
                "updated": "2025-11-21T12:27:53Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    27,
                    53,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17202v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Driven by growing demand for clean energy, triboelectric nanogenerators (TENGs) have emerged as promising self-powered systems, yet achieving high charge density remains a critical challenge. In polymer dielectrics, triboelectricity can be further amplified by incorporating high-dielectric and polar materials for functional adaptability. Conventional dielectrics, including liquid crystals (LCs), offer limited improvement for triboelectrification, whereas the breakthrough ferroelectric nematic liquid crystals (NF-LCs), with giant spontaneous polarization and high dielectric constant, act as highly effective charge boosters. Here, we introduce NF-LC (DIO) as a functional filler in a PVDF-based TENG. Increasing DIO content progressively grows the electroactive phase and effective polarization in PVDF and defines a marked improvement in TENGs electrical performances through favourable dipolar alignment and strengthened charge-trap effects. The optimized composite film achieves an impressive open-circuit voltage of 1.1 kV, short-circuit current of 50 micro Amp, and power density of 110 W/m2 - seven times higher than pure PVDF. The device exhibits excellent charge-storage capability, powers over 500 LEDs without power management. This work establishes NF-LC-based TENGs as a new platform for high-performance self-powered energy harvesting, linking soft matter physics with applied energy technology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Driven by growing demand for clean energy, triboelectric nanogenerators (TENGs) have emerged as promising self-powered systems, yet achieving high charge density remains a critical challenge. In polymer dielectrics, triboelectricity can be further amplified by incorporating high-dielectric and polar materials for functional adaptability. Conventional dielectrics, including liquid crystals (LCs), offer limited improvement for triboelectrification, whereas the breakthrough ferroelectric nematic liquid crystals (NF-LCs), with giant spontaneous polarization and high dielectric constant, act as highly effective charge boosters. Here, we introduce NF-LC (DIO) as a functional filler in a PVDF-based TENG. Increasing DIO content progressively grows the electroactive phase and effective polarization in PVDF and defines a marked improvement in TENGs electrical performances through favourable dipolar alignment and strengthened charge-trap effects. The optimized composite film achieves an impressive open-circuit voltage of 1.1 kV, short-circuit current of 50 micro Amp, and power density of 110 W/m2 - seven times higher than pure PVDF. The device exhibits excellent charge-storage capability, powers over 500 LEDs without power management. This work establishes NF-LC-based TENGs as a new platform for high-performance self-powered energy harvesting, linking soft matter physics with applied energy technology."
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T12:27:53Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    27,
                    53,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph"
                },
                "authors": [
                    {
                        "name": "Jia-Yao Ye"
                    },
                    {
                        "name": "Susanta Chakraborty"
                    },
                    {
                        "name": "Karthick Subramani"
                    },
                    {
                        "name": "Xing-Zhou Tang"
                    },
                    {
                        "name": "Yan-Nan Xie"
                    },
                    {
                        "name": "Bing-Xiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Bing-Xiang Li"
                },
                "author": "Bing-Xiang Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.16943v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.16943v1",
                "title": "RASTP: Representation-Aware Semantic Token Pruning for Generative Recommendation with Semantic Identifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RASTP: Representation-Aware Semantic Token Pruning for Generative Recommendation with Semantic Identifiers"
                },
                "updated": "2025-11-21T04:39:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    4,
                    39,
                    32,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.16943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.16943v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generative recommendation systems typically leverage Semantic Identifiers (SIDs), which represent each item as a sequence of tokens that encode semantic information. However, representing item ID with multiple SIDs significantly increases input sequence length, which is a major determinant of computational complexity and memory consumption. While existing efforts primarily focus on optimizing attention computation and KV cache, we propose RASTP (Representation-Aware Semantic Token Pruning), which directly prunes less informative tokens in the input sequence. Specifically, RASTP evaluates token importance by combining semantic saliency, measured via representation magnitude, and attention centrality, derived from cumulative attention weights. Since RASTP dynamically prunes low-information or irrelevant semantic tokens, experiments on three real-world Amazon datasets show that RASTP reduces training time by 26.7\\%, while maintaining or slightly improving recommendation performance. The code has been open-sourced at https://github.com/Yuzt-zju/RASTP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative recommendation systems typically leverage Semantic Identifiers (SIDs), which represent each item as a sequence of tokens that encode semantic information. However, representing item ID with multiple SIDs significantly increases input sequence length, which is a major determinant of computational complexity and memory consumption. While existing efforts primarily focus on optimizing attention computation and KV cache, we propose RASTP (Representation-Aware Semantic Token Pruning), which directly prunes less informative tokens in the input sequence. Specifically, RASTP evaluates token importance by combining semantic saliency, measured via representation magnitude, and attention centrality, derived from cumulative attention weights. Since RASTP dynamically prunes low-information or irrelevant semantic tokens, experiments on three real-world Amazon datasets show that RASTP reduces training time by 26.7\\%, while maintaining or slightly improving recommendation performance. The code has been open-sourced at https://github.com/Yuzt-zju/RASTP."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T04:39:32Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    4,
                    39,
                    32,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "4 pages",
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Tianyu Zhan"
                    },
                    {
                        "name": "Kairui Fu"
                    },
                    {
                        "name": "Zheqi Lv"
                    },
                    {
                        "name": "Shengyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shengyu Zhang"
                },
                "author": "Shengyu Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.16786v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.16786v1",
                "title": "Revisiting Multimodal KV Cache Compression: A Frequency-Domain-Guided Outlier-KV-Aware Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Multimodal KV Cache Compression: A Frequency-Domain-Guided Outlier-KV-Aware Approach"
                },
                "updated": "2025-11-20T20:25:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    20,
                    20,
                    25,
                    34,
                    3,
                    324,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.16786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.16786v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multimodal large language models suffer from substantial inference overhead since multimodal KV Cache grows proportionally with the visual input length. Existing multimodal KV Cache compression methods mostly rely on attention score to reduce cache size, which makes them are incompatible with established efficient attention kernels (e.g., FlashAttention) and ignores the contribution of value vectors to the attention output. In this work, we revisit multimodal KV Cache compression from the perspective of the KV matrices' distribution. First, we observe that frequency-domain energy of multimodal KV matrices is predominantly concentrated in low-frequency and extract this principal energy via a low-pass filter. Further, we find that removing KV pairs that deviate substantially from this principal energy leads to a pronounced performance drop, which we define as Outlier KVs. Considering Outlier KVs are more likely to encode features critical for inference, we propose FlashCache, a frequency-domain-guided, Outlier-KV-aware KV Cache compression framework. First, we introduce an Outlier KV Recognition Module that models the principal component of multimodal KV matrices in the frequency domain and preferentially retains KV pairs that significantly deviate from it. Furthermore, Dynamic Budget Allocation Module is designed to adaptively determine the per-layer KV Cache size to retain more Outlier KVs. Experiments on multiple MLLMs and benchmarks demonstrate that FlashCache outperforms state-of-the-art multimoal KV compression methods, achieving up to 1.69 times faster decoding with 80% lower KV memory usage while maintaining task performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models suffer from substantial inference overhead since multimodal KV Cache grows proportionally with the visual input length. Existing multimodal KV Cache compression methods mostly rely on attention score to reduce cache size, which makes them are incompatible with established efficient attention kernels (e.g., FlashAttention) and ignores the contribution of value vectors to the attention output. In this work, we revisit multimodal KV Cache compression from the perspective of the KV matrices' distribution. First, we observe that frequency-domain energy of multimodal KV matrices is predominantly concentrated in low-frequency and extract this principal energy via a low-pass filter. Further, we find that removing KV pairs that deviate substantially from this principal energy leads to a pronounced performance drop, which we define as Outlier KVs. Considering Outlier KVs are more likely to encode features critical for inference, we propose FlashCache, a frequency-domain-guided, Outlier-KV-aware KV Cache compression framework. First, we introduce an Outlier KV Recognition Module that models the principal component of multimodal KV matrices in the frequency domain and preferentially retains KV pairs that significantly deviate from it. Furthermore, Dynamic Budget Allocation Module is designed to adaptively determine the per-layer KV Cache size to retain more Outlier KVs. Experiments on multiple MLLMs and benchmarks demonstrate that FlashCache outperforms state-of-the-art multimoal KV compression methods, achieving up to 1.69 times faster decoding with 80% lower KV memory usage while maintaining task performance."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-20T20:25:34Z",
                "published_parsed": [
                    2025,
                    11,
                    20,
                    20,
                    25,
                    34,
                    3,
                    324,
                    0
                ],
                "arxiv_comment": "Under Review",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yaoxin Yang"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Xudong Tan"
                    },
                    {
                        "name": "Chongjun Tu"
                    },
                    {
                        "name": "Maosen Zhao"
                    },
                    {
                        "name": "Jia Hao"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen"
            },
            {
                "id": "http://arxiv.org/abs/2511.15311v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.15311v2",
                "title": "Adapt-As-You-Walk Through the Clouds: Training-Free Online Test-Time Adaptation of 3D Vision-Language Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapt-As-You-Walk Through the Clouds: Training-Free Online Test-Time Adaptation of 3D Vision-Language Foundation Models"
                },
                "updated": "2025-11-20T19:08:56Z",
                "updated_parsed": [
                    2025,
                    11,
                    20,
                    19,
                    8,
                    56,
                    3,
                    324,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.15311v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.15311v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "3D Vision-Language Foundation Models (VLFMs) have shown strong generalization and zero-shot recognition capabilities in open-world point cloud processing tasks. However, these models often underperform in practical scenarios where data are noisy, incomplete, or drawn from a different distribution than the training data. To address this, we propose Uni-Adapter, a novel training-free online test-time adaptation (TTA) strategy for 3D VLFMs based on dynamic prototype learning. We define a 3D cache to store class-specific cluster centers as prototypes, which are continuously updated to capture intra-class variability in heterogeneous data distributions. These dynamic prototypes serve as anchors for cache-based logit computation via similarity scoring. Simultaneously, a graph-based label smoothing module captures inter-prototype similarities to enforce label consistency among similar prototypes. Finally, we unify predictions from the original 3D VLFM and the refined 3D cache using entropy-weighted aggregation for reliable adaptation. Without retraining, Uni-Adapter effectively mitigates distribution shifts, achieving state-of-the-art performance on diverse 3D benchmarks over different 3D VLFMs, improving ModelNet-40C by 10.55%, ScanObjectNN-C by 8.26%, and ShapeNet-C by 4.49% over the source 3D VLFMs. Project page: https://mehran-tam.github.io/Uni-Adapter",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Vision-Language Foundation Models (VLFMs) have shown strong generalization and zero-shot recognition capabilities in open-world point cloud processing tasks. However, these models often underperform in practical scenarios where data are noisy, incomplete, or drawn from a different distribution than the training data. To address this, we propose Uni-Adapter, a novel training-free online test-time adaptation (TTA) strategy for 3D VLFMs based on dynamic prototype learning. We define a 3D cache to store class-specific cluster centers as prototypes, which are continuously updated to capture intra-class variability in heterogeneous data distributions. These dynamic prototypes serve as anchors for cache-based logit computation via similarity scoring. Simultaneously, a graph-based label smoothing module captures inter-prototype similarities to enforce label consistency among similar prototypes. Finally, we unify predictions from the original 3D VLFM and the refined 3D cache using entropy-weighted aggregation for reliable adaptation. Without retraining, Uni-Adapter effectively mitigates distribution shifts, achieving state-of-the-art performance on diverse 3D benchmarks over different 3D VLFMs, improving ModelNet-40C by 10.55%, ScanObjectNN-C by 8.26%, and ShapeNet-C by 4.49% over the source 3D VLFMs. Project page: https://mehran-tam.github.io/Uni-Adapter"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-19T10:22:22Z",
                "published_parsed": [
                    2025,
                    11,
                    19,
                    10,
                    22,
                    22,
                    2,
                    323,
                    0
                ],
                "arxiv_comment": "Accepted by AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Mehran Tamjidi"
                    },
                    {
                        "name": "Hamidreza Dastmalchi"
                    },
                    {
                        "name": "Mohammadreza Alimoradijazi"
                    },
                    {
                        "name": "Ali Cheraghian"
                    },
                    {
                        "name": "Aijun An"
                    },
                    {
                        "name": "Morteza Saberi"
                    }
                ],
                "author_detail": {
                    "name": "Morteza Saberi"
                },
                "author": "Morteza Saberi"
            },
            {
                "id": "http://arxiv.org/abs/2511.16546v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.16546v1",
                "title": "Progressive Supernet Training for Efficient Visual Autoregressive Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progressive Supernet Training for Efficient Visual Autoregressive Modeling"
                },
                "updated": "2025-11-20T16:59:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    20,
                    16,
                    59,
                    24,
                    3,
                    324,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.16546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.16546v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Visual Auto-Regressive (VAR) models significantly reduce inference steps through the \"next-scale\" prediction paradigm. However, progressive multi-scale generation incurs substantial memory overhead due to cumulative KV caching, limiting practical deployment.\n  We observe a scale-depth asymmetric dependency in VAR: early scales exhibit extreme sensitivity to network depth, while later scales remain robust to depth reduction. Inspired by this, we propose VARiant: by equidistant sampling, we select multiple subnets ranging from 16 to 2 layers from the original 30-layer VAR-d30 network. Early scales are processed by the full network, while later scales utilize subnet. Subnet and the full network share weights, enabling flexible depth adjustment within a single model.\n  However, weight sharing between subnet and the entire network can lead to optimization conflicts. To address this, we propose a progressive training strategy that breaks through the Pareto frontier of generation quality for both subnets and the full network under fixed-ratio training, achieving joint optimality.\n  Experiments on ImageNet demonstrate that, compared to the pretrained VAR-d30 (FID 1.95), VARiant-d16 and VARiant-d8 achieve nearly equivalent quality (FID 2.05/2.12) while reducing memory consumption by 40-65%. VARiant-d2 achieves 3.5 times speedup and 80% memory reduction at moderate quality cost (FID 2.97). In terms of deployment, VARiant's single-model architecture supports zero-cost runtime depth switching and provides flexible deployment options from high quality to extreme efficiency, catering to diverse application scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Auto-Regressive (VAR) models significantly reduce inference steps through the \"next-scale\" prediction paradigm. However, progressive multi-scale generation incurs substantial memory overhead due to cumulative KV caching, limiting practical deployment.\n  We observe a scale-depth asymmetric dependency in VAR: early scales exhibit extreme sensitivity to network depth, while later scales remain robust to depth reduction. Inspired by this, we propose VARiant: by equidistant sampling, we select multiple subnets ranging from 16 to 2 layers from the original 30-layer VAR-d30 network. Early scales are processed by the full network, while later scales utilize subnet. Subnet and the full network share weights, enabling flexible depth adjustment within a single model.\n  However, weight sharing between subnet and the entire network can lead to optimization conflicts. To address this, we propose a progressive training strategy that breaks through the Pareto frontier of generation quality for both subnets and the full network under fixed-ratio training, achieving joint optimality.\n  Experiments on ImageNet demonstrate that, compared to the pretrained VAR-d30 (FID 1.95), VARiant-d16 and VARiant-d8 achieve nearly equivalent quality (FID 2.05/2.12) while reducing memory consumption by 40-65%. VARiant-d2 achieves 3.5 times speedup and 80% memory reduction at moderate quality cost (FID 2.97). In terms of deployment, VARiant's single-model architecture supports zero-cost runtime depth switching and provides flexible deployment options from high quality to extreme efficiency, catering to diverse application scenarios."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-20T16:59:24Z",
                "published_parsed": [
                    2025,
                    11,
                    20,
                    16,
                    59,
                    24,
                    3,
                    324,
                    0
                ],
                "arxiv_comment": "Submitted to CVPR 2025. 10 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Xiaoyue Chen"
                    },
                    {
                        "name": "Yuling Shi"
                    },
                    {
                        "name": "Kaiyuan Li"
                    },
                    {
                        "name": "Huandong Wang"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Xiaodong Gu"
                    },
                    {
                        "name": "Xinlei Chen"
                    },
                    {
                        "name": "Mingbao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Mingbao Lin"
                },
                "author": "Mingbao Lin"
            },
            {
                "id": "http://arxiv.org/abs/2502.04420v5",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.04420v5",
                "title": "KVTuner: Sensitivity-Aware Layer-Wise Mixed-Precision KV Cache Quantization for Efficient and Nearly Lossless LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVTuner: Sensitivity-Aware Layer-Wise Mixed-Precision KV Cache Quantization for Efficient and Nearly Lossless LLM Inference"
                },
                "updated": "2025-11-20T15:25:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    20,
                    15,
                    25,
                    17,
                    3,
                    324,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.04420v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.04420v5",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "KV cache quantization can improve Large Language Models (LLMs) inference throughput and latency in long contexts and large batch-size scenarios while preserving LLMs effectiveness. However, current methods have three unsolved issues: overlooking layer-wise sensitivity to KV cache quantization, high overhead of online fine-grained decision-making, and low flexibility to different LLMs and constraints. Therefore, we theoretically analyze the inherent correlation of layer-wise transformer attention patterns to KV cache quantization errors and study why key cache is generally more important than value cache for quantization error reduction. We further propose a simple yet effective framework KVTuner to adaptively search for the optimal hardware-friendly layer-wise KV quantization precision pairs for coarse-grained KV cache with multi-objective optimization and directly utilize the offline searched configurations during online inference. To reduce the computational cost of offline calibration, we utilize the intra-layer KV precision pair pruning and inter-layer clustering to reduce the search space. Experimental results show that we can achieve nearly lossless 3.25-bit mixed precision KV cache quantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive models like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum inference throughput can be improved by 21.25\\% compared with KIVI-KV8 quantization over various context lengths. Our code and searched configurations are available at https://github.com/cmd2001/KVTuner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache quantization can improve Large Language Models (LLMs) inference throughput and latency in long contexts and large batch-size scenarios while preserving LLMs effectiveness. However, current methods have three unsolved issues: overlooking layer-wise sensitivity to KV cache quantization, high overhead of online fine-grained decision-making, and low flexibility to different LLMs and constraints. Therefore, we theoretically analyze the inherent correlation of layer-wise transformer attention patterns to KV cache quantization errors and study why key cache is generally more important than value cache for quantization error reduction. We further propose a simple yet effective framework KVTuner to adaptively search for the optimal hardware-friendly layer-wise KV quantization precision pairs for coarse-grained KV cache with multi-objective optimization and directly utilize the offline searched configurations during online inference. To reduce the computational cost of offline calibration, we utilize the intra-layer KV precision pair pruning and inter-layer clustering to reduce the search space. Experimental results show that we can achieve nearly lossless 3.25-bit mixed precision KV cache quantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive models like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum inference throughput can be improved by 21.25\\% compared with KIVI-KV8 quantization over various context lengths. Our code and searched configurations are available at https://github.com/cmd2001/KVTuner."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-06T15:26:26Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "arxiv_comment": "Accepted by ICML25. Code: https://github.com/cmd2001/KVTuner",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zeyu Xing"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Linping Qu"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan"
            },
            {
                "id": "http://arxiv.org/abs/2511.16298v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.16298v1",
                "title": "Optimizing 3D Gaussian Splattering for Mobile GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing 3D Gaussian Splattering for Mobile GPUs"
                },
                "updated": "2025-11-20T12:25:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    20,
                    12,
                    25,
                    26,
                    3,
                    324,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.16298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.16298v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Image-based 3D scene reconstruction, which transforms multi-view images into a structured 3D representation of the surrounding environment, is a common task across many modern applications. 3D Gaussian Splatting (3DGS) is a new paradigm to address this problem and offers considerable efficiency as compared to the previous methods. Motivated by this, and considering various benefits of mobile device deployment (data privacy, operating without internet connectivity, and potentially faster responses), this paper develops Texture3dgs, an optimized mapping of 3DGS for a mobile GPU. A critical challenge in this area turns out to be optimizing for the two-dimensional (2D) texture cache, which needs to be exploited for faster executions on mobile GPUs. As a sorting method dominates the computations in 3DGS on mobile platforms, the core of Texture3dgs is a novel sorting algorithm where the processing, data movement, and placement are highly optimized for 2D memory. The properties of this algorithm are analyzed in view of a cost model for the texture cache. In addition, we accelerate other steps of the 3DGS algorithm through improved variable layout design and other optimizations. End-to-end evaluation shows that Texture3dgs delivers up to 4.1$\\times$ and 1.7$\\times$ speedup for the sorting and overall 3D scene reconstruction, respectively -- while also reducing memory usage by up to 1.6$\\times$ -- demonstrating the effectiveness of our design for efficient mobile 3D scene reconstruction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image-based 3D scene reconstruction, which transforms multi-view images into a structured 3D representation of the surrounding environment, is a common task across many modern applications. 3D Gaussian Splatting (3DGS) is a new paradigm to address this problem and offers considerable efficiency as compared to the previous methods. Motivated by this, and considering various benefits of mobile device deployment (data privacy, operating without internet connectivity, and potentially faster responses), this paper develops Texture3dgs, an optimized mapping of 3DGS for a mobile GPU. A critical challenge in this area turns out to be optimizing for the two-dimensional (2D) texture cache, which needs to be exploited for faster executions on mobile GPUs. As a sorting method dominates the computations in 3DGS on mobile platforms, the core of Texture3dgs is a novel sorting algorithm where the processing, data movement, and placement are highly optimized for 2D memory. The properties of this algorithm are analyzed in view of a cost model for the texture cache. In addition, we accelerate other steps of the 3DGS algorithm through improved variable layout design and other optimizations. End-to-end evaluation shows that Texture3dgs delivers up to 4.1$\\times$ and 1.7$\\times$ speedup for the sorting and overall 3D scene reconstruction, respectively -- while also reducing memory usage by up to 1.6$\\times$ -- demonstrating the effectiveness of our design for efficient mobile 3D scene reconstruction."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-20T12:25:26Z",
                "published_parsed": [
                    2025,
                    11,
                    20,
                    12,
                    25,
                    26,
                    3,
                    324,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Md Musfiqur Rahman Sanim"
                    },
                    {
                        "name": "Zhihao Shu"
                    },
                    {
                        "name": "Bahram Afsharmanesh"
                    },
                    {
                        "name": "AmirAli Mirian"
                    },
                    {
                        "name": "Jiexiong Guan"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Bin Ren"
                    },
                    {
                        "name": "Gagan Agrawal"
                    }
                ],
                "author_detail": {
                    "name": "Gagan Agrawal"
                },
                "author": "Gagan Agrawal"
            },
            {
                "id": "http://arxiv.org/abs/2511.16138v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.16138v1",
                "title": "On 10x Better Scalability: KV Stores Scale Up KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On 10x Better Scalability: KV Stores Scale Up KV Cache"
                },
                "updated": "2025-11-20T08:22:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    20,
                    8,
                    22,
                    36,
                    3,
                    324,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.16138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.16138v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) rely on Key-Value (KV) cache to reduce time- to-first-token (TTFT) latency, but existing disk-based KV cache systems using file-per-object layouts suffer from severe scalability bottlenecks due to file system metadata overhead, I/O inefficiency, and poor spatial locality. This paper presents SGLANG-LSM, a database-inspired system that leverages Log-Structured Merge- tree (LSM-tree) architectures for scalable KV cache management. SGLANG-LSM implements a layered system design with three coordinated components: (1) a prefix-preserving storage engine that maintains token sequence locality while efficiently storing large KV cache tensors through key-value separation, (2) an adaptive controller that dynamically optimizes LSM-tree configurations based on shifting workload characteristics, and (3) runtime services including batch opera- tions and automatic resource management for production deployment. Evaluation on large-scale dynamic workloads demonstrates that SGLANG-LSM significantly improves cache hits by up to 143% and reduces TTFT by up to 24% compared to state-of-the-art systems, representing the first systematic application of database storage architectures to large-scale LLM cache management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on Key-Value (KV) cache to reduce time- to-first-token (TTFT) latency, but existing disk-based KV cache systems using file-per-object layouts suffer from severe scalability bottlenecks due to file system metadata overhead, I/O inefficiency, and poor spatial locality. This paper presents SGLANG-LSM, a database-inspired system that leverages Log-Structured Merge- tree (LSM-tree) architectures for scalable KV cache management. SGLANG-LSM implements a layered system design with three coordinated components: (1) a prefix-preserving storage engine that maintains token sequence locality while efficiently storing large KV cache tensors through key-value separation, (2) an adaptive controller that dynamically optimizes LSM-tree configurations based on shifting workload characteristics, and (3) runtime services including batch opera- tions and automatic resource management for production deployment. Evaluation on large-scale dynamic workloads demonstrates that SGLANG-LSM significantly improves cache hits by up to 143% and reduces TTFT by up to 24% compared to state-of-the-art systems, representing the first systematic application of database storage architectures to large-scale LLM cache management."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-20T08:22:36Z",
                "published_parsed": [
                    2025,
                    11,
                    20,
                    8,
                    22,
                    36,
                    3,
                    324,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Weiping Yu"
                    },
                    {
                        "name": "Ye Jiarui"
                    },
                    {
                        "name": "He Mengke"
                    },
                    {
                        "name": "Junfeng Liu"
                    },
                    {
                        "name": "Siqiang Luo"
                    }
                ],
                "author_detail": {
                    "name": "Siqiang Luo"
                },
                "author": "Siqiang Luo"
            },
            {
                "id": "http://arxiv.org/abs/2511.16047v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.16047v1",
                "title": "AMS-KV: Adaptive KV Caching in Multi-Scale Visual Autoregressive Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AMS-KV: Adaptive KV Caching in Multi-Scale Visual Autoregressive Transformers"
                },
                "updated": "2025-11-20T05:10:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    20,
                    5,
                    10,
                    12,
                    3,
                    324,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.16047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.16047v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Visual autoregressive modeling (VAR) via next-scale prediction has emerged as a scalable image generation paradigm. While Key and Value (KV) caching in large language models (LLMs) has been extensively studied, next-scale prediction presents unique challenges, and KV caching design for next-scale based VAR transformers remains largely unexplored. A major bottleneck is the excessive KV memory growth with the increasing number of scales-severely limiting scalability. Our systematic investigation reveals that: (1) Attending to tokens from local scales significantly contributes to generation quality (2) Allocating a small amount of memory for the coarsest scales, termed as condensed scales, stabilizes multi-scale image generation (3) Strong KV similarity across finer scales is predominantly observed in cache-efficient layers, whereas cache-demanding layers exhibit weaker inter-scale similarity. Based on the observations, we introduce AMS-KV, a scale-adaptive KV caching policy for next-scale prediction in VAR models. AMS-KV prioritizes storing KVs from condensed and local scales, preserving the most relevant tokens to maintain generation quality. It further optimizes KV cache utilization and computational efficiency identifying cache-demanding layers through inter-scale similarity analysis. Compared to the vanilla next-scale prediction-based VAR models, AMS-KV reduces KV cache usage by up to 84.83% and self-attention latency by 60.48%. Moreover, when the baseline VAR-d30 model encounters out-of-memory failures at a batch size of 128, AMS-KV enables stable scaling to a batch size of 256 with improved throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual autoregressive modeling (VAR) via next-scale prediction has emerged as a scalable image generation paradigm. While Key and Value (KV) caching in large language models (LLMs) has been extensively studied, next-scale prediction presents unique challenges, and KV caching design for next-scale based VAR transformers remains largely unexplored. A major bottleneck is the excessive KV memory growth with the increasing number of scales-severely limiting scalability. Our systematic investigation reveals that: (1) Attending to tokens from local scales significantly contributes to generation quality (2) Allocating a small amount of memory for the coarsest scales, termed as condensed scales, stabilizes multi-scale image generation (3) Strong KV similarity across finer scales is predominantly observed in cache-efficient layers, whereas cache-demanding layers exhibit weaker inter-scale similarity. Based on the observations, we introduce AMS-KV, a scale-adaptive KV caching policy for next-scale prediction in VAR models. AMS-KV prioritizes storing KVs from condensed and local scales, preserving the most relevant tokens to maintain generation quality. It further optimizes KV cache utilization and computational efficiency identifying cache-demanding layers through inter-scale similarity analysis. Compared to the vanilla next-scale prediction-based VAR models, AMS-KV reduces KV cache usage by up to 84.83% and self-attention latency by 60.48%. Moreover, when the baseline VAR-d30 model encounters out-of-memory failures at a batch size of 128, AMS-KV enables stable scaling to a batch size of 256 with improved throughput."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-20T05:10:12Z",
                "published_parsed": [
                    2025,
                    11,
                    20,
                    5,
                    10,
                    12,
                    3,
                    324,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Boxun Xu"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Zihu Wang"
                    },
                    {
                        "name": "Peng Li"
                    }
                ],
                "author_detail": {
                    "name": "Peng Li"
                },
                "author": "Peng Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.16046v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.16046v1",
                "title": "Train Short, Infer Long: Speech-LLM Enables Zero-Shot Streamable Joint ASR and Diarization on Long Audio",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Train Short, Infer Long: Speech-LLM Enables Zero-Shot Streamable Joint ASR and Diarization on Long Audio"
                },
                "updated": "2025-11-20T05:07:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    20,
                    5,
                    7,
                    13,
                    3,
                    324,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.16046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.16046v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Joint automatic speech recognition (ASR) and speaker diarization aim to answer the question \"who spoke what\" in multi-speaker scenarios. In this paper, we present an end-to-end speech large language model (Speech-LLM) for Joint strEamable DIarization and aSr (JEDIS-LLM). The model is trained only on short audio under 20s but is capable of streamable inference on long-form audio without additional training. This is achieved by introducing a Speaker Prompt Cache (SPC) with an on-the-fly update mechanism during chunk-wise streaming inference, inspired by the autoregressive nature of LLMs. The SPC also allows the seamless use of pre-enrolled speaker profiles which is common in many scenarios like meeting transcription. To further enhance diarization capability, we incorporate word-level speaker supervision into the speech encoder during training. Experimental results demonstrate that our system outperforms strong baselines, including Sortformer and Meta-Cat in the local setting on audio up to 20s, and DiarizationLM on long-form audio, despite being fully end-to-end and streamable while DiarizationLM follows a cascaded offline pipeline. To the best of our knowledge, this is the first work enabling zero-shot streamable joint ASR and diarization on long audio using a Speech-LLM trained only on short audio, achieving state-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint automatic speech recognition (ASR) and speaker diarization aim to answer the question \"who spoke what\" in multi-speaker scenarios. In this paper, we present an end-to-end speech large language model (Speech-LLM) for Joint strEamable DIarization and aSr (JEDIS-LLM). The model is trained only on short audio under 20s but is capable of streamable inference on long-form audio without additional training. This is achieved by introducing a Speaker Prompt Cache (SPC) with an on-the-fly update mechanism during chunk-wise streaming inference, inspired by the autoregressive nature of LLMs. The SPC also allows the seamless use of pre-enrolled speaker profiles which is common in many scenarios like meeting transcription. To further enhance diarization capability, we incorporate word-level speaker supervision into the speech encoder during training. Experimental results demonstrate that our system outperforms strong baselines, including Sortformer and Meta-Cat in the local setting on audio up to 20s, and DiarizationLM on long-form audio, despite being fully end-to-end and streamable while DiarizationLM follows a cascaded offline pipeline. To the best of our knowledge, this is the first work enabling zero-shot streamable joint ASR and diarization on long audio using a Speech-LLM trained only on short audio, achieving state-of-the-art performance."
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-20T05:07:13Z",
                "published_parsed": [
                    2025,
                    11,
                    20,
                    5,
                    7,
                    13,
                    3,
                    324,
                    0
                ],
                "arxiv_comment": "Submitted to ICASSP2026",
                "arxiv_primary_category": {
                    "term": "eess.AS"
                },
                "authors": [
                    {
                        "name": "Mohan Shi"
                    },
                    {
                        "name": "Xiong Xiao"
                    },
                    {
                        "name": "Ruchao Fan"
                    },
                    {
                        "name": "Shaoshi Ling"
                    },
                    {
                        "name": "Jinyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinyu Li"
                },
                "author": "Jinyu Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.15028v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.15028v2",
                "title": "Data Layout Polymorphism for Bounding Volume Hierarchies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Layout Polymorphism for Bounding Volume Hierarchies"
                },
                "updated": "2025-11-20T04:37:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    20,
                    4,
                    37,
                    32,
                    3,
                    324,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.15028v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.15028v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Bounding volume hierarchies are ubiquitous acceleration structures in graphics, scientific computing, and data analytics. Their performance depends critically on data layout choices that affect cache utilization, memory bandwidth, and vectorization -- increasingly dominant factors in modern computing. Yet, in most programming systems, these layout choices are hopelessly entangled with the traversal logic. This entanglement prevents developers from independently optimizing data layouts and algorithms across different contexts, perpetuating a false dichotomy between performance and portability. We introduce Scion, a domain-specific language and compiler for specifying the data layouts of bounding volume hierarchies independent of tree traversal algorithms. We show that Scion can express a broad spectrum of layout optimizations used in high performance computing while remaining architecture-agnostic. We demonstrate empirically that Pareto-optimal layouts (along performance and memory footprint axes) vary across algorithms, architectures, and workload characteristics. Through systematic design exploration, we also identify a novel ray tracing layout that combines optimization techniques from prior work, achieving Pareto-optimality across diverse architectures and scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bounding volume hierarchies are ubiquitous acceleration structures in graphics, scientific computing, and data analytics. Their performance depends critically on data layout choices that affect cache utilization, memory bandwidth, and vectorization -- increasingly dominant factors in modern computing. Yet, in most programming systems, these layout choices are hopelessly entangled with the traversal logic. This entanglement prevents developers from independently optimizing data layouts and algorithms across different contexts, perpetuating a false dichotomy between performance and portability. We introduce Scion, a domain-specific language and compiler for specifying the data layouts of bounding volume hierarchies independent of tree traversal algorithms. We show that Scion can express a broad spectrum of layout optimizations used in high performance computing while remaining architecture-agnostic. We demonstrate empirically that Pareto-optimal layouts (along performance and memory footprint axes) vary across algorithms, architectures, and workload characteristics. Through systematic design exploration, we also identify a novel ray tracing layout that combines optimization techniques from prior work, achieving Pareto-optimality across diverse architectures and scenes."
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-19T01:49:42Z",
                "published_parsed": [
                    2025,
                    11,
                    19,
                    1,
                    49,
                    42,
                    2,
                    323,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL"
                },
                "authors": [
                    {
                        "name": "Christophe Gyurgyik"
                    },
                    {
                        "name": "Alexander J Root"
                    },
                    {
                        "name": "Fredrik Kjolstad"
                    }
                ],
                "author_detail": {
                    "name": "Fredrik Kjolstad"
                },
                "author": "Fredrik Kjolstad"
            },
            {
                "id": "http://arxiv.org/abs/2511.15927v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.15927v1",
                "title": "Breaking the Bottleneck with DiffuApriel: High-Throughput Diffusion LMs with Mamba Backbone",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Bottleneck with DiffuApriel: High-Throughput Diffusion LMs with Mamba Backbone"
                },
                "updated": "2025-11-19T23:23:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    19,
                    23,
                    23,
                    49,
                    2,
                    323,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.15927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.15927v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion-based language models have recently emerged as a promising alternative to autoregressive generation, yet their reliance on Transformer backbones limits inference efficiency due to quadratic attention and KV-cache overhead. In this work, we introduce DiffuApriel, a masked diffusion language model built on a bidirectional Mamba backbone that combines the diffusion objective with linear-time sequence modeling. DiffuApriel matches the performance of Transformer-based diffusion models while achieving up to 4.4x higher inference throughput for long sequences with a 1.3B model. We further propose DiffuApriel-H, a hybrid variant that interleaves attention and mamba layers, offering up to 2.6x throughput improvement with balanced global and local context modeling. Our results demonstrate that bidirectional state-space architectures serve as strong denoisers in masked diffusion LMs, providing a practical and scalable foundation for faster, memory-efficient text generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based language models have recently emerged as a promising alternative to autoregressive generation, yet their reliance on Transformer backbones limits inference efficiency due to quadratic attention and KV-cache overhead. In this work, we introduce DiffuApriel, a masked diffusion language model built on a bidirectional Mamba backbone that combines the diffusion objective with linear-time sequence modeling. DiffuApriel matches the performance of Transformer-based diffusion models while achieving up to 4.4x higher inference throughput for long sequences with a 1.3B model. We further propose DiffuApriel-H, a hybrid variant that interleaves attention and mamba layers, offering up to 2.6x throughput improvement with balanced global and local context modeling. Our results demonstrate that bidirectional state-space architectures serve as strong denoisers in masked diffusion LMs, providing a practical and scalable foundation for faster, memory-efficient text generation."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-19T23:23:49Z",
                "published_parsed": [
                    2025,
                    11,
                    19,
                    23,
                    23,
                    49,
                    2,
                    323,
                    0
                ],
                "arxiv_comment": "9 pages, 4 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Vaibhav Singh"
                    },
                    {
                        "name": "Oleksiy Ostapenko"
                    },
                    {
                        "name": "Pierre-Andr Nol"
                    },
                    {
                        "name": "Torsten Scholak"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Scholak"
                },
                "author": "Torsten Scholak"
            },
            {
                "id": "http://arxiv.org/abs/2511.15651v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.15651v1",
                "title": "Material processing by laser-plasma-filament-guided high voltage discharges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Material processing by laser-plasma-filament-guided high voltage discharges"
                },
                "updated": "2025-11-19T17:39:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    19,
                    17,
                    39,
                    12,
                    2,
                    323,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.15651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.15651v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We investigate ablation experiments performed by laser-plasma-filament guided electrical discharges at high-voltages of up to 145 kV. The guiding was accomplished via fs-laser-generated plasma filaments across a gap of 201 mm of air onto steel 1.3343 samples. This method combines remote material processing and enables the steering and deflection of high voltage discharges with the ease-of-use of remote laser processing technology. We observe an increase of the per-pulse-(and-discharge)-ablated volume by a factor of 1.67 over an ablation regime when the discharges are not present and up to a factor of 12.5 over the case when neither discharges nor plasma filaments, only a loosely focused laser beam, are present.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate ablation experiments performed by laser-plasma-filament guided electrical discharges at high-voltages of up to 145 kV. The guiding was accomplished via fs-laser-generated plasma filaments across a gap of 201 mm of air onto steel 1.3343 samples. This method combines remote material processing and enables the steering and deflection of high voltage discharges with the ease-of-use of remote laser processing technology. We observe an increase of the per-pulse-(and-discharge)-ablated volume by a factor of 1.67 over an ablation regime when the discharges are not present and up to a factor of 12.5 over the case when neither discharges nor plasma filaments, only a loosely focused laser beam, are present."
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-19T17:39:12Z",
                "published_parsed": [
                    2025,
                    11,
                    19,
                    17,
                    39,
                    12,
                    2,
                    323,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph"
                },
                "authors": [
                    {
                        "name": "Kristian Cvecek"
                    },
                    {
                        "name": "Markus Dring"
                    },
                    {
                        "name": "Alexander Romboy"
                    },
                    {
                        "name": "Johannes Heberle"
                    },
                    {
                        "name": "Michael Schmidt"
                    }
                ],
                "author_detail": {
                    "name": "Michael Schmidt"
                },
                "author": "Michael Schmidt"
            },
            {
                "id": "http://arxiv.org/abs/2511.15589v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.15589v1",
                "title": "EPSO: A Caching-Based Efficient Superoptimizer for BPF Bytecode",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPSO: A Caching-Based Efficient Superoptimizer for BPF Bytecode"
                },
                "updated": "2025-11-19T16:21:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    19,
                    16,
                    21,
                    20,
                    2,
                    323,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.15589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.15589v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Extended Berkeley Packet Filter (eBPF) allows developers to extend Linux kernel functionality without modifying its source code. To ensure system safety, an in-kernel safety checker, the verifier, enforces strict safety constraints (for example, a limited program size) on eBPF programs loaded into the kernel. These constraints, combined with eBPF's performance-critical use cases, make effective optimization essential. However, existing compilers (such as Clang) offer limited optimization support, and many semantics-preserving transformations are rejected by the verifier, which makes handcrafted optimization rule design both challenging and limited in effectiveness. Superoptimization overcomes the limitations of rule-based methods by automatically discovering optimal transformations, but its high computational cost limits scalability. To address this, we propose EPSO, a caching-based superoptimizer that discovers rewrite rules via offline superoptimization and reuses them to achieve high-quality optimizations with minimal runtime overhead. We evaluate EPSO on benchmarks from the Linux kernel and several eBPF-based projects, including Cilium, Katran, hXDP, Sysdig, Tetragon, and Tracee. EPSO discovers 795 rewrite rules and achieves up to 68.87 percent (average 24.37 percent) reduction in program size compared to Clang's output, outperforming the state-of-the-art BPF optimizer K2 on all benchmarks and Merlin on 92.68 percent of them. Additionally, EPSO reduces program runtime by an average of 6.60 percent, improving throughput and lowering latency in network applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extended Berkeley Packet Filter (eBPF) allows developers to extend Linux kernel functionality without modifying its source code. To ensure system safety, an in-kernel safety checker, the verifier, enforces strict safety constraints (for example, a limited program size) on eBPF programs loaded into the kernel. These constraints, combined with eBPF's performance-critical use cases, make effective optimization essential. However, existing compilers (such as Clang) offer limited optimization support, and many semantics-preserving transformations are rejected by the verifier, which makes handcrafted optimization rule design both challenging and limited in effectiveness. Superoptimization overcomes the limitations of rule-based methods by automatically discovering optimal transformations, but its high computational cost limits scalability. To address this, we propose EPSO, a caching-based superoptimizer that discovers rewrite rules via offline superoptimization and reuses them to achieve high-quality optimizations with minimal runtime overhead. We evaluate EPSO on benchmarks from the Linux kernel and several eBPF-based projects, including Cilium, Katran, hXDP, Sysdig, Tetragon, and Tracee. EPSO discovers 795 rewrite rules and achieves up to 68.87 percent (average 24.37 percent) reduction in program size compared to Clang's output, outperforming the state-of-the-art BPF optimizer K2 on all benchmarks and Merlin on 92.68 percent of them. Additionally, EPSO reduces program runtime by an average of 6.60 percent, improving throughput and lowering latency in network applications."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-19T16:21:20Z",
                "published_parsed": [
                    2025,
                    11,
                    19,
                    16,
                    21,
                    20,
                    2,
                    323,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Qian Zhu"
                    },
                    {
                        "name": "Yuxuan Liu"
                    },
                    {
                        "name": "Ziyuan Zhu"
                    },
                    {
                        "name": "Shangqing Liu"
                    },
                    {
                        "name": "Lei Bu"
                    }
                ],
                "author_detail": {
                    "name": "Lei Bu"
                },
                "author": "Lei Bu"
            },
            {
                "id": "http://arxiv.org/abs/2511.15557v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.15557v1",
                "title": "B+ANN: A Fast Billion-Scale Disk-based Nearest-Neighbor Index",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "B+ANN: A Fast Billion-Scale Disk-based Nearest-Neighbor Index"
                },
                "updated": "2025-11-19T15:50:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    19,
                    15,
                    50,
                    28,
                    2,
                    323,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.15557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.15557v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Storing and processing of embedding vectors by specialized Vector databases (VDBs) has become the linchpin in building modern AI pipelines. Most current VDBs employ variants of a graph-based ap- proximate nearest-neighbor (ANN) index algorithm, HNSW, to an- swer semantic queries over stored vectors. Inspite of its wide-spread use, the HNSW algorithm suffers from several issues: in-memory design and implementation, random memory accesses leading to degradation in cache behavior, limited acceleration scope due to fine-grained pairwise computations, and support of only semantic similarity queries. In this paper, we present a novel disk-based ANN index, B+ANN, to address these issues: it first partitions input data into blocks containing semantically similar items, then builds an B+ tree variant to store blocks both in-memory and on disks, and finally, enables hybrid edge- and block-based in-memory traversals. As demonstrated by our experimantal evaluation, the proposed B+ANN disk-based index improves both quality (Recall value), and execution performance (Queries per second/QPS) over HNSW, by improving spatial and temporal locality for semantic operations, reducing cache misses (19.23% relative gain), and decreasing the memory consumption and disk-based build time by 24x over the DiskANN algorithm. Finally, it enables dissimilarity queries, which are not supported by similarity-oriented ANN indices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Storing and processing of embedding vectors by specialized Vector databases (VDBs) has become the linchpin in building modern AI pipelines. Most current VDBs employ variants of a graph-based ap- proximate nearest-neighbor (ANN) index algorithm, HNSW, to an- swer semantic queries over stored vectors. Inspite of its wide-spread use, the HNSW algorithm suffers from several issues: in-memory design and implementation, random memory accesses leading to degradation in cache behavior, limited acceleration scope due to fine-grained pairwise computations, and support of only semantic similarity queries. In this paper, we present a novel disk-based ANN index, B+ANN, to address these issues: it first partitions input data into blocks containing semantically similar items, then builds an B+ tree variant to store blocks both in-memory and on disks, and finally, enables hybrid edge- and block-based in-memory traversals. As demonstrated by our experimantal evaluation, the proposed B+ANN disk-based index improves both quality (Recall value), and execution performance (Queries per second/QPS) over HNSW, by improving spatial and temporal locality for semantic operations, reducing cache misses (19.23% relative gain), and decreasing the memory consumption and disk-based build time by 24x over the DiskANN algorithm. Finally, it enables dissimilarity queries, which are not supported by similarity-oriented ANN indices."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-19T15:50:28Z",
                "published_parsed": [
                    2025,
                    11,
                    19,
                    15,
                    50,
                    28,
                    2,
                    323,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Selim Furkan Tekin"
                    },
                    {
                        "name": "Rajesh Bordawekar"
                    }
                ],
                "author_detail": {
                    "name": "Rajesh Bordawekar"
                },
                "author": "Rajesh Bordawekar"
            },
            {
                "id": "http://arxiv.org/abs/2508.08343v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.08343v3",
                "title": "A Data-driven ML Approach for Maximizing Performance in LLM-Adapter Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Data-driven ML Approach for Maximizing Performance in LLM-Adapter Serving"
                },
                "updated": "2025-11-19T13:36:14Z",
                "updated_parsed": [
                    2025,
                    11,
                    19,
                    13,
                    36,
                    14,
                    2,
                    323,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.08343v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.08343v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the rapid adoption of Large Language Models (LLMs), LLM-adapters have become increasingly common, providing lightweight specialization of large-scale models. Serving hundreds or thousands of these adapters on a single GPU allows request aggregation, increasing throughput, but may also cause request starvation if GPU memory limits are exceeded. To address this issue, this study focuses on determining the joint configuration of concurrent and parallel adapters that maximizes GPU throughput without inducing starvation, given heterogeneous adapter and traffic properties. We propose a data-driven ML approach leveraging interpretable models to tackle this caching problem and introduce the first Digital Twin capable of reproducing an LLM-adapter serving system, enabling efficient training data generation. Experiments with the vLLM framework and LoRA adapters show that the Digital Twin reproduces throughput within 5.1% of real results, while the ML approach predicts optimal numbers of concurrent and parallel adapters with an error of at most 7.2% under heterogeneous, real-world workloads. The code is publicly available at https://github.com/FerranAgulloLopez/GPULLMAdapterOptimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid adoption of Large Language Models (LLMs), LLM-adapters have become increasingly common, providing lightweight specialization of large-scale models. Serving hundreds or thousands of these adapters on a single GPU allows request aggregation, increasing throughput, but may also cause request starvation if GPU memory limits are exceeded. To address this issue, this study focuses on determining the joint configuration of concurrent and parallel adapters that maximizes GPU throughput without inducing starvation, given heterogeneous adapter and traffic properties. We propose a data-driven ML approach leveraging interpretable models to tackle this caching problem and introduce the first Digital Twin capable of reproducing an LLM-adapter serving system, enabling efficient training data generation. Experiments with the vLLM framework and LoRA adapters show that the Digital Twin reproduces throughput within 5.1% of real results, while the ML approach predicts optimal numbers of concurrent and parallel adapters with an error of at most 7.2% under heterogeneous, real-world workloads. The code is publicly available at https://github.com/FerranAgulloLopez/GPULLMAdapterOptimization."
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-11T10:47:35Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    47,
                    35,
                    0,
                    223,
                    0
                ],
                "arxiv_comment": "Accepted in a computer science workshop",
                "arxiv_primary_category": {
                    "term": "cs.PF"
                },
                "authors": [
                    {
                        "name": "Ferran Agullo"
                    },
                    {
                        "name": "Joan Oliveras"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Alberto Gutierrez-Torre"
                    },
                    {
                        "name": "Olivier Tardieu"
                    },
                    {
                        "name": "Alaa Youssef"
                    },
                    {
                        "name": "Jordi Torres"
                    },
                    {
                        "name": "Josep Ll. Berral"
                    }
                ],
                "author_detail": {
                    "name": "Josep Ll. Berral"
                },
                "author": "Josep Ll. Berral"
            },
            {
                "id": "http://arxiv.org/abs/2509.03951v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.03951v3",
                "title": "ANTS: Adaptive Negative Textual Space Shaping for OOD Detection via Test-Time MLLM Understanding and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ANTS: Adaptive Negative Textual Space Shaping for OOD Detection via Test-Time MLLM Understanding and Reasoning"
                },
                "updated": "2025-11-19T13:23:53Z",
                "updated_parsed": [
                    2025,
                    11,
                    19,
                    13,
                    23,
                    53,
                    2,
                    323,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.03951v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.03951v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The introduction of negative labels (NLs) has proven effective in enhancing Out-of-Distribution (OOD) detection. However, existing methods often lack an understanding of OOD images, making it difficult to construct an accurate negative space. Furthermore, the absence of negative labels semantically similar to ID labels constrains their capability in near-OOD detection. To address these issues, we propose shaping an Adaptive Negative Textual Space (ANTS) by leveraging the understanding and reasoning capabilities of multimodal large language models (MLLMs). Specifically, we cache images likely to be OOD samples from the historical test images and prompt the MLLM to describe these images, generating expressive negative sentences that precisely characterize the OOD distribution and enhance far-OOD detection. For the near-OOD setting, where OOD samples resemble the in-distribution (ID) subset, we cache the subset of ID classes that are visually similar to historical test images and then leverage MLLM reasoning to generate visually similar negative labels tailored to this subset, effectively reducing false negatives and improving near-OOD detection. To balance these two types of negative textual spaces, we design an adaptive weighted score that enables the method to handle different OOD task settings (near-OOD and far-OOD), making it highly adaptable in open environments. On the ImageNet benchmark, our ANTS significantly reduces the FPR95 by 3.1\\%, establishing a new state-of-the-art. Furthermore, our method is training-free and zero-shot, enabling high scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The introduction of negative labels (NLs) has proven effective in enhancing Out-of-Distribution (OOD) detection. However, existing methods often lack an understanding of OOD images, making it difficult to construct an accurate negative space. Furthermore, the absence of negative labels semantically similar to ID labels constrains their capability in near-OOD detection. To address these issues, we propose shaping an Adaptive Negative Textual Space (ANTS) by leveraging the understanding and reasoning capabilities of multimodal large language models (MLLMs). Specifically, we cache images likely to be OOD samples from the historical test images and prompt the MLLM to describe these images, generating expressive negative sentences that precisely characterize the OOD distribution and enhance far-OOD detection. For the near-OOD setting, where OOD samples resemble the in-distribution (ID) subset, we cache the subset of ID classes that are visually similar to historical test images and then leverage MLLM reasoning to generate visually similar negative labels tailored to this subset, effectively reducing false negatives and improving near-OOD detection. To balance these two types of negative textual spaces, we design an adaptive weighted score that enables the method to handle different OOD task settings (near-OOD and far-OOD), making it highly adaptable in open environments. On the ImageNet benchmark, our ANTS significantly reduces the FPR95 by 3.1\\%, establishing a new state-of-the-art. Furthermore, our method is training-free and zero-shot, enabling high scalability."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-04T07:26:20Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    7,
                    26,
                    20,
                    3,
                    247,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Wenjie Zhu"
                    },
                    {
                        "name": "Yabin Zhang"
                    },
                    {
                        "name": "Xin Jin"
                    },
                    {
                        "name": "Wenjun Zeng"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.15367v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.15367v1",
                "title": "DARE: An Irregularity-Tolerant Matrix Processing Unit with a Densifying ISA and Filtered Runahead Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DARE: An Irregularity-Tolerant Matrix Processing Unit with a Densifying ISA and Filtered Runahead Execution"
                },
                "updated": "2025-11-19T11:52:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    19,
                    11,
                    52,
                    3,
                    2,
                    323,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.15367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.15367v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deep Neural Networks (DNNs) are widely applied across domains and have shown strong effectiveness. As DNN workloads increasingly run on CPUs, dedicated Matrix Processing Units (MPUs) and Matrix Instruction Set Architectures (ISAs) have been introduced. At the same time, sparsity techniques are widely adopted in algorithms to reduce computational cost.\n  Despite these advances, insufficient hardware-algorithm co-optimization leads to suboptimal performance. On the memory side, sparse DNNs incur irregular access patterns that cause high cache miss rates. While runahead execution is a promising prefetching technique, its direct application to MPUs is often ineffective due to significant prefetch redundancy. On the compute side, stride constraints in current Matrix ISAs prevent the densification of multiple logically related sparse operations, resulting in poor utilization of MPU processing elements.\n  To address these irregularities, we propose DARE, an irregularity-tolerant MPU with a Densifying ISA and filtered Runahead Execution. DARE extends the ISA to support densifying sparse operations and equips a lightweight runahead mechanism with filtering capability. Experimental results show that DARE improves performance by 1.04$\\times$ to 4.44$\\times$ and increases energy efficiency by 1.00$\\times$ to 22.8$\\times$ over the baseline, with 3.91$\\times$ lower hardware overhead than NVR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Neural Networks (DNNs) are widely applied across domains and have shown strong effectiveness. As DNN workloads increasingly run on CPUs, dedicated Matrix Processing Units (MPUs) and Matrix Instruction Set Architectures (ISAs) have been introduced. At the same time, sparsity techniques are widely adopted in algorithms to reduce computational cost.\n  Despite these advances, insufficient hardware-algorithm co-optimization leads to suboptimal performance. On the memory side, sparse DNNs incur irregular access patterns that cause high cache miss rates. While runahead execution is a promising prefetching technique, its direct application to MPUs is often ineffective due to significant prefetch redundancy. On the compute side, stride constraints in current Matrix ISAs prevent the densification of multiple logically related sparse operations, resulting in poor utilization of MPU processing elements.\n  To address these irregularities, we propose DARE, an irregularity-tolerant MPU with a Densifying ISA and filtered Runahead Execution. DARE extends the ISA to support densifying sparse operations and equips a lightweight runahead mechanism with filtering capability. Experimental results show that DARE improves performance by 1.04$\\times$ to 4.44$\\times$ and increases energy efficiency by 1.00$\\times$ to 22.8$\\times$ over the baseline, with 3.91$\\times$ lower hardware overhead than NVR."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-19T11:52:03Z",
                "published_parsed": [
                    2025,
                    11,
                    19,
                    11,
                    52,
                    3,
                    2,
                    323,
                    0
                ],
                "arxiv_comment": "8 pages, 9 figures, accepted to DATE 2026",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Xin Yang"
                    },
                    {
                        "name": "Xin Fan"
                    },
                    {
                        "name": "Zengshi Wang"
                    },
                    {
                        "name": "Jun Han"
                    }
                ],
                "author_detail": {
                    "name": "Jun Han"
                },
                "author": "Jun Han"
            },
            {
                "id": "http://arxiv.org/abs/2508.18983v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.18983v2",
                "title": "Enabling MoE on the Edge via Importance-Driven Expert Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling MoE on the Edge via Importance-Driven Expert Scheduling"
                },
                "updated": "2025-11-19T06:48:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    19,
                    6,
                    48,
                    57,
                    2,
                    323,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.18983v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.18983v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Mixture of Experts (MoE) architecture has emerged as a key technique for scaling Large Language Models by activating only a subset of experts per query. Deploying MoE on consumer-grade edge hardware, however, is constrained by limited device memory, making dynamic expert offloading essential. Unlike prior work that treats offloading purely as a scheduling problem, we leverage expert importance to guide decisions, substituting low-importance activated experts with functionally similar ones already cached in GPU memory, thereby preserving accuracy. As a result, this design reduces memory usage and data transfer, while largely eliminating PCIe overhead. In addition, we introduce a scheduling policy that maximizes the reuse ratio of GPU-cached experts, further boosting efficiency. Extensive evaluations show that our approach delivers 48% lower decoding latency with over 60% expert cache hit rate, while maintaining nearly lossless accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) architecture has emerged as a key technique for scaling Large Language Models by activating only a subset of experts per query. Deploying MoE on consumer-grade edge hardware, however, is constrained by limited device memory, making dynamic expert offloading essential. Unlike prior work that treats offloading purely as a scheduling problem, we leverage expert importance to guide decisions, substituting low-importance activated experts with functionally similar ones already cached in GPU memory, thereby preserving accuracy. As a result, this design reduces memory usage and data transfer, while largely eliminating PCIe overhead. In addition, we introduce a scheduling policy that maximizes the reuse ratio of GPU-cached experts, further boosting efficiency. Extensive evaluations show that our approach delivers 48% lower decoding latency with over 60% expert cache hit rate, while maintaining nearly lossless accuracy."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-26T12:32:09Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    32,
                    9,
                    1,
                    238,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Guoying Zhu"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Haipeng Dai"
                    },
                    {
                        "name": "Xuechen Liu"
                    },
                    {
                        "name": "Weijun Wang"
                    },
                    {
                        "name": "Keran Li"
                    },
                    {
                        "name": "Jun xiao"
                    },
                    {
                        "name": "Ligeng Chen"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang"
            },
            {
                "id": "http://arxiv.org/abs/2511.14400v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.14400v2",
                "title": "PIM or CXL-PIM? Understanding Architectural Trade-offs Through Large-Scale Benchmarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM or CXL-PIM? Understanding Architectural Trade-offs Through Large-Scale Benchmarking"
                },
                "updated": "2025-11-19T04:13:40Z",
                "updated_parsed": [
                    2025,
                    11,
                    19,
                    4,
                    13,
                    40,
                    2,
                    323,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.14400v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.14400v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Processing-in-memory (PIM) reduces data movement by executing near memory, but our large-scale characterization on real PIM hardware shows that end-to-end performance is often limited by disjoint host and device address spaces that force explicit staging transfers. In contrast, CXL-PIM provides a unified address space and cache-coherent access at the cost of higher access latency. These opposing interface models create workload-dependent tradeoffs that are not captured by small-scale studies. This work presents a side-by-side, large-scale comparison of PIM and CXL-PIM using measurements from real PIM hardware and trace-driven CXL modeling. We identify when unified-address access amortizes link latency enough to overcome transfer bottlenecks, and when tightly coupled PIM remains preferable. Our results reveal phase- and dataset-size regimes in which the relative ranking between the two architectures reverses, offering practical guidance for future near-memory system design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing-in-memory (PIM) reduces data movement by executing near memory, but our large-scale characterization on real PIM hardware shows that end-to-end performance is often limited by disjoint host and device address spaces that force explicit staging transfers. In contrast, CXL-PIM provides a unified address space and cache-coherent access at the cost of higher access latency. These opposing interface models create workload-dependent tradeoffs that are not captured by small-scale studies. This work presents a side-by-side, large-scale comparison of PIM and CXL-PIM using measurements from real PIM hardware and trace-driven CXL modeling. We identify when unified-address access amortizes link latency enough to overcome transfer bottlenecks, and when tightly coupled PIM remains preferable. Our results reveal phase- and dataset-size regimes in which the relative ranking between the two architectures reverses, offering practical guidance for future near-memory system design."
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-18T12:05:31Z",
                "published_parsed": [
                    2025,
                    11,
                    18,
                    12,
                    5,
                    31,
                    1,
                    322,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET"
                },
                "authors": [
                    {
                        "name": "I-Ting Lee"
                    },
                    {
                        "name": "Bao-Kai Wang"
                    },
                    {
                        "name": "Liang-Chi Chen"
                    },
                    {
                        "name": "Wen Sheng Lim"
                    },
                    {
                        "name": "Da-Wei Chang"
                    },
                    {
                        "name": "Yu-Ming Chang"
                    },
                    {
                        "name": "Chieng-Chung Ho"
                    }
                ],
                "author_detail": {
                    "name": "Chieng-Chung Ho"
                },
                "author": "Chieng-Chung Ho"
            },
            {
                "id": "http://arxiv.org/abs/2511.15098v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.15098v1",
                "title": "A Comprehensive Study on Visual Token Redundancy for Discrete Diffusion-based Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Study on Visual Token Redundancy for Discrete Diffusion-based Multimodal Large Language Models"
                },
                "updated": "2025-11-19T04:13:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    19,
                    4,
                    13,
                    36,
                    2,
                    323,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.15098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.15098v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Discrete diffusion-based multimodal large language models (dMLLMs) have emerged as a promising alternative to autoregressive MLLMs thanks to their advantages in parallel decoding and bidirectional context modeling, but most existing dMLLMs incur significant computational overhead during inference due to the full-sequence attention computation in each denoising step. Pioneer studies attempt to resolve this issue from a modality-agnostic perspective via key-value cache optimization or efficient sampling but most of them overlook modality-specific visual token redundancy. In this work, we conduct a comprehensive study on how visual token redundancy evolves with different dMLLM architectures and tasks and how visual token pruning affects dMLLM responses and efficiency. Specifically, our study reveals that visual redundancy emerges only in from-scratch dMLLMs while handling long-answer tasks. In addition, we validate that visual token pruning introduces non-negligible information loss in dMLLMs and only from-scratch dMLLMs can recover the lost information progressively during late denoising steps. Furthermore, our study shows that layer-skipping is promising for accelerating AR-to-diffusion dMLLMs, whereas progressive or late-step pruning is more effective for from-scratch dMLLMs. Overall, this work offers a new perspective on efficiency optimization for dMLLMs, greatly advancing their applicability across various multimodal understanding tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete diffusion-based multimodal large language models (dMLLMs) have emerged as a promising alternative to autoregressive MLLMs thanks to their advantages in parallel decoding and bidirectional context modeling, but most existing dMLLMs incur significant computational overhead during inference due to the full-sequence attention computation in each denoising step. Pioneer studies attempt to resolve this issue from a modality-agnostic perspective via key-value cache optimization or efficient sampling but most of them overlook modality-specific visual token redundancy. In this work, we conduct a comprehensive study on how visual token redundancy evolves with different dMLLM architectures and tasks and how visual token pruning affects dMLLM responses and efficiency. Specifically, our study reveals that visual redundancy emerges only in from-scratch dMLLMs while handling long-answer tasks. In addition, we validate that visual token pruning introduces non-negligible information loss in dMLLMs and only from-scratch dMLLMs can recover the lost information progressively during late denoising steps. Furthermore, our study shows that layer-skipping is promising for accelerating AR-to-diffusion dMLLMs, whereas progressive or late-step pruning is more effective for from-scratch dMLLMs. Overall, this work offers a new perspective on efficiency optimization for dMLLMs, greatly advancing their applicability across various multimodal understanding tasks."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-19T04:13:36Z",
                "published_parsed": [
                    2025,
                    11,
                    19,
                    4,
                    13,
                    36,
                    2,
                    323,
                    0
                ],
                "arxiv_comment": "14 pages, 2 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Duo Li"
                    },
                    {
                        "name": "Zuhao Yang"
                    },
                    {
                        "name": "Xiaoqin Zhang"
                    },
                    {
                        "name": "Ling Shao"
                    },
                    {
                        "name": "Shijian Lu"
                    }
                ],
                "author_detail": {
                    "name": "Shijian Lu"
                },
                "author": "Shijian Lu"
            },
            {
                "id": "http://arxiv.org/abs/2511.14974v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.14974v1",
                "title": "Electric-Field-Dependent Thermal Conductivity in Fresh and Aged Bulk Single Crystalline $\\mathrm{BaTiO_3}$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric-Field-Dependent Thermal Conductivity in Fresh and Aged Bulk Single Crystalline $\\mathrm{BaTiO_3}$"
                },
                "updated": "2025-11-18T23:42:50Z",
                "updated_parsed": [
                    2025,
                    11,
                    18,
                    23,
                    42,
                    50,
                    1,
                    322,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.14974v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.14974v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Active thermal management requires advances in thermal switching materials, whose thermal conductivity responds to external stimuli. The electric field, as one of the most convenient and effective stimuli, has shown great potential in tuning the thermal conductivity of ferroelectric materials. While previous studies on electric-field-induced ferroelectric thermal switching have primarily focused on thin films and bulk solid solutions with strong extrinsic interface and defect scatterings, bulk single crystals, which can offer clear insights into intrinsic thermal switching mechanisms, have received comparatively less attention. Here, we demonstrate electric-field-induced thermal switching in bulk single-crystalline $\\mathrm{BaTiO_3}$ (BTO) at room temperature and elucidate the critical role of domain evolution and aging in governing heat transport. Using a customized steady-state platform with in-situ electric fields up to $\\pm$10 kV/cm, we observe a modulation of thermal conductivity up to 35% in fresh BTO driven by polarization reorientation and domain restructuring. First-principles finite-temperature lattice-dynamics calculations confirm that the switching behavior primarily originates from anisotropic phonon transport associated with domain configuration rather than strain-induced changes in phonon velocities. We further reveal that both ambient aging and controlled thermal aging can enhance the switching contrast through the formation and alignment of defect dipoles that modulate phonon-defect scattering. These results establish defect-domain interactions as a powerful design parameter for ferroelectric thermal switches and demonstrate a versatile experimental platform for exploring field-tunable heat transport and phase behavior in bulk functional materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active thermal management requires advances in thermal switching materials, whose thermal conductivity responds to external stimuli. The electric field, as one of the most convenient and effective stimuli, has shown great potential in tuning the thermal conductivity of ferroelectric materials. While previous studies on electric-field-induced ferroelectric thermal switching have primarily focused on thin films and bulk solid solutions with strong extrinsic interface and defect scatterings, bulk single crystals, which can offer clear insights into intrinsic thermal switching mechanisms, have received comparatively less attention. Here, we demonstrate electric-field-induced thermal switching in bulk single-crystalline $\\mathrm{BaTiO_3}$ (BTO) at room temperature and elucidate the critical role of domain evolution and aging in governing heat transport. Using a customized steady-state platform with in-situ electric fields up to $\\pm$10 kV/cm, we observe a modulation of thermal conductivity up to 35% in fresh BTO driven by polarization reorientation and domain restructuring. First-principles finite-temperature lattice-dynamics calculations confirm that the switching behavior primarily originates from anisotropic phonon transport associated with domain configuration rather than strain-induced changes in phonon velocities. We further reveal that both ambient aging and controlled thermal aging can enhance the switching contrast through the formation and alignment of defect dipoles that modulate phonon-defect scattering. These results establish defect-domain interactions as a powerful design parameter for ferroelectric thermal switches and demonstrate a versatile experimental platform for exploring field-tunable heat transport and phase behavior in bulk functional materials."
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-18T23:42:50Z",
                "published_parsed": [
                    2025,
                    11,
                    18,
                    23,
                    42,
                    50,
                    1,
                    322,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci"
                },
                "authors": [
                    {
                        "name": "Fanghao Zhang"
                    },
                    {
                        "name": "Guanchun Rui"
                    },
                    {
                        "name": "Yujie Quan"
                    },
                    {
                        "name": "Shantal Adajian"
                    },
                    {
                        "name": "Matthew Delmont"
                    },
                    {
                        "name": "Q. M. Zhang"
                    },
                    {
                        "name": "Bolin Liao"
                    }
                ],
                "author_detail": {
                    "name": "Bolin Liao"
                },
                "author": "Bolin Liao"
            },
            {
                "id": "http://arxiv.org/abs/2511.12201v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.12201v2",
                "title": "OmniSparse: Training-Aware Fine-Grained Sparse Attention for Long-Video MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniSparse: Training-Aware Fine-Grained Sparse Attention for Long-Video MLLMs"
                },
                "updated": "2025-11-18T23:07:41Z",
                "updated_parsed": [
                    2025,
                    11,
                    18,
                    23,
                    7,
                    41,
                    1,
                    322,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.12201v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.12201v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Existing sparse attention methods primarily target inference-time acceleration by selecting critical tokens under predefined sparsity patterns. However, they often fail to bridge the training-inference gap and lack the capacity for fine-grained token selection across multiple dimensions such as queries, key-values (KV), and heads, leading to suboptimal performance and limited acceleration gains. In this paper, we introduce OmniSparse, a training-aware fine-grained sparse attention framework for long-video MLLMs, which operates in both training and inference with dynamic token budget allocation. Specifically, OmniSparse contains three adaptive and complementary mechanisms: (1) query selection via lazy-active classification, retaining active queries that capture broad semantic similarity while discarding most lazy ones that focus on limited local context and exhibit high functional redundancy; (2) KV selection with head-level dynamic budget allocation, where a shared budget is determined based on the flattest head and applied uniformly across all heads to ensure attention recall; and (3) KV cache slimming to reduce head-level redundancy by selectively fetching visual KV cache according to the head-level decoding query pattern. Experimental results show that OmniSparse matches the performance of full attention while achieving up to 2.7x speedup during prefill and 2.4x memory reduction during decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing sparse attention methods primarily target inference-time acceleration by selecting critical tokens under predefined sparsity patterns. However, they often fail to bridge the training-inference gap and lack the capacity for fine-grained token selection across multiple dimensions such as queries, key-values (KV), and heads, leading to suboptimal performance and limited acceleration gains. In this paper, we introduce OmniSparse, a training-aware fine-grained sparse attention framework for long-video MLLMs, which operates in both training and inference with dynamic token budget allocation. Specifically, OmniSparse contains three adaptive and complementary mechanisms: (1) query selection via lazy-active classification, retaining active queries that capture broad semantic similarity while discarding most lazy ones that focus on limited local context and exhibit high functional redundancy; (2) KV selection with head-level dynamic budget allocation, where a shared budget is determined based on the flattest head and applied uniformly across all heads to ensure attention recall; and (3) KV cache slimming to reduce head-level redundancy by selectively fetching visual KV cache according to the head-level decoding query pattern. Experimental results show that OmniSparse matches the performance of full attention while achieving up to 2.7x speedup during prefill and 2.4x memory reduction during decoding."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-15T13:14:17Z",
                "published_parsed": [
                    2025,
                    11,
                    15,
                    13,
                    14,
                    17,
                    5,
                    319,
                    0
                ],
                "arxiv_comment": "Accepted by AAAI2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Shaoxuan He"
                    },
                    {
                        "name": "Yuanyu He"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Lequan Lin"
                    },
                    {
                        "name": "Akide Liu"
                    },
                    {
                        "name": "Zhaoyang Li"
                    },
                    {
                        "name": "Jiyuan Zhang"
                    },
                    {
                        "name": "Zhenbang Sun"
                    },
                    {
                        "name": "Bohan Zhuang"
                    },
                    {
                        "name": "Qi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Wu"
                },
                "author": "Qi Wu"
            },
            {
                "id": "http://arxiv.org/abs/2507.04416v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.04416v3",
                "title": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based Sequence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based Sequence Modeling"
                },
                "updated": "2025-11-18T22:29:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    18,
                    22,
                    29,
                    13,
                    1,
                    322,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.04416v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.04416v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Transformers have become the cornerstone of modern large-scale language models, but their reliance on softmax attention poses a computational bottleneck at both training and inference. Recurrent models offer high efficiency, but compressing the full sequence into a fixed-size and holistic representation can suffer from memory degradation in long contexts and limit fine-grained retrieval. To address this, we propose RAT, an intermediate design that bridges the efficiency of RNNs and capacity of attention. RAT partitions the input into chunks, applies recurrence within each chunk for local dependencies, and softmax-based attention across chunks for long-range interactions. This design mitigates memory degradation and enables direct access to distant tokens, while retaining computational efficiency. Empirically, with a chunk size of 16, the RAT block achieves a 7$\\times$ improvement in training speed for 100K sequence length and 9$times$ in generation at the 4K position, while maintaining similar performance compared to standard attention. We demonstrate this by training 1.3B parameter models from scratch and performing large-scale evaluations, including short- and long-context benchmarks, as well as supervised fine-tuning~(SFT). We further propose a hybrid architecture that interleaves RAT with local attention. By combining efficient long-range modeling with strong local interactions, this hybrid design not only improves inference speed and reduces cache memory usage, but also consistently enhances performance and shows the overall best results. Code is available at https://github.com/CLAIRE-Labo/RAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the cornerstone of modern large-scale language models, but their reliance on softmax attention poses a computational bottleneck at both training and inference. Recurrent models offer high efficiency, but compressing the full sequence into a fixed-size and holistic representation can suffer from memory degradation in long contexts and limit fine-grained retrieval. To address this, we propose RAT, an intermediate design that bridges the efficiency of RNNs and capacity of attention. RAT partitions the input into chunks, applies recurrence within each chunk for local dependencies, and softmax-based attention across chunks for long-range interactions. This design mitigates memory degradation and enables direct access to distant tokens, while retaining computational efficiency. Empirically, with a chunk size of 16, the RAT block achieves a 7$\\times$ improvement in training speed for 100K sequence length and 9$times$ in generation at the 4K position, while maintaining similar performance compared to standard attention. We demonstrate this by training 1.3B parameter models from scratch and performing large-scale evaluations, including short- and long-context benchmarks, as well as supervised fine-tuning~(SFT). We further propose a hybrid architecture that interleaves RAT with local attention. By combining efficient long-range modeling with strong local interactions, this hybrid design not only improves inference speed and reduces cache memory usage, but also consistently enhances performance and shows the overall best results. Code is available at https://github.com/CLAIRE-Labo/RAT."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-06T15:08:49Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Xiuying Wei"
                    },
                    {
                        "name": "Anunay Yadav"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre"
            },
            {
                "id": "http://arxiv.org/abs/2511.14927v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.14927v1",
                "title": "CPSL: Representing Volumetric Video via Content-Promoted Scene Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CPSL: Representing Volumetric Video via Content-Promoted Scene Layers"
                },
                "updated": "2025-11-18T21:26:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    18,
                    21,
                    26,
                    13,
                    1,
                    322,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.14927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.14927v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Volumetric video enables immersive and interactive visual experiences by supporting free viewpoint exploration and realistic motion parallax. However, existing volumetric representations from explicit point clouds to implicit neural fields, remain costly in capture, computation, and rendering, which limits their scalability for on-demand video and reduces their feasibility for real-time communication.\n  To bridge this gap, we propose Content-Promoted Scene Layers (CPSL), a compact 2.5D video representation that brings the perceptual benefits of volumetric video to conventional 2D content. Guided by per-frame depth and content saliency, CPSL decomposes each frame into a small set of geometry-consistent layers equipped with soft alpha bands and an edge-depth cache that jointly preserve occlusion ordering and boundary continuity. These lightweight, 2D-encodable assets enable parallax-corrected novel-view synthesis via depth-weighted warping and front-to-back alpha compositing, bypassing expensive 3D reconstruction. Temporally, CPSL maintains inter-frame coherence using motion-guided propagation and per-layer encoding, supporting real-time playback with standard video codecs. Across multiple benchmarks, CPSL achieves superior perceptual quality and boundary fidelity compared with layer-based and neural-field baselines while reducing storage and rendering cost by several folds. Our approach offer a practical path from 2D video to scalable 2.5D immersive media.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Volumetric video enables immersive and interactive visual experiences by supporting free viewpoint exploration and realistic motion parallax. However, existing volumetric representations from explicit point clouds to implicit neural fields, remain costly in capture, computation, and rendering, which limits their scalability for on-demand video and reduces their feasibility for real-time communication.\n  To bridge this gap, we propose Content-Promoted Scene Layers (CPSL), a compact 2.5D video representation that brings the perceptual benefits of volumetric video to conventional 2D content. Guided by per-frame depth and content saliency, CPSL decomposes each frame into a small set of geometry-consistent layers equipped with soft alpha bands and an edge-depth cache that jointly preserve occlusion ordering and boundary continuity. These lightweight, 2D-encodable assets enable parallax-corrected novel-view synthesis via depth-weighted warping and front-to-back alpha compositing, bypassing expensive 3D reconstruction. Temporally, CPSL maintains inter-frame coherence using motion-guided propagation and per-layer encoding, supporting real-time playback with standard video codecs. Across multiple benchmarks, CPSL achieves superior perceptual quality and boundary fidelity compared with layer-based and neural-field baselines while reducing storage and rendering cost by several folds. Our approach offer a practical path from 2D video to scalable 2.5D immersive media."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-18T21:26:13Z",
                "published_parsed": [
                    2025,
                    11,
                    18,
                    21,
                    26,
                    13,
                    1,
                    322,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Kaiyuan Hu"
                    },
                    {
                        "name": "Yili Jin"
                    },
                    {
                        "name": "Junhua Liu"
                    },
                    {
                        "name": "Xize Duan"
                    },
                    {
                        "name": "Hong Kang"
                    },
                    {
                        "name": "Xue Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xue Liu"
                },
                "author": "Xue Liu"
            },
            {
                "id": "http://arxiv.org/abs/2511.14881v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.14881v1",
                "title": "SilverTorch: A Unified Model-based System to Democratize Large-Scale Recommendation on GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SilverTorch: A Unified Model-based System to Democratize Large-Scale Recommendation on GPUs"
                },
                "updated": "2025-11-18T20:00:19Z",
                "updated_parsed": [
                    2025,
                    11,
                    18,
                    20,
                    0,
                    19,
                    1,
                    322,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.14881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.14881v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Serving deep learning based recommendation models (DLRM) at scale is challenging. Existing systems rely on CPU-based ANN indexing and filtering services, suffering from non-negligible costs and forgoing joint optimization opportunities. Such inefficiency makes them difficult to support more complex model architectures, such as learned similarities and multi-task retrieval.\n  In this paper, we propose SilverTorch, a model-based system for serving recommendation models on GPUs. SilverTorch unifies model serving by replacing standalone indexing and filtering services with layers of served models. We propose a Bloom index algorithm on GPUs for feature filtering and a tensor-native fused Int8 ANN kernel on GPUs for nearest neighbor search. We further co-design the ANN search index and filtering index to reduce GPU memory utilization and eliminate unnecessary computation. Benefit from SilverTorch's serving paradigm, we introduce a OverArch scoring layer and a Value Model to aggregate results across multi-tasks. These advancements improve the accuracy for retrieval and enable future studies for serving more complex models. For ranking, SilverTorch's design accelerates item embedding calculation by caching the pre-calculated embeddings inside the serving model.\n  Our evaluation on the industry-scale datasets show that SilverTorch achieves up to 5.6x lower latency and 23.7x higher throughput compared to the state-of-the-art approaches. We also demonstrate that SilverTorch's solution is 13.35x more cost-efficient than CPU-based solution while improving accuracy via serving more complex models. SilverTorch serves over hundreds of models online across major products and recommends contents for billions of daily active users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving deep learning based recommendation models (DLRM) at scale is challenging. Existing systems rely on CPU-based ANN indexing and filtering services, suffering from non-negligible costs and forgoing joint optimization opportunities. Such inefficiency makes them difficult to support more complex model architectures, such as learned similarities and multi-task retrieval.\n  In this paper, we propose SilverTorch, a model-based system for serving recommendation models on GPUs. SilverTorch unifies model serving by replacing standalone indexing and filtering services with layers of served models. We propose a Bloom index algorithm on GPUs for feature filtering and a tensor-native fused Int8 ANN kernel on GPUs for nearest neighbor search. We further co-design the ANN search index and filtering index to reduce GPU memory utilization and eliminate unnecessary computation. Benefit from SilverTorch's serving paradigm, we introduce a OverArch scoring layer and a Value Model to aggregate results across multi-tasks. These advancements improve the accuracy for retrieval and enable future studies for serving more complex models. For ranking, SilverTorch's design accelerates item embedding calculation by caching the pre-calculated embeddings inside the serving model.\n  Our evaluation on the industry-scale datasets show that SilverTorch achieves up to 5.6x lower latency and 23.7x higher throughput compared to the state-of-the-art approaches. We also demonstrate that SilverTorch's solution is 13.35x more cost-efficient than CPU-based solution while improving accuracy via serving more complex models. SilverTorch serves over hundreds of models online across major products and recommends contents for billions of daily active users."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-18T20:00:19Z",
                "published_parsed": [
                    2025,
                    11,
                    18,
                    20,
                    0,
                    19,
                    1,
                    322,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Bi Xue"
                    },
                    {
                        "name": "Hong Wu"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Chao Yang"
                    },
                    {
                        "name": "Yiming Ma"
                    },
                    {
                        "name": "Fei Ding"
                    },
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Xiaoheng Mao"
                    },
                    {
                        "name": "Ke Huang"
                    },
                    {
                        "name": "Xialu Li"
                    },
                    {
                        "name": "Peng Xia"
                    },
                    {
                        "name": "Rui Jian"
                    },
                    {
                        "name": "Yanli Zhao"
                    },
                    {
                        "name": "Yanzun Huang"
                    },
                    {
                        "name": "Yijie Deng"
                    },
                    {
                        "name": "Harry Tran"
                    },
                    {
                        "name": "Ryan Chang"
                    },
                    {
                        "name": "Min Yu"
                    },
                    {
                        "name": "Eric Dong"
                    },
                    {
                        "name": "Jiazhou Wang"
                    },
                    {
                        "name": "Qianqian Zhang"
                    },
                    {
                        "name": "Keke Zhai"
                    },
                    {
                        "name": "Hongzhang Yin"
                    },
                    {
                        "name": "Pawel Garbacki"
                    },
                    {
                        "name": "Zheng Fang"
                    },
                    {
                        "name": "Yiyi Pan"
                    },
                    {
                        "name": "Min Ni"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu"
            },
            {
                "id": "http://arxiv.org/abs/2511.14748v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.14748v1",
                "title": "Cloud-Native Vector Search: A Comprehensive Performance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud-Native Vector Search: A Comprehensive Performance Analysis"
                },
                "updated": "2025-11-18T18:50:15Z",
                "updated_parsed": [
                    2025,
                    11,
                    18,
                    18,
                    50,
                    15,
                    1,
                    322,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.14748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.14748v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vector search has been widely employed in recommender system and retrieval-augmented-generation pipelines, commonly performed with vector indexes to efficiently find similar items in large datasets. Recent growths in both data and task complexity have motivated placing vector indexes onto remote storage -- cloud-native vector search, which cloud providers have recently introduced services for. Yet, despite varying workload characteristics and various available vector index forms, providers default to using cluster-based indexes, which on paper do adapt well to differences between disk and cloud-based environment: their fetch granularities and lack of notable intra-query dependencies aligns with the large optimal fetch sizes and minimizes costly round-trips (i.e., as opposed to graph-based indexes) to remote storage, respectively.\n  This paper systematically studies cloud-native vector search: What and how should indexes be built and used for on-cloud vector search? We analyze bottlenecks of two common index classes, cluster and graph indexes, on remote storage, and show that despite current standardized adoption of cluster indexes on the cloud, graph indexes are favored in workloads requiring high concurrency and recall, or operating on high-dimensional data or large datatypes. We further find that on-cloud search demands significantly different indexing and search parameterizations versus on-disk search for optimal performance. Finally, we incorporate existing cloud-based caching setups into vector search and find that certain index optimizations work against caching, and study how this can be mitigated to maximize gains under various available cache sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector search has been widely employed in recommender system and retrieval-augmented-generation pipelines, commonly performed with vector indexes to efficiently find similar items in large datasets. Recent growths in both data and task complexity have motivated placing vector indexes onto remote storage -- cloud-native vector search, which cloud providers have recently introduced services for. Yet, despite varying workload characteristics and various available vector index forms, providers default to using cluster-based indexes, which on paper do adapt well to differences between disk and cloud-based environment: their fetch granularities and lack of notable intra-query dependencies aligns with the large optimal fetch sizes and minimizes costly round-trips (i.e., as opposed to graph-based indexes) to remote storage, respectively.\n  This paper systematically studies cloud-native vector search: What and how should indexes be built and used for on-cloud vector search? We analyze bottlenecks of two common index classes, cluster and graph indexes, on remote storage, and show that despite current standardized adoption of cluster indexes on the cloud, graph indexes are favored in workloads requiring high concurrency and recall, or operating on high-dimensional data or large datatypes. We further find that on-cloud search demands significantly different indexing and search parameterizations versus on-disk search for optimal performance. Finally, we incorporate existing cloud-based caching setups into vector search and find that certain index optimizations work against caching, and study how this can be mitigated to maximize gains under various available cache sizes."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-18T18:50:15Z",
                "published_parsed": [
                    2025,
                    11,
                    18,
                    18,
                    50,
                    15,
                    1,
                    322,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Zhaoheng Li"
                    },
                    {
                        "name": "Wei Ding"
                    },
                    {
                        "name": "Silu Huang"
                    },
                    {
                        "name": "Zikang Wang"
                    },
                    {
                        "name": "Yuanjin Lin"
                    },
                    {
                        "name": "Ke Wu"
                    },
                    {
                        "name": "Yongjoo Park"
                    },
                    {
                        "name": "Jianjun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jianjun Chen"
                },
                "author": "Jianjun Chen"
            },
            {
                "id": "http://arxiv.org/abs/2511.14712v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.14712v1",
                "title": "FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation"
                },
                "updated": "2025-11-18T17:56:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    18,
                    17,
                    56,
                    4,
                    1,
                    322,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.14712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.14712v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The quadratic time and memory complexity of the attention mechanism in modern Transformer based video generators makes end-to-end training for ultra high resolution videos prohibitively expensive. Motivated by this limitation, we introduce a training-free approach that leverages video Diffusion Transformers pretrained at their native scale to synthesize higher resolution videos without any additional training or adaptation. At the core of our method lies an inward sliding window attention mechanism, which originates from a key observation: maintaining each query token's training scale receptive field is crucial for preserving visual fidelity and detail. However, naive local window attention, unfortunately, often leads to repetitive content and exhibits a lack of global coherence in the generated results. To overcome this challenge, we devise a dual-path pipeline that backs up window attention with a novel cross-attention override strategy, enabling the semantic content produced by local attention to be guided by another branch with a full receptive field and, therefore, ensuring holistic consistency. Furthermore, to improve efficiency, we incorporate a cross-attention caching strategy for this branch to avoid the frequent computation of full 3D attention. Extensive experiments demonstrate that our method delivers ultra-high-resolution videos with fine-grained visual details and high efficiency in a training-free paradigm. Meanwhile, it achieves superior performance on VBench, even compared to training-based alternatives, with competitive or improved efficiency. Codes are available at: https://github.com/WillWu111/FreeSwim",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic time and memory complexity of the attention mechanism in modern Transformer based video generators makes end-to-end training for ultra high resolution videos prohibitively expensive. Motivated by this limitation, we introduce a training-free approach that leverages video Diffusion Transformers pretrained at their native scale to synthesize higher resolution videos without any additional training or adaptation. At the core of our method lies an inward sliding window attention mechanism, which originates from a key observation: maintaining each query token's training scale receptive field is crucial for preserving visual fidelity and detail. However, naive local window attention, unfortunately, often leads to repetitive content and exhibits a lack of global coherence in the generated results. To overcome this challenge, we devise a dual-path pipeline that backs up window attention with a novel cross-attention override strategy, enabling the semantic content produced by local attention to be guided by another branch with a full receptive field and, therefore, ensuring holistic consistency. Furthermore, to improve efficiency, we incorporate a cross-attention caching strategy for this branch to avoid the frequent computation of full 3D attention. Extensive experiments demonstrate that our method delivers ultra-high-resolution videos with fine-grained visual details and high efficiency in a training-free paradigm. Meanwhile, it achieves superior performance on VBench, even compared to training-based alternatives, with competitive or improved efficiency. Codes are available at: https://github.com/WillWu111/FreeSwim"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-18T17:56:04Z",
                "published_parsed": [
                    2025,
                    11,
                    18,
                    17,
                    56,
                    4,
                    1,
                    322,
                    0
                ],
                "arxiv_comment": "13 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yunfeng Wu"
                    },
                    {
                        "name": "Jiayi Song"
                    },
                    {
                        "name": "Zhenxiong Tan"
                    },
                    {
                        "name": "Zihao He"
                    },
                    {
                        "name": "Songhua Liu"
                    }
                ],
                "author_detail": {
                    "name": "Songhua Liu"
                },
                "author": "Songhua Liu"
            },
            {
                "id": "http://arxiv.org/abs/2511.14694v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.14694v1",
                "title": "Near-Lossless Model Compression Enables Longer Context Inference in DNA Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Near-Lossless Model Compression Enables Longer Context Inference in DNA Large Language Models"
                },
                "updated": "2025-11-18T17:29:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    18,
                    17,
                    29,
                    39,
                    1,
                    322,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.14694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.14694v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Trained on massive cross-species DNA corpora, DNA large language models (LLMs) learn the fundamental \"grammar\" and evolutionary patterns of genomic sequences. This makes them powerful priors for DNA sequence modeling, particularly over long ranges. However, two major constraints hinder their use in practice: the quadratic computational cost of self-attention and the growing memory required for key-value (KV) caches during autoregressive decoding. These constraints force the use of heuristics such as fixed-window truncation or sliding windows, which compromise fidelity on ultra-long sequences by discarding distant information. We introduce FOCUS (Feature-Oriented Compression for Ultra-long Self-attention), a progressive context-compression module that can be plugged into pretrained DNA LLMs. FOCUS combines the established k-mer representation in genomics with learnable hierarchical compression: it inserts summary tokens at k-mer granularity and progressively compresses attention key and value activations across multiple Transformer layers, retaining only the summary KV states across windows while discarding ordinary-token KV. A shared-boundary windowing scheme yields a stationary cross-window interface that propagates long-range information with minimal loss. We validate FOCUS on an Evo-2-based DNA LLM fine-tuned on GRCh38 chromosome 1 with self-supervised training and randomized compression schedules to promote robustness across compression ratios. On held-out human chromosomes, FOCUS achieves near-lossless fidelity: compressing a 1 kb context into only 10 summary tokens (about 100x) shifts the average per-nucleotide probability by only about 0.0004. Compared to a baseline without compression, FOCUS reduces KV-cache memory and converts effective inference scaling from O(N^2) to near-linear O(N), enabling about 100x longer inference windows on commodity GPUs with near-lossless fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trained on massive cross-species DNA corpora, DNA large language models (LLMs) learn the fundamental \"grammar\" and evolutionary patterns of genomic sequences. This makes them powerful priors for DNA sequence modeling, particularly over long ranges. However, two major constraints hinder their use in practice: the quadratic computational cost of self-attention and the growing memory required for key-value (KV) caches during autoregressive decoding. These constraints force the use of heuristics such as fixed-window truncation or sliding windows, which compromise fidelity on ultra-long sequences by discarding distant information. We introduce FOCUS (Feature-Oriented Compression for Ultra-long Self-attention), a progressive context-compression module that can be plugged into pretrained DNA LLMs. FOCUS combines the established k-mer representation in genomics with learnable hierarchical compression: it inserts summary tokens at k-mer granularity and progressively compresses attention key and value activations across multiple Transformer layers, retaining only the summary KV states across windows while discarding ordinary-token KV. A shared-boundary windowing scheme yields a stationary cross-window interface that propagates long-range information with minimal loss. We validate FOCUS on an Evo-2-based DNA LLM fine-tuned on GRCh38 chromosome 1 with self-supervised training and randomized compression schedules to promote robustness across compression ratios. On held-out human chromosomes, FOCUS achieves near-lossless fidelity: compressing a 1 kb context into only 10 summary tokens (about 100x) shifts the average per-nucleotide probability by only about 0.0004. Compared to a baseline without compression, FOCUS reduces KV-cache memory and converts effective inference scaling from O(N^2) to near-linear O(N), enabling about 100x longer inference windows on commodity GPUs with near-lossless fidelity."
                },
                "tags": [
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-18T17:29:39Z",
                "published_parsed": [
                    2025,
                    11,
                    18,
                    17,
                    29,
                    39,
                    1,
                    322,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.GN"
                },
                "authors": [
                    {
                        "name": "Rui Zhu"
                    },
                    {
                        "name": "Xiaopu Zhou"
                    },
                    {
                        "name": "Haixu Tang"
                    },
                    {
                        "name": "Stephen W. Scherer"
                    },
                    {
                        "name": "Lucila Ohno-Machado"
                    }
                ],
                "author_detail": {
                    "name": "Lucila Ohno-Machado"
                },
                "author": "Lucila Ohno-Machado"
            },
            {
                "id": "http://arxiv.org/abs/2511.14629v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.14629v1",
                "title": "Scalable Enforcement of Fine Grained Access Control Policies in Relational Database Management Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Enforcement of Fine Grained Access Control Policies in Relational Database Management Systems"
                },
                "updated": "2025-11-18T16:20:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    18,
                    16,
                    20,
                    47,
                    1,
                    322,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.14629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.14629v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The proliferation of smart technologies and evolving privacy regulations such as the GDPR and CPRA has increased the need to manage fine-grained access control (FGAC) policies in database management systems (DBMSs). Existing approaches to enforcing FGAC policies do not scale to thousands of policies, leading to degraded query performance and reduced system effectiveness. We present Sieve, a middleware for relational DBMSs that combines query rewriting and caching to optimize FGAC policy enforcement. Sieve rewrites a query with guarded expressions that group and filter policies and can efficiently use indexes in the DBMS. It also integrates a caching mechanism with an effective replacement strategy and a refresh mechanism to adapt to dynamic workloads. Experiments on two DBMSs with real and synthetic datasets show that Sieve scales to large datasets and policy corpora, maintaining low query latency and system load and improving policy evaluation performance by between 2x and 10x on workloads with 200 to 1,200 policies. The caching extension further improves query performance by between 6 and 22 percent under dynamic workloads, especially with larger cache sizes. These results highlight Sieve's applicability for real-time access control in smart environments and its support for efficient, scalable management of user preferences and privacy policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of smart technologies and evolving privacy regulations such as the GDPR and CPRA has increased the need to manage fine-grained access control (FGAC) policies in database management systems (DBMSs). Existing approaches to enforcing FGAC policies do not scale to thousands of policies, leading to degraded query performance and reduced system effectiveness. We present Sieve, a middleware for relational DBMSs that combines query rewriting and caching to optimize FGAC policy enforcement. Sieve rewrites a query with guarded expressions that group and filter policies and can efficiently use indexes in the DBMS. It also integrates a caching mechanism with an effective replacement strategy and a refresh mechanism to adapt to dynamic workloads. Experiments on two DBMSs with real and synthetic datasets show that Sieve scales to large datasets and policy corpora, maintaining low query latency and system load and improving policy evaluation performance by between 2x and 10x on workloads with 200 to 1,200 policies. The caching extension further improves query performance by between 6 and 22 percent under dynamic workloads, especially with larger cache sizes. These results highlight Sieve's applicability for real-time access control in smart environments and its support for efficient, scalable management of user preferences and privacy policies."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-18T16:20:47Z",
                "published_parsed": [
                    2025,
                    11,
                    18,
                    16,
                    20,
                    47,
                    1,
                    322,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Anadi Shakya"
                    },
                    {
                        "name": "Primal Pappachan"
                    },
                    {
                        "name": "David Maier"
                    },
                    {
                        "name": "Roberto Yus"
                    },
                    {
                        "name": "Sharad Mehrotra"
                    },
                    {
                        "name": "Johann-Christoph Freytag"
                    }
                ],
                "author_detail": {
                    "name": "Johann-Christoph Freytag"
                },
                "author": "Johann-Christoph Freytag"
            },
            {
                "id": "http://arxiv.org/abs/2511.14510v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.14510v1",
                "title": "CLO: Efficient LLM Inference System with CPU-Light KVCache Offloading via Algorithm-System Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLO: Efficient LLM Inference System with CPU-Light KVCache Offloading via Algorithm-System Co-Design"
                },
                "updated": "2025-11-18T14:03:21Z",
                "updated_parsed": [
                    2025,
                    11,
                    18,
                    14,
                    3,
                    21,
                    1,
                    322,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.14510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.14510v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The growth of million-token LLMs exposes the scalability limits of inference systems, where the KVCache dominates memory usage and data transfer overhead. Recent offloading systems migrate the KVCache to CPU memory and incorporate top-k attention to reduce the volume of data transferred from the CPU, while further applying system-level optimizations such as on-GPU caching and prefetching to lower transfer overhead. However, they overlook the CPU bottleneck in three aspects: (1) substantial overhead of fine-grained dynamic cache management performed on the CPU side, (2) significant transfer overhead from poor PCIe bandwidth utilization caused by heavy gathering operations at the CPU side, and (3) GPU runtime bubbles introduced by coarse-grained CPU-centric synchronization. To address these challenges, we propose CLO, a CPU-light KVCache offloading system via algorithm-system co-design. CLO features: (1) a coarse-grained head-wise approximate on-GPU caching strategy with negligible cache management cost, (2) seamless combination of data prefetching and on-GPU persistent caching for lower transfer overhead, (3) a zero-copy transfer engine to fully exploit PCIe bandwidth, and a GPU-centric synchronization method to eliminate GPU stalls. Evaluation on two widely-used LLMs demonstrates that CLO achieves comparable accuracy to state-of-the-art systems, while substantially minimizing CPU overhead, fully utilizing PCIe bandwidth, thus improving decoding throughput by 9.3%-66.6%. Our results highlight that algorithm-system co-design is essential for memory-constrained LLM inference on modern GPU platforms. We open source CLO at https://github.com/CommediaJW/CLO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growth of million-token LLMs exposes the scalability limits of inference systems, where the KVCache dominates memory usage and data transfer overhead. Recent offloading systems migrate the KVCache to CPU memory and incorporate top-k attention to reduce the volume of data transferred from the CPU, while further applying system-level optimizations such as on-GPU caching and prefetching to lower transfer overhead. However, they overlook the CPU bottleneck in three aspects: (1) substantial overhead of fine-grained dynamic cache management performed on the CPU side, (2) significant transfer overhead from poor PCIe bandwidth utilization caused by heavy gathering operations at the CPU side, and (3) GPU runtime bubbles introduced by coarse-grained CPU-centric synchronization. To address these challenges, we propose CLO, a CPU-light KVCache offloading system via algorithm-system co-design. CLO features: (1) a coarse-grained head-wise approximate on-GPU caching strategy with negligible cache management cost, (2) seamless combination of data prefetching and on-GPU persistent caching for lower transfer overhead, (3) a zero-copy transfer engine to fully exploit PCIe bandwidth, and a GPU-centric synchronization method to eliminate GPU stalls. Evaluation on two widely-used LLMs demonstrates that CLO achieves comparable accuracy to state-of-the-art systems, while substantially minimizing CPU overhead, fully utilizing PCIe bandwidth, thus improving decoding throughput by 9.3%-66.6%. Our results highlight that algorithm-system co-design is essential for memory-constrained LLM inference on modern GPU platforms. We open source CLO at https://github.com/CommediaJW/CLO."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-18T14:03:21Z",
                "published_parsed": [
                    2025,
                    11,
                    18,
                    14,
                    3,
                    21,
                    1,
                    322,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Jiawei Yi"
                    },
                    {
                        "name": "Ping Gong"
                    },
                    {
                        "name": "Youhui Bai"
                    },
                    {
                        "name": "Jiaqi Ruan"
                    },
                    {
                        "name": "Shengnan Wang"
                    },
                    {
                        "name": "Pengcheng Wang"
                    },
                    {
                        "name": "Haibo Wang"
                    },
                    {
                        "name": "Weiguang Wang"
                    },
                    {
                        "name": "Xia Zhu"
                    },
                    {
                        "name": "Feng Wu"
                    },
                    {
                        "name": "Cheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Li"
                },
                "author": "Cheng Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.14225v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.14225v1",
                "title": "Magnetic atoms with a large electric dipole moment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetic atoms with a large electric dipole moment"
                },
                "updated": "2025-11-18T07:58:55Z",
                "updated_parsed": [
                    2025,
                    11,
                    18,
                    7,
                    58,
                    55,
                    1,
                    322,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.14225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.14225v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We experimentally show that an electric dipole moment of more than 1 Debye can be induced in the dysprosium (Dy) atom, in a long-lived state that is about 17513 cm$^{-1}$ above the ground state. This metastable state is part of a strongly coupled opposite-parity doublet. Using optically detected microwave spectroscopy in an atomic beam, we determine the approximately 1.12 cm$^{-1}$ doublet spacing for the five stable bosonic isotopes of Dy with kHz-level accuracy. From the shift of the microwave transition frequency in low electric fields (below 150 V/cm) and from optical spectra in high electric fields (up to 150 kV/cm), a reduced transition dipole moment of 7.65 $\\pm$ 0.05 Debye between the doublet states is extracted. In high electric fields the doublet interacts with a third state at 17727 cm$^{-1}$, that connects to the ground state via an electric-dipole transition. The three-state Stark interaction enables preparation of Dy atoms in the metastable state via single-photon excitation from the ground state.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We experimentally show that an electric dipole moment of more than 1 Debye can be induced in the dysprosium (Dy) atom, in a long-lived state that is about 17513 cm$^{-1}$ above the ground state. This metastable state is part of a strongly coupled opposite-parity doublet. Using optically detected microwave spectroscopy in an atomic beam, we determine the approximately 1.12 cm$^{-1}$ doublet spacing for the five stable bosonic isotopes of Dy with kHz-level accuracy. From the shift of the microwave transition frequency in low electric fields (below 150 V/cm) and from optical spectra in high electric fields (up to 150 kV/cm), a reduced transition dipole moment of 7.65 $\\pm$ 0.05 Debye between the doublet states is extracted. In high electric fields the doublet interacts with a third state at 17727 cm$^{-1}$, that connects to the ground state via an electric-dipole transition. The three-state Stark interaction enables preparation of Dy atoms in the metastable state via single-photon excitation from the ground state."
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.quant-gas",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-18T07:58:55Z",
                "published_parsed": [
                    2025,
                    11,
                    18,
                    7,
                    58,
                    55,
                    1,
                    322,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.atom-ph"
                },
                "authors": [
                    {
                        "name": "Johannes Seifert"
                    },
                    {
                        "name": "Sid C. Wright"
                    },
                    {
                        "name": "Boris G. Sartakov"
                    },
                    {
                        "name": "Giacomo Valtolina"
                    },
                    {
                        "name": "Gerard Meijer"
                    }
                ],
                "author_detail": {
                    "name": "Gerard Meijer"
                },
                "author": "Gerard Meijer"
            },
            {
                "id": "http://arxiv.org/abs/2412.18911v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2412.18911v2",
                "title": "Rethinking Token-wise Feature Caching: Accelerating Diffusion Transformers with Dual Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Token-wise Feature Caching: Accelerating Diffusion Transformers with Dual Feature Caching"
                },
                "updated": "2025-11-18T06:21:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    18,
                    6,
                    21,
                    13,
                    1,
                    322,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2412.18911v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2412.18911v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Transformers (DiT) have become the dominant methods in image and video generation yet still suffer substantial computational costs. As an effective approach for DiT acceleration, feature caching methods are designed to cache the features of DiT in previous timesteps and reuse them in the next timesteps, allowing us to skip the computation in the next timesteps. Among them, token-wise feature caching has been introduced to perform different caching ratios for different tokens in DiTs, aiming to skip the computation for unimportant tokens while still computing the important ones. In this paper, we propose to carefully check the effectiveness in token-wise feature caching with the following two questions: (1) Is it really necessary to compute the so-called \"important\" tokens in each step? (2) Are so-called important tokens really important? Surprisingly, this paper gives some counter-intuition answers, demonstrating that consistently computing the selected ``important tokens'' in all steps is not necessary. The selection of the so-called ``important tokens'' is often ineffective, and even sometimes shows inferior performance than random selection. Based on these observations, this paper introduces dual feature caching referred to as DuCa, which performs aggressive caching strategy and conservative caching strategy iteratively and selects the tokens for computing randomly. Extensive experimental results demonstrate the effectiveness of our method in DiT, PixArt, FLUX, and OpenSora, demonstrating significant improvements than the previous token-wise feature caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have become the dominant methods in image and video generation yet still suffer substantial computational costs. As an effective approach for DiT acceleration, feature caching methods are designed to cache the features of DiT in previous timesteps and reuse them in the next timesteps, allowing us to skip the computation in the next timesteps. Among them, token-wise feature caching has been introduced to perform different caching ratios for different tokens in DiTs, aiming to skip the computation for unimportant tokens while still computing the important ones. In this paper, we propose to carefully check the effectiveness in token-wise feature caching with the following two questions: (1) Is it really necessary to compute the so-called \"important\" tokens in each step? (2) Are so-called important tokens really important? Surprisingly, this paper gives some counter-intuition answers, demonstrating that consistently computing the selected ``important tokens'' in all steps is not necessary. The selection of the so-called ``important tokens'' is often ineffective, and even sometimes shows inferior performance than random selection. Based on these observations, this paper introduces dual feature caching referred to as DuCa, which performs aggressive caching strategy and conservative caching strategy iteratively and selects the tokens for computing randomly. Extensive experimental results demonstrate the effectiveness of our method in DiT, PixArt, FLUX, and OpenSora, demonstrating significant improvements than the previous token-wise feature caching."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-12-25T14:00:14Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Runlin Guo"
                    },
                    {
                        "name": "Haohang Xu"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2510.19670v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.19670v3",
                "title": "CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware Cloud-Edge Cooperation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware Cloud-Edge Cooperation"
                },
                "updated": "2025-11-18T06:18:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    18,
                    6,
                    18,
                    28,
                    1,
                    322,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.19670v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.19670v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present CoSense-LLM, an edge-first framework that turns continuous multimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and lightweight vision) into compact, verifiable semantic tokens and coordinates with large language models under explicit latency, energy, bandwidth, and privacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight encoder that aligns sensor embeddings with language and compresses them into short discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer that grounds generation in site specific policies and notes; (iii) PromptRouter, a cost and uncertainty aware policy that selects edge only generation, edge plus retrieval, or compact cloud escalation; and (iv) Secure Execution, an auditable redaction path that enforces data minimization so raw waveforms never leave the device. The system works with modern serving optimizations, including paged or streaming KV caches, FlashAttention style kernels, speculative decoding, and quantized LoRA adapters, and supports on device personalization and federated updates under non IID drift. Across home, office, and clinic deployments, CoSense-LLM delivers grounded explanations while meeting tight service level objectives: it sustains sub second (p95) end to end latency on edge dominant paths, reduces inter tier token and bandwidth costs by preferring local retrieval grounded responses, and preserves privacy by transmitting only discrete codes and redacted metadata. Ablations show that Edge-RAG improves factual consistency and reduces contradictions, calibrated uncertainty enables selective abstention and controlled escalations, and KV plus decoding accelerators lower energy per decision. The results support an edge first design that treats semantics, privacy, and predictable latency as co equal goals for large model deployments in interference prone environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present CoSense-LLM, an edge-first framework that turns continuous multimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and lightweight vision) into compact, verifiable semantic tokens and coordinates with large language models under explicit latency, energy, bandwidth, and privacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight encoder that aligns sensor embeddings with language and compresses them into short discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer that grounds generation in site specific policies and notes; (iii) PromptRouter, a cost and uncertainty aware policy that selects edge only generation, edge plus retrieval, or compact cloud escalation; and (iv) Secure Execution, an auditable redaction path that enforces data minimization so raw waveforms never leave the device. The system works with modern serving optimizations, including paged or streaming KV caches, FlashAttention style kernels, speculative decoding, and quantized LoRA adapters, and supports on device personalization and federated updates under non IID drift. Across home, office, and clinic deployments, CoSense-LLM delivers grounded explanations while meeting tight service level objectives: it sustains sub second (p95) end to end latency on edge dominant paths, reduces inter tier token and bandwidth costs by preferring local retrieval grounded responses, and preserves privacy by transmitting only discrete codes and redacted metadata. Ablations show that Edge-RAG improves factual consistency and reduces contradictions, calibrated uncertainty enables selective abstention and controlled escalations, and KV plus decoding accelerators lower energy per decision. The results support an edge first design that treats semantics, privacy, and predictable latency as co equal goals for large model deployments in interference prone environments."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-22T15:16:56Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    16,
                    56,
                    2,
                    295,
                    0
                ],
                "arxiv_comment": "19 pages,8 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Hasan Akgul"
                    },
                    {
                        "name": "Mari Eplik"
                    },
                    {
                        "name": "Javier Rojas"
                    },
                    {
                        "name": "Aina Binti Abdullah"
                    },
                    {
                        "name": "Pieter van der Merwe"
                    }
                ],
                "author_detail": {
                    "name": "Pieter van der Merwe"
                },
                "author": "Pieter van der Merwe"
            },
            {
                "id": "http://arxiv.org/abs/2511.14148v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.14148v1",
                "title": "AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models"
                },
                "updated": "2025-11-18T05:21:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    18,
                    5,
                    21,
                    11,
                    1,
                    322,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.14148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.14148v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-language-action (VLA) models have recently emerged as a powerful paradigm for building generalist robots. However, traditional VLA models that generate actions through flow matching (FM) typically rely on rigid and uniform time schedules, i.e., synchronous FM (SFM). Without action context awareness and asynchronous self-correction, SFM becomes unstable in long-horizon tasks, where a single action error can cascade into failure. In this work, we propose asynchronous flow matching VLA (AsyncVLA), a novel framework that introduces temporal flexibility in asynchronous FM (AFM) and enables self-correction in action generation. AsyncVLA breaks from the vanilla SFM in VLA models by generating the action tokens in a non-uniform time schedule with action context awareness. Besides, our method introduces the confidence rater to extract confidence of the initially generated actions, enabling the model to selectively refine inaccurate action tokens before execution. Moreover, we propose a unified training procedure for SFM and AFM that endows a single model with both modes, improving KV-cache utilization. Extensive experiments on robotic manipulation benchmarks demonstrate that AsyncVLA is data-efficient and exhibits self-correction ability. AsyncVLA achieves state-of-the-art results across general embodied evaluations due to its asynchronous generation in AFM. Our code is available at https://github.com/YuhuaJiang2002/AsyncVLA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language-action (VLA) models have recently emerged as a powerful paradigm for building generalist robots. However, traditional VLA models that generate actions through flow matching (FM) typically rely on rigid and uniform time schedules, i.e., synchronous FM (SFM). Without action context awareness and asynchronous self-correction, SFM becomes unstable in long-horizon tasks, where a single action error can cascade into failure. In this work, we propose asynchronous flow matching VLA (AsyncVLA), a novel framework that introduces temporal flexibility in asynchronous FM (AFM) and enables self-correction in action generation. AsyncVLA breaks from the vanilla SFM in VLA models by generating the action tokens in a non-uniform time schedule with action context awareness. Besides, our method introduces the confidence rater to extract confidence of the initially generated actions, enabling the model to selectively refine inaccurate action tokens before execution. Moreover, we propose a unified training procedure for SFM and AFM that endows a single model with both modes, improving KV-cache utilization. Extensive experiments on robotic manipulation benchmarks demonstrate that AsyncVLA is data-efficient and exhibits self-correction ability. AsyncVLA achieves state-of-the-art results across general embodied evaluations due to its asynchronous generation in AFM. Our code is available at https://github.com/YuhuaJiang2002/AsyncVLA."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-18T05:21:11Z",
                "published_parsed": [
                    2025,
                    11,
                    18,
                    5,
                    21,
                    11,
                    1,
                    322,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Yuhua Jiang"
                    },
                    {
                        "name": "Shuang Cheng"
                    },
                    {
                        "name": "Yan Ding"
                    },
                    {
                        "name": "Feifei Gao"
                    },
                    {
                        "name": "Biqing Qi"
                    }
                ],
                "author_detail": {
                    "name": "Biqing Qi"
                },
                "author": "Biqing Qi"
            },
            {
                "id": "http://arxiv.org/abs/2511.14124v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.14124v1",
                "title": "10Cache: Heterogeneous Resource-Aware Tensor Caching and Migration for LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "10Cache: Heterogeneous Resource-Aware Tensor Caching and Migration for LLM Training"
                },
                "updated": "2025-11-18T04:17:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    18,
                    4,
                    17,
                    44,
                    1,
                    322,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.14124v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.14124v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Training large language models (LLMs) in the cloud faces growing memory bottlenecks due to the limited capacity and high cost of GPUs. While GPU memory offloading to CPU and NVMe has made large-scale training more feasible, existing approaches suffer from high tensor migration latency and suboptimal device memory utilization, ultimately increasing training time and cloud costs. To address these challenges, we present 10Cache, a resource-aware tensor caching and migration system that accelerates LLM training by intelligently coordinating memory usage across GPU, CPU, and NVMe tiers. 10Cache profiles tensor execution order to construct prefetch policies, allocates memory buffers in pinned memory based on tensor size distributions, and reuses memory buffers to minimize allocation overhead.\n  Designed for cloud-scale deployments, 10Cache improves memory efficiency and reduces reliance on high-end GPUs. Across diverse LLM workloads, it achieves up to 2x speedup in training time, improves GPU cache hit rate by up to 86.6x, and increases CPU/GPU memory utilization by up to 2.15x and 1.33x, respectively, compared to state-of-the-art offloading methods. These results demonstrate that 10Cache is a practical and scalable solution for optimizing LLM training throughput and resource efficiency in cloud environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) in the cloud faces growing memory bottlenecks due to the limited capacity and high cost of GPUs. While GPU memory offloading to CPU and NVMe has made large-scale training more feasible, existing approaches suffer from high tensor migration latency and suboptimal device memory utilization, ultimately increasing training time and cloud costs. To address these challenges, we present 10Cache, a resource-aware tensor caching and migration system that accelerates LLM training by intelligently coordinating memory usage across GPU, CPU, and NVMe tiers. 10Cache profiles tensor execution order to construct prefetch policies, allocates memory buffers in pinned memory based on tensor size distributions, and reuses memory buffers to minimize allocation overhead.\n  Designed for cloud-scale deployments, 10Cache improves memory efficiency and reduces reliance on high-end GPUs. Across diverse LLM workloads, it achieves up to 2x speedup in training time, improves GPU cache hit rate by up to 86.6x, and increases CPU/GPU memory utilization by up to 2.15x and 1.33x, respectively, compared to state-of-the-art offloading methods. These results demonstrate that 10Cache is a practical and scalable solution for optimizing LLM training throughput and resource efficiency in cloud environments."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-18T04:17:44Z",
                "published_parsed": [
                    2025,
                    11,
                    18,
                    4,
                    17,
                    44,
                    1,
                    322,
                    0
                ],
                "arxiv_comment": "This paper accepted for presentation to the 16th ACM Symposium on Cloud Computing (SOCC'25)",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Sabiha Afroz"
                    },
                    {
                        "name": "Redwan Ibne Seraj Khan"
                    },
                    {
                        "name": "Hadeel Albahar"
                    },
                    {
                        "name": "Jingoo Han"
                    },
                    {
                        "name": "Ali R. Butt"
                    }
                ],
                "author_detail": {
                    "name": "Ali R. Butt"
                },
                "author": "Ali R. Butt"
            },
            {
                "id": "http://arxiv.org/abs/2511.13717v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13717v1",
                "title": "TZ-LLM: Protecting On-Device Large Language Models with Arm TrustZone",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TZ-LLM: Protecting On-Device Large Language Models with Arm TrustZone"
                },
                "updated": "2025-11-17T18:59:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    59,
                    20,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13717v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) deployed on mobile devices offer benefits like user privacy and reduced network latency, but introduce a significant security risk: the leakage of proprietary models to end users.\n  To mitigate this risk, we propose a system design for protecting on-device LLMs using Arm Trusted Execution Environment (TEE), TrustZone. Our system addresses two primary challenges: (1) The dilemma between memory efficiency and fast inference (caching model parameters within TEE memory). (2) The lack of efficient and secure Neural Processing Unit (NPU) time-sharing between Rich Execution Environment (REE) and TEE.\n  Our approach incorporates two key innovations. First, we employ pipelined restoration, leveraging the deterministic memory access patterns of LLM inference to prefetch parameters on demand, hiding memory allocation, I/O and decryption latency under computation time. Second, we introduce a co-driver design, creating a minimal data plane NPU driver in the TEE that collaborates with the full-fledged REE driver. This reduces the TEE TCB size and eliminates control plane reinitialization overhead during NPU world switches.\n  We implemented our system on the emerging OpenHarmony OS and the llama.cpp inference framework, and evaluated it with various LLMs on an Arm Rockchip device. Compared to a strawman TEE baseline lacking our optimizations, our system reduces TTFT by up to 90.9% and increases decoding speed by up to 23.2%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) deployed on mobile devices offer benefits like user privacy and reduced network latency, but introduce a significant security risk: the leakage of proprietary models to end users.\n  To mitigate this risk, we propose a system design for protecting on-device LLMs using Arm Trusted Execution Environment (TEE), TrustZone. Our system addresses two primary challenges: (1) The dilemma between memory efficiency and fast inference (caching model parameters within TEE memory). (2) The lack of efficient and secure Neural Processing Unit (NPU) time-sharing between Rich Execution Environment (REE) and TEE.\n  Our approach incorporates two key innovations. First, we employ pipelined restoration, leveraging the deterministic memory access patterns of LLM inference to prefetch parameters on demand, hiding memory allocation, I/O and decryption latency under computation time. Second, we introduce a co-driver design, creating a minimal data plane NPU driver in the TEE that collaborates with the full-fledged REE driver. This reduces the TEE TCB size and eliminates control plane reinitialization overhead during NPU world switches.\n  We implemented our system on the emerging OpenHarmony OS and the llama.cpp inference framework, and evaluated it with various LLMs on an Arm Rockchip device. Compared to a strawman TEE baseline lacking our optimizations, our system reduces TTFT by up to 90.9% and increases decoding speed by up to 23.2%."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T18:59:20Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    59,
                    20,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Xunjie Wang"
                    },
                    {
                        "name": "Jiacheng Shi"
                    },
                    {
                        "name": "Zihan Zhao"
                    },
                    {
                        "name": "Yang Yu"
                    },
                    {
                        "name": "Zhichao Hua"
                    },
                    {
                        "name": "Jinyu Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jinyu Gu"
                },
                "author": "Jinyu Gu"
            },
            {
                "id": "http://arxiv.org/abs/2511.13679v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13679v1",
                "title": "QUILL: An Algorithm-Architecture Co-Design for Cache-Local Deformable Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QUILL: An Algorithm-Architecture Co-Design for Cache-Local Deformable Attention"
                },
                "updated": "2025-11-17T18:34:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    34,
                    4,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13679v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deformable transformers deliver state-of-the-art detection but map poorly to hardware due to irregular memory access and low arithmetic intensity. We introduce QUILL, a schedule-aware accelerator that turns deformable attention into cache-friendly, single-pass work. At its core, Distance-based Out-of-Order Querying (DOOQ) orders queries by spatial proximity; the look-ahead drives a region prefetch into an alternate buffer--forming a schedule-aware prefetch loop that overlaps memory and compute. A fused MSDeformAttn engine executes interpolation, Softmax, aggregation, and the final projection (W''m) in one pass without spilling intermediates, while small tensors are kept on-chip and surrounding dense layers run on integrated GEMMs. Implemented as RTL and evaluated end-to-end, QUILL achieves up to 7.29x higher throughput and 47.3x better energy efficiency than an RTX 4090, and exceeds prior accelerators by 3.26-9.82x in throughput and 2.01-6.07x in energy efficiency. With mixed-precision quantization, accuracy tracks FP32 within <=0.9 AP across Deformable and Sparse DETR variants. By converting sparsity into locality--and locality into utilization--QUILL delivers consistent, end-to-end speedups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deformable transformers deliver state-of-the-art detection but map poorly to hardware due to irregular memory access and low arithmetic intensity. We introduce QUILL, a schedule-aware accelerator that turns deformable attention into cache-friendly, single-pass work. At its core, Distance-based Out-of-Order Querying (DOOQ) orders queries by spatial proximity; the look-ahead drives a region prefetch into an alternate buffer--forming a schedule-aware prefetch loop that overlaps memory and compute. A fused MSDeformAttn engine executes interpolation, Softmax, aggregation, and the final projection (W''m) in one pass without spilling intermediates, while small tensors are kept on-chip and surrounding dense layers run on integrated GEMMs. Implemented as RTL and evaluated end-to-end, QUILL achieves up to 7.29x higher throughput and 47.3x better energy efficiency than an RTX 4090, and exceeds prior accelerators by 3.26-9.82x in throughput and 2.01-6.07x in energy efficiency. With mixed-precision quantization, accuracy tracks FP32 within <=0.9 AP across Deformable and Sparse DETR variants. By converting sparsity into locality--and locality into utilization--QUILL delivers consistent, end-to-end speedups."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T18:34:04Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    34,
                    4,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "Accepted to DATE 2026",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Hyunwoo Oh"
                    },
                    {
                        "name": "Hanning Chen"
                    },
                    {
                        "name": "Sanggeon Yun"
                    },
                    {
                        "name": "Yang Ni"
                    },
                    {
                        "name": "Wenjun Huang"
                    },
                    {
                        "name": "Tamoghno Das"
                    },
                    {
                        "name": "Suyeon Jang"
                    },
                    {
                        "name": "Mohsen Imani"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Imani"
                },
                "author": "Mohsen Imani"
            },
            {
                "id": "http://arxiv.org/abs/2511.13644v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13644v1",
                "title": "CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding"
                },
                "updated": "2025-11-17T17:56:14Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    56,
                    14,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13644v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Long-form video question answering (VQA) overwhelms current vision-language models (VLMs) because attention and key-value (KV) caches grow with runtime, forcing either expensive inference or near-sighted sliding windows. We introduce CacheFlow, a training-free pipeline that pairs Dynamic Token Dropping (DTD) with a compressive long-term memory. DTD prunes per-patch tokens online via cosine similarity to the previous frame, and surviving tokens are packed into fixed-size blocks. This online, per-frame processing makes our approach fundamentally suited for live streaming VQA. As blocks are processed, each one's keys are summarized by a tiny recurrent encoder to form a retrieval index, while the block's full KV pairs are offloaded and later rehydrated for generation, preserving answer fidelity. At inference, a consensus-based retrieval mechanism retrieves only the Top-K most relevant blocks and attends over both the retrieved and local context for precise, long-range reasoning. CacheFlow is drop-in, architecture-agnostic, and requires no fine-tuning. Experiments on both offline and streaming VQA benchmarks demonstrate that CacheFlow outperforms current strong baselines, while processing up to 87% less tokens. Our dual approach enables VLMs to be both efficient and context-aware, paving the way for practical long-form video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-form video question answering (VQA) overwhelms current vision-language models (VLMs) because attention and key-value (KV) caches grow with runtime, forcing either expensive inference or near-sighted sliding windows. We introduce CacheFlow, a training-free pipeline that pairs Dynamic Token Dropping (DTD) with a compressive long-term memory. DTD prunes per-patch tokens online via cosine similarity to the previous frame, and surviving tokens are packed into fixed-size blocks. This online, per-frame processing makes our approach fundamentally suited for live streaming VQA. As blocks are processed, each one's keys are summarized by a tiny recurrent encoder to form a retrieval index, while the block's full KV pairs are offloaded and later rehydrated for generation, preserving answer fidelity. At inference, a consensus-based retrieval mechanism retrieves only the Top-K most relevant blocks and attends over both the retrieved and local context for precise, long-range reasoning. CacheFlow is drop-in, architecture-agnostic, and requires no fine-tuning. Experiments on both offline and streaming VQA benchmarks demonstrate that CacheFlow outperforms current strong baselines, while processing up to 87% less tokens. Our dual approach enables VLMs to be both efficient and context-aware, paving the way for practical long-form video understanding."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T17:56:14Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    56,
                    14,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Shrenik Patel"
                    },
                    {
                        "name": "Daivik Patel"
                    }
                ],
                "author_detail": {
                    "name": "Daivik Patel"
                },
                "author": "Daivik Patel"
            },
            {
                "id": "http://arxiv.org/abs/2511.13587v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13587v1",
                "title": "VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping"
                },
                "updated": "2025-11-17T16:50:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    50,
                    58,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13587v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Visual autoregressive (AR) generation models have demonstrated strong potential for image generation, yet their next-token-prediction paradigm introduces considerable inference latency. Although speculative decoding (SD) has been proven effective for accelerating visual AR models, its \"draft one step, then verify one step\" paradigm prevents a direct reduction of the forward passes, thus restricting acceleration potential. Motivated by the visual token interchangeability, we for the first time to explore verification skipping in the SD process of visual AR model generation to explicitly cut the number of target model forward passes, thereby reducing inference latency. Based on an analysis of the drafting stage's characteristics, we observe that verification redundancy and stale feature reusability are key factors to retain generation quality and speedup for verification-free steps. Inspired by these two observations, we propose a novel SD framework VVS to accelerate visual AR generation via partial verification skipping, which integrates three complementary modules: (1) a verification-free token selector with dynamical truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped step scheduling. Consequently, VVS reduces the number of target model forward passes by a factor of $2.8\\times$ relative to vanilla AR decoding while maintaining competitive generation quality, offering a superior speed-quality trade-off over conventional SD frameworks and revealing strong potential to reshape the SD paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual autoregressive (AR) generation models have demonstrated strong potential for image generation, yet their next-token-prediction paradigm introduces considerable inference latency. Although speculative decoding (SD) has been proven effective for accelerating visual AR models, its \"draft one step, then verify one step\" paradigm prevents a direct reduction of the forward passes, thus restricting acceleration potential. Motivated by the visual token interchangeability, we for the first time to explore verification skipping in the SD process of visual AR model generation to explicitly cut the number of target model forward passes, thereby reducing inference latency. Based on an analysis of the drafting stage's characteristics, we observe that verification redundancy and stale feature reusability are key factors to retain generation quality and speedup for verification-free steps. Inspired by these two observations, we propose a novel SD framework VVS to accelerate visual AR generation via partial verification skipping, which integrates three complementary modules: (1) a verification-free token selector with dynamical truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped step scheduling. Consequently, VVS reduces the number of target model forward passes by a factor of $2.8\\times$ relative to vanilla AR decoding while maintaining competitive generation quality, offering a superior speed-quality trade-off over conventional SD frameworks and revealing strong potential to reshape the SD paradigm."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T16:50:58Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    50,
                    58,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Haotian Dong"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Rongwei Lu"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang"
            },
            {
                "id": "http://arxiv.org/abs/2511.13412v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13412v1",
                "title": "Microwave-acoustic-driven power electronics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microwave-acoustic-driven power electronics"
                },
                "updated": "2025-11-17T14:25:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    25,
                    37,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13412v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Electrical isolation is critical to ensure safety and minimize electromagnetic interference (EMI), yet existing methods struggle to simultaneously transmit power and signals through a unified channel. Here we demonstrate a mechanically-isolated gate driver based on microwave-frequency surface acoustic wave (SAW) device on lithium niobate that achieves galvanic isolation of 2.75 kV with ultralow isolation capacitance (0.032 pF) over 1.25 mm mechanical propagation length, delivering 13.4 V open-circuit voltage and 44.4 mA short-circuit current. We demonstrate isolated gate driving for a gallium nitride (GaN) high-electron-mobility transistor, achieving a turn-on time of 108.8 ns comparable to commercial drivers and validate its operation in a buck converter. In addition, our SAW device operates over an ultrawide temperature range from 0.5 K (-272.6 C) to 544 K (271 C). The microwave-frequency SAW devices offer inherent EMI immunity and potential for heterogeneous integration on multiple semiconductor platforms, enabling compact, high-performance isolated power and signal transmission in advanced power electronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electrical isolation is critical to ensure safety and minimize electromagnetic interference (EMI), yet existing methods struggle to simultaneously transmit power and signals through a unified channel. Here we demonstrate a mechanically-isolated gate driver based on microwave-frequency surface acoustic wave (SAW) device on lithium niobate that achieves galvanic isolation of 2.75 kV with ultralow isolation capacitance (0.032 pF) over 1.25 mm mechanical propagation length, delivering 13.4 V open-circuit voltage and 44.4 mA short-circuit current. We demonstrate isolated gate driving for a gallium nitride (GaN) high-electron-mobility transistor, achieving a turn-on time of 108.8 ns comparable to commercial drivers and validate its operation in a buck converter. In addition, our SAW device operates over an ultrawide temperature range from 0.5 K (-272.6 C) to 544 K (271 C). The microwave-frequency SAW devices offer inherent EMI immunity and potential for heterogeneous integration on multiple semiconductor platforms, enabling compact, high-performance isolated power and signal transmission in advanced power electronics."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T14:25:37Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    25,
                    37,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Liyang Jin"
                    },
                    {
                        "name": "Zichen Xi"
                    },
                    {
                        "name": "Joseph G. Thomas"
                    },
                    {
                        "name": "Jun Ji"
                    },
                    {
                        "name": "Yuanzhi Zhang"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Yizheng Zhu"
                    },
                    {
                        "name": "Linbo Shao"
                    },
                    {
                        "name": "Liyan Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Liyan Zhu"
                },
                "author": "Liyan Zhu"
            },
            {
                "id": "http://arxiv.org/abs/2505.20334v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.20334v2",
                "title": "Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query"
                },
                "updated": "2025-11-17T13:29:25Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    29,
                    25,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.20334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.20334v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) rely on key-value cache (KV cache) to accelerate decoding by reducing redundant computations. However, the KV cache memory usage grows substantially with longer text sequences, posing challenges for efficient deployment. Existing KV cache eviction methods prune tokens using prefilling-stage attention scores, causing inconsistency with actual inference queries, especially under tight memory budgets. In this paper, we propose Lookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost pseudo lookahead queries to better approximate the true decoding-stage queries. By using these lookahead queries as the observation window for importance estimation, LAQ achieves more consistent and accurate KV cache eviction aligned with real inference scenarios. Experimental results on LongBench and Needle-in-a-Haystack benchmarks show that LAQ outperforms existing methods across various budget levels, achieving a 1 $\\sim$ 4 point improvement on LongBench under limited cache budget. Moreover, LAQ is complementary to existing approaches and can be flexibly combined to yield further improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on key-value cache (KV cache) to accelerate decoding by reducing redundant computations. However, the KV cache memory usage grows substantially with longer text sequences, posing challenges for efficient deployment. Existing KV cache eviction methods prune tokens using prefilling-stage attention scores, causing inconsistency with actual inference queries, especially under tight memory budgets. In this paper, we propose Lookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost pseudo lookahead queries to better approximate the true decoding-stage queries. By using these lookahead queries as the observation window for importance estimation, LAQ achieves more consistent and accurate KV cache eviction aligned with real inference scenarios. Experimental results on LongBench and Needle-in-a-Haystack benchmarks show that LAQ outperforms existing methods across various budget levels, achieving a 1 $\\sim$ 4 point improvement on LongBench under limited cache budget. Moreover, LAQ is complementary to existing approaches and can be flexibly combined to yield further improvements."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-24T10:34:38Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    10,
                    34,
                    38,
                    5,
                    144,
                    0
                ],
                "arxiv_comment": "Accepted by EMNLP 2025 Main",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shiyu Ji"
                    },
                    {
                        "name": "Yijun Liu"
                    },
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che"
            },
            {
                "id": "http://arxiv.org/abs/2511.13319v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13319v1",
                "title": "Whistledown: Combining User-Level Privacy with Conversational Coherence in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whistledown: Combining User-Level Privacy with Conversational Coherence in LLMs"
                },
                "updated": "2025-11-17T12:56:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    12,
                    56,
                    33,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13319v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Users increasingly rely on large language models (LLMs) for personal, emotionally charged, and socially sensitive conversations. However, prompts sent to cloud-hosted models can contain personally identifiable information (PII) that users do not want logged, retained, or leaked. We observe this to be especially acute when users discuss friends, coworkers, or adversaries, i.e., when they spill the tea. Enterprises face the same challenge when they want to use LLMs for internal communication and decision-making.\n  In this whitepaper, we present Whistledown, a best-effort privacy layer that modifies prompts before they are sent to the LLM. Whistledown combines pseudonymization and $$-local differential privacy ($$-LDP) with transformation caching to provide best-effort privacy protection without sacrificing conversational utility. Whistledown is designed to have low compute and memory overhead, allowing it to be deployed directly on a client's device in the case of individual users. For enterprise users, Whistledown is deployed centrally within a zero-trust gateway that runs on an enterprise's trusted infrastructure. Whistledown requires no changes to the existing APIs of popular LLM providers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Users increasingly rely on large language models (LLMs) for personal, emotionally charged, and socially sensitive conversations. However, prompts sent to cloud-hosted models can contain personally identifiable information (PII) that users do not want logged, retained, or leaked. We observe this to be especially acute when users discuss friends, coworkers, or adversaries, i.e., when they spill the tea. Enterprises face the same challenge when they want to use LLMs for internal communication and decision-making.\n  In this whitepaper, we present Whistledown, a best-effort privacy layer that modifies prompts before they are sent to the LLM. Whistledown combines pseudonymization and $$-local differential privacy ($$-LDP) with transformation caching to provide best-effort privacy protection without sacrificing conversational utility. Whistledown is designed to have low compute and memory overhead, allowing it to be deployed directly on a client's device in the case of individual users. For enterprise users, Whistledown is deployed centrally within a zero-trust gateway that runs on an enterprise's trusted infrastructure. Whistledown requires no changes to the existing APIs of popular LLM providers."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T12:56:33Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    12,
                    56,
                    33,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Chelsea McMurray"
                    },
                    {
                        "name": "Hayder Tirmazi"
                    }
                ],
                "author_detail": {
                    "name": "Hayder Tirmazi"
                },
                "author": "Hayder Tirmazi"
            },
            {
                "id": "http://arxiv.org/abs/2506.01215v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.01215v2",
                "title": "Compress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers"
                },
                "updated": "2025-11-17T12:29:07Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    12,
                    29,
                    7,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.01215v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.01215v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As large language models increasingly gain popularity in real-world applications, processing extremely long contexts, often exceeding the model's pre-trained context limits, has emerged as a critical challenge. While existing approaches to efficient long-context processing show promise, recurrent compression-based methods struggle with information preservation, whereas random access approaches require substantial memory resources. We introduce REFORM, a novel inference framework that efficiently handles long contexts through a two-phase approach. First, it incrementally processes input chunks while maintaining a compressed KV cache, constructs cross-layer context embeddings, and utilizes early exit strategy for improved efficiency. Second, it identifies and gathers essential tokens via similarity matching and selectively recomputes the KV cache. Compared to baselines, REFORM achieves over 52% and 34% performance gains on RULER and BABILong respectively at 1M context length. It also outperforms baselines on Infinite-Bench, RepoEval, and MM-NIAH, demonstrating flexibility across diverse tasks and domains. Additionally, REFORM reduces inference time by 30% and peak memory usage by 5%, achieving both efficiency and superior performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models increasingly gain popularity in real-world applications, processing extremely long contexts, often exceeding the model's pre-trained context limits, has emerged as a critical challenge. While existing approaches to efficient long-context processing show promise, recurrent compression-based methods struggle with information preservation, whereas random access approaches require substantial memory resources. We introduce REFORM, a novel inference framework that efficiently handles long contexts through a two-phase approach. First, it incrementally processes input chunks while maintaining a compressed KV cache, constructs cross-layer context embeddings, and utilizes early exit strategy for improved efficiency. Second, it identifies and gathers essential tokens via similarity matching and selectively recomputes the KV cache. Compared to baselines, REFORM achieves over 52% and 34% performance gains on RULER and BABILong respectively at 1M context length. It also outperforms baselines on Infinite-Bench, RepoEval, and MM-NIAH, demonstrating flexibility across diverse tasks and domains. Additionally, REFORM reduces inference time by 30% and peak memory usage by 5%, achieving both efficiency and superior performance."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-01T23:49:14Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    23,
                    49,
                    14,
                    6,
                    152,
                    0
                ],
                "arxiv_comment": "NeurIPS 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Woomin Song"
                    },
                    {
                        "name": "Sai Muralidhar Jayanthi"
                    },
                    {
                        "name": "Srikanth Ronanki"
                    },
                    {
                        "name": "Kanthashree Mysore Sathyendra"
                    },
                    {
                        "name": "Jinwoo Shin"
                    },
                    {
                        "name": "Aram Galstyan"
                    },
                    {
                        "name": "Shubham Katiyar"
                    },
                    {
                        "name": "Sravan Babu Bodapati"
                    }
                ],
                "author_detail": {
                    "name": "Sravan Babu Bodapati"
                },
                "author": "Sravan Babu Bodapati"
            },
            {
                "id": "http://arxiv.org/abs/2504.06261v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.06261v4",
                "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention"
                },
                "updated": "2025-11-17T11:11:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    11,
                    11,
                    28,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.06261v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.06261v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM \"workers\" in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the LLM instances to come up with their own collaboration strategy for the problem at hand, all the while \"seeing\" each other's memory in the concurrent KV cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with \"instant\" access to each other's memory. Hogwild! Inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM \"workers\" in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the LLM instances to come up with their own collaboration strategy for the problem at hand, all the while \"seeing\" each other's memory in the concurrent KV cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with \"instant\" access to each other's memory. Hogwild! Inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-08T17:59:41Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    41,
                    1,
                    98,
                    0
                ],
                "arxiv_comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Gleb Rodionov"
                    },
                    {
                        "name": "Roman Garipov"
                    },
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "George Yakushev"
                    },
                    {
                        "name": "Erik Schultheis"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Anton Sinitsin"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh"
            },
            {
                "id": "http://arxiv.org/abs/2511.13078v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13078v1",
                "title": "A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning"
                },
                "updated": "2025-11-17T07:27:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    7,
                    27,
                    52,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13078v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Emergency Medical Technicians (EMTs) operate in high-pressure environments, making rapid, life-critical decisions under heavy cognitive and operational loads. We present EMSGlass, a smart-glasses system powered by EMSNet, the first multimodal multitask model for Emergency Medical Services (EMS), and EMSServe, a low-latency multimodal serving framework tailored to EMS scenarios. EMSNet integrates text, vital signs, and scene images to construct a unified real-time understanding of EMS incidents. Trained on real-world multimodal EMS datasets, EMSNet simultaneously supports up to five critical EMS tasks with superior accuracy compared to state-of-the-art unimodal baselines. Built on top of PyTorch, EMSServe introduces a modality-aware model splitter and a feature caching mechanism, achieving adaptive and efficient inference across heterogeneous hardware while addressing the challenge of asynchronous modality arrival in the field. By optimizing multimodal inference execution in EMS scenarios, EMSServe achieves 1.9x -- 11.7x speedup over direct PyTorch multimodal inference. A user study evaluation with six professional EMTs demonstrates that EMSGlass enhances real-time situational awareness, decision-making speed, and operational efficiency through intuitive on-glass interaction. In addition, qualitative insights from the user study provide actionable directions for extending EMSGlass toward next-generation AI-enabled EMS systems, bridging multimodal intelligence with real-world emergency response workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergency Medical Technicians (EMTs) operate in high-pressure environments, making rapid, life-critical decisions under heavy cognitive and operational loads. We present EMSGlass, a smart-glasses system powered by EMSNet, the first multimodal multitask model for Emergency Medical Services (EMS), and EMSServe, a low-latency multimodal serving framework tailored to EMS scenarios. EMSNet integrates text, vital signs, and scene images to construct a unified real-time understanding of EMS incidents. Trained on real-world multimodal EMS datasets, EMSNet simultaneously supports up to five critical EMS tasks with superior accuracy compared to state-of-the-art unimodal baselines. Built on top of PyTorch, EMSServe introduces a modality-aware model splitter and a feature caching mechanism, achieving adaptive and efficient inference across heterogeneous hardware while addressing the challenge of asynchronous modality arrival in the field. By optimizing multimodal inference execution in EMS scenarios, EMSServe achieves 1.9x -- 11.7x speedup over direct PyTorch multimodal inference. A user study evaluation with six professional EMTs demonstrates that EMSGlass enhances real-time situational awareness, decision-making speed, and operational efficiency through intuitive on-glass interaction. In addition, qualitative insights from the user study provide actionable directions for extending EMSGlass toward next-generation AI-enabled EMS systems, bridging multimodal intelligence with real-world emergency response workflows."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T07:27:52Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    7,
                    27,
                    52,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Liuyi Jin"
                    },
                    {
                        "name": "Pasan Gunawardena"
                    },
                    {
                        "name": "Amran Haroon"
                    },
                    {
                        "name": "Runzhi Wang"
                    },
                    {
                        "name": "Sangwoo Lee"
                    },
                    {
                        "name": "Radu Stoleru"
                    },
                    {
                        "name": "Michael Middleton"
                    },
                    {
                        "name": "Zepeng Huo"
                    },
                    {
                        "name": "Jeeeun Kim"
                    },
                    {
                        "name": "Jason Moats"
                    }
                ],
                "author_detail": {
                    "name": "Jason Moats"
                },
                "author": "Jason Moats"
            },
            {
                "id": "http://arxiv.org/abs/2507.21761v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.21761v3",
                "title": "IMC-Net: A Lightweight Content-Conditioned Encoder with Multi-Pass Processing for Image Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IMC-Net: A Lightweight Content-Conditioned Encoder with Multi-Pass Processing for Image Classification"
                },
                "updated": "2025-11-17T06:40:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    6,
                    40,
                    18,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.21761v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.21761v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present a compact encoder for image categorization that emphasizes computation economy through content-conditioned multi-pass processing. The model employs a single lightweight core block that can be re-applied a small number of times, while a simple score-based selector decides whether further passes are beneficial for each region unit in the feature map. This design provides input-conditioned depth without introducing heavy auxiliary modules or specialized pretraining. On standard benchmarks, the approach attains competitive accuracy with reduced parameters, lower floating-point operations, and faster inference compared to similarly sized baselines. The method keeps the architecture minimal, implements module reuse to control footprint, and preserves stable training via mild regularization on selection scores. We discuss implementation choices for efficient masking, pass control, and representation caching, and show that the multi-pass strategy transfers well to several datasets without requiring task-specific customization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a compact encoder for image categorization that emphasizes computation economy through content-conditioned multi-pass processing. The model employs a single lightweight core block that can be re-applied a small number of times, while a simple score-based selector decides whether further passes are beneficial for each region unit in the feature map. This design provides input-conditioned depth without introducing heavy auxiliary modules or specialized pretraining. On standard benchmarks, the approach attains competitive accuracy with reduced parameters, lower floating-point operations, and faster inference compared to similarly sized baselines. The method keeps the architecture minimal, implements module reuse to control footprint, and preserves stable training via mild regularization on selection scores. We discuss implementation choices for efficient masking, pass control, and representation caching, and show that the multi-pass strategy transfers well to several datasets without requiring task-specific customization."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-29T12:46:36Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    12,
                    46,
                    36,
                    1,
                    210,
                    0
                ],
                "arxiv_comment": "13 pages,6 figuers",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "YiZhou Li"
                    }
                ],
                "author_detail": {
                    "name": "YiZhou Li"
                },
                "author": "YiZhou Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.12979v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.12979v1",
                "title": "RAGPulse: An Open-Source RAG Workload Trace to Optimize RAG Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGPulse: An Open-Source RAG Workload Trace to Optimize RAG Serving Systems"
                },
                "updated": "2025-11-17T05:06:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    5,
                    6,
                    47,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.12979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.12979v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Retrieval-Augmented Generation (RAG) is a critical paradigm for building reliable, knowledge-intensive Large Language Model (LLM) applications. However, the multi-stage pipeline (retrieve, generate) and unique workload characteristics (e.g., knowledge dependency) of RAG systems pose significant challenges for serving performance optimization. Existing generic LLM inference traces fail to capture these RAG-specific dynamics, creating a significant performance gap between academic research and real-world deployment. To bridge this gap, this paper introduces RAGPulse, an open-source RAG workload trace dataset. This dataset was collected from an university-wide Q&A system serving that has served more than 40,000 students and faculties since April 2024. We detail RAGPulse's system architecture, its privacy-preserving hash-based data format, and provide an in-depth statistical analysis. Our analysis reveals that real-world RAG workloads exhibit significant temporal locality and a highly skewed hot document access pattern. RAGPulse provides a high-fidelity foundation for researchers to develop and validate novel optimization strategies for RAG systems, such as content-aware batching and retrieval caching, ultimately enhancing the efficiency and reliability of RAG services. The code is available at https://github.com/flashserve/RAGPulse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) is a critical paradigm for building reliable, knowledge-intensive Large Language Model (LLM) applications. However, the multi-stage pipeline (retrieve, generate) and unique workload characteristics (e.g., knowledge dependency) of RAG systems pose significant challenges for serving performance optimization. Existing generic LLM inference traces fail to capture these RAG-specific dynamics, creating a significant performance gap between academic research and real-world deployment. To bridge this gap, this paper introduces RAGPulse, an open-source RAG workload trace dataset. This dataset was collected from an university-wide Q&A system serving that has served more than 40,000 students and faculties since April 2024. We detail RAGPulse's system architecture, its privacy-preserving hash-based data format, and provide an in-depth statistical analysis. Our analysis reveals that real-world RAG workloads exhibit significant temporal locality and a highly skewed hot document access pattern. RAGPulse provides a high-fidelity foundation for researchers to develop and validate novel optimization strategies for RAG systems, such as content-aware batching and retrieval caching, ultimately enhancing the efficiency and reliability of RAG services. The code is available at https://github.com/flashserve/RAGPulse."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T05:06:47Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    5,
                    6,
                    47,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Zhengchao Wang"
                    },
                    {
                        "name": "Yitao Hu"
                    },
                    {
                        "name": "Jianing Ye"
                    },
                    {
                        "name": "Zhuxuan Chang"
                    },
                    {
                        "name": "Jiazheng Yu"
                    },
                    {
                        "name": "Youpeng Deng"
                    },
                    {
                        "name": "Keqiu Li"
                    }
                ],
                "author_detail": {
                    "name": "Keqiu Li"
                },
                "author": "Keqiu Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.00090v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.00090v2",
                "title": "LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation"
                },
                "updated": "2025-11-17T02:55:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    2,
                    55,
                    48,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.00090v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.00090v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present LeMiCa, a training-free and efficient acceleration framework for diffusion-based video generation. While existing caching strategies primarily focus on reducing local heuristic errors, they often overlook the accumulation of global errors, leading to noticeable content degradation between accelerated and original videos. To address this issue, we formulate cache scheduling as a directed graph with error-weighted edges and introduce a Lexicographic Minimax Path Optimization strategy that explicitly bounds the worst-case path error. This approach substantially improves the consistency of global content and style across generated frames. Extensive experiments on multiple text-to-video benchmarks demonstrate that LeMiCa delivers dual improvements in both inference speed and generation quality. Notably, our method achieves a 2.9x speedup on the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming prior caching techniques. Importantly, these gains come with minimal perceptual quality degradation, making LeMiCa a robust and generalizable paradigm for accelerating diffusion-based video generation. We believe this approach can serve as a strong foundation for future research on efficient and reliable video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LeMiCa, a training-free and efficient acceleration framework for diffusion-based video generation. While existing caching strategies primarily focus on reducing local heuristic errors, they often overlook the accumulation of global errors, leading to noticeable content degradation between accelerated and original videos. To address this issue, we formulate cache scheduling as a directed graph with error-weighted edges and introduce a Lexicographic Minimax Path Optimization strategy that explicitly bounds the worst-case path error. This approach substantially improves the consistency of global content and style across generated frames. Extensive experiments on multiple text-to-video benchmarks demonstrate that LeMiCa delivers dual improvements in both inference speed and generation quality. Notably, our method achieves a 2.9x speedup on the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming prior caching techniques. Importantly, these gains come with minimal perceptual quality degradation, making LeMiCa a robust and generalizable paradigm for accelerating diffusion-based video generation. We believe this approach can serve as a strong foundation for future research on efficient and reliable video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-30T04:57:26Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    4,
                    57,
                    26,
                    3,
                    303,
                    0
                ],
                "arxiv_comment": "NeurIPS 2025 Spotlight",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Huanlin Gao"
                    },
                    {
                        "name": "Ping Chen"
                    },
                    {
                        "name": "Fuyuan Shi"
                    },
                    {
                        "name": "Chao Tan"
                    },
                    {
                        "name": "Zhaoxiang Liu"
                    },
                    {
                        "name": "Fang Zhao"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Shiguo Lian"
                    }
                ],
                "author_detail": {
                    "name": "Shiguo Lian"
                },
                "author": "Shiguo Lian"
            },
            {
                "id": "http://arxiv.org/abs/2511.12876v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.12876v1",
                "title": "Think, Speak, Decide: Language-Augmented Multi-Agent Reinforcement Learning for Economic Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think, Speak, Decide: Language-Augmented Multi-Agent Reinforcement Learning for Economic Decision-Making"
                },
                "updated": "2025-11-17T02:09:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    2,
                    9,
                    18,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.12876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.12876v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Economic decision-making depends not only on structured signals such as prices and taxes, but also on unstructured language, including peer dialogue and media narratives. While multi-agent reinforcement learning (MARL) has shown promise in optimizing economic decisions, it struggles with the semantic ambiguity and contextual richness of language. We propose LAMP (Language-Augmented Multi-Agent Policy), a framework that integrates language into economic decision-making and narrows the gap to real-world settings. LAMP follows a Think-Speak-Decide pipeline: (1) Think interprets numerical observations to extract short-term shocks and long-term trends, caching high-value reasoning trajectories; (2) Speak crafts and exchanges strategic messages based on reasoning, updating beliefs by parsing peer communications; and (3) Decide fuses numerical data, reasoning, and reflections into a MARL policy to optimize language-augmented decision-making. Experiments in economic simulation show that LAMP outperforms both MARL and LLM-only baselines in cumulative return (+63.5%, +34.0%), robustness (+18.8%, +59.4%), and interpretability. These results demonstrate the potential of language-augmented policies to deliver more effective and robust economic strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Economic decision-making depends not only on structured signals such as prices and taxes, but also on unstructured language, including peer dialogue and media narratives. While multi-agent reinforcement learning (MARL) has shown promise in optimizing economic decisions, it struggles with the semantic ambiguity and contextual richness of language. We propose LAMP (Language-Augmented Multi-Agent Policy), a framework that integrates language into economic decision-making and narrows the gap to real-world settings. LAMP follows a Think-Speak-Decide pipeline: (1) Think interprets numerical observations to extract short-term shocks and long-term trends, caching high-value reasoning trajectories; (2) Speak crafts and exchanges strategic messages based on reasoning, updating beliefs by parsing peer communications; and (3) Decide fuses numerical data, reasoning, and reflections into a MARL policy to optimize language-augmented decision-making. Experiments in economic simulation show that LAMP outperforms both MARL and LLM-only baselines in cumulative return (+63.5%, +34.0%), robustness (+18.8%, +59.4%), and interpretability. These results demonstrate the potential of language-augmented policies to deliver more effective and robust economic strategies."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T02:09:18Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    2,
                    9,
                    18,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "Extended version of a submission to AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Heyang Ma"
                    },
                    {
                        "name": "Qirui Mi"
                    },
                    {
                        "name": "Qipeng Yang"
                    },
                    {
                        "name": "Zijun Fan"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Haifeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Haifeng Zhang"
                },
                "author": "Haifeng Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.06838v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.06838v3",
                "title": "P3-LLM: An Integrated NPU-PIM Accelerator for LLM Inference Using Hybrid Numerical Formats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "P3-LLM: An Integrated NPU-PIM Accelerator for LLM Inference Using Hybrid Numerical Formats"
                },
                "updated": "2025-11-16T22:19:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    16,
                    22,
                    19,
                    39,
                    6,
                    320,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.06838v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.06838v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The substantial memory bandwidth and computational demands of large language models (LLMs) present critical challenges for efficient inference. To tackle this, the literature has explored heterogeneous systems that combine neural processing units (NPUs) with DRAM-based processing-in-memory (PIM) for LLM acceleration. However, existing high-precision (e.g., FP16) PIM compute units incur significant area and power overhead in DRAM technology, limiting the effective computation throughput. In this paper, we introduce P3-LLM, a novel NPU-PIM integrated accelerator for LLM inference using hybrid numerical formats. Our approach is threefold: First, we propose a flexible mixed-precision quantization scheme, which leverages hybrid numerical formats to quantize different LLM operands with high compression efficiency and minimal accuracy loss. Second, we architect an efficient PIM accelerator for P3-LLM, featuring enhanced compute units to support hybrid numerical formats. Our careful choice of numerical formats allows to co-design low-precision PIM compute units that significantly boost the computation throughput under iso-area constraints. Third, we optimize the low-precision dataflow of different LLM modules by applying operator fusion to minimize the overhead of runtime dequantization. Evaluation on a diverse set of representative LLMs and tasks demonstrates that P3-LLM achieves state-of-the-art accuracy in terms of both KV-cache quantization and weight-activation quantization. Combining the proposed quantization scheme with PIM architecture co-design, P3-LLM yields an average of $4.9\\times$, $2.0\\times$, and $3.4\\times$ speedups over the state-of-the-art LLM accelerators HBM-PIM, Ecco, and Pimba, respectively. Our quantization code is available at https://github.com/yc2367/P3-LLM.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The substantial memory bandwidth and computational demands of large language models (LLMs) present critical challenges for efficient inference. To tackle this, the literature has explored heterogeneous systems that combine neural processing units (NPUs) with DRAM-based processing-in-memory (PIM) for LLM acceleration. However, existing high-precision (e.g., FP16) PIM compute units incur significant area and power overhead in DRAM technology, limiting the effective computation throughput. In this paper, we introduce P3-LLM, a novel NPU-PIM integrated accelerator for LLM inference using hybrid numerical formats. Our approach is threefold: First, we propose a flexible mixed-precision quantization scheme, which leverages hybrid numerical formats to quantize different LLM operands with high compression efficiency and minimal accuracy loss. Second, we architect an efficient PIM accelerator for P3-LLM, featuring enhanced compute units to support hybrid numerical formats. Our careful choice of numerical formats allows to co-design low-precision PIM compute units that significantly boost the computation throughput under iso-area constraints. Third, we optimize the low-precision dataflow of different LLM modules by applying operator fusion to minimize the overhead of runtime dequantization. Evaluation on a diverse set of representative LLMs and tasks demonstrates that P3-LLM achieves state-of-the-art accuracy in terms of both KV-cache quantization and weight-activation quantization. Combining the proposed quantization scheme with PIM architecture co-design, P3-LLM yields an average of $4.9\\times$, $2.0\\times$, and $3.4\\times$ speedups over the state-of-the-art LLM accelerators HBM-PIM, Ecco, and Pimba, respectively. Our quantization code is available at https://github.com/yc2367/P3-LLM.git"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-10T08:29:34Z",
                "published_parsed": [
                    2025,
                    11,
                    10,
                    8,
                    29,
                    34,
                    0,
                    314,
                    0
                ],
                "arxiv_comment": "Preprint. Under review",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Yuzong Chen"
                    },
                    {
                        "name": "Chao Fang"
                    },
                    {
                        "name": "Xilai Dai"
                    },
                    {
                        "name": "Yuheng Wu"
                    },
                    {
                        "name": "Thierry Tambe"
                    },
                    {
                        "name": "Marian Verhelst"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah"
            },
            {
                "id": "http://arxiv.org/abs/2511.12752v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.12752v1",
                "title": "Whose Narrative is it Anyway? A KV Cache Manipulation Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whose Narrative is it Anyway? A KV Cache Manipulation Attack"
                },
                "updated": "2025-11-16T19:38:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    16,
                    19,
                    38,
                    28,
                    6,
                    320,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.12752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.12752v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Key Value(KV) cache is an important component for efficient inference in autoregressive Large Language Models (LLMs), but its role as a representation of the model's internal state makes it a potential target for integrity attacks. This paper introduces \"History Swapping,\" a novel block-level attack that manipulates the KV cache to steer model generation without altering the user-facing prompt. The attack involves overwriting a contiguous segment of the active generation's cache with a precomputed cache from a different topic. We empirically evaluate this method across 324 configurations on the Qwen 3 family of models, analyzing the impact of timing, magnitude, and layer depth of the cache overwrite. Our findings reveal that only full-layer overwrites can successfully hijack the conversation's topic, leading to three distinct behaviors: immediate and persistent topic shift, partial recovery, or a delayed hijack. Furthermore, we observe that high-level structural plans are encoded early in the generation process and local discourse structure is maintained by the final layers of the model. This work demonstrates that the KV cache is a significant vector for security analysis, as it encodes not just context but also topic trajectory and structural planning, making it a powerful interface for manipulating model behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key Value(KV) cache is an important component for efficient inference in autoregressive Large Language Models (LLMs), but its role as a representation of the model's internal state makes it a potential target for integrity attacks. This paper introduces \"History Swapping,\" a novel block-level attack that manipulates the KV cache to steer model generation without altering the user-facing prompt. The attack involves overwriting a contiguous segment of the active generation's cache with a precomputed cache from a different topic. We empirically evaluate this method across 324 configurations on the Qwen 3 family of models, analyzing the impact of timing, magnitude, and layer depth of the cache overwrite. Our findings reveal that only full-layer overwrites can successfully hijack the conversation's topic, leading to three distinct behaviors: immediate and persistent topic shift, partial recovery, or a delayed hijack. Furthermore, we observe that high-level structural plans are encoded early in the generation process and local discourse structure is maintained by the final layers of the model. This work demonstrates that the KV cache is a significant vector for security analysis, as it encodes not just context but also topic trajectory and structural planning, making it a powerful interface for manipulating model behavior."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-16T19:38:28Z",
                "published_parsed": [
                    2025,
                    11,
                    16,
                    19,
                    38,
                    28,
                    6,
                    320,
                    0
                ],
                "arxiv_comment": "7 pages, 10 figures",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Mukkesh Ganesh"
                    },
                    {
                        "name": "Kaushik Iyer"
                    },
                    {
                        "name": "Arun Baalaaji Sankar Ananthan"
                    }
                ],
                "author_detail": {
                    "name": "Arun Baalaaji Sankar Ananthan"
                },
                "author": "Arun Baalaaji Sankar Ananthan"
            },
            {
                "id": "http://arxiv.org/abs/2511.12631v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.12631v1",
                "title": "Multivariate Diffusion Transformer with Decoupled Attention for High-Fidelity Mask-Text Collaborative Facial Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivariate Diffusion Transformer with Decoupled Attention for High-Fidelity Mask-Text Collaborative Facial Generation"
                },
                "updated": "2025-11-16T14:52:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    16,
                    14,
                    52,
                    54,
                    6,
                    320,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.12631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.12631v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While significant progress has been achieved in multimodal facial generation using semantic masks and textual descriptions, conventional feature fusion approaches often fail to enable effective cross-modal interactions, thereby leading to suboptimal generation outcomes. To address this challenge, we introduce MDiTFace--a customized diffusion transformer framework that employs a unified tokenization strategy to process semantic mask and text inputs, eliminating discrepancies between heterogeneous modality representations. The framework facilitates comprehensive multimodal feature interaction through stacked, newly designed multivariate transformer blocks that process all conditions synchronously. Additionally, we design a novel decoupled attention mechanism by dissociating implicit dependencies between mask tokens and temporal embeddings. This mechanism segregates internal computations into dynamic and static pathways, enabling caching and reuse of features computed in static pathways after initial calculation, thereby reducing additional computational overhead introduced by mask condition by over 94% while maintaining performance. Extensive experiments demonstrate that MDiTFace significantly outperforms other competing methods in terms of both facial fidelity and conditional consistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While significant progress has been achieved in multimodal facial generation using semantic masks and textual descriptions, conventional feature fusion approaches often fail to enable effective cross-modal interactions, thereby leading to suboptimal generation outcomes. To address this challenge, we introduce MDiTFace--a customized diffusion transformer framework that employs a unified tokenization strategy to process semantic mask and text inputs, eliminating discrepancies between heterogeneous modality representations. The framework facilitates comprehensive multimodal feature interaction through stacked, newly designed multivariate transformer blocks that process all conditions synchronously. Additionally, we design a novel decoupled attention mechanism by dissociating implicit dependencies between mask tokens and temporal embeddings. This mechanism segregates internal computations into dynamic and static pathways, enabling caching and reuse of features computed in static pathways after initial calculation, thereby reducing additional computational overhead introduced by mask condition by over 94% while maintaining performance. Extensive experiments demonstrate that MDiTFace significantly outperforms other competing methods in terms of both facial fidelity and conditional consistency."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-16T14:52:54Z",
                "published_parsed": [
                    2025,
                    11,
                    16,
                    14,
                    52,
                    54,
                    6,
                    320,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yushe Cao"
                    },
                    {
                        "name": "Dianxi Shi"
                    },
                    {
                        "name": "Xing Fu"
                    },
                    {
                        "name": "Xuechao Zou"
                    },
                    {
                        "name": "Haikuo Peng"
                    },
                    {
                        "name": "Xueqi Li"
                    },
                    {
                        "name": "Chun Yu"
                    },
                    {
                        "name": "Junliang Xing"
                    }
                ],
                "author_detail": {
                    "name": "Junliang Xing"
                },
                "author": "Junliang Xing"
            },
            {
                "id": "http://arxiv.org/abs/2511.12286v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.12286v1",
                "title": "Sangam: Chiplet-Based DRAM-PIM Accelerator with CXL Integration for LLM Inferencing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sangam: Chiplet-Based DRAM-PIM Accelerator with CXL Integration for LLM Inferencing"
                },
                "updated": "2025-11-15T16:39:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    15,
                    16,
                    39,
                    51,
                    5,
                    319,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.12286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.12286v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are becoming increasingly data-intensive due to growing model sizes, and they are becoming memory-bound as the context length and, consequently, the key-value (KV) cache size increase. Inference, particularly the decoding phase, is dominated by memory-bound GEMV or flat GEMM operations with low operational intensity (OI), making it well-suited for processing-in-memory (PIM) approaches. However, existing in/near-memory solutions face critical limitations such as reduced memory capacity due to the high area cost of integrating processing elements (PEs) within DRAM chips, and limited PE capability due to the constraints of DRAM fabrication technology. This work presents a chiplet-based memory module that addresses these limitations by decoupling logic and memory into chiplets fabricated in heterogeneous technology nodes and connected via an interposer. The logic chiplets sustain high bandwidth access to the DRAM chiplets, which house the memory banks, and enable the integration of advanced processing components such as systolic arrays and SRAM-based buffers to accelerate memory-bound GEMM kernels, capabilities that were not feasible in prior PIM architectures. We propose Sangam, a CXL-attached PIM-chiplet based memory module that can either act as a drop-in replacement for GPUs or co-executes along side the GPUs. Sangam achieves speedup of 3.93, 4.22, 2.82x speedup in end-to-end query latency, 10.3, 9.5, 6.36x greater decoding throughput, and order of magnitude energy savings compared to an H100 GPU for varying input size, output length, and batch size on LLaMA 2-7B, Mistral-7B, and LLaMA 3-70B, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are becoming increasingly data-intensive due to growing model sizes, and they are becoming memory-bound as the context length and, consequently, the key-value (KV) cache size increase. Inference, particularly the decoding phase, is dominated by memory-bound GEMV or flat GEMM operations with low operational intensity (OI), making it well-suited for processing-in-memory (PIM) approaches. However, existing in/near-memory solutions face critical limitations such as reduced memory capacity due to the high area cost of integrating processing elements (PEs) within DRAM chips, and limited PE capability due to the constraints of DRAM fabrication technology. This work presents a chiplet-based memory module that addresses these limitations by decoupling logic and memory into chiplets fabricated in heterogeneous technology nodes and connected via an interposer. The logic chiplets sustain high bandwidth access to the DRAM chiplets, which house the memory banks, and enable the integration of advanced processing components such as systolic arrays and SRAM-based buffers to accelerate memory-bound GEMM kernels, capabilities that were not feasible in prior PIM architectures. We propose Sangam, a CXL-attached PIM-chiplet based memory module that can either act as a drop-in replacement for GPUs or co-executes along side the GPUs. Sangam achieves speedup of 3.93, 4.22, 2.82x speedup in end-to-end query latency, 10.3, 9.5, 6.36x greater decoding throughput, and order of magnitude energy savings compared to an H100 GPU for varying input size, output length, and batch size on LLaMA 2-7B, Mistral-7B, and LLaMA 3-70B, respectively."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-15T16:39:51Z",
                "published_parsed": [
                    2025,
                    11,
                    15,
                    16,
                    39,
                    51,
                    5,
                    319,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Khyati Kiyawat"
                    },
                    {
                        "name": "Zhenxing Fan"
                    },
                    {
                        "name": "Yasas Seneviratne"
                    },
                    {
                        "name": "Morteza Baradaran"
                    },
                    {
                        "name": "Akhil Shekar"
                    },
                    {
                        "name": "Zihan Xia"
                    },
                    {
                        "name": "Mingu Kang"
                    },
                    {
                        "name": "Kevin Skadron"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Skadron"
                },
                "author": "Kevin Skadron"
            },
            {
                "id": "http://arxiv.org/abs/2511.12136v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.12136v1",
                "title": "Compression and Inference of Spiking Neural Networks on Resource-Constrained Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compression and Inference of Spiking Neural Networks on Resource-Constrained Hardware"
                },
                "updated": "2025-11-15T10:02:23Z",
                "updated_parsed": [
                    2025,
                    11,
                    15,
                    10,
                    2,
                    23,
                    5,
                    319,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.12136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.12136v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Spiking neural networks (SNNs) communicate via discrete spikes in time rather than continuous activations. Their event-driven nature offers advantages for temporal processing and energy efficiency on resource-constrained hardware, but training and deployment remain challenging. We present a lightweight C-based runtime for SNN inference on edge devices and optimizations that reduce latency and memory without sacrificing accuracy. Trained models exported from SNNTorch are translated to a compact C representation; static, cache-friendly data layouts and preallocation avoid interpreter and allocation overheads. We further exploit sparse spiking activity to prune inactive neurons and synapses, shrinking computation in upstream convolutional layers. Experiments on N-MNIST and ST-MNIST show functional parity with the Python baseline while achieving ~10 speedups on desktop CPU and additional gains with pruning, together with large memory reductions that enable microcontroller deployment (Arduino Portenta H7). Results indicate that SNNs can be executed efficiently on conventional embedded platforms when paired with an optimized runtime and spike-driven model compression. Code: https://github.com/karol-jurzec/snn-generator/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking neural networks (SNNs) communicate via discrete spikes in time rather than continuous activations. Their event-driven nature offers advantages for temporal processing and energy efficiency on resource-constrained hardware, but training and deployment remain challenging. We present a lightweight C-based runtime for SNN inference on edge devices and optimizations that reduce latency and memory without sacrificing accuracy. Trained models exported from SNNTorch are translated to a compact C representation; static, cache-friendly data layouts and preallocation avoid interpreter and allocation overheads. We further exploit sparse spiking activity to prune inactive neurons and synapses, shrinking computation in upstream convolutional layers. Experiments on N-MNIST and ST-MNIST show functional parity with the Python baseline while achieving ~10 speedups on desktop CPU and additional gains with pruning, together with large memory reductions that enable microcontroller deployment (Arduino Portenta H7). Results indicate that SNNs can be executed efficiently on conventional embedded platforms when paired with an optimized runtime and spike-driven model compression. Code: https://github.com/karol-jurzec/snn-generator/"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-15T10:02:23Z",
                "published_parsed": [
                    2025,
                    11,
                    15,
                    10,
                    2,
                    23,
                    5,
                    319,
                    0
                ],
                "arxiv_comment": "6 pages, 6 figures, 1 table; code available at https://github.com/karol-jurzec/snn-generator/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Karol C. Jurzec"
                    },
                    {
                        "name": "Tomasz Szydlo"
                    },
                    {
                        "name": "Maciej Wielgosz"
                    }
                ],
                "author_detail": {
                    "name": "Maciej Wielgosz"
                },
                "author": "Maciej Wielgosz"
            },
            {
                "id": "http://arxiv.org/abs/2511.12031v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.12031v1",
                "title": "Striking the Right Balance between Compute and Copy: Improving LLM Inferencing Under Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Striking the Right Balance between Compute and Copy: Improving LLM Inferencing Under Speculative Decoding"
                },
                "updated": "2025-11-15T04:49:23Z",
                "updated_parsed": [
                    2025,
                    11,
                    15,
                    4,
                    49,
                    23,
                    5,
                    319,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.12031v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.12031v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the skyrocketing costs of GPUs and their virtual instances in the cloud, there is a significant desire to use CPUs for large language model (LLM) inference. KV cache update, often implemented as allocation, copying, and in-place strided update for each generated token, incurs significant overhead. As the sequence length increases, the allocation and copy overheads dominate the performance. Alternate approaches may allocate large KV tensors upfront to enable in-place updates, but these matrices (with zero-padded rows) cause redundant computations. In this work, we propose a new KV cache allocation mechanism called Balancing Memory and Compute (BMC). BMC allocates, once every r iterations, KV tensors with r redundant rows, allowing in-place update without copy overhead for those iterations, but at the expense of a small amount of redundant computation. Second, we make an interesting observation that the extra rows allocated in the KV tensors and the resulting redundant computation can be repurposed for Speculative Decoding (SD) that improves token generation efficiency. Last, BMC represents a spectrum of design points with different values of r. To identify the best-performing design point(s), we derive a simple analytical model for BMC. The proposed BMC method achieves an average throughput acceleration of up to 3.2x over baseline HuggingFace (without SD). Importantly when we apply BMC with SD, it results in an additional speedup of up to 1.39x, over and above the speedup offered by SD. Further, BMC achieves a throughput acceleration of up to 1.36x and 2.29x over state-of-the-art inference servers vLLM and DeepSpeed, respectively. Although the BMC technique is evaluated extensively across different classes of CPUs (desktop and server class), we also evaluate the scheme with GPUs and demonstrate that it works well for GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the skyrocketing costs of GPUs and their virtual instances in the cloud, there is a significant desire to use CPUs for large language model (LLM) inference. KV cache update, often implemented as allocation, copying, and in-place strided update for each generated token, incurs significant overhead. As the sequence length increases, the allocation and copy overheads dominate the performance. Alternate approaches may allocate large KV tensors upfront to enable in-place updates, but these matrices (with zero-padded rows) cause redundant computations. In this work, we propose a new KV cache allocation mechanism called Balancing Memory and Compute (BMC). BMC allocates, once every r iterations, KV tensors with r redundant rows, allowing in-place update without copy overhead for those iterations, but at the expense of a small amount of redundant computation. Second, we make an interesting observation that the extra rows allocated in the KV tensors and the resulting redundant computation can be repurposed for Speculative Decoding (SD) that improves token generation efficiency. Last, BMC represents a spectrum of design points with different values of r. To identify the best-performing design point(s), we derive a simple analytical model for BMC. The proposed BMC method achieves an average throughput acceleration of up to 3.2x over baseline HuggingFace (without SD). Importantly when we apply BMC with SD, it results in an additional speedup of up to 1.39x, over and above the speedup offered by SD. Further, BMC achieves a throughput acceleration of up to 1.36x and 2.29x over state-of-the-art inference servers vLLM and DeepSpeed, respectively. Although the BMC technique is evaluated extensively across different classes of CPUs (desktop and server class), we also evaluate the scheme with GPUs and demonstrate that it works well for GPUs."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-15T04:49:23Z",
                "published_parsed": [
                    2025,
                    11,
                    15,
                    4,
                    49,
                    23,
                    5,
                    319,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Arun Ramachandran"
                    },
                    {
                        "name": "Ramaswamy Govindarajan"
                    },
                    {
                        "name": "Murali Annavaram"
                    },
                    {
                        "name": "Prakash Raghavendra"
                    },
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Chaoyi Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Chaoyi Jiang"
                },
                "author": "Chaoyi Jiang"
            },
            {
                "id": "http://arxiv.org/abs/2511.11907v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.11907v1",
                "title": "KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference"
                },
                "updated": "2025-11-14T22:37:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    14,
                    22,
                    37,
                    57,
                    4,
                    318,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.11907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.11907v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Language models (LMs) underpin emerging mobile and embedded AI applications like meeting and video summarization and document analysis, which often require processing multiple long-context inputs. Running an LM locally on-device improves privacy, enables offline use, and reduces cost, but long-context inference quickly hits a \\emph{memory capacity wall} as the key-value (KV) cache grows linearly with context length and batch size.\n  We present KVSwap, a software framework to break this memory wall by offloading the KV cache to non-volatile secondary storage (disk). KVSwap leverages the observation that only a small, dynamically changing subset of KV entries is critical for generation. It stores the full cache on disk, uses a compact in-memory metadata to predict which entries to preload, overlaps computation with hardware-aware disk access, and orchestrates read patterns to match storage device characteristics. Our evaluation shows that across representative LMs and storage types, KVSwap delivers higher throughput under tight memory budgets while maintaining the generation quality when compared with existing KV cache offloading schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models (LMs) underpin emerging mobile and embedded AI applications like meeting and video summarization and document analysis, which often require processing multiple long-context inputs. Running an LM locally on-device improves privacy, enables offline use, and reduces cost, but long-context inference quickly hits a \\emph{memory capacity wall} as the key-value (KV) cache grows linearly with context length and batch size.\n  We present KVSwap, a software framework to break this memory wall by offloading the KV cache to non-volatile secondary storage (disk). KVSwap leverages the observation that only a small, dynamically changing subset of KV entries is critical for generation. It stores the full cache on disk, uses a compact in-memory metadata to predict which entries to preload, overlaps computation with hardware-aware disk access, and orchestrates read patterns to match storage device characteristics. Our evaluation shows that across representative LMs and storage types, KVSwap delivers higher throughput under tight memory budgets while maintaining the generation quality when compared with existing KV cache offloading schemes."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-14T22:37:57Z",
                "published_parsed": [
                    2025,
                    11,
                    14,
                    22,
                    37,
                    57,
                    4,
                    318,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Huawei Zhang"
                    },
                    {
                        "name": "Chunwei Xia"
                    },
                    {
                        "name": "Zheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Wang"
                },
                "author": "Zheng Wang"
            },
            {
                "id": "http://arxiv.org/abs/2511.03092v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.03092v4",
                "title": "SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators"
                },
                "updated": "2025-11-14T19:14:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    14,
                    19,
                    14,
                    59,
                    4,
                    318,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.03092v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.03092v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+ context length support have resulted in increasing demands for on-chip memory to support large KV caches. Techniques such as StreamingLLM and SnapKV demonstrate how to control KV cache size while maintaining model accuracy. Yet, these techniques are not commonly used within industrial deployments using frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static graphs and continuous batching methodology employed by these frameworks make it difficult to admit modifications to the standard multi-head attention algorithm, while on the other hand, the accuracy implications of such techniques on modern instruction-following and reasoning models are not well understood, obfuscating the need for implementing these techniques. In this paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators running at 128k context length and up to 1832 tokens per second in a real production setting. SnapStream enables $4\\times$ improved on-chip memory usage and introduces minimal accuracy degradation on LongBench-v2, AIME24 and LiveCodeBench. To the best of our knowledge, this is the first implementation of sparse KV attention techniques deployed in a production inference system with static graphs and continuous batching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+ context length support have resulted in increasing demands for on-chip memory to support large KV caches. Techniques such as StreamingLLM and SnapKV demonstrate how to control KV cache size while maintaining model accuracy. Yet, these techniques are not commonly used within industrial deployments using frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static graphs and continuous batching methodology employed by these frameworks make it difficult to admit modifications to the standard multi-head attention algorithm, while on the other hand, the accuracy implications of such techniques on modern instruction-following and reasoning models are not well understood, obfuscating the need for implementing these techniques. In this paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators running at 128k context length and up to 1832 tokens per second in a real production setting. SnapStream enables $4\\times$ improved on-chip memory usage and introduces minimal accuracy degradation on LongBench-v2, AIME24 and LiveCodeBench. To the best of our knowledge, this is the first implementation of sparse KV attention techniques deployed in a production inference system with static graphs and continuous batching."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-05T00:38:31Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    0,
                    38,
                    31,
                    2,
                    309,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jonathan Li"
                    },
                    {
                        "name": "Nasim Farahini"
                    },
                    {
                        "name": "Evgenii Iuliugin"
                    },
                    {
                        "name": "Magnus Vesterlund"
                    },
                    {
                        "name": "Christian Hggstrm"
                    },
                    {
                        "name": "Guangtao Wang"
                    },
                    {
                        "name": "Shubhangi Upasani"
                    },
                    {
                        "name": "Ayush Sachdeva"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Faline Fu"
                    },
                    {
                        "name": "Chen Wu"
                    },
                    {
                        "name": "Ayesha Siddiqua"
                    },
                    {
                        "name": "John Long"
                    },
                    {
                        "name": "Tuowen Zhao"
                    },
                    {
                        "name": "Matheen Musaddiq"
                    },
                    {
                        "name": "Hkan Zeffer"
                    },
                    {
                        "name": "Yun Du"
                    },
                    {
                        "name": "Mingran Wang"
                    },
                    {
                        "name": "Qinghua Li"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Urmish Thakker"
                    },
                    {
                        "name": "Raghu Prabhakar"
                    }
                ],
                "author_detail": {
                    "name": "Raghu Prabhakar"
                },
                "author": "Raghu Prabhakar"
            },
            {
                "id": "http://arxiv.org/abs/2511.11519v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.11519v1",
                "title": "Experience-Guided Adaptation of Inference-Time Reasoning Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experience-Guided Adaptation of Inference-Time Reasoning Strategies"
                },
                "updated": "2025-11-14T17:45:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    14,
                    17,
                    45,
                    28,
                    4,
                    318,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.11519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.11519v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-14T17:45:28Z",
                "published_parsed": [
                    2025,
                    11,
                    14,
                    17,
                    45,
                    28,
                    4,
                    318,
                    0
                ],
                "arxiv_comment": "29 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Adam Stein"
                    },
                    {
                        "name": "Matthew Trager"
                    },
                    {
                        "name": "Benjamin Bowman"
                    },
                    {
                        "name": "Michael Kleinman"
                    },
                    {
                        "name": "Aditya Chattopadhyay"
                    },
                    {
                        "name": "Wei Xia"
                    },
                    {
                        "name": "Stefano Soatto"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Soatto"
                },
                "author": "Stefano Soatto"
            },
            {
                "id": "http://arxiv.org/abs/2508.07570v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.07570v2",
                "title": "Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language Models"
                },
                "updated": "2025-11-14T15:34:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    14,
                    15,
                    34,
                    24,
                    4,
                    318,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.07570v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.07570v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot generalization but suffer performance degradation under distribution shifts in downstream tasks, particularly in the absence of labeled data. Test-Time Adaptation (TTA) addresses this challenge by enabling online optimization of VLMs during inference, eliminating the need for annotated data. Cache-based TTA methods exploit historical knowledge by maintaining a dynamic memory cache of low-entropy or high-confidence samples, promoting efficient adaptation to out-of-distribution data. Nevertheless, these methods face two critical challenges: (1) unreliable confidence metrics under significant distribution shifts, resulting in error accumulation within the cache and degraded adaptation performance; and (2) rigid decision boundaries that fail to accommodate substantial distributional variations, leading to suboptimal predictions. To overcome these limitations, we introduce the Adaptive Cache Enhancement (ACE) framework, which constructs a robust cache by selectively storing high-confidence or low-entropy image embeddings per class, guided by dynamic, class-specific thresholds initialized from zero-shot statistics and iteratively refined using an exponential moving average and exploration-augmented updates. This approach enables adaptive, class-wise decision boundaries, ensuring robust and accurate predictions across diverse visual distributions. Extensive experiments on 15 diverse benchmark datasets demonstrate that ACE achieves state-of-the-art performance, delivering superior robustness and generalization compared to existing TTA methods in challenging out-of-distribution scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) exhibit remarkable zero-shot generalization but suffer performance degradation under distribution shifts in downstream tasks, particularly in the absence of labeled data. Test-Time Adaptation (TTA) addresses this challenge by enabling online optimization of VLMs during inference, eliminating the need for annotated data. Cache-based TTA methods exploit historical knowledge by maintaining a dynamic memory cache of low-entropy or high-confidence samples, promoting efficient adaptation to out-of-distribution data. Nevertheless, these methods face two critical challenges: (1) unreliable confidence metrics under significant distribution shifts, resulting in error accumulation within the cache and degraded adaptation performance; and (2) rigid decision boundaries that fail to accommodate substantial distributional variations, leading to suboptimal predictions. To overcome these limitations, we introduce the Adaptive Cache Enhancement (ACE) framework, which constructs a robust cache by selectively storing high-confidence or low-entropy image embeddings per class, guided by dynamic, class-specific thresholds initialized from zero-shot statistics and iteratively refined using an exponential moving average and exploration-augmented updates. This approach enables adaptive, class-wise decision boundaries, ensuring robust and accurate predictions across diverse visual distributions. Extensive experiments on 15 diverse benchmark datasets demonstrate that ACE achieves state-of-the-art performance, delivering superior robustness and generalization compared to existing TTA methods in challenging out-of-distribution scenarios."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-11T03:03:34Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    3,
                    3,
                    34,
                    0,
                    223,
                    0
                ],
                "arxiv_comment": "12 pages, Under review",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Khanh-Binh Nguyen"
                    },
                    {
                        "name": "Phuoc-Nguyen Bui"
                    },
                    {
                        "name": "Hyunseung Choo"
                    },
                    {
                        "name": "Duc Thanh Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Duc Thanh Nguyen"
                },
                "author": "Duc Thanh Nguyen"
            },
            {
                "id": "http://arxiv.org/abs/2508.19257v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.19257v3",
                "title": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models"
                },
                "updated": "2025-11-14T12:35:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    14,
                    12,
                    35,
                    36,
                    4,
                    318,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.19257v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.19257v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-Language-Action (VLA) models process visual inputs independently at each timestep, discarding valuable temporal information inherent in robotic manipulation tasks. This frame-by-frame processing makes models vulnerable to visual noise while ignoring the substantial coherence between consecutive frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a training-free approach that intelligently integrates historical and current visual representations to enhance VLA inference quality. Our method employs dual-dimension detection combining efficient grayscale pixel difference analysis with attention-based semantic relevance assessment, enabling selective temporal token fusion through hard fusion strategies and keyframe anchoring to prevent error accumulation. Comprehensive experiments across LIBERO, SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0 percentage points average on LIBERO (72.4\\% vs 68.4\\% baseline), cross-environment validation on SimplerEnv (4.8\\% relative improvement), and 8.7\\% relative improvement on real robot tasks. Our approach proves model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably, TTF reveals that selective Query matrix reuse in attention mechanisms enhances rather than compromises performance, suggesting promising directions for direct KQV matrix reuse strategies that achieve computational acceleration while improving task success rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models process visual inputs independently at each timestep, discarding valuable temporal information inherent in robotic manipulation tasks. This frame-by-frame processing makes models vulnerable to visual noise while ignoring the substantial coherence between consecutive frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a training-free approach that intelligently integrates historical and current visual representations to enhance VLA inference quality. Our method employs dual-dimension detection combining efficient grayscale pixel difference analysis with attention-based semantic relevance assessment, enabling selective temporal token fusion through hard fusion strategies and keyframe anchoring to prevent error accumulation. Comprehensive experiments across LIBERO, SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0 percentage points average on LIBERO (72.4\\% vs 68.4\\% baseline), cross-environment validation on SimplerEnv (4.8\\% relative improvement), and 8.7\\% relative improvement on real robot tasks. Our approach proves model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably, TTF reveals that selective Query matrix reuse in attention mechanisms enhances rather than compromises performance, suggesting promising directions for direct KQV matrix reuse strategies that achieve computational acceleration while improving task success rates."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-15T12:03:34Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    3,
                    34,
                    4,
                    227,
                    0
                ],
                "arxiv_comment": "Accepted to AAAI 2026. Camera-ready version",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Chenghao Liu"
                    },
                    {
                        "name": "Jiachen Zhang"
                    },
                    {
                        "name": "Chengxuan Li"
                    },
                    {
                        "name": "Zhimu Zhou"
                    },
                    {
                        "name": "Shixin Wu"
                    },
                    {
                        "name": "Songfang Huang"
                    },
                    {
                        "name": "Huiling Duan"
                    }
                ],
                "author_detail": {
                    "name": "Huiling Duan"
                },
                "author": "Huiling Duan"
            },
            {
                "id": "http://arxiv.org/abs/2407.15743v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2407.15743v3",
                "title": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter Optimization"
                },
                "updated": "2025-11-14T11:01:15Z",
                "updated_parsed": [
                    2025,
                    11,
                    14,
                    11,
                    1,
                    15,
                    4,
                    318,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2407.15743v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2407.15743v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Cache-aided MIMO communications aims to jointly exploit both coded caching~(CC) and spatial multiplexing gains to enhance communication efficiency. In this paper, we analyze both the achievable degrees of freedom~(DoF) under linear processing constraint and the finite-SNR performance of a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\) transmit antennas communicates with \\(K\\) users, each equipped with \\(G\\) receive antennas. We first demonstrate that the enhanced DoF of \\(\\max_{, } \\times \\) is achievable with linear processing, where the number of users \\(\\) served in each transmission is fine-tuned to maximize DoF, and \\(\\le \\min\\big(G, \\nicefrac{L \\binom{-1}{t}}{\\big(1 + (- t - 1)\\binom{-1}{t}}\\big)\\big)\\) represents the number of parallel streams decoded by each user. Then, we propose a new class of MIMO-CC schemes using a novel scheduling mechanism leveraging maximal multicasting opportunities to maximize delivery rates at given SNR levels while still adhering to linear processing constraints. This new class of schemes is paired with an efficient linear multicast beamformer design, resulting in a more practical, high-performance solution for integrating CC in future MIMO systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-aided MIMO communications aims to jointly exploit both coded caching~(CC) and spatial multiplexing gains to enhance communication efficiency. In this paper, we analyze both the achievable degrees of freedom~(DoF) under linear processing constraint and the finite-SNR performance of a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\) transmit antennas communicates with \\(K\\) users, each equipped with \\(G\\) receive antennas. We first demonstrate that the enhanced DoF of \\(\\max_{, } \\times \\) is achievable with linear processing, where the number of users \\(\\) served in each transmission is fine-tuned to maximize DoF, and \\(\\le \\min\\big(G, \\nicefrac{L \\binom{-1}{t}}{\\big(1 + (- t - 1)\\binom{-1}{t}}\\big)\\big)\\) represents the number of parallel streams decoded by each user. Then, we propose a new class of MIMO-CC schemes using a novel scheduling mechanism leveraging maximal multicasting opportunities to maximize delivery rates at given SNR levels while still adhering to linear processing constraints. This new class of schemes is paired with an efficient linear multicast beamformer design, resulting in a more practical, high-performance solution for integrating CC in future MIMO systems."
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-07-22T15:42:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    42,
                    59,
                    0,
                    204,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT"
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tlli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tlli"
                },
                "author": "Antti Tlli"
            },
            {
                "id": "http://arxiv.org/abs/2511.11106v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.11106v1",
                "title": "AccKV: Towards Efficient Audio-Video LLMs Inference via Adaptive-Focusing and Cross-Calibration KV Cache Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AccKV: Towards Efficient Audio-Video LLMs Inference via Adaptive-Focusing and Cross-Calibration KV Cache Optimization"
                },
                "updated": "2025-11-14T09:31:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    14,
                    9,
                    31,
                    11,
                    4,
                    318,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.11106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.11106v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advancements in Audio-Video Large Language Models (AV-LLMs) have enhanced their capabilities in tasks like audio-visual question answering and multimodal dialog systems. Video and audio introduce an extended temporal dimension, resulting in a larger key-value (KV) cache compared to static image embedding. A naive optimization strategy is to selectively focus on and retain KV caches of audio or video based on task. However, in the experiment, we observed that the attention of AV-LLMs to various modalities in the high layers is not strictly dependent on the task. In higher layers, the attention of AV-LLMs shifts more towards the video modality. In addition, we also found that directly integrating temporal KV of audio and spatial-temporal KV of video may lead to information confusion and significant performance degradation of AV-LLMs. If audio and video are processed indiscriminately, it may also lead to excessive compression or reservation of a certain modality, thereby disrupting the alignment between modalities. To address these challenges, we propose AccKV, an Adaptive-Focusing and Cross-Calibration KV cache optimization framework designed specifically for efficient AV-LLMs inference. Our method is based on layer adaptive focusing technology, selectively focusing on key modalities according to the characteristics of different layers, and enhances the recognition of heavy hitter tokens through attention redistribution. In addition, we propose a Cross-Calibration technique that first integrates inefficient KV caches within the audio and video modalities, and then aligns low-priority modalities with high-priority modalities to selectively evict KV cache of low-priority modalities. The experimental results show that AccKV can significantly improve the computational efficiency of AV-LLMs while maintaining accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Audio-Video Large Language Models (AV-LLMs) have enhanced their capabilities in tasks like audio-visual question answering and multimodal dialog systems. Video and audio introduce an extended temporal dimension, resulting in a larger key-value (KV) cache compared to static image embedding. A naive optimization strategy is to selectively focus on and retain KV caches of audio or video based on task. However, in the experiment, we observed that the attention of AV-LLMs to various modalities in the high layers is not strictly dependent on the task. In higher layers, the attention of AV-LLMs shifts more towards the video modality. In addition, we also found that directly integrating temporal KV of audio and spatial-temporal KV of video may lead to information confusion and significant performance degradation of AV-LLMs. If audio and video are processed indiscriminately, it may also lead to excessive compression or reservation of a certain modality, thereby disrupting the alignment between modalities. To address these challenges, we propose AccKV, an Adaptive-Focusing and Cross-Calibration KV cache optimization framework designed specifically for efficient AV-LLMs inference. Our method is based on layer adaptive focusing technology, selectively focusing on key modalities according to the characteristics of different layers, and enhances the recognition of heavy hitter tokens through attention redistribution. In addition, we propose a Cross-Calibration technique that first integrates inefficient KV caches within the audio and video modalities, and then aligns low-priority modalities with high-priority modalities to selectively evict KV cache of low-priority modalities. The experimental results show that AccKV can significantly improve the computational efficiency of AV-LLMs while maintaining accuracy."
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-14T09:31:11Z",
                "published_parsed": [
                    2025,
                    11,
                    14,
                    9,
                    31,
                    11,
                    4,
                    318,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM"
                },
                "authors": [
                    {
                        "name": "Zhonghua Jiang"
                    },
                    {
                        "name": "Kui Chen"
                    },
                    {
                        "name": "Kunxi Li"
                    },
                    {
                        "name": "Keting Yin"
                    },
                    {
                        "name": "Yiyun Zhou"
                    },
                    {
                        "name": "Zhaode Wang"
                    },
                    {
                        "name": "Chengfei Lv"
                    },
                    {
                        "name": "Shengyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shengyu Zhang"
                },
                "author": "Shengyu Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.11031v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.11031v1",
                "title": "Accelerating Controllable Generation via Hybrid-grained Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Controllable Generation via Hybrid-grained Cache"
                },
                "updated": "2025-11-14T07:35:50Z",
                "updated_parsed": [
                    2025,
                    11,
                    14,
                    7,
                    35,
                    50,
                    4,
                    318,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.11031v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.11031v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Controllable generative models have been widely used to improve the realism of synthetic visual content. However, such models must handle control conditions and content generation computational requirements, resulting in generally low generation efficiency. To address this issue, we propose a Hybrid-Grained Cache (HGC) approach that reduces computational overhead by adopting cache strategies with different granularities at different computational stages. Specifically, (1) we use a coarse-grained cache (block-level) based on feature reuse to dynamically bypass redundant computations in encoder-decoder blocks between each step of model reasoning. (2) We design a fine-grained cache (prompt-level) that acts within a module, where the fine-grained cache reuses cross-attention maps within consecutive reasoning steps and extends them to the corresponding module computations of adjacent steps. These caches of different granularities can be seamlessly integrated into each computational link of the controllable generation process. We verify the effectiveness of HGC on four benchmark datasets, especially its advantages in balancing generation efficiency and visual quality. For example, on the COCO-Stuff segmentation benchmark, our HGC significantly reduces the computational cost (MACs) by 63% (from 18.22T to 6.70T), while keeping the loss of semantic fidelity (quantized performance degradation) within 1.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controllable generative models have been widely used to improve the realism of synthetic visual content. However, such models must handle control conditions and content generation computational requirements, resulting in generally low generation efficiency. To address this issue, we propose a Hybrid-Grained Cache (HGC) approach that reduces computational overhead by adopting cache strategies with different granularities at different computational stages. Specifically, (1) we use a coarse-grained cache (block-level) based on feature reuse to dynamically bypass redundant computations in encoder-decoder blocks between each step of model reasoning. (2) We design a fine-grained cache (prompt-level) that acts within a module, where the fine-grained cache reuses cross-attention maps within consecutive reasoning steps and extends them to the corresponding module computations of adjacent steps. These caches of different granularities can be seamlessly integrated into each computational link of the controllable generation process. We verify the effectiveness of HGC on four benchmark datasets, especially its advantages in balancing generation efficiency and visual quality. For example, on the COCO-Stuff segmentation benchmark, our HGC significantly reduces the computational cost (MACs) by 63% (from 18.22T to 6.70T), while keeping the loss of semantic fidelity (quantized performance degradation) within 1.5%."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-14T07:35:50Z",
                "published_parsed": [
                    2025,
                    11,
                    14,
                    7,
                    35,
                    50,
                    4,
                    318,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Huixia Ben"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Shengeng Tang"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao"
            },
            {
                "id": "http://arxiv.org/abs/2511.10991v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10991v1",
                "title": "Rethinking Autoregressive Models for Lossless Image Compression via Hierarchical Parallelism and Progressive Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Autoregressive Models for Lossless Image Compression via Hierarchical Parallelism and Progressive Adaptation"
                },
                "updated": "2025-11-14T06:27:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    14,
                    6,
                    27,
                    58,
                    4,
                    318,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10991v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Autoregressive (AR) models, the theoretical performance benchmark for learned lossless image compression, are often dismissed as impractical due to prohibitive computational cost. This work re-thinks this paradigm, introducing a framework built on hierarchical parallelism and progressive adaptation that re-establishes pure autoregression as a top-performing and practical solution. Our approach is embodied in the Hierarchical Parallel Autoregressive ConvNet (HPAC), an ultra-lightweight pre-trained model using a hierarchical factorized structure and content-aware convolutional gating to efficiently capture spatial dependencies. We introduce two key optimizations for practicality: Cache-then-Select Inference (CSI), which accelerates coding by eliminating redundant computations, and Adaptive Focus Coding (AFC), which efficiently extends the framework to high bit-depth images. Building on this efficient foundation, our progressive adaptation strategy is realized by Spatially-Aware Rate-Guided Progressive Fine-tuning (SARP-FT). This instance-level strategy fine-tunes the model for each test image by optimizing low-rank adapters on progressively larger, spatially-continuous regions selected via estimated information density. Experiments on diverse datasets (natural, satellite, medical) validate that our method achieves new state-of-the-art compression. Notably, our approach sets a new benchmark in learned lossless compression, showing a carefully designed AR framework can offer significant gains over existing methods with a small parameter count and competitive coding speeds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) models, the theoretical performance benchmark for learned lossless image compression, are often dismissed as impractical due to prohibitive computational cost. This work re-thinks this paradigm, introducing a framework built on hierarchical parallelism and progressive adaptation that re-establishes pure autoregression as a top-performing and practical solution. Our approach is embodied in the Hierarchical Parallel Autoregressive ConvNet (HPAC), an ultra-lightweight pre-trained model using a hierarchical factorized structure and content-aware convolutional gating to efficiently capture spatial dependencies. We introduce two key optimizations for practicality: Cache-then-Select Inference (CSI), which accelerates coding by eliminating redundant computations, and Adaptive Focus Coding (AFC), which efficiently extends the framework to high bit-depth images. Building on this efficient foundation, our progressive adaptation strategy is realized by Spatially-Aware Rate-Guided Progressive Fine-tuning (SARP-FT). This instance-level strategy fine-tunes the model for each test image by optimizing low-rank adapters on progressively larger, spatially-continuous regions selected via estimated information density. Experiments on diverse datasets (natural, satellite, medical) validate that our method achieves new state-of-the-art compression. Notably, our approach sets a new benchmark in learned lossless compression, showing a carefully designed AR framework can offer significant gains over existing methods with a small parameter count and competitive coding speeds."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-14T06:27:58Z",
                "published_parsed": [
                    2025,
                    11,
                    14,
                    6,
                    27,
                    58,
                    4,
                    318,
                    0
                ],
                "arxiv_comment": "15 pages",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Daxin Li"
                    },
                    {
                        "name": "Yuanchao Bai"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Wenbo Zhao"
                    },
                    {
                        "name": "Junjun Jiang"
                    },
                    {
                        "name": "Xianming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xianming Liu"
                },
                "author": "Xianming Liu"
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v8",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.16242v8",
                "title": "Robustifying Learning-Augmented Caching Efficiently without Compromising 1-Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying Learning-Augmented Caching Efficiently without Compromising 1-Consistency"
                },
                "updated": "2025-11-14T03:18:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    14,
                    3,
                    18,
                    36,
                    4,
                    318,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.16242v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.16242v8",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The online caching problem aims to minimize cache misses when serving a sequence of requests under a limited cache size. While naive learning-augmented caching algorithms achieve ideal $1$-consistency, they lack robustness guarantees. Existing robustification methods either sacrifice $1$-consistency or introduce excessive computational overhead. In this paper, we introduce Guard, a lightweight robustification framework that enhances the robustness of a broad class of learning-augmented caching algorithms to $2H_{k-1} + 2$, while preserving their $1$-consistency. Guard achieves the current best-known trade-off between consistency and robustness, with only O(1) additional per-request overhead, thereby maintaining the original time complexity of the base algorithm. Extensive experiments across multiple real-world datasets and prediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a sequence of requests under a limited cache size. While naive learning-augmented caching algorithms achieve ideal $1$-consistency, they lack robustness guarantees. Existing robustification methods either sacrifice $1$-consistency or introduce excessive computational overhead. In this paper, we introduce Guard, a lightweight robustification framework that enhances the robustness of a broad class of learning-augmented caching algorithms to $2H_{k-1} + 2$, while preserving their $1$-consistency. Guard achieves the current best-known trade-off between consistency and robustness, with only O(1) additional per-request overhead, thereby maintaining the original time complexity of the base algorithm. Extensive experiments across multiple real-world datasets and prediction models validate the effectiveness of Guard in practice."
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "arxiv_comment": "Accepted to NeurIPS 2025. https://neurips.cc/virtual/2025/poster/116615",
                "arxiv_primary_category": {
                    "term": "cs.DS"
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng"
            },
            {
                "id": "http://arxiv.org/abs/2509.00625v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.00625v2",
                "title": "NetGent: Agent-Based Automation of Network Application Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NetGent: Agent-Based Automation of Network Application Workflows"
                },
                "updated": "2025-11-14T01:23:14Z",
                "updated_parsed": [
                    2025,
                    11,
                    14,
                    1,
                    23,
                    14,
                    4,
                    318,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.00625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.00625v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present NetGent, an AI-agent framework for automating complex application workflows to generate realistic network traffic datasets. Developing generalizable ML models for networking requires data collection from network environments with traffic that results from a diverse set of real-world web applications. However, using existing browser automation tools that are diverse, repeatable, realistic, and efficient remains fragile and costly. NetGent addresses this challenge by allowing users to specify workflows as natural-language rules that define state-dependent actions. These abstract specifications are compiled into nondeterministic finite automata (NFAs), which a state synthesis component translates into reusable, executable code. This design enables deterministic replay, reduces redundant LLM calls through state caching, and adapts quickly when application interfaces change. In experiments, NetGent automated more than 50+ workflows spanning video-on-demand streaming, live video streaming, video conferencing, social media, and web scraping, producing realistic traffic traces while remaining robust to UI variability. By combining the flexibility of language-based agents with the reliability of compiled execution, NetGent provides a scalable foundation for generating the diverse, repeatable datasets needed to advance ML in networking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present NetGent, an AI-agent framework for automating complex application workflows to generate realistic network traffic datasets. Developing generalizable ML models for networking requires data collection from network environments with traffic that results from a diverse set of real-world web applications. However, using existing browser automation tools that are diverse, repeatable, realistic, and efficient remains fragile and costly. NetGent addresses this challenge by allowing users to specify workflows as natural-language rules that define state-dependent actions. These abstract specifications are compiled into nondeterministic finite automata (NFAs), which a state synthesis component translates into reusable, executable code. This design enables deterministic replay, reduces redundant LLM calls through state caching, and adapts quickly when application interfaces change. In experiments, NetGent automated more than 50+ workflows spanning video-on-demand streaming, live video streaming, video conferencing, social media, and web scraping, producing realistic traffic traces while remaining robust to UI variability. By combining the flexibility of language-based agents with the reliability of compiled execution, NetGent provides a scalable foundation for generating the diverse, repeatable datasets needed to advance ML in networking."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-30T22:47:15Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    22,
                    47,
                    15,
                    5,
                    242,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jaber Daneshamooz"
                    },
                    {
                        "name": "Eugene Vuong"
                    },
                    {
                        "name": "Laasya Koduru"
                    },
                    {
                        "name": "Sanjay Chandrasekaran"
                    },
                    {
                        "name": "Arpit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Arpit Gupta"
                },
                "author": "Arpit Gupta"
            },
            {
                "id": "http://arxiv.org/abs/2508.16166v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.16166v2",
                "title": "Terahertz third-harmonic generation of lightwave driven Weyl fermions far from equilibrium",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz third-harmonic generation of lightwave driven Weyl fermions far from equilibrium"
                },
                "updated": "2025-11-13T15:44:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    44,
                    30,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.16166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.16166v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We report on time-resolved ultrafast terahertz third-harmonic generation spectroscopy of nonequilibrium dynamics of Weyl fermions in a nanometer thin film of the Weyl semimetal TaP. Terahertz third-harmonic generation is observed at room temperature under the drive of a multicycle narrowband terahertz pulse with a peak field strength of down to tens of kV/cm. The observed terahertz third-harmonic generation exhibits a perturbative cubic power-law dependence on the terahertz drive. By varying the polarization of the drive pulse from linear to elliptical, we realize a sensitive tuning of the third harmonic yield. By carrying out theoretical analysis based on the Boltzmann transport theory, we can properly describe the experimental results and ascribe the observed THz nonlinearity to field-driven kinetics of the Weyl fermions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on time-resolved ultrafast terahertz third-harmonic generation spectroscopy of nonequilibrium dynamics of Weyl fermions in a nanometer thin film of the Weyl semimetal TaP. Terahertz third-harmonic generation is observed at room temperature under the drive of a multicycle narrowband terahertz pulse with a peak field strength of down to tens of kV/cm. The observed terahertz third-harmonic generation exhibits a perturbative cubic power-law dependence on the terahertz drive. By varying the polarization of the drive pulse from linear to elliptical, we realize a sensitive tuning of the third harmonic yield. By carrying out theoretical analysis based on the Boltzmann transport theory, we can properly describe the experimental results and ascribe the observed THz nonlinearity to field-driven kinetics of the Weyl fermions."
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-22T07:42:10Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    42,
                    10,
                    4,
                    234,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el"
                },
                "authors": [
                    {
                        "name": "Patrick Pilch"
                    },
                    {
                        "name": "Changqing Zhu"
                    },
                    {
                        "name": "Sergey Kovalev"
                    },
                    {
                        "name": "Renato M. A. Dantas"
                    },
                    {
                        "name": "Amilcar Bedoya-Pinto"
                    },
                    {
                        "name": "Stuart S. P. Parkin"
                    },
                    {
                        "name": "Zhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Wang"
                },
                "author": "Zhe Wang"
            },
            {
                "id": "http://arxiv.org/abs/2511.10394v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10394v1",
                "title": "LLM-YOLOMS: Large Language Model-based Semantic Interpretation and Fault Diagnosis for Wind Turbine Components",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-YOLOMS: Large Language Model-based Semantic Interpretation and Fault Diagnosis for Wind Turbine Components"
                },
                "updated": "2025-11-13T15:14:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    14,
                    34,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10394v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The health condition of wind turbine (WT) components is crucial for ensuring stable and reliable operation. However, existing fault detection methods are largely limited to visual recognition, producing structured outputs that lack semantic interpretability and fail to support maintenance decision-making. To address these limitations, this study proposes an integrated framework that combines YOLOMS with a large language model (LLM) for intelligent fault analysis and diagnosis. Specifically, YOLOMS employs multi-scale detection and sliding-window cropping to enhance fault feature extraction, while a lightweight key-value (KV) mapping module bridges the gap between visual outputs and textual inputs. This module converts YOLOMS detection results into structured textual representations enriched with both qualitative and quantitative attributes. A domain-tuned LLM then performs semantic reasoning to generate interpretable fault analyses and maintenance recommendations. Experiments on real-world datasets demonstrate that the proposed framework achieves a fault detection accuracy of 90.6\\% and generates maintenance reports with an average accuracy of 89\\%, thereby improving the interpretability of diagnostic results and providing practical decision support for the operation and maintenance of wind turbines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The health condition of wind turbine (WT) components is crucial for ensuring stable and reliable operation. However, existing fault detection methods are largely limited to visual recognition, producing structured outputs that lack semantic interpretability and fail to support maintenance decision-making. To address these limitations, this study proposes an integrated framework that combines YOLOMS with a large language model (LLM) for intelligent fault analysis and diagnosis. Specifically, YOLOMS employs multi-scale detection and sliding-window cropping to enhance fault feature extraction, while a lightweight key-value (KV) mapping module bridges the gap between visual outputs and textual inputs. This module converts YOLOMS detection results into structured textual representations enriched with both qualitative and quantitative attributes. A domain-tuned LLM then performs semantic reasoning to generate interpretable fault analyses and maintenance recommendations. Experiments on real-world datasets demonstrate that the proposed framework achieves a fault detection accuracy of 90.6\\% and generates maintenance reports with an average accuracy of 89\\%, thereby improving the interpretability of diagnostic results and providing practical decision support for the operation and maintenance of wind turbines."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T15:14:34Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    14,
                    34,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "Journal resubmission",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yaru Li"
                    },
                    {
                        "name": "Yanxue Wang"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Xinming Li"
                    },
                    {
                        "name": "Jianbo Feng"
                    }
                ],
                "author_detail": {
                    "name": "Jianbo Feng"
                },
                "author": "Jianbo Feng"
            },
            {
                "id": "http://arxiv.org/abs/2511.05534v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.05534v2",
                "title": "FlowMM: Cross-Modal Information Flow Guided KV Cache Merging for Efficient Multimodal Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowMM: Cross-Modal Information Flow Guided KV Cache Merging for Efficient Multimodal Context Inference"
                },
                "updated": "2025-11-13T14:25:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    25,
                    8,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.05534v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.05534v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Traditional KV cache eviction strategies, which discard less critical KV-pairs based on attention scores, often degrade generation quality, causing context loss or hallucinations. Recent efforts shift toward KV merging, merging eviction tokens with retention tokens based on similarity. However, in multimodal scenarios, distributional biases across modality tokens and attentional biases in cross-modal interactions limit its effectiveness. This work introduces FlowMM, an adaptive framework for cross-modal information flow-guided multimodal KV cache merging. FlowMM leverages cross-modal information flow to dynamically apply layer-specific merging strategies, capturing modality-specific patterns while preserving contextual integrity. Furthermore, we introduce a sensitivity-adaptive token matching mechanism that jointly evaluates token similarity and task-critical sensitivity, merging low-risk tokens while safeguarding high-sensitivity ones. Extensive experiments across diverse leading MLLMs show that FlowMM reduces KV cache memory by 80% to 95% and decoding latency by 1.3-1.8x, while maintaining competitive task performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional KV cache eviction strategies, which discard less critical KV-pairs based on attention scores, often degrade generation quality, causing context loss or hallucinations. Recent efforts shift toward KV merging, merging eviction tokens with retention tokens based on similarity. However, in multimodal scenarios, distributional biases across modality tokens and attentional biases in cross-modal interactions limit its effectiveness. This work introduces FlowMM, an adaptive framework for cross-modal information flow-guided multimodal KV cache merging. FlowMM leverages cross-modal information flow to dynamically apply layer-specific merging strategies, capturing modality-specific patterns while preserving contextual integrity. Furthermore, we introduce a sensitivity-adaptive token matching mechanism that jointly evaluates token similarity and task-critical sensitivity, merging low-risk tokens while safeguarding high-sensitivity ones. Extensive experiments across diverse leading MLLMs show that FlowMM reduces KV cache memory by 80% to 95% and decoding latency by 1.3-1.8x, while maintaining competitive task performance."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-29T13:20:16Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    20,
                    16,
                    2,
                    302,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Kunxi Li"
                    },
                    {
                        "name": "Yufan Xiong"
                    },
                    {
                        "name": "Zhonghua Jiang"
                    },
                    {
                        "name": "Yiyun Zhou"
                    },
                    {
                        "name": "Zhaode Wang"
                    },
                    {
                        "name": "Chengfei Lv"
                    },
                    {
                        "name": "Shengyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shengyu Zhang"
                },
                "author": "Shengyu Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2411.19248v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2411.19248v2",
                "title": "Reconfigurable Intelligent Surface-Assisted Multiple-Antenna Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable Intelligent Surface-Assisted Multiple-Antenna Coded Caching"
                },
                "updated": "2025-11-13T11:36:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    11,
                    36,
                    29,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2411.19248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2411.19248v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reconfigurable Intelligent Surface (RIS) has emerged as a promising technology to enhance the wireless propagation environment for next-generation wireless communication systems. This paper introduces a new RIS-assisted multiple-antenna coded caching problem. Unlike the existing multi-antenna coded caching models, our considered model incorporates a passive RIS with a limited number of elements aimed at enhancing the multicast gain (i.e., Degrees of Freedom (DoF)). The system consists of a server equipped with multiple antennas and several single-antenna users. The RIS, which functions as a passive and configurable relay, improves communication by selectively erasing certain transmission paths between transmit and receive antennas, thereby reducing interference. We first propose a new RIS-assisted interference nulling algorithm to determine the phase-shift coefficients of the RIS. This algorithm achieves faster convergence compared to the existing approach. By strategically nulling certain interference paths in each time slot, the transmission process is divided into multiple interference-free groups. Each group consists of a set of transmit antennas that serve a corresponding set of users without any interference from other groups. The optimal grouping strategy to maximize the DoF is formulated as a combinatorial optimization problem. To efficiently solve this, we design a low-complexity algorithm that identifies the optimal solution and develops a corresponding coded caching scheme to achieve the maximum DoF. Building on the optimal grouping strategy, we introduce a new framework, referred to as RIS-assisted Multiple-Antenna Placement Delivery Array (RMAPDA), to construct the cache placement and delivery phases. Then we propose a general RMAPDA design to achieve the maximum DoF under the optimal grouping strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable Intelligent Surface (RIS) has emerged as a promising technology to enhance the wireless propagation environment for next-generation wireless communication systems. This paper introduces a new RIS-assisted multiple-antenna coded caching problem. Unlike the existing multi-antenna coded caching models, our considered model incorporates a passive RIS with a limited number of elements aimed at enhancing the multicast gain (i.e., Degrees of Freedom (DoF)). The system consists of a server equipped with multiple antennas and several single-antenna users. The RIS, which functions as a passive and configurable relay, improves communication by selectively erasing certain transmission paths between transmit and receive antennas, thereby reducing interference. We first propose a new RIS-assisted interference nulling algorithm to determine the phase-shift coefficients of the RIS. This algorithm achieves faster convergence compared to the existing approach. By strategically nulling certain interference paths in each time slot, the transmission process is divided into multiple interference-free groups. Each group consists of a set of transmit antennas that serve a corresponding set of users without any interference from other groups. The optimal grouping strategy to maximize the DoF is formulated as a combinatorial optimization problem. To efficiently solve this, we design a low-complexity algorithm that identifies the optimal solution and develops a corresponding coded caching scheme to achieve the maximum DoF. Building on the optimal grouping strategy, we introduce a new framework, referred to as RIS-assisted Multiple-Antenna Placement Delivery Array (RMAPDA), to construct the cache placement and delivery phases. Then we propose a general RMAPDA design to achieve the maximum DoF under the optimal grouping strategy."
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-11-28T16:35:22Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    35,
                    22,
                    3,
                    333,
                    0
                ],
                "arxiv_comment": "Submitted to IEEE Trans. Information Theory, 40 pages",
                "arxiv_primary_category": {
                    "term": "cs.IT"
                },
                "authors": [
                    {
                        "name": "Xiaofan Niu"
                    },
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Robert Caiming Qiu"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire"
            },
            {
                "id": "http://arxiv.org/abs/2511.10116v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10116v1",
                "title": "Lanthanides-Based Nanoparticles Conjugated with Rose Bengal for FRET-Mediated X-Ray-Induced PDT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lanthanides-Based Nanoparticles Conjugated with Rose Bengal for FRET-Mediated X-Ray-Induced PDT"
                },
                "updated": "2025-11-13T09:20:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    9,
                    20,
                    38,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10116v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In order to find a good candidate for F{}rster Resonance Energy Transfer (FRET)-mediated X-ray-induced photodynamic therapy (X-PDT) for the treatment of cancer, lanthanide (Ln)-based AGuIX nanoparticles (NPs) conjugated with Rose Bengal (RB) as a photosensitizer (PS) were synthesized. X-PDT overcomes the problem of the poor penetration of visible light into tissues, which limits the efficacy of PDT in the treatment of deep-seated tumors. It is essential to optimize FRET efficiency by maximizing the overlap integral between donor emission and acceptor absorption and lengthening the duration of the donor emission. In this study, we optimized energy transfer between a scintillator (Sc) as a donor and a PS as an acceptor. Terbium (Tb) and Gadolinium (Gd) as Scs and Rose RB as a PS were chosen. The study of energy transfer between Tb, Gd and RB in solution and chelated on AGuIX NPs proved to be FRET-like. RB was conjugated directly onto AGuIX NPs (i.e., AGuIX Ln@RB), and the use of a spacer arm (i.e., AGuIX Ln@spacer arm-RB) increased FRET efficiency. Singlet oxygen production by these NPs was observed under UV--visible illumination and X-ray irradiation. The in vitro bioassay demonstrated 52% cell death of U-251MG derived from human malignant glioblastoma multiforme at a concentration of 1 $$M RB after illumination and irradiation (2 Gy, 320 kV, 10 mA, 3 Gy/min at 47 cm). In addition, the RB-coupled NRP-1-targeting peptide (i.e., K(RB)DKPPR) was conjugated onto AGuIX NPs by a thiol-maleimide click chemistry reaction, and an affinity in the nM range was observed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In order to find a good candidate for F{}rster Resonance Energy Transfer (FRET)-mediated X-ray-induced photodynamic therapy (X-PDT) for the treatment of cancer, lanthanide (Ln)-based AGuIX nanoparticles (NPs) conjugated with Rose Bengal (RB) as a photosensitizer (PS) were synthesized. X-PDT overcomes the problem of the poor penetration of visible light into tissues, which limits the efficacy of PDT in the treatment of deep-seated tumors. It is essential to optimize FRET efficiency by maximizing the overlap integral between donor emission and acceptor absorption and lengthening the duration of the donor emission. In this study, we optimized energy transfer between a scintillator (Sc) as a donor and a PS as an acceptor. Terbium (Tb) and Gadolinium (Gd) as Scs and Rose RB as a PS were chosen. The study of energy transfer between Tb, Gd and RB in solution and chelated on AGuIX NPs proved to be FRET-like. RB was conjugated directly onto AGuIX NPs (i.e., AGuIX Ln@RB), and the use of a spacer arm (i.e., AGuIX Ln@spacer arm-RB) increased FRET efficiency. Singlet oxygen production by these NPs was observed under UV--visible illumination and X-ray irradiation. The in vitro bioassay demonstrated 52% cell death of U-251MG derived from human malignant glioblastoma multiforme at a concentration of 1 $$M RB after illumination and irradiation (2 Gy, 320 kV, 10 mA, 3 Gy/min at 47 cm). In addition, the RB-coupled NRP-1-targeting peptide (i.e., K(RB)DKPPR) was conjugated onto AGuIX NPs by a thiol-maleimide click chemistry reaction, and an affinity in the nM range was observed."
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T09:20:38Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    9,
                    20,
                    38,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph"
                },
                "arxiv_journal_ref": "Pharmaceuticals, 2025, 18 (5), pp.672",
                "authors": [
                    {
                        "name": "Batoul Dhaini"
                    },
                    {
                        "name": "Jol Daouk"
                    },
                    {
                        "name": "Herv Schohn"
                    },
                    {
                        "name": "Philippe Arnoux"
                    },
                    {
                        "name": "Valrie Jouan-Hureaux"
                    },
                    {
                        "name": "Albert Moussaron"
                    },
                    {
                        "name": "Agns Hagege"
                    },
                    {
                        "name": "Mathilde Achard"
                    },
                    {
                        "name": "Samir Acherar"
                    },
                    {
                        "name": "Tayssir Hamieh"
                    },
                    {
                        "name": "Cline Frochot"
                    }
                ],
                "author_detail": {
                    "name": "Cline Frochot"
                },
                "arxiv_affiliation": "LRGP",
                "author": "Cline Frochot"
            },
            {
                "id": "http://arxiv.org/abs/2511.09956v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.09956v1",
                "title": "Optimizing CPU Cache Utilization in Cloud VMs with Accurate Cache Abstraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing CPU Cache Utilization in Cloud VMs with Accurate Cache Abstraction"
                },
                "updated": "2025-11-13T04:37:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    4,
                    37,
                    52,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.09956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.09956v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper shows that cache-based optimizations are often ineffective in cloud virtual machines (VMs) due to limited visibility into and control over provisioned caches. In public clouds, CPU caches can be partitioned or shared among VMs, but a VM is unaware of cache provisioning details. Moreover, a VM cannot influence cache usage via page placement policies, as memory-to-cache mappings are hidden. The paper proposes a novel solution, CacheX, which probes accurate and fine-grained cache abstraction within VMs using eviction sets without requiring hardware or hypervisor support, and showcases the utility of the probed information with two new techniques: LLC contention-aware task scheduling and virtual color-aware page cache management. Our evaluation of CacheX's implementation in x86 Linux kernel demonstrates that it can effectively improve cache utilization for various workloads in public cloud VMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper shows that cache-based optimizations are often ineffective in cloud virtual machines (VMs) due to limited visibility into and control over provisioned caches. In public clouds, CPU caches can be partitioned or shared among VMs, but a VM is unaware of cache provisioning details. Moreover, a VM cannot influence cache usage via page placement policies, as memory-to-cache mappings are hidden. The paper proposes a novel solution, CacheX, which probes accurate and fine-grained cache abstraction within VMs using eviction sets without requiring hardware or hypervisor support, and showcases the utility of the probed information with two new techniques: LLC contention-aware task scheduling and virtual color-aware page cache management. Our evaluation of CacheX's implementation in x86 Linux kernel demonstrates that it can effectively improve cache utilization for various workloads in public cloud VMs."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T04:37:52Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    4,
                    37,
                    52,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Mani Tofigh"
                    },
                    {
                        "name": "Edward Guo"
                    },
                    {
                        "name": "Weiwei Jia"
                    },
                    {
                        "name": "Xiaoning Ding"
                    },
                    {
                        "name": "Jianchen Shan"
                    }
                ],
                "author_detail": {
                    "name": "Jianchen Shan"
                },
                "author": "Jianchen Shan"
            },
            {
                "id": "http://arxiv.org/abs/2411.17741v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2411.17741v2",
                "title": "Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM Inference Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM Inference Environments"
                },
                "updated": "2025-11-12T22:17:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    12,
                    22,
                    17,
                    44,
                    2,
                    316,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2411.17741v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2411.17741v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3725843.3756083",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "The widespread adoption of LLMs has driven an exponential rise in their deployment, imposing substantial demands on inference clusters. These clusters must handle numerous concurrent queries for different LLM downstream tasks. To handle multi-task settings with vast LLM parameter counts, methods like Low-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most of the base LLM model across tasks. Hence, they allow concurrent task serving with minimal memory requirements. However, existing LLM serving systems face inefficiencies: they overlook workload heterogeneity, impose high link bandwidth from frequent adapter loading, and suffer from head-of-line blocking in their schedulers. To address these challenges, we present Chameleon, a novel LLM serving system optimized for many adapter environments, that relies on two core ideas: adapter caching and adapter-aware scheduling. First, Chameleon caches popular adapters in GPU memory, minimizing the adapter loading times. Importantly, it uses the otherwise idle GPU memory, avoiding extra memory costs. Second, Chameleon uses a non-preemptive multi-queue scheduling to efficiently account for workload heterogeneity. In this way, Chameleon simultaneously prevents head of line blocking and starvation. We implement Chameleon on top of a state-of-the-art LLM serving platform and evaluate it with real-world production traces and open-source LLMs. Under high loads, Chameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively, while improving throughput by 1.5x compared to state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of LLMs has driven an exponential rise in their deployment, imposing substantial demands on inference clusters. These clusters must handle numerous concurrent queries for different LLM downstream tasks. To handle multi-task settings with vast LLM parameter counts, methods like Low-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most of the base LLM model across tasks. Hence, they allow concurrent task serving with minimal memory requirements. However, existing LLM serving systems face inefficiencies: they overlook workload heterogeneity, impose high link bandwidth from frequent adapter loading, and suffer from head-of-line blocking in their schedulers. To address these challenges, we present Chameleon, a novel LLM serving system optimized for many adapter environments, that relies on two core ideas: adapter caching and adapter-aware scheduling. First, Chameleon caches popular adapters in GPU memory, minimizing the adapter loading times. Importantly, it uses the otherwise idle GPU memory, avoiding extra memory costs. Second, Chameleon uses a non-preemptive multi-queue scheduling to efficiently account for workload heterogeneity. In this way, Chameleon simultaneously prevents head of line blocking and starvation. We implement Chameleon on top of a state-of-the-art LLM serving platform and evaluate it with real-world production traces and open-source LLMs. Under high loads, Chameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively, while improving throughput by 1.5x compared to state-of-the-art baselines."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-11-24T16:20:57Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    16,
                    20,
                    57,
                    6,
                    329,
                    0
                ],
                "arxiv_comment": "Accepted at MICRO '25",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "arxiv_journal_ref": "MICRO 58, 2025, 15",
                "authors": [
                    {
                        "name": "Nikoleta Iliakopoulou"
                    },
                    {
                        "name": "Jovan Stojkovic"
                    },
                    {
                        "name": "Chloe Alverti"
                    },
                    {
                        "name": "Tianyin Xu"
                    },
                    {
                        "name": "Hubertus Franke"
                    },
                    {
                        "name": "Josep Torrellas"
                    }
                ],
                "author_detail": {
                    "name": "Josep Torrellas"
                },
                "author": "Josep Torrellas",
                "arxiv_doi": "10.1145/3725843.3756083"
            },
            {
                "id": "http://arxiv.org/abs/2508.15212v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.15212v3",
                "title": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning"
                },
                "updated": "2025-11-12T16:02:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    12,
                    16,
                    2,
                    3,
                    2,
                    316,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.15212v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.15212v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Long-context inference in large language models (LLMs) is increasingly constrained by the KV cache bottleneck: memory usage grows linearly with sequence length, while attention computation scales quadratically. Existing approaches address this issue by compressing the KV cache along the temporal axis through strategies such as token eviction or merging to reduce memory and computational overhead. However, these methods often neglect fine-grained importance variations across feature dimensions (i.e., the channel axis), thereby limiting their ability to effectively balance efficiency and model accuracy. In reality, we observe that channel saliency varies dramatically across both queries and positions: certain feature channels carry near-zero information for a given query, while others spike in relevance. To address this oversight, we propose SPARK, a training-free plug-and-play method that applies unstructured sparsity by pruning KV at the channel level, while dynamically restoring the pruned entries during attention score computation. Notably, our approach is orthogonal to existing KV compression and quantization techniques, making it compatible for integration with them to achieve further acceleration. By reducing channel-level redundancy, SPARK enables processing of longer sequences within the same memory budget. For sequences of equal length, SPARK not only preserves or improves model accuracy but also reduces KV cache storage by over 30% compared to eviction-based methods. Furthermore, even with an aggressive pruning ratio of 80%, SPARK maintains performance with less degradation than 5% compared to the baseline eviction method, demonstrating its robustness and effectiveness. Our code will be available at https://github.com/Xnhyacinth/SparK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference in large language models (LLMs) is increasingly constrained by the KV cache bottleneck: memory usage grows linearly with sequence length, while attention computation scales quadratically. Existing approaches address this issue by compressing the KV cache along the temporal axis through strategies such as token eviction or merging to reduce memory and computational overhead. However, these methods often neglect fine-grained importance variations across feature dimensions (i.e., the channel axis), thereby limiting their ability to effectively balance efficiency and model accuracy. In reality, we observe that channel saliency varies dramatically across both queries and positions: certain feature channels carry near-zero information for a given query, while others spike in relevance. To address this oversight, we propose SPARK, a training-free plug-and-play method that applies unstructured sparsity by pruning KV at the channel level, while dynamically restoring the pruned entries during attention score computation. Notably, our approach is orthogonal to existing KV compression and quantization techniques, making it compatible for integration with them to achieve further acceleration. By reducing channel-level redundancy, SPARK enables processing of longer sequences within the same memory budget. For sequences of equal length, SPARK not only preserves or improves model accuracy but also reduces KV cache storage by over 30% compared to eviction-based methods. Furthermore, even with an aggressive pruning ratio of 80%, SPARK maintains performance with less degradation than 5% compared to the baseline eviction method, demonstrating its robustness and effectiveness. Our code will be available at https://github.com/Xnhyacinth/SparK."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-21T03:48:28Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    48,
                    28,
                    3,
                    233,
                    0
                ],
                "arxiv_comment": "accepted to AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Guanchen Li"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Emad Barsoum"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu"
            },
            {
                "id": "http://arxiv.org/abs/2511.09052v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.09052v1",
                "title": "Efficient Distributed Exact Subgraph Matching via GNN-PE: Load Balancing, Cache Optimization, and Query Plan Ranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Distributed Exact Subgraph Matching via GNN-PE: Load Balancing, Cache Optimization, and Query Plan Ranking"
                },
                "updated": "2025-11-12T07:06:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    12,
                    7,
                    6,
                    33,
                    2,
                    316,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.09052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.09052v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Exact subgraph matching on large-scale graphs remains a challenging problem due to high computational complexity and distributed system constraints. Existing GNN-based path embedding (GNN-PE) frameworks achieve efficient exact matching on single machines but lack scalability and optimization for distributed environments. To address this gap, we propose three core innovations to extend GNN-PE to distributed systems: (1) a lightweight dynamic correlation-aware load balancing and hot migration mechanism that fuses multi-dimensional metrics (CPU, communication, memory) and guarantees index consistency; (2) an online incremental learning-based multi-GPU collaborative dynamic caching strategy with heterogeneous GPU adaptation and graph-structure-aware replacement; (3) a query plan ranking method driven by dominance embedding pruning potential (PE-score) that optimizes execution order. Through METIS partitioning, parallel offline preprocessing, and lightweight metadata management, our approach achieves \"minimum edge cut + load balancing + non-interruptible queries\" in distributed scenarios (tens of machines), significantly improving the efficiency and stability of distributed subgraph matching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exact subgraph matching on large-scale graphs remains a challenging problem due to high computational complexity and distributed system constraints. Existing GNN-based path embedding (GNN-PE) frameworks achieve efficient exact matching on single machines but lack scalability and optimization for distributed environments. To address this gap, we propose three core innovations to extend GNN-PE to distributed systems: (1) a lightweight dynamic correlation-aware load balancing and hot migration mechanism that fuses multi-dimensional metrics (CPU, communication, memory) and guarantees index consistency; (2) an online incremental learning-based multi-GPU collaborative dynamic caching strategy with heterogeneous GPU adaptation and graph-structure-aware replacement; (3) a query plan ranking method driven by dominance embedding pruning potential (PE-score) that optimizes execution order. Through METIS partitioning, parallel offline preprocessing, and lightweight metadata management, our approach achieves \"minimum edge cut + load balancing + non-interruptible queries\" in distributed scenarios (tens of machines), significantly improving the efficiency and stability of distributed subgraph matching."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-12T07:06:33Z",
                "published_parsed": [
                    2025,
                    11,
                    12,
                    7,
                    6,
                    33,
                    2,
                    316,
                    0
                ],
                "arxiv_comment": "10 pages",
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Jiake Ge"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang"
            },
            {
                "id": "http://arxiv.org/abs/2511.06029v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.06029v2",
                "title": "Lethe: Layer- and Time-Adaptive KV Cache Pruning for Reasoning-Intensive LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lethe: Layer- and Time-Adaptive KV Cache Pruning for Reasoning-Intensive LLM Serving"
                },
                "updated": "2025-11-12T03:53:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    12,
                    3,
                    53,
                    30,
                    2,
                    316,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.06029v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.06029v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generative reasoning with large language models (LLMs) often involves long decoding sequences, leading to substantial memory and latency overheads from accumulating key-value (KV) caches. While existing KV compression methods primarily focus on reducing prefill memory from long input sequences, they fall short in addressing the dynamic and layer-sensitive nature of long-form generation, which is central to reasoning tasks. We propose Lethe, a dynamic KV cache management framework that introduces adaptivity along both the spatial and temporal dimensions of decoding. Along the spatial dimension, Lethe performs layerwise sparsity-aware allocation, assigning token pruning budgets to each transformer layer based on estimated attention redundancy. Along the temporal dimension, Lethe conducts multi-round token pruning during generation, driven by a Recency-Aware Selective Retention} (RASR) mechanism. RASR extends traditional recency-based heuristics by also considering token relevance derived from evolving attention patterns, enabling informed decisions about which tokens to retain or evict. Empirical results demonstrate that Lethe achieves a favorable balance between efficiency and generation quality across diverse models and tasks, increases throughput by up to 2.56x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative reasoning with large language models (LLMs) often involves long decoding sequences, leading to substantial memory and latency overheads from accumulating key-value (KV) caches. While existing KV compression methods primarily focus on reducing prefill memory from long input sequences, they fall short in addressing the dynamic and layer-sensitive nature of long-form generation, which is central to reasoning tasks. We propose Lethe, a dynamic KV cache management framework that introduces adaptivity along both the spatial and temporal dimensions of decoding. Along the spatial dimension, Lethe performs layerwise sparsity-aware allocation, assigning token pruning budgets to each transformer layer based on estimated attention redundancy. Along the temporal dimension, Lethe conducts multi-round token pruning during generation, driven by a Recency-Aware Selective Retention} (RASR) mechanism. RASR extends traditional recency-based heuristics by also considering token relevance derived from evolving attention patterns, enabling informed decisions about which tokens to retain or evict. Empirical results demonstrate that Lethe achieves a favorable balance between efficiency and generation quality across diverse models and tasks, increases throughput by up to 2.56x."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-08T14:52:43Z",
                "published_parsed": [
                    2025,
                    11,
                    8,
                    14,
                    52,
                    43,
                    5,
                    312,
                    0
                ],
                "arxiv_comment": "aaai26 camera-ready version, 12 pages",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Hui Zeng"
                    },
                    {
                        "name": "Daming Zhao"
                    },
                    {
                        "name": "Pengfei Yang"
                    },
                    {
                        "name": "WenXuan Hou"
                    },
                    {
                        "name": "Tianyang Zheng"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Weiye Ji"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai"
            },
            {
                "id": "http://arxiv.org/abs/2511.08923v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.08923v1",
                "title": "TiDAR: Think in Diffusion, Talk in Autoregression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TiDAR: Think in Diffusion, Talk in Autoregression"
                },
                "updated": "2025-11-12T02:59:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    12,
                    2,
                    59,
                    33,
                    2,
                    316,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.08923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.08923v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion language models hold the promise of fast parallel generation, while autoregressive (AR) models typically excel in quality due to their causal structure aligning naturally with language modeling. This raises a fundamental question: can we achieve a synergy with high throughput, higher GPU utilization, and AR level quality? Existing methods fail to effectively balance these two aspects, either prioritizing AR using a weaker model for sequential drafting (speculative decoding), leading to lower drafting efficiency, or using some form of left-to-right (AR-like) decoding logic for diffusion, which still suffers from quality degradation and forfeits its potential parallelizability. We introduce TiDAR, a sequence-level hybrid architecture that drafts tokens (Thinking) in Diffusion and samples final outputs (Talking) AutoRegressively - all within a single forward pass using specially designed structured attention masks. This design exploits the free GPU compute density, achieving a strong balance between drafting and verification capacity. Moreover, TiDAR is designed to be serving-friendly (low overhead) as a standalone model. We extensively evaluate TiDAR against AR models, speculative decoding, and diffusion variants across generative and likelihood tasks at 1.5B and 8B scales. Thanks to the parallel drafting and sampling as well as exact KV cache support, TiDAR outperforms speculative decoding in measured throughput and surpasses diffusion models like Dream and Llada in both efficiency and quality. Most notably, TiDAR is the first architecture to close the quality gap with AR models while delivering 4.71x to 5.91x more tokens per second.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models hold the promise of fast parallel generation, while autoregressive (AR) models typically excel in quality due to their causal structure aligning naturally with language modeling. This raises a fundamental question: can we achieve a synergy with high throughput, higher GPU utilization, and AR level quality? Existing methods fail to effectively balance these two aspects, either prioritizing AR using a weaker model for sequential drafting (speculative decoding), leading to lower drafting efficiency, or using some form of left-to-right (AR-like) decoding logic for diffusion, which still suffers from quality degradation and forfeits its potential parallelizability. We introduce TiDAR, a sequence-level hybrid architecture that drafts tokens (Thinking) in Diffusion and samples final outputs (Talking) AutoRegressively - all within a single forward pass using specially designed structured attention masks. This design exploits the free GPU compute density, achieving a strong balance between drafting and verification capacity. Moreover, TiDAR is designed to be serving-friendly (low overhead) as a standalone model. We extensively evaluate TiDAR against AR models, speculative decoding, and diffusion variants across generative and likelihood tasks at 1.5B and 8B scales. Thanks to the parallel drafting and sampling as well as exact KV cache support, TiDAR outperforms speculative decoding in measured throughput and surpasses diffusion models like Dream and Llada in both efficiency and quality. Most notably, TiDAR is the first architecture to close the quality gap with AR models while delivering 4.71x to 5.91x more tokens per second."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-12T02:59:33Z",
                "published_parsed": [
                    2025,
                    11,
                    12,
                    2,
                    59,
                    33,
                    2,
                    316,
                    0
                ],
                "arxiv_comment": "NVIDIA-Tech Report",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jingyu Liu"
                    },
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Zhifan Ye"
                    },
                    {
                        "name": "Rishabh Mehta"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Vartika Singh"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Ce Zhang"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov"
            },
            {
                "id": "http://arxiv.org/abs/2510.25977v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.25977v3",
                "title": "NeuronMM: High-Performance Matrix Multiplication for LLM Inference on AWS Trainium",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuronMM: High-Performance Matrix Multiplication for LLM Inference on AWS Trainium"
                },
                "updated": "2025-11-11T23:18:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    11,
                    23,
                    18,
                    58,
                    1,
                    315,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.25977v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.25977v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "AI accelerators, customized to AI workloads, provide cost-effective and high-performance solutions for training and inference. Trainium, an AI accelerator recently developed by Amazon Web Services (AWS), provides an attractive option for LLM training and inference through its heterogeneous architecture. However, leveraging Trainium architecture for high performance can be challenging because of its systolic array architecture and special requirement on data layout. In this paper, we design high-performance matrix multiplication (matmul), a critical compute kernel, for LLM inference on Trainium. We introduce a series of techniques customized to Trainium based on kernel fusion and novel caching strategies to reduce data movement across the software-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive matrix transpose. Evaluating with nine datasets and four recent LLMs, we show that our system largely outperforms the state-of-the-art matmul implemented by AWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x speedup (up to 2.22x), which translates to an average 1.66x speedup (up to 2.49x) for end-to-end LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI accelerators, customized to AI workloads, provide cost-effective and high-performance solutions for training and inference. Trainium, an AI accelerator recently developed by Amazon Web Services (AWS), provides an attractive option for LLM training and inference through its heterogeneous architecture. However, leveraging Trainium architecture for high performance can be challenging because of its systolic array architecture and special requirement on data layout. In this paper, we design high-performance matrix multiplication (matmul), a critical compute kernel, for LLM inference on Trainium. We introduce a series of techniques customized to Trainium based on kernel fusion and novel caching strategies to reduce data movement across the software-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive matrix transpose. Evaluating with nine datasets and four recent LLMs, we show that our system largely outperforms the state-of-the-art matmul implemented by AWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x speedup (up to 2.22x), which translates to an average 1.66x speedup (up to 2.49x) for end-to-end LLM inference."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-29T21:22:08Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    21,
                    22,
                    8,
                    2,
                    302,
                    0
                ],
                "arxiv_comment": "12 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Dinghong Song"
                    },
                    {
                        "name": "Jierui Xu"
                    },
                    {
                        "name": "Weichu Yang"
                    },
                    {
                        "name": "Pengfei Su"
                    },
                    {
                        "name": "Dong Li"
                    }
                ],
                "author_detail": {
                    "name": "Dong Li"
                },
                "author": "Dong Li"
            },
            {
                "id": "http://arxiv.org/abs/2510.25979v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.25979v3",
                "title": "AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache"
                },
                "updated": "2025-11-11T23:07:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    11,
                    23,
                    7,
                    38,
                    1,
                    315,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.25979v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.25979v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are widely used in generative applications such as chatting, code generation, and reasoning. However, many realworld workloads such as classification, question answering, recommendation, and text embedding rely solely on the prefill stage of inference, where the model encodes input sequences without performing autoregressive decoding. In these prefill only scenarios, the self-attention computation becomes the primary performance bottleneck due to its quadratic complexity with respect to sequence length. In this paper, we observe that semantically different sentences often produce similar attention maps across layers and heads. Building on this insight, we propose AttnCache, a framework that accelerates the prefill stage of LLM inference by retrieving and reusing similar attention maps. Based on an attention map memorization database, AttnCache employs efficient caching and similarity search techniques to identify and reuse pre-cached attention maps during inference, thereby reducing the computational overhead of self-attention. Experimental results show that AttnCache achieves an average of 1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x attention speedup on GPU, with negligible accuracy degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used in generative applications such as chatting, code generation, and reasoning. However, many realworld workloads such as classification, question answering, recommendation, and text embedding rely solely on the prefill stage of inference, where the model encodes input sequences without performing autoregressive decoding. In these prefill only scenarios, the self-attention computation becomes the primary performance bottleneck due to its quadratic complexity with respect to sequence length. In this paper, we observe that semantically different sentences often produce similar attention maps across layers and heads. Building on this insight, we propose AttnCache, a framework that accelerates the prefill stage of LLM inference by retrieving and reusing similar attention maps. Based on an attention map memorization database, AttnCache employs efficient caching and similarity search techniques to identify and reuse pre-cached attention maps during inference, thereby reducing the computational overhead of self-attention. Experimental results show that AttnCache achieves an average of 1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x attention speedup on GPU, with negligible accuracy degradation."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-29T21:26:17Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    21,
                    26,
                    17,
                    2,
                    302,
                    0
                ],
                "arxiv_comment": "10 pages, 6 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Dinghong Song"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Shangye Chen"
                    },
                    {
                        "name": "Cyril Guyot"
                    },
                    {
                        "name": "Filip Blagojevic"
                    },
                    {
                        "name": "Hyeran Jeon"
                    },
                    {
                        "name": "Pengfei Su"
                    },
                    {
                        "name": "Dong Li"
                    }
                ],
                "author_detail": {
                    "name": "Dong Li"
                },
                "author": "Dong Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.08826v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.08826v1",
                "title": "FlashMap: A Flash Optimized Key-Value Store",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashMap: A Flash Optimized Key-Value Store"
                },
                "updated": "2025-11-11T22:48:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    11,
                    22,
                    48,
                    29,
                    1,
                    315,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.08826v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.08826v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Key-value stores are a fundamental class of NoSQL databases that offer a simple yet powerful model for data storage and retrieval, representing information as pairs of unique keys and associated values. Their minimal structure enables exceptionally fast access times, scalability, and flexibility in storing diverse data types, making them ideal for high-performance applications such as caching, session management, and distributed systems. As modern computing increasingly demands responsiveness and scalability, key-value stores have become a critical component of the data infrastructure in both industry and research contexts. In this work, we present FlashMap, a high-performance key-value store optimized for Flash-based solid-state drives (SSDs). Experiments show that FlashMap achieves outstanding throughput, averaging 19.8 million inserts and 23.8 million random lookups per second with a 100-byte payload, all on a single data center-grade server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value stores are a fundamental class of NoSQL databases that offer a simple yet powerful model for data storage and retrieval, representing information as pairs of unique keys and associated values. Their minimal structure enables exceptionally fast access times, scalability, and flexibility in storing diverse data types, making them ideal for high-performance applications such as caching, session management, and distributed systems. As modern computing increasingly demands responsiveness and scalability, key-value stores have become a critical component of the data infrastructure in both industry and research contexts. In this work, we present FlashMap, a high-performance key-value store optimized for Flash-based solid-state drives (SSDs). Experiments show that FlashMap achieves outstanding throughput, averaging 19.8 million inserts and 23.8 million random lookups per second with a 100-byte payload, all on a single data center-grade server."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-11T22:48:29Z",
                "published_parsed": [
                    2025,
                    11,
                    11,
                    22,
                    48,
                    29,
                    1,
                    315,
                    0
                ],
                "arxiv_comment": "6 pages, 2 figures, 3 tables",
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Zonglin Guo"
                    },
                    {
                        "name": "Tony Givargis"
                    }
                ],
                "author_detail": {
                    "name": "Tony Givargis"
                },
                "author": "Tony Givargis"
            },
            {
                "id": "http://arxiv.org/abs/2511.08568v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.08568v1",
                "title": "Machine Learning-Guided Memory Optimization for DLRM Inference on Tiered Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Learning-Guided Memory Optimization for DLRM Inference on Tiered Memory"
                },
                "updated": "2025-11-11T18:49:53Z",
                "updated_parsed": [
                    2025,
                    11,
                    11,
                    18,
                    49,
                    53,
                    1,
                    315,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.08568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.08568v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deep learning recommendation models (DLRMs) are widely used in industry, and their memory capacity requirements reach the terabyte scale. Tiered memory architectures provide a cost-effective solution but introduce challenges in embedding-vector placement due to complex embedding-access patterns. We propose RecMG, a machine learning (ML)-guided system for vector caching and prefetching on tiered memory. RecMG accurately predicts accesses to embedding vectors with long reuse distances or few reuses. The design of RecMG focuses on making ML feasible in the context of DLRM inference by addressing unique challenges in data labeling and navigating the search space for embedding-vector placement. By employing separate ML models for caching and prefetching, plus a novel differentiable loss function, RecMG narrows the prefetching search space and minimizes on-demand fetches. Compared to state-of-the-art temporal, spatial, and ML-based prefetchers, RecMG reduces on-demand fetches by 2.2x, 2.8x, and 1.5x, respectively. In industrial-scale DLRM inference scenarios, RecMG effectively reduces end-to-end DLRM inference time by up to 43%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning recommendation models (DLRMs) are widely used in industry, and their memory capacity requirements reach the terabyte scale. Tiered memory architectures provide a cost-effective solution but introduce challenges in embedding-vector placement due to complex embedding-access patterns. We propose RecMG, a machine learning (ML)-guided system for vector caching and prefetching on tiered memory. RecMG accurately predicts accesses to embedding vectors with long reuse distances or few reuses. The design of RecMG focuses on making ML feasible in the context of DLRM inference by addressing unique challenges in data labeling and navigating the search space for embedding-vector placement. By employing separate ML models for caching and prefetching, plus a novel differentiable loss function, RecMG narrows the prefetching search space and minimizes on-demand fetches. Compared to state-of-the-art temporal, spatial, and ML-based prefetchers, RecMG reduces on-demand fetches by 2.2x, 2.8x, and 1.5x, respectively. In industrial-scale DLRM inference scenarios, RecMG effectively reduces end-to-end DLRM inference time by up to 43%."
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-11T18:49:53Z",
                "published_parsed": [
                    2025,
                    11,
                    11,
                    18,
                    49,
                    53,
                    1,
                    315,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF"
                },
                "authors": [
                    {
                        "name": "Jie Ren"
                    },
                    {
                        "name": "Bin Ma"
                    },
                    {
                        "name": "Shuangyan Yang"
                    },
                    {
                        "name": "Benjamin Francis"
                    },
                    {
                        "name": "Ehsan K. Ardestani"
                    },
                    {
                        "name": "Min Si"
                    },
                    {
                        "name": "Dong Li"
                    }
                ],
                "author_detail": {
                    "name": "Dong Li"
                },
                "author": "Dong Li"
            },
            {
                "id": "http://arxiv.org/abs/2505.15683v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.15683v3",
                "title": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting Framework for Large Language Models"
                },
                "updated": "2025-11-11T13:52:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    11,
                    13,
                    52,
                    57,
                    1,
                    315,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.15683v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.15683v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Private data holds promise for improving LLMs due to its high quality, but its scattered distribution across data silos and the high computational demands of LLMs limit their deployment in federated environments. To address this, the transformer-based federated split models are proposed, which offload most model parameters to the server (or distributed clients) while retaining only a small portion on the client to ensure data privacy. Despite this design, they still face three challenges: 1) Peer-to-peer key encryption struggles to secure transmitted vectors effectively; 2) The auto-regressive nature of LLMs means that federated split learning can only train and infer sequentially, causing high communication overhead; 3) Fixed partition points lack adaptability to downstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure, Efficient, and Adaptive Federated splitting framework based on LLaMA2. First, we inject Gaussian noise into forward-pass hidden states to enable secure end-to-end vector transmission. Second, we employ attention-mask compression and KV cache collaboration to reduce communication costs, accelerating training and inference. Third, we allow users to dynamically adjust the partition points for input/output blocks based on specific task requirements. Experiments on natural language understanding, summarization, and conversational QA tasks show that FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and achieves up to 8x speedups in training and inference. Further analysis of privacy attacks and different partition points also demonstrates the effectiveness of FedSEA-LLaMA in security and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private data holds promise for improving LLMs due to its high quality, but its scattered distribution across data silos and the high computational demands of LLMs limit their deployment in federated environments. To address this, the transformer-based federated split models are proposed, which offload most model parameters to the server (or distributed clients) while retaining only a small portion on the client to ensure data privacy. Despite this design, they still face three challenges: 1) Peer-to-peer key encryption struggles to secure transmitted vectors effectively; 2) The auto-regressive nature of LLMs means that federated split learning can only train and infer sequentially, causing high communication overhead; 3) Fixed partition points lack adaptability to downstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure, Efficient, and Adaptive Federated splitting framework based on LLaMA2. First, we inject Gaussian noise into forward-pass hidden states to enable secure end-to-end vector transmission. Second, we employ attention-mask compression and KV cache collaboration to reduce communication costs, accelerating training and inference. Third, we allow users to dynamically adjust the partition points for input/output blocks based on specific task requirements. Experiments on natural language understanding, summarization, and conversational QA tasks show that FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and achieves up to 8x speedups in training and inference. Further analysis of privacy attacks and different partition points also demonstrates the effectiveness of FedSEA-LLaMA in security and adaptability."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-21T15:58:08Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zishuai Zhang"
                    },
                    {
                        "name": "Hainan zhang"
                    },
                    {
                        "name": "Weihua Li"
                    },
                    {
                        "name": "Qinnan zhang"
                    },
                    {
                        "name": "jin Dong"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng"
            },
            {
                "id": "http://arxiv.org/abs/2507.10069v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.10069v3",
                "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism"
                },
                "updated": "2025-11-11T12:44:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    11,
                    12,
                    44,
                    32,
                    1,
                    315,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.10069v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.10069v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multimodal large language models (MLLMs) extend LLMs to handle images, videos, and audio by incorporating feature extractors and projection modules. However, these additional components -- combined with complex inference pipelines and heterogeneous workloads -- introduce significant inference overhead. Therefore, efficiently serving MLLMs remains a major challenge. Current tightly coupled serving architectures struggle to distinguish between mixed request types or adapt parallelism strategies to different inference stages, leading to increased time-to-first-token (TTFT) latency and poor resource utilization. To address this, we introduce Elastic Multimodal Parallelism (EMP), a new serving paradigm that elastically adapts to resource heterogeneity across request types and inference stages. Building upon EMP, we develop ElasticMM, an MLLM serving system that (1) separates requests into independent modality groups with dynamic resource allocation via a modality-aware load balancer; (2) decouples inference stages and enables parallelism adjustment and adaptive scaling via elastic partition scheduling; and (3) improves inference efficiency through unified multimodal prefix caching and non-blocking encoding. Experiments on diverse real-world datasets show that ElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by up to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level objectives (SLOs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) extend LLMs to handle images, videos, and audio by incorporating feature extractors and projection modules. However, these additional components -- combined with complex inference pipelines and heterogeneous workloads -- introduce significant inference overhead. Therefore, efficiently serving MLLMs remains a major challenge. Current tightly coupled serving architectures struggle to distinguish between mixed request types or adapt parallelism strategies to different inference stages, leading to increased time-to-first-token (TTFT) latency and poor resource utilization. To address this, we introduce Elastic Multimodal Parallelism (EMP), a new serving paradigm that elastically adapts to resource heterogeneity across request types and inference stages. Building upon EMP, we develop ElasticMM, an MLLM serving system that (1) separates requests into independent modality groups with dynamic resource allocation via a modality-aware load balancer; (2) decouples inference stages and enables parallelism adjustment and adaptive scaling via elastic partition scheduling; and (3) improves inference efficiency through unified multimodal prefix caching and non-blocking encoding. Experiments on diverse real-world datasets show that ElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by up to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level objectives (SLOs)."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-14T08:53:48Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "arxiv_comment": "Accepted at NeurIPS 2025 Oral (Thirty-Ninth Conference on Neural Information Processing Systems)",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Zedong Liu"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Guangming Tan"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Dingwen Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dingwen Tao"
                },
                "author": "Dingwen Tao"
            },
            {
                "id": "http://arxiv.org/abs/2502.20969v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.20969v3",
                "title": "TeleRAG: Efficient Retrieval-Augmented Generation Inference with Lookahead Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeleRAG: Efficient Retrieval-Augmented Generation Inference with Lookahead Retrieval"
                },
                "updated": "2025-11-11T09:41:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    11,
                    9,
                    41,
                    30,
                    1,
                    315,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.20969v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.20969v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Retrieval-augmented generation (RAG) extends large language models (LLMs) with external data sources to enhance factual correctness and domain coverage. Modern RAG pipelines rely on large datastores, creating a significant system challenge: achieving high throughput and low latency is difficult, especially when GPU memory is limited. To address these challenges, we propose TeleRAG, an efficient inference system that reduces latency and improves throughput with minimal GPU memory requirements. The core innovation of TeleRAG is lookahead retrieval, a prefetching mechanism that predicts required data and transfers them from CPU to GPU in parallel with LLM generation. In addition, TeleRAG adopts a prefetching scheduler and a cache-aware scheduler to support efficient multi-GPU inference with minimal overhead. Evaluations show TeleRAG achieves up to a 1.53x average end-to-end latency reduction (single-query) and 1.83x higher average throughput (batched), as well as good scalability in throughput. This confirms the practical utility of TeleRAG for faster and more memory-efficient deployments of RAG applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) extends large language models (LLMs) with external data sources to enhance factual correctness and domain coverage. Modern RAG pipelines rely on large datastores, creating a significant system challenge: achieving high throughput and low latency is difficult, especially when GPU memory is limited. To address these challenges, we propose TeleRAG, an efficient inference system that reduces latency and improves throughput with minimal GPU memory requirements. The core innovation of TeleRAG is lookahead retrieval, a prefetching mechanism that predicts required data and transfers them from CPU to GPU in parallel with LLM generation. In addition, TeleRAG adopts a prefetching scheduler and a cache-aware scheduler to support efficient multi-GPU inference with minimal overhead. Evaluations show TeleRAG achieves up to a 1.53x average end-to-end latency reduction (single-query) and 1.83x higher average throughput (batched), as well as good scalability in throughput. This confirms the practical utility of TeleRAG for faster and more memory-efficient deployments of RAG applications."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-28T11:32:22Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    32,
                    22,
                    4,
                    59,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Keisuke Kamahori"
                    },
                    {
                        "name": "Yiyu Liu"
                    },
                    {
                        "name": "Xiaoxiang Shi"
                    },
                    {
                        "name": "Madhav Kashyap"
                    },
                    {
                        "name": "Yile Gu"
                    },
                    {
                        "name": "Rulin Shao"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Rohan Kadekodi"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Baris Kasikci"
                    }
                ],
                "author_detail": {
                    "name": "Baris Kasikci"
                },
                "author": "Baris Kasikci"
            },
            {
                "id": "http://arxiv.org/abs/2511.08003v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.08003v1",
                "title": "Sharp Eyes and Memory for VideoLLMs: Information-Aware Visual Token Pruning for Efficient and Reliable VideoLLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sharp Eyes and Memory for VideoLLMs: Information-Aware Visual Token Pruning for Efficient and Reliable VideoLLM Reasoning"
                },
                "updated": "2025-11-11T09:07:40Z",
                "updated_parsed": [
                    2025,
                    11,
                    11,
                    9,
                    7,
                    40,
                    1,
                    315,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.08003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.08003v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current Video Large Language Models (VideoLLMs) suffer from quadratic computational complexity and key-value cache scaling, due to their reliance on processing excessive redundant visual tokens. To address this problem, we propose SharpV, a minimalist and efficient method for adaptive pruning of visual tokens and KV cache. Different from most uniform compression approaches, SharpV dynamically adjusts pruning ratios based on spatial-temporal information. Remarkably, this adaptive mechanism occasionally achieves performance gains over dense models, offering a novel paradigm for adaptive pruning. During the KV cache pruning stage, based on observations of visual information degradation, SharpV prunes degraded visual features via a self-calibration manner, guided by similarity to original visual features. In this way, SharpV achieves hierarchical cache pruning from the perspective of information bottleneck, offering a new insight into VideoLLMs' information flow. Experiments on multiple public benchmarks demonstrate the superiority of SharpV. Moreover, to the best of our knowledge, SharpV is notably the first two-stage pruning framework that operates without requiring access to exposed attention scores, ensuring full compatibility with hardware acceleration techniques like Flash Attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Video Large Language Models (VideoLLMs) suffer from quadratic computational complexity and key-value cache scaling, due to their reliance on processing excessive redundant visual tokens. To address this problem, we propose SharpV, a minimalist and efficient method for adaptive pruning of visual tokens and KV cache. Different from most uniform compression approaches, SharpV dynamically adjusts pruning ratios based on spatial-temporal information. Remarkably, this adaptive mechanism occasionally achieves performance gains over dense models, offering a novel paradigm for adaptive pruning. During the KV cache pruning stage, based on observations of visual information degradation, SharpV prunes degraded visual features via a self-calibration manner, guided by similarity to original visual features. In this way, SharpV achieves hierarchical cache pruning from the perspective of information bottleneck, offering a new insight into VideoLLMs' information flow. Experiments on multiple public benchmarks demonstrate the superiority of SharpV. Moreover, to the best of our knowledge, SharpV is notably the first two-stage pruning framework that operates without requiring access to exposed attention scores, ensuring full compatibility with hardware acceleration techniques like Flash Attention."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-11T09:07:40Z",
                "published_parsed": [
                    2025,
                    11,
                    11,
                    9,
                    7,
                    40,
                    1,
                    315,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jialong Qin"
                    },
                    {
                        "name": "Xin Zou"
                    },
                    {
                        "name": "Di Lu"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu"
            },
            {
                "id": "http://arxiv.org/abs/2511.07969v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.07969v1",
                "title": "Unified Work Embeddings: Contrastive Learning of a Bidirectional Multi-task Ranker",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Work Embeddings: Contrastive Learning of a Bidirectional Multi-task Ranker"
                },
                "updated": "2025-11-11T08:28:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    11,
                    8,
                    28,
                    26,
                    1,
                    315,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.07969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.07969v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Workforce transformation across diverse industries has driven an increased demand for specialized natural language processing capabilities. Nevertheless, tasks derived from work-related contexts inherently reflect real-world complexities, characterized by long-tailed distributions, extreme multi-label target spaces, and scarce data availability. The rise of generalist embedding models prompts the question of their performance in the work domain, especially as progress in the field has focused mainly on individual tasks. To this end, we introduce WorkBench, the first unified evaluation suite spanning six work-related tasks formulated explicitly as ranking problems, establishing a common ground for multi-task progress. Based on this benchmark, we find significant positive cross-task transfer, and use this insight to compose task-specific bipartite graphs from real-world data, synthetically enriched through grounding. This leads to Unified Work Embeddings (UWE), a task-agnostic bi-encoder that exploits our training-data structure with a many-to-many InfoNCE objective, and leverages token-level embeddings with task-agnostic soft late interaction. UWE demonstrates zero-shot ranking performance on unseen target spaces in the work domain, enables low-latency inference by caching the task target space embeddings, and shows significant gains in macro-averaged MAP and RP@10 over generalist embedding models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Workforce transformation across diverse industries has driven an increased demand for specialized natural language processing capabilities. Nevertheless, tasks derived from work-related contexts inherently reflect real-world complexities, characterized by long-tailed distributions, extreme multi-label target spaces, and scarce data availability. The rise of generalist embedding models prompts the question of their performance in the work domain, especially as progress in the field has focused mainly on individual tasks. To this end, we introduce WorkBench, the first unified evaluation suite spanning six work-related tasks formulated explicitly as ranking problems, establishing a common ground for multi-task progress. Based on this benchmark, we find significant positive cross-task transfer, and use this insight to compose task-specific bipartite graphs from real-world data, synthetically enriched through grounding. This leads to Unified Work Embeddings (UWE), a task-agnostic bi-encoder that exploits our training-data structure with a many-to-many InfoNCE objective, and leverages token-level embeddings with task-agnostic soft late interaction. UWE demonstrates zero-shot ranking performance on unseen target spaces in the work domain, enables low-latency inference by caching the task target space embeddings, and shows significant gains in macro-averaged MAP and RP@10 over generalist embedding models."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-11T08:28:26Z",
                "published_parsed": [
                    2025,
                    11,
                    11,
                    8,
                    28,
                    26,
                    1,
                    315,
                    0
                ],
                "arxiv_comment": "Preprint",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Matthias De Lange"
                    },
                    {
                        "name": "Jens-Joris Decorte"
                    },
                    {
                        "name": "Jeroen Van Hautte"
                    }
                ],
                "author_detail": {
                    "name": "Jeroen Van Hautte"
                },
                "author": "Jeroen Van Hautte"
            },
            {
                "id": "http://arxiv.org/abs/2511.07762v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.07762v1",
                "title": "Kerr Polarization Transport: Accuracy and Performance in General Relativistic Light Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kerr Polarization Transport: Accuracy and Performance in General Relativistic Light Propagation"
                },
                "updated": "2025-11-11T02:26:27Z",
                "updated_parsed": [
                    2025,
                    11,
                    11,
                    2,
                    26,
                    27,
                    1,
                    315,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.07762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.07762v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present a compact and reproducible method for general relativistic polarization transport in the Kerr metric that achieves median electric vector position angle (EVPA) residuals of $\\langle \\mathrm{PA} \\rangle \\approx 0.09^\\circ$, a 95th percentile of $0.31^\\circ$, and a worst case $\\mathrm{PA} \\lesssim 0.32^\\circ$ for spins up to $|a/M|=0.9$, while maintaining a fivefold or greater speedup relative to a strict reference integrator. Across the benchmark grid, typical residuals remain at the sub-tenth-degree level, with only modest degradation ($\\mathrm{PA} \\lesssim 2^\\circ$) near the Thorne spin limit. Photon four-momenta $k^$ and polarization four-vectors $f^$ are advanced using a fourth order Runge-Kutta scheme with cached Christoffel symbols, maintaining the constraints $u\\cdot f=0$ and $n\\cdot f=0$, where $u^$ is the ZAMO four-velocity and $n^$ is the disk normal, while keeping $k\\cdot f \\simeq 0$. A physically motivated gauge is enforced by projecting the polarization into the local zero-angular-momentum observer (ZAMO) screen at every substep, ensuring numerical stability of the orthogonality constraints. Accuracy and performance are benchmarked over a representative grid in spin, inclination, image-plane azimuth, and radius. The method comfortably meets IXPE and NICER polarization tolerances and approaches EHT requirements. The approach provides a practical foundation for future general relativistic polarimetry and simulation pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a compact and reproducible method for general relativistic polarization transport in the Kerr metric that achieves median electric vector position angle (EVPA) residuals of $\\langle \\mathrm{PA} \\rangle \\approx 0.09^\\circ$, a 95th percentile of $0.31^\\circ$, and a worst case $\\mathrm{PA} \\lesssim 0.32^\\circ$ for spins up to $|a/M|=0.9$, while maintaining a fivefold or greater speedup relative to a strict reference integrator. Across the benchmark grid, typical residuals remain at the sub-tenth-degree level, with only modest degradation ($\\mathrm{PA} \\lesssim 2^\\circ$) near the Thorne spin limit. Photon four-momenta $k^$ and polarization four-vectors $f^$ are advanced using a fourth order Runge-Kutta scheme with cached Christoffel symbols, maintaining the constraints $u\\cdot f=0$ and $n\\cdot f=0$, where $u^$ is the ZAMO four-velocity and $n^$ is the disk normal, while keeping $k\\cdot f \\simeq 0$. A physically motivated gauge is enforced by projecting the polarization into the local zero-angular-momentum observer (ZAMO) screen at every substep, ensuring numerical stability of the orthogonality constraints. Accuracy and performance are benchmarked over a representative grid in spin, inclination, image-plane azimuth, and radius. The method comfortably meets IXPE and NICER polarization tolerances and approaches EHT requirements. The approach provides a practical foundation for future general relativistic polarimetry and simulation pipelines."
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-11T02:26:27Z",
                "published_parsed": [
                    2025,
                    11,
                    11,
                    2,
                    26,
                    27,
                    1,
                    315,
                    0
                ],
                "arxiv_comment": "11 pages, 2 figures. Submitted to arXiv; to be submitted to The Astrophysical Journal",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE"
                },
                "authors": [
                    {
                        "name": "Shakibul Chowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Shakibul Chowdhury"
                },
                "author": "Shakibul Chowdhury"
            },
            {
                "id": "http://arxiv.org/abs/2511.07399v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.07399v1",
                "title": "StreamDiffusionV2: A Streaming System for Dynamic and Interactive Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamDiffusionV2: A Streaming System for Dynamic and Interactive Video Generation"
                },
                "updated": "2025-11-10T18:51:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    18,
                    51,
                    28,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.07399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.07399v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generative models are reshaping the live-streaming industry by redefining how content is created, styled, and delivered. Previous image-based streaming diffusion models have powered efficient and creative live streaming products but have hit limits on temporal consistency due to the foundation of image-based designs. Recent advances in video diffusion have markedly improved temporal consistency and sampling efficiency for offline generation. However, offline generation systems primarily optimize throughput by batching large workloads. In contrast, live online streaming operates under strict service-level objectives (SLOs): time-to-first-frame must be minimal, and every frame must meet a per-frame deadline with low jitter. Besides, scalable multi-GPU serving for real-time streams remains largely unresolved so far. To address this, we present StreamDiffusionV2, a training-free pipeline for interactive live streaming with video diffusion models. StreamDiffusionV2 integrates an SLO-aware batching scheduler and a block scheduler, together with a sink-token--guided rolling KV cache, a motion-aware noise controller, and other system-level optimizations. Moreover, we introduce a scalable pipeline orchestration that parallelizes the diffusion process across denoising steps and network layers, achieving near-linear FPS scaling without violating latency guarantees. The system scales seamlessly across heterogeneous GPU environments and supports flexible denoising steps (e.g., 1--4), enabling both ultra-low-latency and higher-quality modes. Without TensorRT or quantization, StreamDiffusionV2 renders the first frame within 0.5s and attains 58.28 FPS with a 14B-parameter model and 64.52 FPS with a 1.3B-parameter model on four H100 GPUs, making state-of-the-art generative live streaming practical and accessible--from individual creators to enterprise-scale platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models are reshaping the live-streaming industry by redefining how content is created, styled, and delivered. Previous image-based streaming diffusion models have powered efficient and creative live streaming products but have hit limits on temporal consistency due to the foundation of image-based designs. Recent advances in video diffusion have markedly improved temporal consistency and sampling efficiency for offline generation. However, offline generation systems primarily optimize throughput by batching large workloads. In contrast, live online streaming operates under strict service-level objectives (SLOs): time-to-first-frame must be minimal, and every frame must meet a per-frame deadline with low jitter. Besides, scalable multi-GPU serving for real-time streams remains largely unresolved so far. To address this, we present StreamDiffusionV2, a training-free pipeline for interactive live streaming with video diffusion models. StreamDiffusionV2 integrates an SLO-aware batching scheduler and a block scheduler, together with a sink-token--guided rolling KV cache, a motion-aware noise controller, and other system-level optimizations. Moreover, we introduce a scalable pipeline orchestration that parallelizes the diffusion process across denoising steps and network layers, achieving near-linear FPS scaling without violating latency guarantees. The system scales seamlessly across heterogeneous GPU environments and supports flexible denoising steps (e.g., 1--4), enabling both ultra-low-latency and higher-quality modes. Without TensorRT or quantization, StreamDiffusionV2 renders the first frame within 0.5s and attains 58.28 FPS with a 14B-parameter model and 64.52 FPS with a 1.3B-parameter model on four H100 GPUs, making state-of-the-art generative live streaming practical and accessible--from individual creators to enterprise-scale platforms."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-10T18:51:28Z",
                "published_parsed": [
                    2025,
                    11,
                    10,
                    18,
                    51,
                    28,
                    0,
                    314,
                    0
                ],
                "arxiv_comment": "Project Page: http://streamdiffusionv2.github.io",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Tianrui Feng"
                    },
                    {
                        "name": "Zhi Li"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Haocheng Xi"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Lvmin Zhang"
                    },
                    {
                        "name": "Keting Yang"
                    },
                    {
                        "name": "Kelly Peng"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Maneesh Agrawala"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Akio Kodaira"
                    },
                    {
                        "name": "Chenfeng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chenfeng Xu"
                },
                "author": "Chenfeng Xu"
            },
            {
                "id": "http://arxiv.org/abs/2511.07278v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.07278v1",
                "title": "StreamKV: Streaming Video Question-Answering with Segment-based KV Cache Retrieval and Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamKV: Streaming Video Question-Answering with Segment-based KV Cache Retrieval and Compression"
                },
                "updated": "2025-11-10T16:25:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    16,
                    25,
                    3,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.07278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.07278v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Video Large Language Models (Video-LLMs) have demonstrated significant potential in the areas of video captioning, search, and summarization. However, current Video-LLMs still face challenges with long real-world videos. Recent methods have introduced a retrieval mechanism that retrieves query-relevant KV caches for question answering, enhancing the efficiency and accuracy of long real-world videos. However, the compression and retrieval of KV caches are still not fully explored. In this paper, we propose \\textbf{StreamKV}, a training-free framework that seamlessly equips Video-LLMs with advanced KV cache retrieval and compression. Compared to previous methods that used uniform partitioning, StreamKV dynamically partitions video streams into semantic segments, which better preserves semantic information. For KV cache retrieval, StreamKV calculates a summary vector for each segment to retain segment-level information essential for retrieval. For KV cache compression, StreamKV introduces a guidance prompt designed to capture the key semantic elements within each segment, ensuring only the most informative KV caches are retained for answering questions. Moreover, StreamKV unifies KV cache retrieval and compression within a single module, performing both in a layer-adaptive manner, thereby further improving the effectiveness of streaming video question answering. Extensive experiments on public StreamingVQA benchmarks demonstrate that StreamKV significantly outperforms existing Online Video-LLMs, achieving superior accuracy while substantially improving both memory efficiency and computational latency. The code has been released at https://github.com/sou1p0wer/StreamKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (Video-LLMs) have demonstrated significant potential in the areas of video captioning, search, and summarization. However, current Video-LLMs still face challenges with long real-world videos. Recent methods have introduced a retrieval mechanism that retrieves query-relevant KV caches for question answering, enhancing the efficiency and accuracy of long real-world videos. However, the compression and retrieval of KV caches are still not fully explored. In this paper, we propose \\textbf{StreamKV}, a training-free framework that seamlessly equips Video-LLMs with advanced KV cache retrieval and compression. Compared to previous methods that used uniform partitioning, StreamKV dynamically partitions video streams into semantic segments, which better preserves semantic information. For KV cache retrieval, StreamKV calculates a summary vector for each segment to retain segment-level information essential for retrieval. For KV cache compression, StreamKV introduces a guidance prompt designed to capture the key semantic elements within each segment, ensuring only the most informative KV caches are retained for answering questions. Moreover, StreamKV unifies KV cache retrieval and compression within a single module, performing both in a layer-adaptive manner, thereby further improving the effectiveness of streaming video question answering. Extensive experiments on public StreamingVQA benchmarks demonstrate that StreamKV significantly outperforms existing Online Video-LLMs, achieving superior accuracy while substantially improving both memory efficiency and computational latency. The code has been released at https://github.com/sou1p0wer/StreamKV."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-10T16:25:03Z",
                "published_parsed": [
                    2025,
                    11,
                    10,
                    16,
                    25,
                    3,
                    0,
                    314,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Xiang Bai"
                    },
                    {
                        "name": "Zhibin Wang"
                    },
                    {
                        "name": "Chengyu Bai"
                    },
                    {
                        "name": "Yuhan Dai"
                    },
                    {
                        "name": "Ming Lu"
                    },
                    {
                        "name": "Shanghang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shanghang Zhang"
                },
                "author": "Shanghang Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.07229v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.07229v1",
                "title": "LLMServingSim2.0: A Unified Simulator for Heterogeneous Hardware and Serving Techniques in LLM Infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMServingSim2.0: A Unified Simulator for Heterogeneous Hardware and Serving Techniques in LLM Infrastructure"
                },
                "updated": "2025-11-10T15:47:53Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    15,
                    47,
                    53,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.07229v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.07229v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/LCA.2025.3628325",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "This paper introduces LLMServingSim2.0, a system simulator designed for exploring heterogeneous hardware in large-scale LLM serving systems. LLMServingSim2.0 addresses two key limitations of its predecessor: (1) integrating hardware models into system-level simulators is non-trivial due to the lack of a clear abstraction, and (2) existing simulators support only a narrow subset of serving techniques, leaving no infrastructure that captures the breadth of approaches in modern LLM serving. To overcome these issues, LLMServingSim2.0 adopts trace-driven performance modeling, accompanied by an operator-level latency profiler, enabling the integration of new accelerators with a single command. It further embeds up-to-date serving techniques while exposing flexible interfaces for request routing, cache management, and scheduling policies. In a TPU case study, our profiler requires 18.5x fewer LoC and outperforms the predecessor's hardware-simulator integration, demonstrating LLMServingSim2.0's low-effort hardware extensibility. Our experiments further show that LLMServingSim2.0 reproduces GPU-based LLM serving with 1.9% error, while maintaining practical simulation time, making it a comprehensive platform for both hardware developers and LLM service providers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces LLMServingSim2.0, a system simulator designed for exploring heterogeneous hardware in large-scale LLM serving systems. LLMServingSim2.0 addresses two key limitations of its predecessor: (1) integrating hardware models into system-level simulators is non-trivial due to the lack of a clear abstraction, and (2) existing simulators support only a narrow subset of serving techniques, leaving no infrastructure that captures the breadth of approaches in modern LLM serving. To overcome these issues, LLMServingSim2.0 adopts trace-driven performance modeling, accompanied by an operator-level latency profiler, enabling the integration of new accelerators with a single command. It further embeds up-to-date serving techniques while exposing flexible interfaces for request routing, cache management, and scheduling policies. In a TPU case study, our profiler requires 18.5x fewer LoC and outperforms the predecessor's hardware-simulator integration, demonstrating LLMServingSim2.0's low-effort hardware extensibility. Our experiments further show that LLMServingSim2.0 reproduces GPU-based LLM serving with 1.9% error, while maintaining practical simulation time, making it a comprehensive platform for both hardware developers and LLM service providers."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-10T15:47:53Z",
                "published_parsed": [
                    2025,
                    11,
                    10,
                    15,
                    47,
                    53,
                    0,
                    314,
                    0
                ],
                "arxiv_comment": "4 pages, 3 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "arxiv_journal_ref": "IEEE Computer Architecture Letters (CAL) 2025",
                "authors": [
                    {
                        "name": "Jaehong Cho"
                    },
                    {
                        "name": "Hyunmin Choi"
                    },
                    {
                        "name": "Jongse Park"
                    }
                ],
                "author_detail": {
                    "name": "Jongse Park"
                },
                "author": "Jongse Park",
                "arxiv_doi": "10.1109/LCA.2025.3628325"
            },
            {
                "id": "http://arxiv.org/abs/2510.18480v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.18480v3",
                "title": "How Efficient Are Diffusion Language Models? A Critical Examination of Efficiency Evaluation Practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Efficient Are Diffusion Language Models? A Critical Examination of Efficiency Evaluation Practices"
                },
                "updated": "2025-11-10T13:10:25Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    13,
                    10,
                    25,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.18480v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.18480v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion language models (DLMs) have emerged as a promising alternative to the long-dominant autoregressive (AR) paradigm, offering a parallelable decoding process that could yield greater efficiency. Yet, in practice, current open-source DLMs often underperform their AR counterparts in speed, limiting their real-world utility. This work presents a systematic study of DLM efficiency, identifying key issues in prior evaluation methods. Through empirical benchmarking and a theoretical analysis, we demonstrate that AR models generally achieve higher throughput, while DLMs consistently lag. We also investigate acceleration strategies, finding that techniques like dual cache and parallel decoding mainly offer gains at small batch sizes, with their benefits diminishing upon scaling. Our findings underscore the necessity of robust evaluation methods and improved acceleration strategies to advance research on DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models (DLMs) have emerged as a promising alternative to the long-dominant autoregressive (AR) paradigm, offering a parallelable decoding process that could yield greater efficiency. Yet, in practice, current open-source DLMs often underperform their AR counterparts in speed, limiting their real-world utility. This work presents a systematic study of DLM efficiency, identifying key issues in prior evaluation methods. Through empirical benchmarking and a theoretical analysis, we demonstrate that AR models generally achieve higher throughput, while DLMs consistently lag. We also investigate acceleration strategies, finding that techniques like dual cache and parallel decoding mainly offer gains at small batch sizes, with their benefits diminishing upon scaling. Our findings underscore the necessity of robust evaluation methods and improved acceleration strategies to advance research on DLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-21T10:00:32Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    0,
                    32,
                    1,
                    294,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Han Peng"
                    },
                    {
                        "name": "Peiyu Liu"
                    },
                    {
                        "name": "Zican Dong"
                    },
                    {
                        "name": "Daixuan Cheng"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Yiru Tang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wayne Xin Zhao"
                },
                "author": "Wayne Xin Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2511.10676v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10676v1",
                "title": "Pre-Attention Expert Prediction and Prefetching for Mixture-of-Experts Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-Attention Expert Prediction and Prefetching for Mixture-of-Experts Large Language Models"
                },
                "updated": "2025-11-10T13:05:07Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    13,
                    5,
                    7,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10676v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Mixture-of-Experts (MoE) Large Language Models (LLMs) efficiently scale-up the model while keeping relatively low inference cost. As MoE models only activate part of the experts, related work has proposed expert prediction and caching methods to prefetch the experts for faster inference. However, existing approaches utilize the activations from the previous layer for prediction, incurring low accuracy and leave the first layer unoptimized. Applying complex layers or even training standalone networks for better prediction introduces high computation overhead. In this paper, we propose pre-attention expert prediction to achieve accurate and lightweight expert prefetching. The key insight is that some functions in LLMs are ranking-preserving, indicating that matching the ranking of selected experts using simple linear functions is possible. Therefore, we utilize the activations before the attention block in the same layer with 2 linear functions and ranking-aware loss to achieve accurate prediction, which also supports prefetching in the first layer. Our lightweight, pre-attention expert routers achieve 93.03% accuracy on DeepSeek V2 Lite, 94.69% on Qwen3-30B, and 97.62% on Phi-mini-MoE, showing about 15% improvement on absolute accuracy over the state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) Large Language Models (LLMs) efficiently scale-up the model while keeping relatively low inference cost. As MoE models only activate part of the experts, related work has proposed expert prediction and caching methods to prefetch the experts for faster inference. However, existing approaches utilize the activations from the previous layer for prediction, incurring low accuracy and leave the first layer unoptimized. Applying complex layers or even training standalone networks for better prediction introduces high computation overhead. In this paper, we propose pre-attention expert prediction to achieve accurate and lightweight expert prefetching. The key insight is that some functions in LLMs are ranking-preserving, indicating that matching the ranking of selected experts using simple linear functions is possible. Therefore, we utilize the activations before the attention block in the same layer with 2 linear functions and ranking-aware loss to achieve accurate prediction, which also supports prefetching in the first layer. Our lightweight, pre-attention expert routers achieve 93.03% accuracy on DeepSeek V2 Lite, 94.69% on Qwen3-30B, and 97.62% on Phi-mini-MoE, showing about 15% improvement on absolute accuracy over the state-of-the-art methods."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-10T13:05:07Z",
                "published_parsed": [
                    2025,
                    11,
                    10,
                    13,
                    5,
                    7,
                    0,
                    314,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Shien Zhu"
                    },
                    {
                        "name": "Samuel Bohl"
                    },
                    {
                        "name": "Robin Oester"
                    },
                    {
                        "name": "Gustavo Alonso"
                    }
                ],
                "author_detail": {
                    "name": "Gustavo Alonso"
                },
                "author": "Gustavo Alonso"
            },
            {
                "id": "http://arxiv.org/abs/2506.08009v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.08009v2",
                "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion"
                },
                "updated": "2025-11-10T04:36:27Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    4,
                    36,
                    27,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.08009v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.08009v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce Self Forcing, a novel training paradigm for autoregressive video diffusion models. It addresses the longstanding issue of exposure bias, where models trained on ground-truth context must generate sequences conditioned on their own imperfect outputs during inference. Unlike prior methods that denoise future frames based on ground-truth context frames, Self Forcing conditions each frame's generation on previously self-generated outputs by performing autoregressive rollout with key-value (KV) caching during training. This strategy enables supervision through a holistic loss at the video level that directly evaluates the quality of the entire generated sequence, rather than relying solely on traditional frame-wise objectives. To ensure training efficiency, we employ a few-step diffusion model along with a stochastic gradient truncation strategy, effectively balancing computational cost and performance. We further introduce a rolling KV cache mechanism that enables efficient autoregressive video extrapolation. Extensive experiments demonstrate that our approach achieves real-time streaming video generation with sub-second latency on a single GPU, while matching or even surpassing the generation quality of significantly slower and non-causal diffusion models. Project website: http://self-forcing.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Self Forcing, a novel training paradigm for autoregressive video diffusion models. It addresses the longstanding issue of exposure bias, where models trained on ground-truth context must generate sequences conditioned on their own imperfect outputs during inference. Unlike prior methods that denoise future frames based on ground-truth context frames, Self Forcing conditions each frame's generation on previously self-generated outputs by performing autoregressive rollout with key-value (KV) caching during training. This strategy enables supervision through a holistic loss at the video level that directly evaluates the quality of the entire generated sequence, rather than relying solely on traditional frame-wise objectives. To ensure training efficiency, we employ a few-step diffusion model along with a stochastic gradient truncation strategy, effectively balancing computational cost and performance. We further introduce a rolling KV cache mechanism that enables efficient autoregressive video extrapolation. Extensive experiments demonstrate that our approach achieves real-time streaming video generation with sub-second latency on a single GPU, while matching or even surpassing the generation quality of significantly slower and non-causal diffusion models. Project website: http://self-forcing.github.io/"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-09T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    17,
                    59,
                    55,
                    0,
                    160,
                    0
                ],
                "arxiv_comment": "NeurIPS 2025 spotlight. Project website: http://self-forcing.github.io/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Xun Huang"
                    },
                    {
                        "name": "Zhengqi Li"
                    },
                    {
                        "name": "Guande He"
                    },
                    {
                        "name": "Mingyuan Zhou"
                    },
                    {
                        "name": "Eli Shechtman"
                    }
                ],
                "author_detail": {
                    "name": "Eli Shechtman"
                },
                "author": "Eli Shechtman"
            },
            {
                "id": "http://arxiv.org/abs/2507.02397v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.02397v2",
                "title": "Direct Reconstruction of Terahertz-driven Subcycle Electron Emission Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Reconstruction of Terahertz-driven Subcycle Electron Emission Dynamics"
                },
                "updated": "2025-11-10T03:55:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    3,
                    55,
                    22,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.02397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.02397v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While field-driven electron emission is theoretically understood down to the subcycle regime, its direct experimental temporal characterization using long-wavelength terahertz (THz) fields remains elusive. Here, by driving a graphite tip with phase-stable quasi-single-cycle THz pulses, we reveal distinct subcycle electron emission dynamics including: (1) At a carrier-envelope phase (CEP) zero, spectral peaks scale linearly with THz field strength, characteristic of subcycle emission; (2) At nearly opposite CEP, dominant deceleration fields generate stationary low-energy peaks. Crucially, we develop a pump-probe-free, direct reconstruction method extracting electron pulse profiles solely from measured energy spectra, obtaining durations from 73.0 to 81.0 fs as the field increases (191-290 kV/cm). Phase-resolved simulations further reveal a 72.8% modulation in the cutoff energy and a near-total (99.7%) suppression of the emission current. This work not only validates the field-emisssion theory under THz excitation but also establishes a general framework for the direct temporal characterization of subcycle electron emission, opening pathways for precise electron control in ultrafast electron sources and lightwave nanoelectronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While field-driven electron emission is theoretically understood down to the subcycle regime, its direct experimental temporal characterization using long-wavelength terahertz (THz) fields remains elusive. Here, by driving a graphite tip with phase-stable quasi-single-cycle THz pulses, we reveal distinct subcycle electron emission dynamics including: (1) At a carrier-envelope phase (CEP) zero, spectral peaks scale linearly with THz field strength, characteristic of subcycle emission; (2) At nearly opposite CEP, dominant deceleration fields generate stationary low-energy peaks. Crucially, we develop a pump-probe-free, direct reconstruction method extracting electron pulse profiles solely from measured energy spectra, obtaining durations from 73.0 to 81.0 fs as the field increases (191-290 kV/cm). Phase-resolved simulations further reveal a 72.8% modulation in the cutoff energy and a near-total (99.7%) suppression of the emission current. This work not only validates the field-emisssion theory under THz excitation but also establishes a general framework for the direct temporal characterization of subcycle electron emission, opening pathways for precise electron control in ultrafast electron sources and lightwave nanoelectronics."
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-03T07:49:18Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    49,
                    18,
                    3,
                    184,
                    0
                ],
                "arxiv_comment": "18 pages, 5 figures, references added",
                "arxiv_primary_category": {
                    "term": "physics.optics"
                },
                "authors": [
                    {
                        "name": "Jiakang Mao"
                    },
                    {
                        "name": "Yushan Zeng"
                    },
                    {
                        "name": "Hongyang Li"
                    },
                    {
                        "name": "Liwei Song"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Ruxin Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruxin Li"
                },
                "author": "Ruxin Li"
            },
            {
                "id": "http://arxiv.org/abs/2510.27070v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.27070v2",
                "title": "Descriptor-Based Object-Aware Memory Systems: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Descriptor-Based Object-Aware Memory Systems: A Comprehensive Review"
                },
                "updated": "2025-11-10T02:03:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    2,
                    3,
                    6,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.27070v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.27070v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The security and efficiency of modern computing systems are fundamentally undermined by the absence of a native architectural mechanism to propagate high-level program semantics, such as object identity, bounds, and lifetime, across the hardware/software interface. This paper presents a comprehensive survey of the architectural paradigm designed to bridge this semantic gap: descriptor-based, object-aware memory systems. By elevating the descriptor to a first-class architectural abstraction, this paradigm enables hardware to dynamically acquire and enforce the rich semantics of software-defined objects. This survey systematically charts the evolution and current landscape of this approach. We establish the foundational concepts of memory objects and descriptors and introduce a novel taxonomy of descriptor addressing modes, providing a structured framework for analyzing and comparing diverse implementations. Our unified analysis reveals how this paradigm holistically addresses the intertwined challenges of memory protection, management, and processing. As a culminating case study, we re-examine the CentroID model, demonstrating how its hybrid tagged-pointer encoding and descriptor processing mechanisms embody the path toward practical and efficient object-aware designs. Finally, we outline how the explicit cross-layer communication of object semantics provides a foundational research direction for next-generation cache hierarchies, unified virtual memory, and even 128-bit architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The security and efficiency of modern computing systems are fundamentally undermined by the absence of a native architectural mechanism to propagate high-level program semantics, such as object identity, bounds, and lifetime, across the hardware/software interface. This paper presents a comprehensive survey of the architectural paradigm designed to bridge this semantic gap: descriptor-based, object-aware memory systems. By elevating the descriptor to a first-class architectural abstraction, this paradigm enables hardware to dynamically acquire and enforce the rich semantics of software-defined objects. This survey systematically charts the evolution and current landscape of this approach. We establish the foundational concepts of memory objects and descriptors and introduce a novel taxonomy of descriptor addressing modes, providing a structured framework for analyzing and comparing diverse implementations. Our unified analysis reveals how this paradigm holistically addresses the intertwined challenges of memory protection, management, and processing. As a culminating case study, we re-examine the CentroID model, demonstrating how its hybrid tagged-pointer encoding and descriptor processing mechanisms embody the path toward practical and efficient object-aware designs. Finally, we outline how the explicit cross-layer communication of object semantics provides a foundational research direction for next-generation cache hierarchies, unified virtual memory, and even 128-bit architectures."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-31T00:39:27Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    0,
                    39,
                    27,
                    4,
                    304,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Dong Tong"
                    }
                ],
                "author_detail": {
                    "name": "Dong Tong"
                },
                "author": "Dong Tong"
            },
            {
                "id": "http://arxiv.org/abs/2511.06605v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.06605v1",
                "title": "DMA Collectives for Efficient ML Communication Offloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DMA Collectives for Efficient ML Communication Offloads"
                },
                "updated": "2025-11-10T01:28:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    1,
                    28,
                    58,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.06605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.06605v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Offloading machine learning (ML) communication collectives to direct memory access (DMA) engines has emerged as an interesting and low-cost solution to efficiently overlap computation and communication in inference and training. Doing so delivers superior concurrent performance by freeing up all GPU cores for computation and also lowers interference in the memory sub-system (caches). While DMA collectives show strong promise, prior works have only studied them in limited context (bandwidth-bound transfer sizes only, performance-only).\n  To address this, we provide a comprehensive performance, power/energy and synchronization costs analysis of offloading ML communication collectives (all-gather, all-to-all) to DMA engines on state-of-the-art AMD Instinct MI300X GPUs. Our analysis reveals that, compared to the state-of-the-art RCCL communication collectives library, DMA collectives are at-par or better for large sizes (10s of MB to GB) in terms of both performance (16% better) and power (32% better). However, they significantly lag for latency-bound small sizes; 4.5X and 2.5X slower for all-gather and all-to-all, respectively. We provide a detailed latency breakdown of a DMA transfer and identify that DMA command scheduling and synchronization costs can limit DMA collective performance. To tackle this, we harness existing DMA architecture innovations, hitherto untapped, to build optimized DMA collectives and demonstrate their efficacy on real hardware. Our optimized implementations considerably close the performance gap for DMA collectives at smaller sizes (30% slower and 20% faster all-gather and all-to-all, respectively) and further improves performance (by 7%) and power savings at larger sizes (3-10%). Overall, this work represents a significant step toward making DMA collectives suitable for adoption in mainstream collective libraries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offloading machine learning (ML) communication collectives to direct memory access (DMA) engines has emerged as an interesting and low-cost solution to efficiently overlap computation and communication in inference and training. Doing so delivers superior concurrent performance by freeing up all GPU cores for computation and also lowers interference in the memory sub-system (caches). While DMA collectives show strong promise, prior works have only studied them in limited context (bandwidth-bound transfer sizes only, performance-only).\n  To address this, we provide a comprehensive performance, power/energy and synchronization costs analysis of offloading ML communication collectives (all-gather, all-to-all) to DMA engines on state-of-the-art AMD Instinct MI300X GPUs. Our analysis reveals that, compared to the state-of-the-art RCCL communication collectives library, DMA collectives are at-par or better for large sizes (10s of MB to GB) in terms of both performance (16% better) and power (32% better). However, they significantly lag for latency-bound small sizes; 4.5X and 2.5X slower for all-gather and all-to-all, respectively. We provide a detailed latency breakdown of a DMA transfer and identify that DMA command scheduling and synchronization costs can limit DMA collective performance. To tackle this, we harness existing DMA architecture innovations, hitherto untapped, to build optimized DMA collectives and demonstrate their efficacy on real hardware. Our optimized implementations considerably close the performance gap for DMA collectives at smaller sizes (30% slower and 20% faster all-gather and all-to-all, respectively) and further improves performance (by 7%) and power savings at larger sizes (3-10%). Overall, this work represents a significant step toward making DMA collectives suitable for adoption in mainstream collective libraries."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-10T01:28:58Z",
                "published_parsed": [
                    2025,
                    11,
                    10,
                    1,
                    28,
                    58,
                    0,
                    314,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Mohamed Assem Ibrahim"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Assem Ibrahim"
                },
                "author": "Mohamed Assem Ibrahim"
            },
            {
                "id": "http://arxiv.org/abs/2502.16002v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.16002v4",
                "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"
                },
                "updated": "2025-11-10T00:11:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    0,
                    11,
                    57,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.16002v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.16002v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in large language models (LLMs). In many LLM applications, different inputs can share overlapping context, such as the same retrieved document appearing in multiple queries. However, the LLMs still need to encode the entire context for each query, leading to redundant computation. In this paper, we investigate a new strategy to eliminate such inefficiency, where the KV cache of each document is precomputed independently. During inference, the KV caches of retrieved documents are concatenated, allowing the model to reuse cached representations instead of recomputing them. To mitigate the performance degradation when using KV caches computed independently for each document, KVLink introduces two key techniques: adjusting positional embeddings of the KV cache at inference to match the global position after concatenation, and using trainable special tokens to restore self-attention across independently encoded documents. Experiments across 7 datasets demonstrate that KVLink improves question answering accuracy by an average of 4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV caches, our approach reduces time-to-first-token by up to 96% compared to standard LLM inference, making it a scalable and efficient solution for context reuse. Additionally, KVLink can be combined with KV cache compression to further save cache loading and storage overhead while outperforming the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in large language models (LLMs). In many LLM applications, different inputs can share overlapping context, such as the same retrieved document appearing in multiple queries. However, the LLMs still need to encode the entire context for each query, leading to redundant computation. In this paper, we investigate a new strategy to eliminate such inefficiency, where the KV cache of each document is precomputed independently. During inference, the KV caches of retrieved documents are concatenated, allowing the model to reuse cached representations instead of recomputing them. To mitigate the performance degradation when using KV caches computed independently for each document, KVLink introduces two key techniques: adjusting positional embeddings of the KV cache at inference to match the global position after concatenation, and using trainable special tokens to restore self-attention across independently encoded documents. Experiments across 7 datasets demonstrate that KVLink improves question answering accuracy by an average of 4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV caches, our approach reduces time-to-first-token by up to 96% compared to standard LLM inference, making it a scalable and efficient solution for context reuse. Additionally, KVLink can be combined with KV cache compression to further save cache loading and storage overhead while outperforming the baselines."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-21T23:34:29Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jingbo Yang"
                    },
                    {
                        "name": "Bairu Hou"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yujia Bao"
                    },
                    {
                        "name": "Shiyu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Chang"
                },
                "author": "Shiyu Chang"
            },
            {
                "id": "http://arxiv.org/abs/2510.13797v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.13797v2",
                "title": "Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression Beacons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression Beacons"
                },
                "updated": "2025-11-10T00:06:46Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    0,
                    6,
                    46,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.13797v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.13797v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The scalability of large language models for long-context reasoning is severely constrained by the linear growth of their Transformer key-value cache, which incurs significant memory and computational costs. We posit that as a model generates reasoning tokens, the informational value of past generated tokens diminishes, creating an opportunity for compression. In this work, we propose to periodically compress the generation KV cache with a learned, special-purpose token and evict compressed entries. We train the model to perform this compression via a modified joint distillation and reinforcement learning (RL) framework. Our training method minimizes overhead over the conventional RL process, as it leverages RL outputs for distillation. Empirically, our method achieves a superior memory-accuracy Pareto frontier compared to both the model without cache compression and training-free compression techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scalability of large language models for long-context reasoning is severely constrained by the linear growth of their Transformer key-value cache, which incurs significant memory and computational costs. We posit that as a model generates reasoning tokens, the informational value of past generated tokens diminishes, creating an opportunity for compression. In this work, we propose to periodically compress the generation KV cache with a learned, special-purpose token and evict compressed entries. We train the model to perform this compression via a modified joint distillation and reinforcement learning (RL) framework. Our training method minimizes overhead over the conventional RL process, as it leverages RL outputs for distillation. Empirically, our method achieves a superior memory-accuracy Pareto frontier compared to both the model without cache compression and training-free compression techniques."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-15T17:57:21Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    57,
                    21,
                    2,
                    288,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Giovanni Monea"
                    },
                    {
                        "name": "Yair Feldman"
                    },
                    {
                        "name": "Shankar Padmanabhan"
                    },
                    {
                        "name": "Kiant Brantley"
                    },
                    {
                        "name": "Yoav Artzi"
                    }
                ],
                "author_detail": {
                    "name": "Yoav Artzi"
                },
                "author": "Yoav Artzi"
            },
            {
                "id": "http://arxiv.org/abs/2511.06460v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.06460v1",
                "title": "Guidelines for Building Indexes on Partially Cache-Coherent CXL Shared Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guidelines for Building Indexes on Partially Cache-Coherent CXL Shared Memory"
                },
                "updated": "2025-11-09T16:55:00Z",
                "updated_parsed": [
                    2025,
                    11,
                    9,
                    16,
                    55,
                    0,
                    6,
                    313,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.06460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.06460v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The \\emph{Partial Cache-Coherence (PCC)} model maintains hardware cache coherence only within subsets of cores, enabling large-scale memory sharing with emerging memory interconnect technologies like Compute Express Link (CXL). However, PCC's relaxation of global cache coherence compromises the correctness of existing single-machine software.\n  This paper focuses on building consistent and efficient indexes on PCC platforms. We present that existing indexes designed for cache-coherent platforms can be made consistent on PCC platforms following SP guidelines, i.e., we identify \\emph{sync-data} and \\emph{protected-data} according to the index's concurrency control mechanisms, and synchronize them accordingly. However, conversion with SP guidelines introduces performance overhead. To mitigate the overhead, we identify several unique performance bottlenecks on PCC platforms, and propose P$^3$ guidelines (i.e., using Out-of-\\underline{P}lace update, Re\\underline{P}licated shared variable, S\\underline{P}eculative Reading) to improve the efficiency of converted indexes on PCC platforms.\n  With SP and P$^3$ guidelines, we convert and optimize two indexes (CLevelHash and BwTree) for PCC platforms. Evaluation shows that converted indexes' throughput improves up to 16$\\times$ following P$^3$ guidelines, and the optimized indexes outperform their message-passing-based and disaggregated-memory-based counterparts by up to 16$\\times$ and 19$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \\emph{Partial Cache-Coherence (PCC)} model maintains hardware cache coherence only within subsets of cores, enabling large-scale memory sharing with emerging memory interconnect technologies like Compute Express Link (CXL). However, PCC's relaxation of global cache coherence compromises the correctness of existing single-machine software.\n  This paper focuses on building consistent and efficient indexes on PCC platforms. We present that existing indexes designed for cache-coherent platforms can be made consistent on PCC platforms following SP guidelines, i.e., we identify \\emph{sync-data} and \\emph{protected-data} according to the index's concurrency control mechanisms, and synchronize them accordingly. However, conversion with SP guidelines introduces performance overhead. To mitigate the overhead, we identify several unique performance bottlenecks on PCC platforms, and propose P$^3$ guidelines (i.e., using Out-of-\\underline{P}lace update, Re\\underline{P}licated shared variable, S\\underline{P}eculative Reading) to improve the efficiency of converted indexes on PCC platforms.\n  With SP and P$^3$ guidelines, we convert and optimize two indexes (CLevelHash and BwTree) for PCC platforms. Evaluation shows that converted indexes' throughput improves up to 16$\\times$ following P$^3$ guidelines, and the optimized indexes outperform their message-passing-based and disaggregated-memory-based counterparts by up to 16$\\times$ and 19$\\times$."
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-09T16:55:00Z",
                "published_parsed": [
                    2025,
                    11,
                    9,
                    16,
                    55,
                    0,
                    6,
                    313,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS"
                },
                "authors": [
                    {
                        "name": "Fangnuo Wu"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Wenjun Cai"
                    },
                    {
                        "name": "Jingsheng Yan"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen"
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2511.17487v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17487v1",
                "title": "Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models"
                },
                "updated": "2025-11-21T18:43:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    18,
                    43,
                    1,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17487v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Scaling up multimodal models has enabled remarkable advances in visual understanding and reasoning, but practical demands call for smaller, efficient systems. In this work, we conduct a principled analysis of downscaling intelligence in multimodal models, examining how reduced large language model (LLM) capacity affects multimodal capabilities. Our initial findings reveal an interesting trend: LLM downscaling disproportionately affects visual capabilities, rather than abilities inherited from the LLM. We then examine whether this drop mainly reflects the expected decline in visual reasoning or a more fundamental loss of perceptual abilities. Isolating the effect of LLM downscaling on perception, we find performance still drops sharply, often matching or exceeding the impact on reasoning. To address this bottleneck, we introduce visual extraction tuning, which explicitly trains the model to extract instruction-relevant visual details consistently across tasks. With these extracted visual details, we then apply step-by-step reasoning to generate answers. Together, these components form our Extract+Think approach, setting a new standard for efficiency and performance in this space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling up multimodal models has enabled remarkable advances in visual understanding and reasoning, but practical demands call for smaller, efficient systems. In this work, we conduct a principled analysis of downscaling intelligence in multimodal models, examining how reduced large language model (LLM) capacity affects multimodal capabilities. Our initial findings reveal an interesting trend: LLM downscaling disproportionately affects visual capabilities, rather than abilities inherited from the LLM. We then examine whether this drop mainly reflects the expected decline in visual reasoning or a more fundamental loss of perceptual abilities. Isolating the effect of LLM downscaling on perception, we find performance still drops sharply, often matching or exceeding the impact on reasoning. To address this bottleneck, we introduce visual extraction tuning, which explicitly trains the model to extract instruction-relevant visual details consistently across tasks. With these extracted visual details, we then apply step-by-step reasoning to generate answers. Together, these components form our Extract+Think approach, setting a new standard for efficiency and performance in this space."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T18:43:01Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    18,
                    43,
                    1,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Website at https://web.stanford.edu/~markendo/projects/downscaling_intelligence",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Mark Endo"
                    },
                    {
                        "name": "Serena Yeung-Levy"
                    }
                ],
                "author_detail": {
                    "name": "Serena Yeung-Levy"
                },
                "author": "Serena Yeung-Levy"
            },
            {
                "id": "http://arxiv.org/abs/2509.09162v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.09162v2",
                "title": "Divide, Interact, Sample: The Two-System Paradigm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Divide, Interact, Sample: The Two-System Paradigm"
                },
                "updated": "2025-11-21T18:29:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    18,
                    29,
                    36,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.09162v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.09162v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Mean-field, ensemble-chain, and adaptive samplers have historically been viewed as distinct approaches to Monte Carlo sampling. In this paper, we present a unifying {two-system} framework that brings all three under one roof. In our approach, an ensemble of particles is split into two interacting subsystems that propose updates for each other in a symmetric, alternating fashion. This cross-system interaction ensures that the overall ensemble has $(x)$ as its invariant distribution in both the finite-particle setting and the mean-field limit. The two-system construction reveals that ensemble-chain samplers can be interpreted as finite-$N$ approximations of an ideal mean-field sampler; conversely, it provides a principled recipe to discretize mean-field Langevin dynamics into tractable parallel MCMC algorithms. The framework also connects naturally to adaptive single-chain methods: by replacing particle-based statistics with time-averaged statistics from a single chain, one recovers analogous adaptive dynamics in the long-time limit without requiring a large ensemble. We derive novel two-system versions of both overdamped and underdamped Langevin MCMC samplers within this paradigm. Across synthetic benchmarks and real-world posterior inference tasks, these two-system samplers exhibit significant performance gains over the popular No-U-Turn Sampler, achieving an order of magnitude higher effective sample sizes per gradient evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mean-field, ensemble-chain, and adaptive samplers have historically been viewed as distinct approaches to Monte Carlo sampling. In this paper, we present a unifying {two-system} framework that brings all three under one roof. In our approach, an ensemble of particles is split into two interacting subsystems that propose updates for each other in a symmetric, alternating fashion. This cross-system interaction ensures that the overall ensemble has $(x)$ as its invariant distribution in both the finite-particle setting and the mean-field limit. The two-system construction reveals that ensemble-chain samplers can be interpreted as finite-$N$ approximations of an ideal mean-field sampler; conversely, it provides a principled recipe to discretize mean-field Langevin dynamics into tractable parallel MCMC algorithms. The framework also connects naturally to adaptive single-chain methods: by replacing particle-based statistics with time-averaged statistics from a single chain, one recovers analogous adaptive dynamics in the long-time limit without requiring a large ensemble. We derive novel two-system versions of both overdamped and underdamped Langevin MCMC samplers within this paradigm. Across synthetic benchmarks and real-world posterior inference tasks, these two-system samplers exhibit significant performance gains over the popular No-U-Turn Sampler, achieving an order of magnitude higher effective sample sizes per gradient evaluation."
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-11T05:42:37Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    5,
                    42,
                    37,
                    3,
                    254,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO"
                },
                "authors": [
                    {
                        "name": "James Chok"
                    },
                    {
                        "name": "Myung Won Lee"
                    },
                    {
                        "name": "Daniel Paulin"
                    },
                    {
                        "name": "Geoffrey M. Vasil"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey M. Vasil"
                },
                "author": "Geoffrey M. Vasil"
            },
            {
                "id": "http://arxiv.org/abs/2511.17473v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17473v1",
                "title": "Masked-and-Reordered Self-Supervision for Reinforcement Learning from Verifiable Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked-and-Reordered Self-Supervision for Reinforcement Learning from Verifiable Rewards"
                },
                "updated": "2025-11-21T18:23:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    18,
                    23,
                    4,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17473v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Test-time scaling has been shown to substantially improve large language models' (LLMs) mathematical reasoning. However, for a large portion of mathematical corpora, especially theorem proving, RLVR's scalability is limited: intermediate reasoning is crucial, while final answers are difficult to directly and reliably verify. Meanwhile, token-level SFT often degenerates into rote memorization rather than inducing longer chains of thought. Inspired by BERT's self-supervised tasks, we propose MR-RLVR (Masked-and-Reordered RLVR), which constructs process-level self-supervised rewards via \"masked-then-fill\" and \"step reordering\" to extract learnable signals from intermediate reasoning. Our training pipeline comprises two stages: we first perform self-supervised training on sampled mathematical calculation and proof data; we then conduct RLVR fine-tuning on mathematical calculation datasets where only outcomes are verifiable. We implement MR-RLVR on Qwen2.5-3B and DeepSeek-R1-Distill-Qwen-1.5B, and evaluate on AIME24, AIME25, AMC23, and MATH500. Under a fixed sampling and decoding budget, MR-RLVR achieves average relative gains over the original RLVR of +9.86% Pass@1, +5.27% Pass@5, and +4.00% Pass@8. These results indicate that incorporating process-aware self-supervised signals can effectively enhance RLVR's scalability and performance in only outcome-verifiable settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling has been shown to substantially improve large language models' (LLMs) mathematical reasoning. However, for a large portion of mathematical corpora, especially theorem proving, RLVR's scalability is limited: intermediate reasoning is crucial, while final answers are difficult to directly and reliably verify. Meanwhile, token-level SFT often degenerates into rote memorization rather than inducing longer chains of thought. Inspired by BERT's self-supervised tasks, we propose MR-RLVR (Masked-and-Reordered RLVR), which constructs process-level self-supervised rewards via \"masked-then-fill\" and \"step reordering\" to extract learnable signals from intermediate reasoning. Our training pipeline comprises two stages: we first perform self-supervised training on sampled mathematical calculation and proof data; we then conduct RLVR fine-tuning on mathematical calculation datasets where only outcomes are verifiable. We implement MR-RLVR on Qwen2.5-3B and DeepSeek-R1-Distill-Qwen-1.5B, and evaluate on AIME24, AIME25, AMC23, and MATH500. Under a fixed sampling and decoding budget, MR-RLVR achieves average relative gains over the original RLVR of +9.86% Pass@1, +5.27% Pass@5, and +4.00% Pass@8. These results indicate that incorporating process-aware self-supervised signals can effectively enhance RLVR's scalability and performance in only outcome-verifiable settings."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T18:23:04Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    18,
                    23,
                    4,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Zhifeng Gao"
                    },
                    {
                        "name": "Guolin Ke"
                    }
                ],
                "author_detail": {
                    "name": "Guolin Ke"
                },
                "author": "Guolin Ke"
            },
            {
                "id": "http://arxiv.org/abs/2511.17467v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17467v1",
                "title": "PersonaAgent with GraphRAG: Community-Aware Knowledge Graphs for Personalized LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonaAgent with GraphRAG: Community-Aware Knowledge Graphs for Personalized LLM"
                },
                "updated": "2025-11-21T18:15:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    18,
                    15,
                    47,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17467v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We propose a novel framework for persona-based language model system, motivated by the need for personalized AI agents that adapt to individual user preferences. In our approach, the agent embodies the user's \"persona\" (e.g. user profile or taste) and is powered by a large language model (LLM). To enable the agent to leverage rich contextual information, we introduce a Knowledge-Graph-enhanced Retrieval-Augmented Generation (Graph RAG) mechanism that constructs an LLM-derived graph index of relevant documents and summarizes communities of related information. Our framework generates personalized prompts by combining: (1) a summary of the user's historical behaviors and preferences extracted from the knowledge graph, and (2) relevant global interaction patterns identified through graph-based community detection. This dynamic prompt engineering approach allows the agent to maintain consistent persona-aligned behaviors while benefiting from collective knowledge. On the LaMP benchmark, our method improves news categorization F1 by 11.1%, movie tagging F1 by 56.1%, and reduces product rating MAE by 10.4% over prior methods. Our code is available at https://anonymous.4open.science/r/PersonaAgentwGraphRAG-DE6F",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel framework for persona-based language model system, motivated by the need for personalized AI agents that adapt to individual user preferences. In our approach, the agent embodies the user's \"persona\" (e.g. user profile or taste) and is powered by a large language model (LLM). To enable the agent to leverage rich contextual information, we introduce a Knowledge-Graph-enhanced Retrieval-Augmented Generation (Graph RAG) mechanism that constructs an LLM-derived graph index of relevant documents and summarizes communities of related information. Our framework generates personalized prompts by combining: (1) a summary of the user's historical behaviors and preferences extracted from the knowledge graph, and (2) relevant global interaction patterns identified through graph-based community detection. This dynamic prompt engineering approach allows the agent to maintain consistent persona-aligned behaviors while benefiting from collective knowledge. On the LaMP benchmark, our method improves news categorization F1 by 11.1%, movie tagging F1 by 56.1%, and reduces product rating MAE by 10.4% over prior methods. Our code is available at https://anonymous.4open.science/r/PersonaAgentwGraphRAG-DE6F"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T18:15:47Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    18,
                    15,
                    47,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Siqi Liang"
                    },
                    {
                        "name": "Yudi Zhang"
                    },
                    {
                        "name": "Yue Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yue Guo"
                },
                "author": "Yue Guo"
            },
            {
                "id": "http://arxiv.org/abs/2508.00086v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.00086v2",
                "title": "Do LLMs produce texts with \"human-like\" lexical diversity?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs produce texts with \"human-like\" lexical diversity?"
                },
                "updated": "2025-11-21T18:13:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    18,
                    13,
                    28,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.00086v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.00086v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The degree to which large language models (LLMs) produce writing that is truly human-like remains unclear despite the extensive empirical attention that this question has received. The present study addresses this question from the perspective of lexical diversity. Specifically, the study investigates patterns of lexical diversity in LLM-generated texts from four ChatGPT models (ChatGPT-3.5, ChatGPT-4, ChatGPT-o4 mini, and ChatGPT-4.5) in comparison with texts written by L1 and L2 English participants (n = 240) across four education levels. Six dimensions of lexical diversity were measured in each text: volume, abundance, variety-repetition, evenness, disparity, and dispersion. Results from one-way MANOVAs, one-way ANOVAs, and Support Vector Machines revealed that the ChatGPT-generated texts differed significantly from human-written texts for each variable, with ChatGPT-o4 mini and ChatGPT-4.5 differing the most. Within these two groups, ChatGPT-4.5 demonstrated higher levels of lexical diversity than older models despite producing fewer tokens. The human writers' lexical diversity did not differ across subgroups (i.e., education, language status). Altogether, the results indicate that ChatGPT models do not produce human-like texts in relation to lexical diversity, and the newer models produce less human-like text than older models. We discuss the implications of these results for language pedagogy and related applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The degree to which large language models (LLMs) produce writing that is truly human-like remains unclear despite the extensive empirical attention that this question has received. The present study addresses this question from the perspective of lexical diversity. Specifically, the study investigates patterns of lexical diversity in LLM-generated texts from four ChatGPT models (ChatGPT-3.5, ChatGPT-4, ChatGPT-o4 mini, and ChatGPT-4.5) in comparison with texts written by L1 and L2 English participants (n = 240) across four education levels. Six dimensions of lexical diversity were measured in each text: volume, abundance, variety-repetition, evenness, disparity, and dispersion. Results from one-way MANOVAs, one-way ANOVAs, and Support Vector Machines revealed that the ChatGPT-generated texts differed significantly from human-written texts for each variable, with ChatGPT-o4 mini and ChatGPT-4.5 differing the most. Within these two groups, ChatGPT-4.5 demonstrated higher levels of lexical diversity than older models despite producing fewer tokens. The human writers' lexical diversity did not differ across subgroups (i.e., education, language status). Altogether, the results indicate that ChatGPT models do not produce human-like texts in relation to lexical diversity, and the newer models produce less human-like text than older models. We discuss the implications of these results for language pedagogy and related applications."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-31T18:22:11Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    18,
                    22,
                    11,
                    3,
                    212,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Kelly Kendro"
                    },
                    {
                        "name": "Jeffrey Maloney"
                    },
                    {
                        "name": "Scott Jarvis"
                    }
                ],
                "author_detail": {
                    "name": "Scott Jarvis"
                },
                "author": "Scott Jarvis"
            },
            {
                "id": "http://arxiv.org/abs/2508.12315v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.12315v3",
                "title": "Deciphering the global production network from cross-border firm transactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deciphering the global production network from cross-border firm transactions"
                },
                "updated": "2025-11-21T17:58:00Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    58,
                    0,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.12315v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.12315v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Critical for policy-making and business operations, the study of global supply chains has been severely hampered by a lack of detailed data. Here we harness international firm-level transaction data covering 20m global firms, and 1 billion cross-border transactions, to infer key inputs for over 1200 products. Transforming this data to a directed network, we find that products are clustered into three large groups including textiles, chemicals and food, and machinery and metals. European industrial nations and China dominate critical intermediate products such as metals, common components and tools, while industrial complexity is highly correlated with embeddedness in densely connected supply chains. We find structural similarities with AIPNET, a product network generated via LLM queries, and strong linkages between products identified in manually-mapped electric vehicle battery and semiconductor supply chains. Finally, both forward and backward linkages are predictive of country-product diversification patterns, with stronger overall evidence for backward (upstream) linkages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critical for policy-making and business operations, the study of global supply chains has been severely hampered by a lack of detailed data. Here we harness international firm-level transaction data covering 20m global firms, and 1 billion cross-border transactions, to infer key inputs for over 1200 products. Transforming this data to a directed network, we find that products are clustered into three large groups including textiles, chemicals and food, and machinery and metals. European industrial nations and China dominate critical intermediate products such as metals, common components and tools, while industrial complexity is highly correlated with embeddedness in densely connected supply chains. We find structural similarities with AIPNET, a product network generated via LLM queries, and strong linkages between products identified in manually-mapped electric vehicle battery and semiconductor supply chains. Finally, both forward and backward linkages are predictive of country-product diversification patterns, with stronger overall evidence for backward (upstream) linkages."
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-17T10:17:23Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    10,
                    17,
                    23,
                    6,
                    229,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN"
                },
                "authors": [
                    {
                        "name": "Neave O'Clery"
                    },
                    {
                        "name": "Ben Radcliffe-Brown"
                    },
                    {
                        "name": "Thomas Spencer"
                    },
                    {
                        "name": "Daniel Tarling-Hunter"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Tarling-Hunter"
                },
                "author": "Daniel Tarling-Hunter"
            },
            {
                "id": "http://arxiv.org/abs/2511.17454v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17454v1",
                "title": "Illustrator's Depth: Monocular Layer Index Prediction for Image Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Illustrator's Depth: Monocular Layer Index Prediction for Image Decomposition"
                },
                "updated": "2025-11-21T17:56:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    56,
                    43,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17454v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce Illustrator's Depth, a novel definition of depth that addresses a key challenge in digital content creation: decomposing flat images into editable, ordered layers. Inspired by an artist's compositional process, illustrator's depth infers a layer index to each pixel, forming an interpretable image decomposition through a discrete, globally consistent ordering of elements optimized for editability. We also propose and train a neural network using a curated dataset of layered vector graphics to predict layering directly from raster inputs. Our layer index inference unlocks a range of powerful downstream applications. In particular, it significantly outperforms state-of-the-art baselines for image vectorization while also enabling high-fidelity text-to-vector-graphics generation, automatic 3D relief generation from 2D images, and intuitive depth-aware editing. By reframing depth from a physical quantity to a creative abstraction, illustrator's depth prediction offers a new foundation for editable image decomposition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Illustrator's Depth, a novel definition of depth that addresses a key challenge in digital content creation: decomposing flat images into editable, ordered layers. Inspired by an artist's compositional process, illustrator's depth infers a layer index to each pixel, forming an interpretable image decomposition through a discrete, globally consistent ordering of elements optimized for editability. We also propose and train a neural network using a curated dataset of layered vector graphics to predict layering directly from raster inputs. Our layer index inference unlocks a range of powerful downstream applications. In particular, it significantly outperforms state-of-the-art baselines for image vectorization while also enabling high-fidelity text-to-vector-graphics generation, automatic 3D relief generation from 2D images, and intuitive depth-aware editing. By reframing depth from a physical quantity to a creative abstraction, illustrator's depth prediction offers a new foundation for editable image decomposition."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T17:56:43Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    56,
                    43,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Nissim Maruani"
                    },
                    {
                        "name": "Peiying Zhang"
                    },
                    {
                        "name": "Siddhartha Chaudhuri"
                    },
                    {
                        "name": "Matthew Fisher"
                    },
                    {
                        "name": "Nanxuan Zhao"
                    },
                    {
                        "name": "Vladimir G. Kim"
                    },
                    {
                        "name": "Pierre Alliez"
                    },
                    {
                        "name": "Mathieu Desbrun"
                    },
                    {
                        "name": "Wang Yifan"
                    }
                ],
                "author_detail": {
                    "name": "Wang Yifan"
                },
                "author": "Wang Yifan"
            },
            {
                "id": "http://arxiv.org/abs/2406.03283v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2406.03283v2",
                "title": "CATCODER: Repository-Level Code Generation with Relevant Code and Type Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CATCODER: Repository-Level Code Generation with Relevant Code and Type Context"
                },
                "updated": "2025-11-21T17:41:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    41,
                    45,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2406.03283v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2406.03283v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, repository-level code generation presents unique challenges, particularly due to the need to utilize information spread across multiple files within a repository. Specifically, successful generation depends on a solid grasp of both general, context-agnostic knowledge and specific, context-dependent knowledge. While LLMs are widely used for the context-agnostic aspect, existing retrieval-based approaches sometimes fall short as they are limited in obtaining a broader and deeper repository context. In this paper, we present CatCoder, a novel code generation framework designed for statically typed programming languages. CatCoder enhances repository-level code generation by integrating relevant code and type context. Specifically, it leverages static analyzers to extract type dependencies and merges this information with retrieved code to create comprehensive prompts for LLMs. To evaluate the effectiveness of CatCoder, we adapt and construct benchmarks that include 199 Java tasks and 90 Rust tasks. The results show that CatCoder outperforms the RepoCoder baseline by up to 14.44% and 17.35%, in terms of compile@k and pass@k scores. In addition, the generalizability of CatCoder is assessed using various LLMs, including both code-specialized models and general-purpose models. Our findings indicate consistent performance improvements across all models, which underlines the practicality of CatCoder. Furthermore, we evaluate the time consumption of CatCoder in a large open source repository, and the results demonstrate the scalability of CatCoder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, repository-level code generation presents unique challenges, particularly due to the need to utilize information spread across multiple files within a repository. Specifically, successful generation depends on a solid grasp of both general, context-agnostic knowledge and specific, context-dependent knowledge. While LLMs are widely used for the context-agnostic aspect, existing retrieval-based approaches sometimes fall short as they are limited in obtaining a broader and deeper repository context. In this paper, we present CatCoder, a novel code generation framework designed for statically typed programming languages. CatCoder enhances repository-level code generation by integrating relevant code and type context. Specifically, it leverages static analyzers to extract type dependencies and merges this information with retrieved code to create comprehensive prompts for LLMs. To evaluate the effectiveness of CatCoder, we adapt and construct benchmarks that include 199 Java tasks and 90 Rust tasks. The results show that CatCoder outperforms the RepoCoder baseline by up to 14.44% and 17.35%, in terms of compile@k and pass@k scores. In addition, the generalizability of CatCoder is assessed using various LLMs, including both code-specialized models and general-purpose models. Our findings indicate consistent performance improvements across all models, which underlines the practicality of CatCoder. Furthermore, we evaluate the time consumption of CatCoder in a large open source repository, and the results demonstrate the scalability of CatCoder."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-06-05T13:56:42Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    13,
                    56,
                    42,
                    2,
                    157,
                    0
                ],
                "arxiv_comment": "Revised and extended version; To be published in ACM Transactions on Software Engineering and Methodology",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Zhiyuan Pan"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Xiaohu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohu Yang"
                },
                "author": "Xiaohu Yang"
            },
            {
                "id": "http://arxiv.org/abs/2511.17442v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17442v1",
                "title": "REMSA: An LLM Agent for Foundation Model Selection in Remote Sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REMSA: An LLM Agent for Foundation Model Selection in Remote Sensing"
                },
                "updated": "2025-11-21T17:41:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    41,
                    26,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17442v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Foundation Models (FMs) are increasingly used in remote sensing (RS) for tasks such as environmental monitoring, disaster assessment, and land-use mapping. These models include unimodal vision encoders trained on a single data modality and multimodal architectures trained on combinations of SAR, multispectral, hyperspectral, and image-text data. They support diverse RS tasks including semantic segmentation, image classification, change detection, and visual question answering. However, selecting an appropriate remote sensing foundation model (RSFM) remains difficult due to scattered documentation, heterogeneous formats, and varied deployment constraints. We introduce the RSFM Database (RS-FMD), a structured resource covering over 150 RSFMs spanning multiple data modalities, resolutions, and learning paradigms. Built on RS-FMD, we present REMSA, the first LLM-based agent for automated RSFM selection from natural language queries. REMSA interprets user requirements, resolves missing constraints, ranks candidate models using in-context learning, and provides transparent justifications. We also propose a benchmark of 75 expert-verified RS query scenarios, producing 900 configurations under an expert-centered evaluation protocol. REMSA outperforms several baselines, including naive agents, dense retrieval, and unstructured RAG-based LLMs. It operates entirely on publicly available metadata and does not access private or sensitive data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Models (FMs) are increasingly used in remote sensing (RS) for tasks such as environmental monitoring, disaster assessment, and land-use mapping. These models include unimodal vision encoders trained on a single data modality and multimodal architectures trained on combinations of SAR, multispectral, hyperspectral, and image-text data. They support diverse RS tasks including semantic segmentation, image classification, change detection, and visual question answering. However, selecting an appropriate remote sensing foundation model (RSFM) remains difficult due to scattered documentation, heterogeneous formats, and varied deployment constraints. We introduce the RSFM Database (RS-FMD), a structured resource covering over 150 RSFMs spanning multiple data modalities, resolutions, and learning paradigms. Built on RS-FMD, we present REMSA, the first LLM-based agent for automated RSFM selection from natural language queries. REMSA interprets user requirements, resolves missing constraints, ranks candidate models using in-context learning, and provides transparent justifications. We also propose a benchmark of 75 expert-verified RS query scenarios, producing 900 configurations under an expert-centered evaluation protocol. REMSA outperforms several baselines, including naive agents, dense retrieval, and unstructured RAG-based LLMs. It operates entirely on publicly available metadata and does not access private or sensitive data."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T17:41:26Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    41,
                    26,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Code and data available at https://github.com/be-chen/REMSA",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Binger Chen"
                    },
                    {
                        "name": "Tacettin Emre Bk"
                    },
                    {
                        "name": "Behnood Rasti"
                    },
                    {
                        "name": "Volker Markl"
                    },
                    {
                        "name": "Begm Demir"
                    }
                ],
                "author_detail": {
                    "name": "Begm Demir"
                },
                "author": "Begm Demir"
            },
            {
                "id": "http://arxiv.org/abs/2511.17438v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17438v1",
                "title": "Iterating marginalized Bayes maps for likelihood maximization with application to nonlinear panel models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterating marginalized Bayes maps for likelihood maximization with application to nonlinear panel models"
                },
                "updated": "2025-11-21T17:35:27Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    35,
                    27,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17438v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Complex dynamic systems can be investigated by fitting mechanistic stochastic dynamic models to time series data. In this context, commonly used Monte Carlo inference procedures for model selection and parameter estimation quickly become computationally unfeasible as the system dimension grows. The increasing prevalence of panel data, characterized by multiple related time series, therefore necessitates the development of inference algorithms that are effective for this class of high-dimensional mechanistic models. Nonlinear, non-Gaussian mechanistic models are routinely fitted to time series data but seldom to panel data, despite its widespread availability, suggesting that the practical difficulties for existing procedures are prohibitive. We investigate the use of iterated filtering algorithms for this purpose. We introduce a novel algorithm that contains a marginalization step that mitigates issues arising from particle filtering in high dimensions. Our approach enables likelihood-based inference for models that were previously considered intractable, thus broadening the scope of dynamic models available for panel data analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex dynamic systems can be investigated by fitting mechanistic stochastic dynamic models to time series data. In this context, commonly used Monte Carlo inference procedures for model selection and parameter estimation quickly become computationally unfeasible as the system dimension grows. The increasing prevalence of panel data, characterized by multiple related time series, therefore necessitates the development of inference algorithms that are effective for this class of high-dimensional mechanistic models. Nonlinear, non-Gaussian mechanistic models are routinely fitted to time series data but seldom to panel data, despite its widespread availability, suggesting that the practical difficulties for existing procedures are prohibitive. We investigate the use of iterated filtering algorithms for this purpose. We introduce a novel algorithm that contains a marginalization step that mitigates issues arising from particle filtering in high dimensions. Our approach enables likelihood-based inference for models that were previously considered intractable, thus broadening the scope of dynamic models available for panel data analysis."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T17:35:27Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    35,
                    27,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Jesse Wheeler"
                    },
                    {
                        "name": "Aaron J. Abkemeier"
                    },
                    {
                        "name": "Edward L. Ionides"
                    }
                ],
                "author_detail": {
                    "name": "Edward L. Ionides"
                },
                "author": "Edward L. Ionides"
            },
            {
                "id": "http://arxiv.org/abs/2511.17432v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17432v1",
                "title": "SMILE: A Composite Lexical-Semantic Metric for Question-Answering Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMILE: A Composite Lexical-Semantic Metric for Question-Answering Evaluation"
                },
                "updated": "2025-11-21T17:30:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    30,
                    18,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17432v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Traditional evaluation metrics for textual and visual question answering, like ROUGE, METEOR, and Exact Match (EM), focus heavily on n-gram based lexical similarity, often missing the deeper semantic understanding needed for accurate assessment. While measures like BERTScore and MoverScore leverage contextual embeddings to address this limitation, they lack flexibility in balancing sentence-level and keyword-level semantics and ignore lexical similarity, which remains important. Large Language Model (LLM) based evaluators, though powerful, come with drawbacks like high costs, bias, inconsistency, and hallucinations. To address these issues, we introduce SMILE: Semantic Metric Integrating Lexical Exactness, a novel approach that combines sentence-level semantic understanding with keyword-level semantic understanding and easy keyword matching. This composite method balances lexical precision and semantic relevance, offering a comprehensive evaluation. Extensive benchmarks across text, image, and video QA tasks show SMILE is highly correlated with human judgments and computationally lightweight, bridging the gap between lexical and semantic evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional evaluation metrics for textual and visual question answering, like ROUGE, METEOR, and Exact Match (EM), focus heavily on n-gram based lexical similarity, often missing the deeper semantic understanding needed for accurate assessment. While measures like BERTScore and MoverScore leverage contextual embeddings to address this limitation, they lack flexibility in balancing sentence-level and keyword-level semantics and ignore lexical similarity, which remains important. Large Language Model (LLM) based evaluators, though powerful, come with drawbacks like high costs, bias, inconsistency, and hallucinations. To address these issues, we introduce SMILE: Semantic Metric Integrating Lexical Exactness, a novel approach that combines sentence-level semantic understanding with keyword-level semantic understanding and easy keyword matching. This composite method balances lexical precision and semantic relevance, offering a comprehensive evaluation. Extensive benchmarks across text, image, and video QA tasks show SMILE is highly correlated with human judgments and computationally lightweight, bridging the gap between lexical and semantic evaluation."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T17:30:18Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    30,
                    18,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "23 pages, 6 tables, 9 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Shrikant Kendre"
                    },
                    {
                        "name": "Austin Xu"
                    },
                    {
                        "name": "Honglu Zhou"
                    },
                    {
                        "name": "Michael Ryoo"
                    },
                    {
                        "name": "Shafiq Joty"
                    },
                    {
                        "name": "Juan Carlos Niebles"
                    }
                ],
                "author_detail": {
                    "name": "Juan Carlos Niebles"
                },
                "author": "Juan Carlos Niebles"
            },
            {
                "id": "http://arxiv.org/abs/2511.13646v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13646v2",
                "title": "Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?"
                },
                "updated": "2025-11-21T17:22:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    22,
                    32,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13646v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-Gdel Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that LIVE-SWE-AGENT can achieve an impressive solve rate of 77.4% without test-time scaling, outperforming all existing software agents, including the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-Gdel Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that LIVE-SWE-AGENT can achieve an impressive solve rate of 77.4% without test-time scaling, outperforming all existing software agents, including the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T17:58:18Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    58,
                    18,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Chunqiu Steven Xia"
                    },
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Yan Yang"
                    },
                    {
                        "name": "Yuxiang Wei"
                    },
                    {
                        "name": "Lingming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lingming Zhang"
                },
                "author": "Lingming Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.17418v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17418v1",
                "title": "MemIntelli: A Generic End-to-End Simulation Framework for Memristive Intelligent Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemIntelli: A Generic End-to-End Simulation Framework for Memristive Intelligent Computing"
                },
                "updated": "2025-11-21T17:17:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    17,
                    12,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17418v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Memristive in-memory computing (IMC) has emerged as a promising solution for addressing the bottleneck in the Von Neumann architecture. However, the couplingbetweenthecircuitandalgorithm in IMC makes computing reliability susceptible to non-ideal effects in devices and peripheral circuits. In this respect, efficient softwarehardwareco-simulationtoolsarehighlydesiredtoembedthedevice and circuit models into the algorithms. In this paper, for the first time, we proposed an end-to-end simulation framework supporting flexible variable-precision computing, named MemIntelli, to realize the pre-verification of diverse intelligent applications on memristive devices. At the device and circuit level, mathematical functions are employed to abstract the devices and circuits through meticulous equivalent circuit modeling. On the architecture level, MemIntelli achieves flexible variable-precision IMC supporting integer and floating data representation with bit-slicing. Moreover, MemIntelli is compatible with NumPy and PyTorch for seamless integration with applications. To demonstrate its capabilities, diverse intelligent algorithms, such as equation solving, data clustering, wavelet transformation, and neural network training and inference, were employed to showcase the robust processing ability of MemIntelli. This research presents a comprehensive simulation tool that facilitates the co-design of the IMC system, spanning from device to application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memristive in-memory computing (IMC) has emerged as a promising solution for addressing the bottleneck in the Von Neumann architecture. However, the couplingbetweenthecircuitandalgorithm in IMC makes computing reliability susceptible to non-ideal effects in devices and peripheral circuits. In this respect, efficient softwarehardwareco-simulationtoolsarehighlydesiredtoembedthedevice and circuit models into the algorithms. In this paper, for the first time, we proposed an end-to-end simulation framework supporting flexible variable-precision computing, named MemIntelli, to realize the pre-verification of diverse intelligent applications on memristive devices. At the device and circuit level, mathematical functions are employed to abstract the devices and circuits through meticulous equivalent circuit modeling. On the architecture level, MemIntelli achieves flexible variable-precision IMC supporting integer and floating data representation with bit-slicing. Moreover, MemIntelli is compatible with NumPy and PyTorch for seamless integration with applications. To demonstrate its capabilities, diverse intelligent algorithms, such as equation solving, data clustering, wavelet transformation, and neural network training and inference, were employed to showcase the robust processing ability of MemIntelli. This research presents a comprehensive simulation tool that facilitates the co-design of the IMC system, spanning from device to application."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T17:17:12Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    17,
                    12,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Houji Zhou"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Zhiwei Zhou"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Xiangshui Miao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangshui Miao"
                },
                "author": "Xiangshui Miao"
            },
            {
                "id": "http://arxiv.org/abs/2511.17415v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17415v1",
                "title": "Bayesian Bridge Gaussian Process Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Bridge Gaussian Process Regression"
                },
                "updated": "2025-11-21T17:13:23Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    13,
                    23,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17415v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The performance of Gaussian Process (GP) regression is often hampered by the curse of dimensionality, which inflates computational cost and reduces predictive power in high-dimensional problems. Variable selection is thus crucial for building efficient and accurate GP models. Inspired by Bayesian bridge regression, we propose the Bayesian Bridge Gaussian Process Regression (B\\textsuperscript{2}GPR) model. This framework places $\\ell_q$-norm constraints on key GP parameters to automatically induce sparsity and identify active variables. We formulate two distinct versions: one for $q=2$ using conjugate Gaussian priors, and another for $0<q<2$ that employs constrained flat priors, leading to non-standard, norm-constrained posterior distributions. To enable posterior inference, we design a Gibbs sampling algorithm that integrates Spherical Hamiltonian Monte Carlo (SphHMC) to efficiently sample from the constrained posteriors when $0<q<2$. Simulations and a real-data application confirm that B\\textsuperscript{2}GPR offers superior variable selection and prediction compared to alternative approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of Gaussian Process (GP) regression is often hampered by the curse of dimensionality, which inflates computational cost and reduces predictive power in high-dimensional problems. Variable selection is thus crucial for building efficient and accurate GP models. Inspired by Bayesian bridge regression, we propose the Bayesian Bridge Gaussian Process Regression (B\\textsuperscript{2}GPR) model. This framework places $\\ell_q$-norm constraints on key GP parameters to automatically induce sparsity and identify active variables. We formulate two distinct versions: one for $q=2$ using conjugate Gaussian priors, and another for $0<q<2$ that employs constrained flat priors, leading to non-standard, norm-constrained posterior distributions. To enable posterior inference, we design a Gibbs sampling algorithm that integrates Spherical Hamiltonian Monte Carlo (SphHMC) to efficiently sample from the constrained posteriors when $0<q<2$. Simulations and a real-data application confirm that B\\textsuperscript{2}GPR offers superior variable selection and prediction compared to alternative approaches."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T17:13:23Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    13,
                    23,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Minshen Xu"
                    },
                    {
                        "name": "Shiwei Lan"
                    },
                    {
                        "name": "Lulu Kang"
                    }
                ],
                "author_detail": {
                    "name": "Lulu Kang"
                },
                "author": "Lulu Kang"
            },
            {
                "id": "http://arxiv.org/abs/2511.17413v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17413v1",
                "title": "Quasar clustering and duty cycle measurements at $0\\leq z\\leq 4$ with the Gaia-unWISE Catalog",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quasar clustering and duty cycle measurements at $0\\leq z\\leq 4$ with the Gaia-unWISE Catalog"
                },
                "updated": "2025-11-21T17:11:55Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    11,
                    55,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17413v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We measure the two-point correlation function of a uniformly selected, all-sky sample of $\\sim$1.3 million quasars with magnitudes $G\\leq20.5$ from the Gaia--unWISE Quasar Catalog (Quaia) over the redshift range $0 \\leq z \\leq 4$ to trace the evolution of the quasar clustering strength across cosmic time. We find a steady increase in the correlation length $r_0$ with redshift, i.e. $r_0 = 6.8 \\pm 0.2\\,h^{-1}\\mathrm{Mpc}$ at $0 \\leq z < 1$, $r_0=8.0 \\pm 0.2\\,h^{-1}\\mathrm{Mpc}$ at $1 \\leq z < 2$, $r_0=10.8 \\pm 0.2\\,h^{-1}\\mathrm{Mpc}$ at $2 \\leq z < 3$, and $r_0=13.9 \\pm 1.2\\,h^{-1}\\mathrm{Mpc}$ at $3 \\leq z < 4$, and slopes consistent with $\\approx 2$. Our measurements suggest a slightly weaker clustering signal at $z>3$ than previous studies, and thus we find a smooth, monotonic rise in clustering strength. Using a bias-halo mass relation and a step-function for the halo occupation distribution, we infer characteristic minimum halo masses of quasar hosts of $\\log_{10}(M_{\\mathrm{min}}/M_\\odot) \\approx 12.8$ across all redshifts. Combining these with the observed quasar number densities yields duty cycles that rise from $f_{\\mathrm{duty}} \\approx 2\\%$ to $\\approx 7\\%$ with increasing redshift, corresponding to integrated quasar lifetimes of $t_{\\rm QSO}\\sim10^8$~years. These results suggest that both the characteristic halo mass of active quasars and their typical lifetimes have remained remarkably stable over more than $12 \\sim$ Gyr of cosmic time, implying a self-regulated growth process largely independent of epoch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We measure the two-point correlation function of a uniformly selected, all-sky sample of $\\sim$1.3 million quasars with magnitudes $G\\leq20.5$ from the Gaia--unWISE Quasar Catalog (Quaia) over the redshift range $0 \\leq z \\leq 4$ to trace the evolution of the quasar clustering strength across cosmic time. We find a steady increase in the correlation length $r_0$ with redshift, i.e. $r_0 = 6.8 \\pm 0.2\\,h^{-1}\\mathrm{Mpc}$ at $0 \\leq z < 1$, $r_0=8.0 \\pm 0.2\\,h^{-1}\\mathrm{Mpc}$ at $1 \\leq z < 2$, $r_0=10.8 \\pm 0.2\\,h^{-1}\\mathrm{Mpc}$ at $2 \\leq z < 3$, and $r_0=13.9 \\pm 1.2\\,h^{-1}\\mathrm{Mpc}$ at $3 \\leq z < 4$, and slopes consistent with $\\approx 2$. Our measurements suggest a slightly weaker clustering signal at $z>3$ than previous studies, and thus we find a smooth, monotonic rise in clustering strength. Using a bias-halo mass relation and a step-function for the halo occupation distribution, we infer characteristic minimum halo masses of quasar hosts of $\\log_{10}(M_{\\mathrm{min}}/M_\\odot) \\approx 12.8$ across all redshifts. Combining these with the observed quasar number densities yields duty cycles that rise from $f_{\\mathrm{duty}} \\approx 2\\%$ to $\\approx 7\\%$ with increasing redshift, corresponding to integrated quasar lifetimes of $t_{\\rm QSO}\\sim10^8$~years. These results suggest that both the characteristic halo mass of active quasars and their typical lifetimes have remained remarkably stable over more than $12 \\sim$ Gyr of cosmic time, implying a self-regulated growth process largely independent of epoch."
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T17:11:55Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    11,
                    55,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "11 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA"
                },
                "authors": [
                    {
                        "name": "Mariona Giner Mascarell"
                    },
                    {
                        "name": "Anna-Christina Eilers"
                    },
                    {
                        "name": "Kate Storey-Fisher"
                    }
                ],
                "author_detail": {
                    "name": "Kate Storey-Fisher"
                },
                "author": "Kate Storey-Fisher"
            },
            {
                "id": "http://arxiv.org/abs/2511.17411v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17411v1",
                "title": "SPEAR-1: Scaling Beyond Robot Demonstrations via 3D Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPEAR-1: Scaling Beyond Robot Demonstrations via 3D Understanding"
                },
                "updated": "2025-11-21T17:09:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    9,
                    43,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17411v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Robotic Foundation Models (RFMs) hold great promise as generalist, end-to-end systems for robot control. Yet their ability to generalize across new environments, tasks, and embodiments remains limited. We argue that a major bottleneck lies in their foundations: most RFMs are built by fine-tuning internet-pretrained Vision-Language Models (VLMs). However, these VLMs are trained on 2D image-language tasks and lack the 3D spatial reasoning inherently required for embodied control in the 3D world. Bridging this gap directly with large-scale robotic data is costly and difficult to scale. Instead, we propose to enrich easy-to-collect non-robotic image data with 3D annotations and enhance a pretrained VLM with 3D understanding capabilities. Following this strategy, we train SPEAR-VLM, a 3D-aware VLM that infers object coordinates in 3D space from a single 2D image. Building on SPEAR-VLM, we introduce our main contribution, $~\\textbf{SPEAR-1}$: a robotic foundation model that integrates grounded 3D perception with language-instructed embodied control. Trained on $\\sim$45M frames from 24 Open X-Embodiment datasets, SPEAR-1 outperforms or matches state-of-the-art models such as $_0$-FAST and $_{0.5}$, while it uses 20$\\times$ fewer robot demonstrations. This carefully-engineered training strategy unlocks new VLM capabilities and as a consequence boosts the reliability of embodied control beyond what is achievable with only robotic data. We make our model weights and 3D-annotated datasets publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic Foundation Models (RFMs) hold great promise as generalist, end-to-end systems for robot control. Yet their ability to generalize across new environments, tasks, and embodiments remains limited. We argue that a major bottleneck lies in their foundations: most RFMs are built by fine-tuning internet-pretrained Vision-Language Models (VLMs). However, these VLMs are trained on 2D image-language tasks and lack the 3D spatial reasoning inherently required for embodied control in the 3D world. Bridging this gap directly with large-scale robotic data is costly and difficult to scale. Instead, we propose to enrich easy-to-collect non-robotic image data with 3D annotations and enhance a pretrained VLM with 3D understanding capabilities. Following this strategy, we train SPEAR-VLM, a 3D-aware VLM that infers object coordinates in 3D space from a single 2D image. Building on SPEAR-VLM, we introduce our main contribution, $~\\textbf{SPEAR-1}$: a robotic foundation model that integrates grounded 3D perception with language-instructed embodied control. Trained on $\\sim$45M frames from 24 Open X-Embodiment datasets, SPEAR-1 outperforms or matches state-of-the-art models such as $_0$-FAST and $_{0.5}$, while it uses 20$\\times$ fewer robot demonstrations. This carefully-engineered training strategy unlocks new VLM capabilities and as a consequence boosts the reliability of embodied control beyond what is achievable with only robotic data. We make our model weights and 3D-annotated datasets publicly available."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T17:09:43Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    9,
                    43,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Nikolay Nikolov"
                    },
                    {
                        "name": "Giuliano Albanese"
                    },
                    {
                        "name": "Sombit Dey"
                    },
                    {
                        "name": "Aleksandar Yanev"
                    },
                    {
                        "name": "Luc Van Gool"
                    },
                    {
                        "name": "Jan-Nico Zaech"
                    },
                    {
                        "name": "Danda Pani Paudel"
                    }
                ],
                "author_detail": {
                    "name": "Danda Pani Paudel"
                },
                "author": "Danda Pani Paudel"
            },
            {
                "id": "http://arxiv.org/abs/2511.17408v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17408v1",
                "title": "That's not natural: The Impact of Off-Policy Training Data on Probe Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "That's not natural: The Impact of Off-Policy Training Data on Probe Performance"
                },
                "updated": "2025-11-21T17:08:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    8,
                    48,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17408v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Probing has emerged as a promising method for monitoring Large Language Models (LLMs), enabling inference-time detection of concerning behaviours such as deception and sycophancy. However, natural examples of many behaviours are rare, forcing researchers to rely on synthetic or off-policy LLM responses for training probes. We systematically evaluate how the use of synthetic and off-policy data influences probe generalisation across eight distinct LLM behaviours. Testing linear and attention probes across multiple LLMs, we find that the response generation strategy can significantly affect probe performance, though the magnitude of this effect varies by behaviour. We find that successful generalisation from off-policy data, to test sets where the model is incentivised to produce the target behaviour, is predictive of successful on-policy generalisation. Leveraging this result, we predict that Deception and Sandbagging probes may fail to generalise from off-policy to on-policy data when used in real monitoring scenarios. Notably, shifts in the training data domain still cause even larger performance degradation, with different-domain test scores being consistently lower than the same-domain ones. These results indicate that, in the absence of on-policy data, using same-domain off-policy data yields more reliable probes than using on-policy data from a different domain, emphasizing the need for methods that can better handle distribution shifts in LLM monitoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing has emerged as a promising method for monitoring Large Language Models (LLMs), enabling inference-time detection of concerning behaviours such as deception and sycophancy. However, natural examples of many behaviours are rare, forcing researchers to rely on synthetic or off-policy LLM responses for training probes. We systematically evaluate how the use of synthetic and off-policy data influences probe generalisation across eight distinct LLM behaviours. Testing linear and attention probes across multiple LLMs, we find that the response generation strategy can significantly affect probe performance, though the magnitude of this effect varies by behaviour. We find that successful generalisation from off-policy data, to test sets where the model is incentivised to produce the target behaviour, is predictive of successful on-policy generalisation. Leveraging this result, we predict that Deception and Sandbagging probes may fail to generalise from off-policy to on-policy data when used in real monitoring scenarios. Notably, shifts in the training data domain still cause even larger performance degradation, with different-domain test scores being consistently lower than the same-domain ones. These results indicate that, in the absence of on-policy data, using same-domain off-policy data yields more reliable probes than using on-policy data from a different domain, emphasizing the need for methods that can better handle distribution shifts in LLM monitoring."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T17:08:48Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    8,
                    48,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "10 pages, EurIPS 2025 Workshop on Private AI Governance",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Nathalie Kirch"
                    },
                    {
                        "name": "Samuel Dower"
                    },
                    {
                        "name": "Adrians Skapars"
                    },
                    {
                        "name": "Ekdeep Singh Lubana"
                    },
                    {
                        "name": "Dmitrii Krasheninnikov"
                    }
                ],
                "author_detail": {
                    "name": "Dmitrii Krasheninnikov"
                },
                "author": "Dmitrii Krasheninnikov"
            },
            {
                "id": "http://arxiv.org/abs/2511.11910v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.11910v2",
                "title": "Seeing the Forest and the Trees: Query-Aware Tokenizer for Long-Video Multimodal Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seeing the Forest and the Trees: Query-Aware Tokenizer for Long-Video Multimodal Language Models"
                },
                "updated": "2025-11-21T17:08:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    8,
                    33,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.11910v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.11910v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Despite the recent advances in the video understanding ability of multimodal large language models (MLLMs), long video understanding remains a challenge. One of the main issues is that the number of vision tokens grows linearly with video length, which causes an explosion in attention cost, memory, and latency. To solve this challenge, we present Query-aware Token Selector (\\textbf{QTSplus}), a lightweight yet powerful visual token selection module that serves as an information gate between the vision encoder and LLMs. Given a text query and video tokens, QTSplus dynamically selects the most important visual evidence for the input text query by (i) scoring visual tokens via cross-attention, (ii) \\emph{predicting} an instance-specific retention budget based on the complexity of the query, and (iii) \\emph{selecting} Top-$n$ tokens with a differentiable straight-through estimator during training and a hard gate at inference. Furthermore, a small re-encoder preserves temporal order using absolute time information, enabling second-level localization while maintaining global coverage.\n  Integrated into Qwen2.5-VL, QTSplus compresses the vision stream by up to \\textbf{89\\%} and reduces end-to-end latency by \\textbf{28\\%} on long videos. The evaluation on eight long video understanding benchmarks shows near-parity accuracy overall when compared with the original Qwen models and outperforms the original model by \\textbf{+20.5} and \\textbf{+5.6} points respectively on TempCompass direction and order accuracies. These results show that QTSplus is an effective, general mechanism for scaling MLLMs to real-world long-video scenarios while preserving task-relevant evidence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent advances in the video understanding ability of multimodal large language models (MLLMs), long video understanding remains a challenge. One of the main issues is that the number of vision tokens grows linearly with video length, which causes an explosion in attention cost, memory, and latency. To solve this challenge, we present Query-aware Token Selector (\\textbf{QTSplus}), a lightweight yet powerful visual token selection module that serves as an information gate between the vision encoder and LLMs. Given a text query and video tokens, QTSplus dynamically selects the most important visual evidence for the input text query by (i) scoring visual tokens via cross-attention, (ii) \\emph{predicting} an instance-specific retention budget based on the complexity of the query, and (iii) \\emph{selecting} Top-$n$ tokens with a differentiable straight-through estimator during training and a hard gate at inference. Furthermore, a small re-encoder preserves temporal order using absolute time information, enabling second-level localization while maintaining global coverage.\n  Integrated into Qwen2.5-VL, QTSplus compresses the vision stream by up to \\textbf{89\\%} and reduces end-to-end latency by \\textbf{28\\%} on long videos. The evaluation on eight long video understanding benchmarks shows near-parity accuracy overall when compared with the original Qwen models and outperforms the original model by \\textbf{+20.5} and \\textbf{+5.6} points respectively on TempCompass direction and order accuracies. These results show that QTSplus is an effective, general mechanism for scaling MLLMs to real-world long-video scenarios while preserving task-relevant evidence."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-14T22:41:27Z",
                "published_parsed": [
                    2025,
                    11,
                    14,
                    22,
                    41,
                    27,
                    4,
                    318,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Siyou Li"
                    },
                    {
                        "name": "Huanan Wu"
                    },
                    {
                        "name": "Juexi Shao"
                    },
                    {
                        "name": "Yinghao Ma"
                    },
                    {
                        "name": "Yujian Gan"
                    },
                    {
                        "name": "Yihao Luo"
                    },
                    {
                        "name": "Yuwei Wang"
                    },
                    {
                        "name": "Dong Nie"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Wengqing Wu"
                    },
                    {
                        "name": "Le Zhang"
                    },
                    {
                        "name": "Massimo Poesio"
                    },
                    {
                        "name": "Juntao Yu"
                    }
                ],
                "author_detail": {
                    "name": "Juntao Yu"
                },
                "author": "Juntao Yu"
            },
            {
                "id": "http://arxiv.org/abs/2511.17405v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17405v1",
                "title": "Beyond Multiple Choice: A Hybrid Framework for Unifying Robust Evaluation and Verifiable Reasoning Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Multiple Choice: A Hybrid Framework for Unifying Robust Evaluation and Verifiable Reasoning Training"
                },
                "updated": "2025-11-21T17:06:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    6,
                    37,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17405v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multiple-choice question answering (MCQA) has been a popular format for evaluating and reinforcement fine-tuning (RFT) of modern multimodal language models. Its constrained output format allows for simplified, deterministic automatic verification. However, we find that the options may leak exploitable signals, which makes the accuracy metrics unreliable for indicating real capabilities and encourages explicit or implicit answer guessing behaviors during RFT. We propose ReVeL (Rewrite and Verify by LLM), a framework that rewrites multiple-choice questions into open-form questions while keeping answers verifiable whenever possible. The framework categorizes questions according to different answer types, apply different rewriting and verification schemes, respectively. When applied for RFT, we converted 20k MCQA examples and use GRPO to finetune Qwen2.5-VL models. Models trained on ReVeL-OpenQA match MCQA accuracy on multiple-choice benchmarks and improve OpenQA accuracy by about six percentage points, indicating better data efficiency and more robust reward signals than MCQA-based training. When used for evaluation, ReVeL also reveals up to 20 percentage points of score inflation in MCQA benchmarks (relative to OpenQA), improves judging accuracy, and reduces both cost and latency. We will release code and data publicly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple-choice question answering (MCQA) has been a popular format for evaluating and reinforcement fine-tuning (RFT) of modern multimodal language models. Its constrained output format allows for simplified, deterministic automatic verification. However, we find that the options may leak exploitable signals, which makes the accuracy metrics unreliable for indicating real capabilities and encourages explicit or implicit answer guessing behaviors during RFT. We propose ReVeL (Rewrite and Verify by LLM), a framework that rewrites multiple-choice questions into open-form questions while keeping answers verifiable whenever possible. The framework categorizes questions according to different answer types, apply different rewriting and verification schemes, respectively. When applied for RFT, we converted 20k MCQA examples and use GRPO to finetune Qwen2.5-VL models. Models trained on ReVeL-OpenQA match MCQA accuracy on multiple-choice benchmarks and improve OpenQA accuracy by about six percentage points, indicating better data efficiency and more robust reward signals than MCQA-based training. When used for evaluation, ReVeL also reveals up to 20 percentage points of score inflation in MCQA benchmarks (relative to OpenQA), improves judging accuracy, and reduces both cost and latency. We will release code and data publicly."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T17:06:37Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    6,
                    37,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Project url: https://flageval-baai.github.io/ReVeL/",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yesheng Liu"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Haiyu Xu"
                    },
                    {
                        "name": "Baoqi Pei"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Mingxuan Zhao"
                    },
                    {
                        "name": "Jingshu Zheng"
                    },
                    {
                        "name": "Zheqi He"
                    },
                    {
                        "name": "JG Yao"
                    },
                    {
                        "name": "Bowen Qin"
                    },
                    {
                        "name": "Xi Yang"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.17401v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17401v1",
                "title": "Feasibility of Embodied Dynamics Based Bayesian Learning for Continuous Pursuit Motion Control of Assistive Mobile Robots in the Built Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feasibility of Embodied Dynamics Based Bayesian Learning for Continuous Pursuit Motion Control of Assistive Mobile Robots in the Built Environment"
                },
                "updated": "2025-11-21T17:00:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    0,
                    48,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17401v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Non-invasive electroencephalography (EEG)-based brain-computer interfaces (BCIs) offer an intuitive means for individuals with severe motor impairments to independently operate assistive robotic wheelchairs and navigate built environments. Despite considerable progress in BCI research, most current motion control systems are limited to discrete commands, rather than supporting continuous pursuit, where users can freely adjust speed and direction in real time. Such natural mobility control is, however, essential for wheelchair users to navigate complex public spaces, such as transit stations, airports, hospitals, and indoor corridors, to interact socially with the dynamic populations with agility, and to move flexibly and comfortably as autonomous driving is refined to allow movement at will. In this study, we address the gap of continuous pursuit motion control in BCIs by proposing and validating a brain-inspired Bayesian inference framework, where embodied dynamics in acceleration-based motor representations are decoded. This approach contrasts with conventional kinematics-level decoding and deep learning-based methods. Using a public dataset with sixteen hours of EEG from four subjects performing motor imagery-based target-following, we demonstrate that our method, utilizing Automatic Relevance Determination for feature selection and continual online learning, reduces the normalized mean squared error between predicted and true velocities by 72% compared to autoregressive and EEGNet-based methods in a session-accumulative transfer learning setting. Theoretically, these findings empirically support embodied cognition theory and reveal the brain's intrinsic motor control dynamics in an embodied and predictive nature. Practically, grounding EEG decoding in the same dynamical principles that govern biological motion offers a promising path toward more stable and intuitive BCI control.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-invasive electroencephalography (EEG)-based brain-computer interfaces (BCIs) offer an intuitive means for individuals with severe motor impairments to independently operate assistive robotic wheelchairs and navigate built environments. Despite considerable progress in BCI research, most current motion control systems are limited to discrete commands, rather than supporting continuous pursuit, where users can freely adjust speed and direction in real time. Such natural mobility control is, however, essential for wheelchair users to navigate complex public spaces, such as transit stations, airports, hospitals, and indoor corridors, to interact socially with the dynamic populations with agility, and to move flexibly and comfortably as autonomous driving is refined to allow movement at will. In this study, we address the gap of continuous pursuit motion control in BCIs by proposing and validating a brain-inspired Bayesian inference framework, where embodied dynamics in acceleration-based motor representations are decoded. This approach contrasts with conventional kinematics-level decoding and deep learning-based methods. Using a public dataset with sixteen hours of EEG from four subjects performing motor imagery-based target-following, we demonstrate that our method, utilizing Automatic Relevance Determination for feature selection and continual online learning, reduces the normalized mean squared error between predicted and true velocities by 72% compared to autoregressive and EEGNet-based methods in a session-accumulative transfer learning setting. Theoretically, these findings empirically support embodied cognition theory and reveal the brain's intrinsic motor control dynamics in an embodied and predictive nature. Practically, grounding EEG decoding in the same dynamical principles that govern biological motion offers a promising path toward more stable and intuitive BCI control."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T17:00:48Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    0,
                    48,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "37 pages, 9 figures, and 7 tables",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Xiaoshan Zhou"
                    },
                    {
                        "name": "Carol C. Menassa"
                    },
                    {
                        "name": "Vineet R. Kamat"
                    }
                ],
                "author_detail": {
                    "name": "Vineet R. Kamat"
                },
                "author": "Vineet R. Kamat"
            },
            {
                "id": "http://arxiv.org/abs/2511.17397v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17397v1",
                "title": "MCMoE: Completing Missing Modalities with Mixture of Experts for Incomplete Multimodal Action Quality Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCMoE: Completing Missing Modalities with Mixture of Experts for Incomplete Multimodal Action Quality Assessment"
                },
                "updated": "2025-11-21T16:56:25Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    16,
                    56,
                    25,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17397v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multimodal Action Quality Assessment (AQA) has recently emerged as a promising paradigm. By leveraging complementary information across shared contextual cues, it enhances the discriminative evaluation of subtle intra-class variations in highly similar action sequences. However, partial modalities are frequently unavailable at the inference stage in reality. The absence of any modality often renders existing multimodal models inoperable. Furthermore, it triggers catastrophic performance degradation due to interruptions in cross-modal interactions. To address this issue, we propose a novel Missing Completion Framework with Mixture of Experts (MCMoE) that unifies unimodal and joint representation learning in single-stage training. Specifically, we propose an adaptive gated modality generator that dynamically fuses available information to reconstruct missing modalities. We then design modality experts to learn unimodal knowledge and dynamically mix the knowledge of all experts to extract cross-modal joint representations. With a mixture of experts, missing modalities are further refined and complemented. Finally, in the training phase, we mine the complete multimodal features and unimodal expert knowledge to guide modality generation and generation-based joint representation extraction. Extensive experiments demonstrate that our MCMoE achieves state-of-the-art results in both complete and incomplete multimodal learning on three public AQA benchmarks. Code is available at https://github.com/XuHuangbiao/MCMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Action Quality Assessment (AQA) has recently emerged as a promising paradigm. By leveraging complementary information across shared contextual cues, it enhances the discriminative evaluation of subtle intra-class variations in highly similar action sequences. However, partial modalities are frequently unavailable at the inference stage in reality. The absence of any modality often renders existing multimodal models inoperable. Furthermore, it triggers catastrophic performance degradation due to interruptions in cross-modal interactions. To address this issue, we propose a novel Missing Completion Framework with Mixture of Experts (MCMoE) that unifies unimodal and joint representation learning in single-stage training. Specifically, we propose an adaptive gated modality generator that dynamically fuses available information to reconstruct missing modalities. We then design modality experts to learn unimodal knowledge and dynamically mix the knowledge of all experts to extract cross-modal joint representations. With a mixture of experts, missing modalities are further refined and complemented. Finally, in the training phase, we mine the complete multimodal features and unimodal expert knowledge to guide modality generation and generation-based joint representation extraction. Extensive experiments demonstrate that our MCMoE achieves state-of-the-art results in both complete and incomplete multimodal learning on three public AQA benchmarks. Code is available at https://github.com/XuHuangbiao/MCMoE."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T16:56:25Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    16,
                    56,
                    25,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Huangbiao Xu"
                    },
                    {
                        "name": "Huanqi Wu"
                    },
                    {
                        "name": "Xiao Ke"
                    },
                    {
                        "name": "Junyi Wu"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Jinglin Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jinglin Xu"
                },
                "author": "Jinglin Xu"
            },
            {
                "id": "http://arxiv.org/abs/2412.19622v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2412.19622v2",
                "title": "Reassessing prediction in the brain: Pre-onset neural encoding during natural listening does not reflect pre-activation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reassessing prediction in the brain: Pre-onset neural encoding during natural listening does not reflect pre-activation"
                },
                "updated": "2025-11-21T16:51:07Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    16,
                    51,
                    7,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2412.19622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2412.19622v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Predictive processing theories propose that the brain continuously anticipates upcoming input. However, direct neural evidence for predictive pre-activation during natural language comprehension remains limited and debated. Previous studies using large language model (LLM)-based encoding models with fMRI and ECoG have reported pre-onset signals that appear to encode upcoming words, but these effects may instead reflect dependencies in the stimulus or autocorrelations in neural activity. Here, we re-examined this question by aligning LLM-derived word embeddings with neural activity recorded during naturalistic listening using magnetoencephalography (MEG) and electrocorticography (ECoG). We replicated pre-onset encoding effects previously observed in ECoG across both modalities, and found that they persist even after controlling for stimulus correlations. Crucially, temporal generalization analyses revealed no stable overlap between pre- and post-onset representations, indicating that pre-onset activity does not reflect pre-activation of the next word. Consistent with this, long-range predictive effects previously reported in fMRI did not replicate in our higher-temporal-resolution data. While we found no evidence for predictive pre-activation, we observed clear signatures of postdiction, with neural activity reflecting persistent encoding of prior words. These results suggest that reported apparent predictive signals do not reflect pre-activation of upcoming input. They call for caution in interpreting LLM-based encoding models and highlight the need for a more nuanced understanding of what constitutes \"prediction\" in language comprehension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictive processing theories propose that the brain continuously anticipates upcoming input. However, direct neural evidence for predictive pre-activation during natural language comprehension remains limited and debated. Previous studies using large language model (LLM)-based encoding models with fMRI and ECoG have reported pre-onset signals that appear to encode upcoming words, but these effects may instead reflect dependencies in the stimulus or autocorrelations in neural activity. Here, we re-examined this question by aligning LLM-derived word embeddings with neural activity recorded during naturalistic listening using magnetoencephalography (MEG) and electrocorticography (ECoG). We replicated pre-onset encoding effects previously observed in ECoG across both modalities, and found that they persist even after controlling for stimulus correlations. Crucially, temporal generalization analyses revealed no stable overlap between pre- and post-onset representations, indicating that pre-onset activity does not reflect pre-activation of the next word. Consistent with this, long-range predictive effects previously reported in fMRI did not replicate in our higher-temporal-resolution data. While we found no evidence for predictive pre-activation, we observed clear signatures of postdiction, with neural activity reflecting persistent encoding of prior words. These results suggest that reported apparent predictive signals do not reflect pre-activation of upcoming input. They call for caution in interpreting LLM-based encoding models and highlight the need for a more nuanced understanding of what constitutes \"prediction\" in language comprehension."
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-12-27T12:49:03Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    12,
                    49,
                    3,
                    4,
                    362,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC"
                },
                "authors": [
                    {
                        "name": "Sahel Azizpour"
                    },
                    {
                        "name": "Britta U. Westner"
                    },
                    {
                        "name": "Jakub Szewczyk"
                    },
                    {
                        "name": "Umut Gl"
                    },
                    {
                        "name": "Linda Geerligs"
                    }
                ],
                "author_detail": {
                    "name": "Linda Geerligs"
                },
                "author": "Linda Geerligs"
            },
            {
                "id": "http://arxiv.org/abs/2511.17379v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17379v1",
                "title": "On treating right-censoring events like treatments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On treating right-censoring events like treatments"
                },
                "updated": "2025-11-21T16:42:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    16,
                    42,
                    36,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17379v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In causal inference literature, potential outcomes are often indexed by the \"elimination of all right-censoring events,\" leading to the perception that such a restriction is necessary for defining well-posed causal estimands. In this paper, we clarify that this restriction is not required: a well-defined estimand can be formulated without indexing on the elimination of such events. Achieving this requires a more precise classification of right-censoring events than has historically been considered, as the nature of these events has direct implications for identification of the target estimand. We provide a framework that distinguishes different types of right-censoring events from a causal perspective, and demonstrate how this framework relates to censoring definitions and assumptions in classical survival analysis literature. By bridging these perspectives, we provide a clearer understanding of how to handle right-censoring events and provide guidance for identifying causal estimands when right-censored events are present.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In causal inference literature, potential outcomes are often indexed by the \"elimination of all right-censoring events,\" leading to the perception that such a restriction is necessary for defining well-posed causal estimands. In this paper, we clarify that this restriction is not required: a well-defined estimand can be formulated without indexing on the elimination of such events. Achieving this requires a more precise classification of right-censoring events than has historically been considered, as the nature of these events has direct implications for identification of the target estimand. We provide a framework that distinguishes different types of right-censoring events from a causal perspective, and demonstrate how this framework relates to censoring definitions and assumptions in classical survival analysis literature. By bridging these perspectives, we provide a clearer understanding of how to handle right-censoring events and provide guidance for identifying causal estimands when right-censored events are present."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T16:42:36Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    16,
                    42,
                    36,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Lan Wen"
                    },
                    {
                        "name": "Aaron L. Sarvet"
                    },
                    {
                        "name": "Jessica G. Young"
                    }
                ],
                "author_detail": {
                    "name": "Jessica G. Young"
                },
                "author": "Jessica G. Young"
            },
            {
                "id": "http://arxiv.org/abs/2511.17362v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17362v1",
                "title": "ATAC: Augmentation-Based Test-Time Adversarial Correction for CLIP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ATAC: Augmentation-Based Test-Time Adversarial Correction for CLIP"
                },
                "updated": "2025-11-21T16:30:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    16,
                    30,
                    6,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17362v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17362v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Despite its remarkable success in zero-shot image-text matching, CLIP remains highly vulnerable to adversarial perturbations on images. As adversarial fine-tuning is prohibitively costly, recent works explore various test-time defense strategies; however, these approaches still exhibit limited robustness. In this work, we revisit this problem and propose a simple yet effective strategy: Augmentation-based Test-time Adversarial Correction (ATAC). Our method operates directly in the embedding space of CLIP, calculating augmentation-induced drift vectors to infer a semantic recovery direction and correcting the embedding based on the angular consistency of these latent drifts. Across a wide range of benchmarks, ATAC consistently achieves remarkably high robustness, surpassing that of previous state-of-the-art methods by nearly 50\\% on average, all while requiring minimal computational overhead. Furthermore, ATAC retains state-of-the-art robustness in unconventional and extreme settings and even achieves nontrivial robustness against adaptive attacks. Our results demonstrate that ATAC is an efficient method in a novel paradigm for test-time adversarial defenses in the embedding space of CLIP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite its remarkable success in zero-shot image-text matching, CLIP remains highly vulnerable to adversarial perturbations on images. As adversarial fine-tuning is prohibitively costly, recent works explore various test-time defense strategies; however, these approaches still exhibit limited robustness. In this work, we revisit this problem and propose a simple yet effective strategy: Augmentation-based Test-time Adversarial Correction (ATAC). Our method operates directly in the embedding space of CLIP, calculating augmentation-induced drift vectors to infer a semantic recovery direction and correcting the embedding based on the angular consistency of these latent drifts. Across a wide range of benchmarks, ATAC consistently achieves remarkably high robustness, surpassing that of previous state-of-the-art methods by nearly 50\\% on average, all while requiring minimal computational overhead. Furthermore, ATAC retains state-of-the-art robustness in unconventional and extreme settings and even achieves nontrivial robustness against adaptive attacks. Our results demonstrate that ATAC is an efficient method in a novel paradigm for test-time adversarial defenses in the embedding space of CLIP."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T16:30:06Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    16,
                    30,
                    6,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "16 pages",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Linxiang Su"
                    },
                    {
                        "name": "Andrs Balogh"
                    }
                ],
                "author_detail": {
                    "name": "Andrs Balogh"
                },
                "author": "Andrs Balogh"
            },
            {
                "id": "http://arxiv.org/abs/2507.07729v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.07729v3",
                "title": "Efficient Stochastic BFGS methods Inspired by Bayesian Principles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Stochastic BFGS methods Inspired by Bayesian Principles"
                },
                "updated": "2025-11-21T16:30:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    16,
                    30,
                    1,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.07729v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.07729v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Quasi-Newton methods are ubiquitous in deterministic local search due to their efficiency and low computational cost. This class of methods uses the history of gradient evaluations to approximate second-order derivatives. However, only noisy gradient observations are accessible in stochastic optimization; thus, deriving quasi-Newton methods in this setting is challenging. Although most existing quasi-Newton methods for stochastic optimization rely on deterministic equations that are modified to circumvent noise, we propose a new approach inspired by Bayesian inference to assimilate noisy gradient information and derive the stochastic counterparts to standard quasi-Newton methods. We focus on the derivations of stochastic BFGS and L-BFGS, but our methodology can also be employed to derive stochastic analogs of other quasi-Newton methods. The resulting stochastic BFGS (S-BFGS) and stochastic L-BFGS (L-S-BFGS) can effectively learn an inverse Hessian approximation even with small batch sizes. For a problem of dimension $d$, the iteration cost of S-BFGS is $\\mathcal{O}(d^2)$, and the cost of L-S-BFGS is $\\mathcal{O}(d)$. Numerical experiments with a dimensionality of up to $30,720$ demonstrate the efficiency and robustness of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quasi-Newton methods are ubiquitous in deterministic local search due to their efficiency and low computational cost. This class of methods uses the history of gradient evaluations to approximate second-order derivatives. However, only noisy gradient observations are accessible in stochastic optimization; thus, deriving quasi-Newton methods in this setting is challenging. Although most existing quasi-Newton methods for stochastic optimization rely on deterministic equations that are modified to circumvent noise, we propose a new approach inspired by Bayesian inference to assimilate noisy gradient information and derive the stochastic counterparts to standard quasi-Newton methods. We focus on the derivations of stochastic BFGS and L-BFGS, but our methodology can also be employed to derive stochastic analogs of other quasi-Newton methods. The resulting stochastic BFGS (S-BFGS) and stochastic L-BFGS (L-S-BFGS) can effectively learn an inverse Hessian approximation even with small batch sizes. For a problem of dimension $d$, the iteration cost of S-BFGS is $\\mathcal{O}(d^2)$, and the cost of L-S-BFGS is $\\mathcal{O}(d)$. Numerical experiments with a dimensionality of up to $30,720$ demonstrate the efficiency and robustness of the proposed method."
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-10T13:08:55Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    13,
                    8,
                    55,
                    3,
                    191,
                    0
                ],
                "arxiv_comment": "18 pages, 4 figures",
                "arxiv_primary_category": {
                    "term": "math.OC"
                },
                "authors": [
                    {
                        "name": "Andr Carlon"
                    },
                    {
                        "name": "Luis Espath"
                    },
                    {
                        "name": "Ral Tempone"
                    }
                ],
                "author_detail": {
                    "name": "Ral Tempone"
                },
                "author": "Ral Tempone"
            },
            {
                "id": "http://arxiv.org/abs/2511.17361v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17361v1",
                "title": "SuperQuadricOcc: Multi-Layer Gaussian Approximation of Superquadrics for Real-Time Self-Supervised Occupancy Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperQuadricOcc: Multi-Layer Gaussian Approximation of Superquadrics for Real-Time Self-Supervised Occupancy Estimation"
                },
                "updated": "2025-11-21T16:26:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    16,
                    26,
                    31,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17361v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Semantic occupancy estimation enables comprehensive scene understanding for automated driving, providing dense spatial and semantic information essential for perception and planning. While Gaussian representations have been widely adopted in self-supervised occupancy estimation, the deployment of a large number of Gaussian primitives drastically increases memory requirements and is not suitable for real-time inference. In contrast, superquadrics permit reduced primitive count and lower memory requirements due to their diverse shape set. However, implementation into a self-supervised occupancy model is nontrivial due to the absence of a superquadric rasterizer to enable model supervision. Our proposed method, SuperQuadricOcc, employs a superquadric-based scene representation. By leveraging a multi-layer icosphere-tessellated Gaussian approximation of superquadrics, we enable Gaussian rasterization for supervision during training. On the Occ3D dataset, SuperQuadricOcc achieves a 75\\% reduction in memory footprint, 124\\% faster inference, and a 5.9\\% improvement in mIoU compared to previous Gaussian-based methods, without the use of temporal labels. To our knowledge, this is the first occupancy model to enable real-time inference while maintaining competitive performance. The use of superquadrics reduces the number of primitives required for scene modeling by 84\\% relative to Gaussian-based approaches. Finally, evaluation against prior methods is facilitated by our fast superquadric voxelization module. The code will be released as open source.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic occupancy estimation enables comprehensive scene understanding for automated driving, providing dense spatial and semantic information essential for perception and planning. While Gaussian representations have been widely adopted in self-supervised occupancy estimation, the deployment of a large number of Gaussian primitives drastically increases memory requirements and is not suitable for real-time inference. In contrast, superquadrics permit reduced primitive count and lower memory requirements due to their diverse shape set. However, implementation into a self-supervised occupancy model is nontrivial due to the absence of a superquadric rasterizer to enable model supervision. Our proposed method, SuperQuadricOcc, employs a superquadric-based scene representation. By leveraging a multi-layer icosphere-tessellated Gaussian approximation of superquadrics, we enable Gaussian rasterization for supervision during training. On the Occ3D dataset, SuperQuadricOcc achieves a 75\\% reduction in memory footprint, 124\\% faster inference, and a 5.9\\% improvement in mIoU compared to previous Gaussian-based methods, without the use of temporal labels. To our knowledge, this is the first occupancy model to enable real-time inference while maintaining competitive performance. The use of superquadrics reduces the number of primitives required for scene modeling by 84\\% relative to Gaussian-based approaches. Finally, evaluation against prior methods is facilitated by our fast superquadric voxelization module. The code will be released as open source."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T16:26:31Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    16,
                    26,
                    31,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Seamie Hayes"
                    },
                    {
                        "name": "Reenu Mohandas"
                    },
                    {
                        "name": "Tim Brophy"
                    },
                    {
                        "name": "Alexandre Boulch"
                    },
                    {
                        "name": "Ganesh Sistu"
                    },
                    {
                        "name": "Ciaran Eising"
                    }
                ],
                "author_detail": {
                    "name": "Ciaran Eising"
                },
                "author": "Ciaran Eising"
            },
            {
                "id": "http://arxiv.org/abs/2507.04482v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.04482v2",
                "title": "A Training-Free Style-Personalization via SVD-Based Feature Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Training-Free Style-Personalization via SVD-Based Feature Decomposition"
                },
                "updated": "2025-11-21T16:25:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    16,
                    25,
                    35,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.04482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.04482v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present a training-free framework for style-personalized image generation that operates during inference using a scale-wise autoregressive model. Our method generates a stylized image guided by a single reference style while preserving semantic consistency and mitigating content leakage. Through a detailed step-wise analysis of the generation process, we identify a pivotal step where the dominant singular values of the internal feature encode style-related components. Building upon this insight, we introduce two lightweight control modules: Principal Feature Blending, which enables precise modulation of style through SVD-based feature reconstruction, and Structural Attention Correction, which stabilizes structural consistency by leveraging content-guided attention correction across fine stages. Without any additional training, extensive experiments demonstrate that our method achieves competitive style fidelity and prompt fidelity compared to fine-tuned baselines, while offering faster inference and greater deployment flexibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a training-free framework for style-personalized image generation that operates during inference using a scale-wise autoregressive model. Our method generates a stylized image guided by a single reference style while preserving semantic consistency and mitigating content leakage. Through a detailed step-wise analysis of the generation process, we identify a pivotal step where the dominant singular values of the internal feature encode style-related components. Building upon this insight, we introduce two lightweight control modules: Principal Feature Blending, which enables precise modulation of style through SVD-based feature reconstruction, and Structural Attention Correction, which stabilizes structural consistency by leveraging content-guided attention correction across fine stages. Without any additional training, extensive experiments demonstrate that our method achieves competitive style fidelity and prompt fidelity compared to fine-tuned baselines, while offering faster inference and greater deployment flexibility."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-06T17:42:11Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    17,
                    42,
                    11,
                    6,
                    187,
                    0
                ],
                "arxiv_comment": "21 pages, 14 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Kyoungmin Lee"
                    },
                    {
                        "name": "Jihun Park"
                    },
                    {
                        "name": "Jongmin Gim"
                    },
                    {
                        "name": "Wonhyeok Choi"
                    },
                    {
                        "name": "Kyumin Hwang"
                    },
                    {
                        "name": "Jaeyeul Kim"
                    },
                    {
                        "name": "Sunghoon Im"
                    }
                ],
                "author_detail": {
                    "name": "Sunghoon Im"
                },
                "author": "Sunghoon Im"
            },
            {
                "id": "http://arxiv.org/abs/2511.17358v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17358v1",
                "title": "Don't Learn, Ground: A Case for Natural Language Inference with Visual Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Learn, Ground: A Case for Natural Language Inference with Visual Grounding"
                },
                "updated": "2025-11-21T16:23:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    16,
                    23,
                    17,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17358v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We propose a zero-shot method for Natural Language Inference (NLI) that leverages multimodal representations by grounding language in visual contexts. Our approach generates visual representations of premises using text-to-image models and performs inference by comparing these representations with textual hypotheses. We evaluate two inference techniques: cosine similarity and visual question answering. Our method achieves high accuracy without task-specific fine-tuning, demonstrating robustness against textual biases and surface heuristics. Additionally, we design a controlled adversarial dataset to validate the robustness of our approach. Our findings suggest that leveraging visual modality as a meaning representation provides a promising direction for robust natural language understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a zero-shot method for Natural Language Inference (NLI) that leverages multimodal representations by grounding language in visual contexts. Our approach generates visual representations of premises using text-to-image models and performs inference by comparing these representations with textual hypotheses. We evaluate two inference techniques: cosine similarity and visual question answering. Our method achieves high accuracy without task-specific fine-tuning, demonstrating robustness against textual biases and surface heuristics. Additionally, we design a controlled adversarial dataset to validate the robustness of our approach. Our findings suggest that leveraging visual modality as a meaning representation provides a promising direction for robust natural language understanding."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T16:23:17Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    16,
                    23,
                    17,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Daniil Ignatev"
                    },
                    {
                        "name": "Ayman Santeer"
                    },
                    {
                        "name": "Albert Gatt"
                    },
                    {
                        "name": "Denis Paperno"
                    }
                ],
                "author_detail": {
                    "name": "Denis Paperno"
                },
                "author": "Denis Paperno"
            },
            {
                "id": "http://arxiv.org/abs/2505.09439v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.09439v3",
                "title": "Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?"
                },
                "updated": "2025-11-21T16:16:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    16,
                    16,
                    38,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.09439v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.09439v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni, on an audio question answering dataset with the reinforcement learning method GRPO. This leads to new State-of-the-Art performance on the recent MMAU and MMAR benchmarks. Omni-R1 achieves the highest accuracies on the sounds, music, speech, and overall average categories, both on the Test-mini and Test-full splits. To understand the performance improvement, we tested models both with and without audio and found that much of the performance improvement from GRPO could be attributed to better text-based reasoning. We also made a surprising discovery that fine-tuning without audio on a text-only dataset was effective at improving the audio-based performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni, on an audio question answering dataset with the reinforcement learning method GRPO. This leads to new State-of-the-Art performance on the recent MMAU and MMAR benchmarks. Omni-R1 achieves the highest accuracies on the sounds, music, speech, and overall average categories, both on the Test-mini and Test-full splits. To understand the performance improvement, we tested models both with and without audio and found that much of the performance improvement from GRPO could be attributed to better text-based reasoning. We also made a surprising discovery that fine-tuning without audio on a text-only dataset was effective at improving the audio-based performance."
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-14T14:47:16Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    14,
                    47,
                    16,
                    2,
                    134,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS"
                },
                "authors": [
                    {
                        "name": "Andrew Rouditchenko"
                    },
                    {
                        "name": "Saurabhchand Bhati"
                    },
                    {
                        "name": "Edson Araujo"
                    },
                    {
                        "name": "Samuel Thomas"
                    },
                    {
                        "name": "Hilde Kuehne"
                    },
                    {
                        "name": "Rogerio Feris"
                    },
                    {
                        "name": "James Glass"
                    }
                ],
                "author_detail": {
                    "name": "James Glass"
                },
                "author": "James Glass"
            },
            {
                "id": "http://arxiv.org/abs/2511.17339v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17339v1",
                "title": "ReBaPL: Repulsive Bayesian Prompt Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReBaPL: Repulsive Bayesian Prompt Learning"
                },
                "updated": "2025-11-21T16:00:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    16,
                    0,
                    13,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17339v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Prompt learning has emerged as an effective technique for fine-tuning large-scale foundation models for downstream tasks. However, conventional prompt tuning methods are prone to overfitting and can struggle with out-of-distribution generalization. To address these limitations, Bayesian prompt learning has been proposed, which frames prompt optimization as a Bayesian inference problem to enhance robustness. This paper introduces Repulsive Bayesian Prompt Learning (ReBaPL), a novel method for Bayesian prompt learning, designed to efficiently explore the complex and often multimodal posterior landscape of prompts. Our method integrates a cyclical step-size schedule with a stochastic gradient Hamiltonian Monte Carlo (SGHMC) algorithm, enabling alternating phases of exploration to discover new modes, and exploitation to refine existing modes. Furthermore, we introduce a repulsive force derived from a potential function over probability metrics (including Maximum Mean Discrepancy and Wasserstein distance) computed on the distributions of representations produced by different prompts. This representation-space repulsion diversifies exploration and prevents premature collapse to a single mode. Our approach allows for a more comprehensive characterization of the prompt posterior distribution, leading to improved generalization. In contrast to prior Bayesian prompt learning methods, our method provides a modular plug-and-play Bayesian extension of any existing prompt learning method based on maximum likelihood estimation. We demonstrate the efficacy of ReBaPL on several benchmark datasets, showing superior performance over state-of-the-art methods for prompt learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt learning has emerged as an effective technique for fine-tuning large-scale foundation models for downstream tasks. However, conventional prompt tuning methods are prone to overfitting and can struggle with out-of-distribution generalization. To address these limitations, Bayesian prompt learning has been proposed, which frames prompt optimization as a Bayesian inference problem to enhance robustness. This paper introduces Repulsive Bayesian Prompt Learning (ReBaPL), a novel method for Bayesian prompt learning, designed to efficiently explore the complex and often multimodal posterior landscape of prompts. Our method integrates a cyclical step-size schedule with a stochastic gradient Hamiltonian Monte Carlo (SGHMC) algorithm, enabling alternating phases of exploration to discover new modes, and exploitation to refine existing modes. Furthermore, we introduce a repulsive force derived from a potential function over probability metrics (including Maximum Mean Discrepancy and Wasserstein distance) computed on the distributions of representations produced by different prompts. This representation-space repulsion diversifies exploration and prevents premature collapse to a single mode. Our approach allows for a more comprehensive characterization of the prompt posterior distribution, leading to improved generalization. In contrast to prior Bayesian prompt learning methods, our method provides a modular plug-and-play Bayesian extension of any existing prompt learning method based on maximum likelihood estimation. We demonstrate the efficacy of ReBaPL on several benchmark datasets, showing superior performance over state-of-the-art methods for prompt learning."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T16:00:13Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    16,
                    0,
                    13,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Under review",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yassir Bendou"
                    },
                    {
                        "name": "Omar Ezzahir"
                    },
                    {
                        "name": "Eduardo Fernandes Montesuma"
                    },
                    {
                        "name": "Gabriel Mahuas"
                    },
                    {
                        "name": "Victoria Shevchenko"
                    },
                    {
                        "name": "Mike Gartrell"
                    }
                ],
                "author_detail": {
                    "name": "Mike Gartrell"
                },
                "author": "Mike Gartrell"
            },
            {
                "id": "http://arxiv.org/abs/2511.17335v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17335v1",
                "title": "Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM"
                },
                "updated": "2025-11-21T15:55:25Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    55,
                    25,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17335v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Human-robot collaboration towards a shared goal requires robots to understand human action and interaction with the surrounding environment. This paper focuses on human-robot interaction (HRI) based on human-robot dialogue that relies on the robot action confirmation and action step generation using multimodal scene understanding. The state-of-the-art approach uses multimodal transformers to generate robot action steps aligned with robot action confirmation from a single clip showing a task composed of multiple micro steps. Although actions towards a long-horizon task depend on each other throughout an entire video, the current approaches mainly focus on clip-level processing and do not leverage long-context information. This paper proposes a long-context Q-former incorporating left and right context dependency in full videos. Furthermore, this paper proposes a text-conditioning approach to feed text embeddings directly into the LLM decoder to mitigate the high abstraction of the information in text by Q-former. Experiments with the YouCook2 corpus show that the accuracy of confirmation generation is a major factor in the performance of action planning. Furthermore, we demonstrate that the long-context Q-former improves the confirmation and action planning by integrating VideoLLaMA3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-robot collaboration towards a shared goal requires robots to understand human action and interaction with the surrounding environment. This paper focuses on human-robot interaction (HRI) based on human-robot dialogue that relies on the robot action confirmation and action step generation using multimodal scene understanding. The state-of-the-art approach uses multimodal transformers to generate robot action steps aligned with robot action confirmation from a single clip showing a task composed of multiple micro steps. Although actions towards a long-horizon task depend on each other throughout an entire video, the current approaches mainly focus on clip-level processing and do not leverage long-context information. This paper proposes a long-context Q-former incorporating left and right context dependency in full videos. Furthermore, this paper proposes a text-conditioning approach to feed text embeddings directly into the LLM decoder to mitigate the high abstraction of the information in text by Q-former. Experiments with the YouCook2 corpus show that the accuracy of confirmation generation is a major factor in the performance of action planning. Furthermore, we demonstrate that the long-context Q-former improves the confirmation and action planning by integrating VideoLLaMA3."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T15:55:25Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    55,
                    25,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Accepted to ASRU 2025",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Chiori Hori"
                    },
                    {
                        "name": "Yoshiki Masuyama"
                    },
                    {
                        "name": "Siddarth Jain"
                    },
                    {
                        "name": "Radu Corcodel"
                    },
                    {
                        "name": "Devesh Jha"
                    },
                    {
                        "name": "Diego Romeres"
                    },
                    {
                        "name": "Jonathan Le Roux"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Le Roux"
                },
                "author": "Jonathan Le Roux"
            },
            {
                "id": "http://arxiv.org/abs/2511.17330v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17330v1",
                "title": "Agentic Program Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Program Verification"
                },
                "updated": "2025-11-21T15:51:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    51,
                    48,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17330v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Automatically generated code is gaining traction recently, owing to the prevalence of Large Language Models (LLMs). Further, the AlphaProof initiative has demonstrated the possibility of using AI for general mathematical reasoning. Reasoning about computer programs (software) can be accomplished via general mathematical reasoning; however, it tends to be more structured and richer in contexts. This forms an attractive proposition, since then AI agents can be used to reason about voluminous code that gets generated by AI.\n  In this work, we present a first LLM agent, AutoRocq, for conducting program verification. Unlike past works, which rely on extensive training of LLMs on proof examples, our agent learns on-the-fly and improves the proof via an iterative refinement loop. The iterative improvement of the proof is achieved by the proof agent communicating with the Rocq (formerly Coq) theorem prover to get additional context and feedback. The final result of the iteration is a proof derivation checked by the Rocq theorem prover. In this way, our proof construction involves autonomous collaboration between the proof agent and the theorem prover. This autonomy facilitates the search for proofs and decision-making in deciding on the structure of the proof tree.\n  Experimental evaluation on SV-COMP benchmarks and on Linux kernel modules shows promising efficacy in achieving automated program verification. As automation in code generation becomes more widespread, we posit that our proof agent can be potentially integrated with AI coding agents to achieve a generate and validate loop, thus moving closer to the vision of trusted automatic programming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically generated code is gaining traction recently, owing to the prevalence of Large Language Models (LLMs). Further, the AlphaProof initiative has demonstrated the possibility of using AI for general mathematical reasoning. Reasoning about computer programs (software) can be accomplished via general mathematical reasoning; however, it tends to be more structured and richer in contexts. This forms an attractive proposition, since then AI agents can be used to reason about voluminous code that gets generated by AI.\n  In this work, we present a first LLM agent, AutoRocq, for conducting program verification. Unlike past works, which rely on extensive training of LLMs on proof examples, our agent learns on-the-fly and improves the proof via an iterative refinement loop. The iterative improvement of the proof is achieved by the proof agent communicating with the Rocq (formerly Coq) theorem prover to get additional context and feedback. The final result of the iteration is a proof derivation checked by the Rocq theorem prover. In this way, our proof construction involves autonomous collaboration between the proof agent and the theorem prover. This autonomy facilitates the search for proofs and decision-making in deciding on the structure of the proof tree.\n  Experimental evaluation on SV-COMP benchmarks and on Linux kernel modules shows promising efficacy in achieving automated program verification. As automation in code generation becomes more widespread, we posit that our proof agent can be potentially integrated with AI coding agents to achieve a generate and validate loop, thus moving closer to the vision of trusted automatic programming."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T15:51:48Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    51,
                    48,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "21 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Haoxin Tu"
                    },
                    {
                        "name": "Huan Zhao"
                    },
                    {
                        "name": "Yahui Song"
                    },
                    {
                        "name": "Mehtab Zafar"
                    },
                    {
                        "name": "Ruijie Meng"
                    },
                    {
                        "name": "Abhik Roychoudhury"
                    }
                ],
                "author_detail": {
                    "name": "Abhik Roychoudhury"
                },
                "author": "Abhik Roychoudhury"
            },
            {
                "id": "http://arxiv.org/abs/2511.17326v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17326v1",
                "title": "Spectral Clustering with Side Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectral Clustering with Side Information"
                },
                "updated": "2025-11-21T15:49:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    49,
                    12,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17326v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In the graph clustering problem with a planted solution, the input is a graph on $n$ vertices partitioned into $k$ clusters, and the task is to infer the clusters from graph structure. A standard assumption is that clusters induce well-connected subgraphs (i.e. $(1)$-expanders), and form $$-sparse cuts. Such a graph defines the clustering uniquely up to $\\approx $ misclassification rate, and efficient algorithms for achieving this rate are known. While this vanilla version of graph clustering is well studied, in practice, vertices of the graph are typically equipped with labels that provide additional information on cluster ids of the vertices. For example, each vertex could have a cluster label that is corrupted independently with probability $$. Using only one of the two sources of information leads to misclassification rate $\\min\\{, \\}$, but can they be combined to achieve a rate of $\\approx $?\n  In this paper, we give an affirmative answer to this question and present a sublinear-time algorithm in the number of vertices $n$. Our key algorithmic insight is a new observation on ``spectrally ambiguous'' vertices in a well-clusterable graph.\n  While our sublinear-time classifier achieves the nearly optimal $\\approx \\widetilde O()$ misclassification rate, the approximate clusters that it outputs do not necessarily induce expanders in the graph $G$. In our second result, we give a polynomial-time algorithm that reweights edges of the original $(k, , (1))$-clusterable graph to transform it into a $(k, \\widetilde O(), (1))$-clusterable one (for constant $k$), improving sparsity of cuts nearly optimally and preserving expansion properties of the communities - an algorithm for refining community structure of the input graph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the graph clustering problem with a planted solution, the input is a graph on $n$ vertices partitioned into $k$ clusters, and the task is to infer the clusters from graph structure. A standard assumption is that clusters induce well-connected subgraphs (i.e. $(1)$-expanders), and form $$-sparse cuts. Such a graph defines the clustering uniquely up to $\\approx $ misclassification rate, and efficient algorithms for achieving this rate are known. While this vanilla version of graph clustering is well studied, in practice, vertices of the graph are typically equipped with labels that provide additional information on cluster ids of the vertices. For example, each vertex could have a cluster label that is corrupted independently with probability $$. Using only one of the two sources of information leads to misclassification rate $\\min\\{, \\}$, but can they be combined to achieve a rate of $\\approx $?\n  In this paper, we give an affirmative answer to this question and present a sublinear-time algorithm in the number of vertices $n$. Our key algorithmic insight is a new observation on ``spectrally ambiguous'' vertices in a well-clusterable graph.\n  While our sublinear-time classifier achieves the nearly optimal $\\approx \\widetilde O()$ misclassification rate, the approximate clusters that it outputs do not necessarily induce expanders in the graph $G$. In our second result, we give a polynomial-time algorithm that reweights edges of the original $(k, , (1))$-clusterable graph to transform it into a $(k, \\widetilde O(), (1))$-clusterable one (for constant $k$), improving sparsity of cuts nearly optimally and preserving expansion properties of the communities - an algorithm for refining community structure of the input graph."
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T15:49:12Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    49,
                    12,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "The arxiv landing page contains a shortened abstract",
                "arxiv_primary_category": {
                    "term": "cs.DS"
                },
                "authors": [
                    {
                        "name": "Hendrik Fichtenberger"
                    },
                    {
                        "name": "Michael Kapralov"
                    },
                    {
                        "name": "Ekaterina Kochetkova"
                    },
                    {
                        "name": "Silvio Lattanzi"
                    },
                    {
                        "name": "Davide Mazzali"
                    },
                    {
                        "name": "Weronika Wrzos-Kaminska"
                    }
                ],
                "author_detail": {
                    "name": "Weronika Wrzos-Kaminska"
                },
                "author": "Weronika Wrzos-Kaminska"
            },
            {
                "id": "http://arxiv.org/abs/2511.17321v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17321v1",
                "title": "Parameter Inference from Final-State Entanglement in Higgs Decays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter Inference from Final-State Entanglement in Higgs Decays"
                },
                "updated": "2025-11-21T15:40:23Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    40,
                    23,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17321v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The decay out-states of unstable Standard Model (SM) particles provide a unique, well-defined intrinsic quantum-information probe of the SM parameter space. We use Higgs decays as a test case: after tracing out kinematics, we compute entanglement among final-state spins and colors across all decay channels and impose a near-maximal entanglement-entropy criterion. This criterion yields quantitative indications for fundamental parameters. Within the SM, the entanglement entropy exhibits a global maximum close to the observed Higgs mass and the measured $W$ mass, the latter being equivalent to the $SU(2)_L$ gauge coupling. In a two-parameter kappa framework, applying the same criterion points to an SM-like balance between vector and fermion couplings, constraining the ratio of the sector-wide rescalings. These results suggest that entanglement extremality can serve as a complementary handle on fundamental parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The decay out-states of unstable Standard Model (SM) particles provide a unique, well-defined intrinsic quantum-information probe of the SM parameter space. We use Higgs decays as a test case: after tracing out kinematics, we compute entanglement among final-state spins and colors across all decay channels and impose a near-maximal entanglement-entropy criterion. This criterion yields quantitative indications for fundamental parameters. Within the SM, the entanglement entropy exhibits a global maximum close to the observed Higgs mass and the measured $W$ mass, the latter being equivalent to the $SU(2)_L$ gauge coupling. In a two-parameter kappa framework, applying the same criterion points to an SM-like balance between vector and fermion couplings, constraining the ratio of the sector-wide rescalings. These results suggest that entanglement extremality can serve as a complementary handle on fundamental parameters."
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T15:40:23Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    40,
                    23,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "17 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "hep-ph"
                },
                "authors": [
                    {
                        "name": "Jia Liu"
                    },
                    {
                        "name": "Masanori Tanaka"
                    },
                    {
                        "name": "Xiao-Ping Wang"
                    },
                    {
                        "name": "Jing-Jun Zhang"
                    },
                    {
                        "name": "Zifan Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zifan Zheng"
                },
                "author": "Zifan Zheng"
            },
            {
                "id": "http://arxiv.org/abs/2511.17315v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17315v1",
                "title": "Humanlike Multi-user Agent (HUMA): Designing a Deceptively Human AI Facilitator for Group Chats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humanlike Multi-user Agent (HUMA): Designing a Deceptively Human AI Facilitator for Group Chats"
                },
                "updated": "2025-11-21T15:34:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    34,
                    42,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17315v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Conversational agents built on large language models (LLMs) are becoming increasingly prevalent, yet most systems are designed for one-on-one, turn-based exchanges rather than natural, asynchronous group chats. As AI assistants become widespread throughout digital platforms, from virtual assistants to customer service, developing natural and humanlike interaction patterns seems crucial for maintaining user trust and engagement. We present the Humanlike Multi-user Agent (HUMA), an LLM-based facilitator that participates in multi-party conversations using human-like strategies and timing. HUMA extends prior multi-user chatbot work with an event-driven architecture that handles messages, replies, reactions and introduces realistic response-time simulation. HUMA comprises three components-Router, Action Agent, and Reflection-which together adapt LLMs to group conversation dynamics.\n  We evaluate HUMA in a controlled study with 97 participants in four-person role-play chats, comparing AI and human community managers (CMs). Participants classified CMs as human at near-chance rates in both conditions, indicating they could not reliably distinguish HUMA agents from humans. Subjective experience was comparable across conditions: community-manager effectiveness, social presence, and engagement/satisfaction differed only modestly with small effect sizes. Our results suggest that, in natural group chat settings, an AI facilitator can match human quality while remaining difficult to identify as nonhuman.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational agents built on large language models (LLMs) are becoming increasingly prevalent, yet most systems are designed for one-on-one, turn-based exchanges rather than natural, asynchronous group chats. As AI assistants become widespread throughout digital platforms, from virtual assistants to customer service, developing natural and humanlike interaction patterns seems crucial for maintaining user trust and engagement. We present the Humanlike Multi-user Agent (HUMA), an LLM-based facilitator that participates in multi-party conversations using human-like strategies and timing. HUMA extends prior multi-user chatbot work with an event-driven architecture that handles messages, replies, reactions and introduces realistic response-time simulation. HUMA comprises three components-Router, Action Agent, and Reflection-which together adapt LLMs to group conversation dynamics.\n  We evaluate HUMA in a controlled study with 97 participants in four-person role-play chats, comparing AI and human community managers (CMs). Participants classified CMs as human at near-chance rates in both conditions, indicating they could not reliably distinguish HUMA agents from humans. Subjective experience was comparable across conditions: community-manager effectiveness, social presence, and engagement/satisfaction differed only modestly with small effect sizes. Our results suggest that, in natural group chat settings, an AI facilitator can match human quality while remaining difficult to identify as nonhuman."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T15:34:42Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    34,
                    42,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "9 pages, 4 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Mateusz Jacniacki"
                    },
                    {
                        "name": "Mart Carmona Serrat"
                    }
                ],
                "author_detail": {
                    "name": "Mart Carmona Serrat"
                },
                "author": "Mart Carmona Serrat"
            },
            {
                "id": "http://arxiv.org/abs/2507.04224v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.04224v3",
                "title": "Fairness Evaluation of Large Language Models in Academic Library Reference Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fairness Evaluation of Large Language Models in Academic Library Reference Services"
                },
                "updated": "2025-11-21T15:33:14Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    33,
                    14,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.04224v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.04224v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As libraries explore large language models (LLMs) for use in virtual reference services, a key question arises: Can LLMs serve all users equitably, regardless of demographics or social status? While they offer great potential for scalable support, LLMs may also reproduce societal biases embedded in their training data, risking the integrity of libraries' commitment to equitable service. To address this concern, we evaluate whether LLMs differentiate responses across user identities by prompting six state-of-the-art LLMs to assist patrons differing in sex, race/ethnicity, and institutional role. We find no evidence of differentiation by race or ethnicity, and only minor evidence of stereotypical bias against women in one model. LLMs demonstrate nuanced accommodation of institutional roles through the use of linguistic choices related to formality, politeness, and domain-specific vocabularies, reflecting professional norms rather than discriminatory treatment. These findings suggest that current LLMs show a promising degree of readiness to support equitable and contextually appropriate communication in academic library reference services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As libraries explore large language models (LLMs) for use in virtual reference services, a key question arises: Can LLMs serve all users equitably, regardless of demographics or social status? While they offer great potential for scalable support, LLMs may also reproduce societal biases embedded in their training data, risking the integrity of libraries' commitment to equitable service. To address this concern, we evaluate whether LLMs differentiate responses across user identities by prompting six state-of-the-art LLMs to assist patrons differing in sex, race/ethnicity, and institutional role. We find no evidence of differentiation by race or ethnicity, and only minor evidence of stereotypical bias against women in one model. LLMs demonstrate nuanced accommodation of institutional roles through the use of linguistic choices related to formality, politeness, and domain-specific vocabularies, reflecting professional norms rather than discriminatory treatment. These findings suggest that current LLMs show a promising degree of readiness to support equitable and contextually appropriate communication in academic library reference services."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-06T03:28:24Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    3,
                    28,
                    24,
                    6,
                    187,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Haining Wang"
                    },
                    {
                        "name": "Jason Clark"
                    },
                    {
                        "name": "Yueru Yan"
                    },
                    {
                        "name": "Star Bradley"
                    },
                    {
                        "name": "Ruiyang Chen"
                    },
                    {
                        "name": "Yiqiong Zhang"
                    },
                    {
                        "name": "Hengyi Fu"
                    },
                    {
                        "name": "Zuoyu Tian"
                    }
                ],
                "author_detail": {
                    "name": "Zuoyu Tian"
                },
                "author": "Zuoyu Tian"
            },
            {
                "id": "http://arxiv.org/abs/2511.17313v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17313v1",
                "title": "On the baryon budget in the X-ray-emitting circumgalactic medium of Milky Way-mass galaxies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the baryon budget in the X-ray-emitting circumgalactic medium of Milky Way-mass galaxies"
                },
                "updated": "2025-11-21T15:31:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    31,
                    54,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17313v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent observations with SRG/eROSITA have revealed the average X-ray surface brightness profile of the X-ray-emitting circumgalactic medium (CGM) around Milky Way (MW)-mass galaxies, offering valuable insights into the baryon mass in these systems. However, the estimation of the baryon mass depends critically on several assumptions regarding the gas density profile, temperature, metallicity, and the underlying halo mass distribution. Here, we assess how these assumptions affect the inferred baryon mass of the X-ray-emitting CGM in MW-mass galaxies, based on the stacked eROSITA signal. We find that variations in temperature profiles and uncertainties in the halo mass introduce the dominant sources of uncertainty, resulting in X-ray-emitting baryon mass estimates that vary by nearly a factor of four ($0.8-3.5\\times10^{11} M_\\odot$). Assumptions about metallicity contribute an additional uncertainty of approximately $50\\%$. We emphasize that accurate X-ray spectral constraints on gas temperature and metallicity, along with careful modeling of halo mass uncertainty, are essential for accurately estimating the baryon mass for MW-mass galaxies. Future X-ray microcalorimeter missions will be crucial for determining the hot CGM properties and closing the baryon census at the MW-mass scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent observations with SRG/eROSITA have revealed the average X-ray surface brightness profile of the X-ray-emitting circumgalactic medium (CGM) around Milky Way (MW)-mass galaxies, offering valuable insights into the baryon mass in these systems. However, the estimation of the baryon mass depends critically on several assumptions regarding the gas density profile, temperature, metallicity, and the underlying halo mass distribution. Here, we assess how these assumptions affect the inferred baryon mass of the X-ray-emitting CGM in MW-mass galaxies, based on the stacked eROSITA signal. We find that variations in temperature profiles and uncertainties in the halo mass introduce the dominant sources of uncertainty, resulting in X-ray-emitting baryon mass estimates that vary by nearly a factor of four ($0.8-3.5\\times10^{11} M_\\odot$). Assumptions about metallicity contribute an additional uncertainty of approximately $50\\%$. We emphasize that accurate X-ray spectral constraints on gas temperature and metallicity, along with careful modeling of halo mass uncertainty, are essential for accurately estimating the baryon mass for MW-mass galaxies. Future X-ray microcalorimeter missions will be crucial for determining the hot CGM properties and closing the baryon census at the MW-mass scale."
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T15:31:54Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    31,
                    54,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "12 pages, 3 figures, 3 tables. Accepted by A&A",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA"
                },
                "authors": [
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Soumya Shreeram"
                    },
                    {
                        "name": "Gabriele Ponti"
                    },
                    {
                        "name": "Johan Comparat"
                    },
                    {
                        "name": "Andrea Merloni"
                    },
                    {
                        "name": "Zhijie Qu"
                    },
                    {
                        "name": "Jiangtao Li"
                    },
                    {
                        "name": "N. Joel Bregman"
                    },
                    {
                        "name": "Taotao Fang"
                    }
                ],
                "author_detail": {
                    "name": "Taotao Fang"
                },
                "author": "Taotao Fang"
            },
            {
                "id": "http://arxiv.org/abs/2511.02894v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.02894v3",
                "title": "Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models"
                },
                "updated": "2025-11-21T15:30:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    30,
                    31,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.02894v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.02894v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The widespread integration of wearable sensing devices in Internet of Things (IoT) ecosystems, particularly in healthcare, smart homes, and industrial applications, has required robust human activity recognition (HAR) techniques to improve functionality and user experience. Although machine learning models have advanced HAR, they are increasingly susceptible to data poisoning attacks that compromise the data integrity and reliability of these systems. Conventional approaches to defending against such attacks often require extensive task-specific training with large, labeled datasets, which limits adaptability in dynamic IoT environments. This work proposes a novel framework that uses large language models (LLMs) to perform poisoning detection and sanitization in HAR systems, utilizing zero-shot, one-shot, and few-shot learning paradigms. Our approach incorporates \\textit{role play} prompting, whereby the LLM assumes the role of expert to contextualize and evaluate sensor anomalies, and \\textit{think step-by-step} reasoning, guiding the LLM to infer poisoning indicators in the raw sensor data and plausible clean alternatives. These strategies minimize reliance on curation of extensive datasets and enable robust, adaptable defense mechanisms in real-time. We perform an extensive evaluation of the framework, quantifying detection accuracy, sanitization quality, latency, and communication cost, thus demonstrating the practicality and effectiveness of LLMs in improving the security and reliability of wearable IoT systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread integration of wearable sensing devices in Internet of Things (IoT) ecosystems, particularly in healthcare, smart homes, and industrial applications, has required robust human activity recognition (HAR) techniques to improve functionality and user experience. Although machine learning models have advanced HAR, they are increasingly susceptible to data poisoning attacks that compromise the data integrity and reliability of these systems. Conventional approaches to defending against such attacks often require extensive task-specific training with large, labeled datasets, which limits adaptability in dynamic IoT environments. This work proposes a novel framework that uses large language models (LLMs) to perform poisoning detection and sanitization in HAR systems, utilizing zero-shot, one-shot, and few-shot learning paradigms. Our approach incorporates \\textit{role play} prompting, whereby the LLM assumes the role of expert to contextualize and evaluate sensor anomalies, and \\textit{think step-by-step} reasoning, guiding the LLM to infer poisoning indicators in the raw sensor data and plausible clean alternatives. These strategies minimize reliance on curation of extensive datasets and enable robust, adaptable defense mechanisms in real-time. We perform an extensive evaluation of the framework, quantifying detection accuracy, sanitization quality, latency, and communication cost, thus demonstrating the practicality and effectiveness of LLMs in improving the security and reliability of wearable IoT systems."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-04T15:59:10Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    59,
                    10,
                    1,
                    308,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "W. K. M Mithsara"
                    },
                    {
                        "name": "Ning Yang"
                    },
                    {
                        "name": "Ahmed Imteaj"
                    },
                    {
                        "name": "Hussein Zangoti"
                    },
                    {
                        "name": "Abdur R. Shahid"
                    }
                ],
                "author_detail": {
                    "name": "Abdur R. Shahid"
                },
                "author": "Abdur R. Shahid"
            },
            {
                "id": "http://arxiv.org/abs/2511.16544v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.16544v2",
                "title": "WER is Unaware: Assessing How ASR Errors Distort Clinical Understanding in Patient Facing Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WER is Unaware: Assessing How ASR Errors Distort Clinical Understanding in Patient Facing Dialogue"
                },
                "updated": "2025-11-21T15:29:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    29,
                    43,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.16544v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.16544v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As Automatic Speech Recognition (ASR) is increasingly deployed in clinical dialogue, standard evaluations still rely heavily on Word Error Rate (WER). This paper challenges that standard, investigating whether WER or other common metrics correlate with the clinical impact of transcription errors. We establish a gold-standard benchmark by having expert clinicians compare ground-truth utterances to their ASR-generated counterparts, labeling the clinical impact of any discrepancies found in two distinct doctor-patient dialogue datasets. Our analysis reveals that WER and a comprehensive suite of existing metrics correlate poorly with the clinician-assigned risk labels (No, Minimal, or Significant Impact). To bridge this evaluation gap, we introduce an LLM-as-a-Judge, programmatically optimized using GEPA through DSPy to replicate expert clinical assessment. The optimized judge (Gemini-2.5-Pro) achieves human-comparable performance, obtaining 90% accuracy and a strong Cohen's $$ of 0.816. This work provides a validated, automated framework for moving ASR evaluation beyond simple textual fidelity to a necessary, scalable assessment of safety in clinical dialogue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Automatic Speech Recognition (ASR) is increasingly deployed in clinical dialogue, standard evaluations still rely heavily on Word Error Rate (WER). This paper challenges that standard, investigating whether WER or other common metrics correlate with the clinical impact of transcription errors. We establish a gold-standard benchmark by having expert clinicians compare ground-truth utterances to their ASR-generated counterparts, labeling the clinical impact of any discrepancies found in two distinct doctor-patient dialogue datasets. Our analysis reveals that WER and a comprehensive suite of existing metrics correlate poorly with the clinician-assigned risk labels (No, Minimal, or Significant Impact). To bridge this evaluation gap, we introduce an LLM-as-a-Judge, programmatically optimized using GEPA through DSPy to replicate expert clinical assessment. The optimized judge (Gemini-2.5-Pro) achieves human-comparable performance, obtaining 90% accuracy and a strong Cohen's $$ of 0.816. This work provides a validated, automated framework for moving ASR evaluation beyond simple textual fidelity to a necessary, scalable assessment of safety in clinical dialogue."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-20T16:59:20Z",
                "published_parsed": [
                    2025,
                    11,
                    20,
                    16,
                    59,
                    20,
                    3,
                    324,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zachary Ellis"
                    },
                    {
                        "name": "Jared Joselowitz"
                    },
                    {
                        "name": "Yash Deo"
                    },
                    {
                        "name": "Yajie He"
                    },
                    {
                        "name": "Anna Kalygina"
                    },
                    {
                        "name": "Aisling Higham"
                    },
                    {
                        "name": "Mana Rahimzadeh"
                    },
                    {
                        "name": "Yan Jia"
                    },
                    {
                        "name": "Ibrahim Habli"
                    },
                    {
                        "name": "Ernest Lim"
                    }
                ],
                "author_detail": {
                    "name": "Ernest Lim"
                },
                "author": "Ernest Lim"
            },
            {
                "id": "http://arxiv.org/abs/2511.17308v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17308v1",
                "title": "SpatialGeo:Boosting Spatial Reasoning in Multimodal LLMs via Geometry-Semantics Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpatialGeo:Boosting Spatial Reasoning in Multimodal LLMs via Geometry-Semantics Fusion"
                },
                "updated": "2025-11-21T15:24:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    24,
                    33,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17308v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multimodal large language models (MLLMs) have achieved significant progress in image and language tasks due to the strong reasoning capability of large language models (LLMs). Nevertheless, most MLLMs suffer from limited spatial reasoning ability to interpret and infer spatial arrangements in three-dimensional space. In this work, we propose a novel vision encoder based on hierarchical fusion of geometry and semantics features, generating spatial-aware visual embedding and boosting the spatial grounding capability of MLLMs. Specifically, we first unveil that the spatial ambiguity shortcoming stems from the lossy embedding of the vision encoder utilized in most existing MLLMs (e.g., CLIP), restricted to instance-level semantic features. This motivates us to complement CLIP with the geometry features from vision-only self-supervised learning via a hierarchical adapter, enhancing the spatial awareness in the proposed SpatialGeo. The network is efficiently trained using pretrained LLaVA model and optimized with random feature dropping to avoid trivial solutions relying solely on the CLIP encoder. Experimental results show that SpatialGeo improves the accuracy in spatial reasoning tasks, enhancing state-of-the-art models by at least 8.0% in SpatialRGPT-Bench with approximately 50% less memory cost during inference. The source code is available via https://ricky-plus.github.io/SpatialGeoPages/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have achieved significant progress in image and language tasks due to the strong reasoning capability of large language models (LLMs). Nevertheless, most MLLMs suffer from limited spatial reasoning ability to interpret and infer spatial arrangements in three-dimensional space. In this work, we propose a novel vision encoder based on hierarchical fusion of geometry and semantics features, generating spatial-aware visual embedding and boosting the spatial grounding capability of MLLMs. Specifically, we first unveil that the spatial ambiguity shortcoming stems from the lossy embedding of the vision encoder utilized in most existing MLLMs (e.g., CLIP), restricted to instance-level semantic features. This motivates us to complement CLIP with the geometry features from vision-only self-supervised learning via a hierarchical adapter, enhancing the spatial awareness in the proposed SpatialGeo. The network is efficiently trained using pretrained LLaVA model and optimized with random feature dropping to avoid trivial solutions relying solely on the CLIP encoder. Experimental results show that SpatialGeo improves the accuracy in spatial reasoning tasks, enhancing state-of-the-art models by at least 8.0% in SpatialRGPT-Bench with approximately 50% less memory cost during inference. The source code is available via https://ricky-plus.github.io/SpatialGeoPages/."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T15:24:33Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    24,
                    33,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jiajie Guo"
                    },
                    {
                        "name": "Qingpeng Zhu"
                    },
                    {
                        "name": "Jin Zeng"
                    },
                    {
                        "name": "Xiaolong Wu"
                    },
                    {
                        "name": "Changyong He"
                    },
                    {
                        "name": "Weida Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weida Wang"
                },
                "author": "Weida Wang"
            },
            {
                "id": "http://arxiv.org/abs/2501.16852v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2501.16852v2",
                "title": "Generalized framework for likelihood-based field-level inference of growth rate from velocity and density fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized framework for likelihood-based field-level inference of growth rate from velocity and density fields"
                },
                "updated": "2025-11-21T15:16:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    16,
                    36,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2501.16852v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2501.16852v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1051/0004-6361/202554319",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Measuring the growth rate of large-scale structures (f) as a function of redshift has the potential to break degeneracies between modified gravity and dark energy models, when combined with expansion-rate probes. Direct estimates of peculiar velocities of galaxies have attracted interest as a means of estimating $f_8$. In particular, field-level methods can be used to fit the field nuisance parameter along with cosmological parameters simultaneously. This article aims to provide the community with a unified framework for the theoretical modeling of the likelihood-based field-level inference by performing fast field covariance calculations for velocity and density fields. Our purpose is to lay the foundations for a nonlinear extension of the likelihood-based method at the field level. We have developed a generalized framework, implemented in the dedicated software flip to perform a likelihood-based inference of $f_8$. We derived a new field covariance model, which includes wide-angle corrections. We also included the models previously described in the literature inside our framework. We compared their performance against ours, and we validated our model by comparing it with the two-point statistics of a recent N-body simulation. The tests we performed have allowed us to validate our software and determine the appropriate wavenumber range to integrate our covariance model and its validity in terms of separation. Our framework allows for a wider wavenumber coverage to be used in our calculations than in previous works. Finally, our generalized framework allows us to efficiently perform a survey geometry-dependent Fisher forecast of the $f_8$ parameter. We show that the Fisher forecast method we developed gives an error bar that is 30 % closer to a full likelihood-based estimation than a standard volume Fisher forecast.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring the growth rate of large-scale structures (f) as a function of redshift has the potential to break degeneracies between modified gravity and dark energy models, when combined with expansion-rate probes. Direct estimates of peculiar velocities of galaxies have attracted interest as a means of estimating $f_8$. In particular, field-level methods can be used to fit the field nuisance parameter along with cosmological parameters simultaneously. This article aims to provide the community with a unified framework for the theoretical modeling of the likelihood-based field-level inference by performing fast field covariance calculations for velocity and density fields. Our purpose is to lay the foundations for a nonlinear extension of the likelihood-based method at the field level. We have developed a generalized framework, implemented in the dedicated software flip to perform a likelihood-based inference of $f_8$. We derived a new field covariance model, which includes wide-angle corrections. We also included the models previously described in the literature inside our framework. We compared their performance against ours, and we validated our model by comparing it with the two-point statistics of a recent N-body simulation. The tests we performed have allowed us to validate our software and determine the appropriate wavenumber range to integrate our covariance model and its validity in terms of separation. Our framework allows for a wider wavenumber coverage to be used in our calculations than in previous works. Finally, our generalized framework allows us to efficiently perform a survey geometry-dependent Fisher forecast of the $f_8$ parameter. We show that the Fisher forecast method we developed gives an error bar that is 30 % closer to a full likelihood-based estimation than a standard volume Fisher forecast."
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-01-28T10:58:54Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    10,
                    58,
                    54,
                    1,
                    28,
                    0
                ],
                "arxiv_comment": "21 pages, 13 figures",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO"
                },
                "arxiv_journal_ref": "A&A 698, A273 (2025)",
                "authors": [
                    {
                        "name": "Corentin Ravoux"
                    },
                    {
                        "name": "Bastien Carreres"
                    },
                    {
                        "name": "Damiano Rosselli"
                    },
                    {
                        "name": "Julian Bautista"
                    },
                    {
                        "name": "Anthony Carr"
                    },
                    {
                        "name": "Tyann Dummerchat"
                    },
                    {
                        "name": "Alex G. Kim"
                    },
                    {
                        "name": "David Parkinson"
                    },
                    {
                        "name": "Benjamin Racine"
                    },
                    {
                        "name": "Dominique Fouchez"
                    },
                    {
                        "name": "Fabrice Feinstein"
                    }
                ],
                "author_detail": {
                    "name": "Fabrice Feinstein"
                },
                "author": "Fabrice Feinstein",
                "arxiv_doi": "10.1051/0004-6361/202554319"
            },
            {
                "id": "http://arxiv.org/abs/2511.17302v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17302v1",
                "title": "Covariate Connectivity Combined Clustering for Weighted Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Covariate Connectivity Combined Clustering for Weighted Networks"
                },
                "updated": "2025-11-21T15:16:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    16,
                    33,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17302v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Community detection is a central task in network analysis, with applications in social, biological, and technological systems. Traditional algorithms rely primarily on network topology, which can fail when community signals are partly encoded in node-specific attributes. Existing covariate-assisted methods often assume the number of clusters is known, involve computationally intensive inference, or are not designed for weighted networks. We propose $\\text{C}^4$: Covariate Connectivity Combined Clustering, an adaptive spectral clustering algorithm that integrates network connectivity and node-level covariates into a unified similarity representation. $\\text{C}^4$ balances the two sources of information through a data-driven tuning parameter, estimates the number of communities via an eigengap heuristic, and avoids reliance on costly sampling-based procedures. Simulation studies show that $\\text{C}^4$ achieves higher accuracy and robustness than competing approaches across diverse scenarios. Application to an airport reachability network demonstrates the method's scalability, interpretability, and practical utility for real-world weighted networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Community detection is a central task in network analysis, with applications in social, biological, and technological systems. Traditional algorithms rely primarily on network topology, which can fail when community signals are partly encoded in node-specific attributes. Existing covariate-assisted methods often assume the number of clusters is known, involve computationally intensive inference, or are not designed for weighted networks. We propose $\\text{C}^4$: Covariate Connectivity Combined Clustering, an adaptive spectral clustering algorithm that integrates network connectivity and node-level covariates into a unified similarity representation. $\\text{C}^4$ balances the two sources of information through a data-driven tuning parameter, estimates the number of communities via an eigengap heuristic, and avoids reliance on costly sampling-based procedures. Simulation studies show that $\\text{C}^4$ achieves higher accuracy and robustness than competing approaches across diverse scenarios. Application to an airport reachability network demonstrates the method's scalability, interpretability, and practical utility for real-world weighted networks."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T15:16:33Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    16,
                    33,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Zeyu Hu"
                    },
                    {
                        "name": "Wenrui Li"
                    },
                    {
                        "name": "Jun Yan"
                    },
                    {
                        "name": "Panpan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Panpan Zhang"
                },
                "author": "Panpan Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.17301v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17301v1",
                "title": "Large Language Models for Sentiment Analysis to Detect Social Challenges: A Use Case with South African Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Sentiment Analysis to Detect Social Challenges: A Use Case with South African Languages"
                },
                "updated": "2025-11-21T15:14:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    14,
                    32,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17301v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Sentiment analysis can aid in understanding people's opinions and emotions on social issues. In multilingual communities sentiment analysis systems can be used to quickly identify social challenges in social media posts, enabling government departments to detect and address these issues more precisely and effectively. Recently, large-language models (LLMs) have become available to the wide public and initial analyses have shown that they exhibit magnificent zero-shot sentiment analysis abilities in English. However, there is no work that has investigated to leverage LLMs for sentiment analysis on social media posts in South African languages and detect social challenges. Consequently, in this work, we analyse the zero-shot performance of the state-of-the-art LLMs GPT-3.5, GPT-4, LlaMa 2, PaLM 2, and Dolly 2 to investigate the sentiment polarities of the 10 most emerging topics in English, Sepedi and Setswana social media posts that fall within the jurisdictional areas of 10 South African government departments. Our results demonstrate that there are big differences between the various LLMs, topics, and languages. In addition, we show that a fusion of the outcomes of different LLMs provides large gains in sentiment classification performance with sentiment classification errors below 1%. Consequently, it is now feasible to provide systems that generate reliable information about sentiment analysis to detect social challenges and draw conclusions about possible needs for actions on specific topics and within different language groups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sentiment analysis can aid in understanding people's opinions and emotions on social issues. In multilingual communities sentiment analysis systems can be used to quickly identify social challenges in social media posts, enabling government departments to detect and address these issues more precisely and effectively. Recently, large-language models (LLMs) have become available to the wide public and initial analyses have shown that they exhibit magnificent zero-shot sentiment analysis abilities in English. However, there is no work that has investigated to leverage LLMs for sentiment analysis on social media posts in South African languages and detect social challenges. Consequently, in this work, we analyse the zero-shot performance of the state-of-the-art LLMs GPT-3.5, GPT-4, LlaMa 2, PaLM 2, and Dolly 2 to investigate the sentiment polarities of the 10 most emerging topics in English, Sepedi and Setswana social media posts that fall within the jurisdictional areas of 10 South African government departments. Our results demonstrate that there are big differences between the various LLMs, topics, and languages. In addition, we show that a fusion of the outcomes of different LLMs provides large gains in sentiment classification performance with sentiment classification errors below 1%. Consequently, it is now feasible to provide systems that generate reliable information about sentiment analysis to detect social challenges and draw conclusions about possible needs for actions on specific topics and within different language groups."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T15:14:32Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    14,
                    32,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Published in the Proceedings of The Southern African Conference on AI Research (SACAIR 2024), Bloemfontein, South Africa, 2-6 December 2024. ISBN: 978-0-7961-6069-0",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Koena Ronny Mabokela"
                    },
                    {
                        "name": "Tim Schlippe"
                    },
                    {
                        "name": "Matthias Wlfel"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Wlfel"
                },
                "author": "Matthias Wlfel"
            },
            {
                "id": "http://arxiv.org/abs/2511.17300v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17300v1",
                "title": "MolSight: Optical Chemical Structure Recognition with SMILES Pretraining, Multi-Granularity Learning and Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MolSight: Optical Chemical Structure Recognition with SMILES Pretraining, Multi-Granularity Learning and Reinforcement Learning"
                },
                "updated": "2025-11-21T15:11:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    11,
                    47,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17300v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Optical Chemical Structure Recognition (OCSR) plays a pivotal role in modern chemical informatics, enabling the automated conversion of chemical structure images from scientific literature, patents, and educational materials into machine-readable molecular representations. This capability is essential for large-scale chemical data mining, drug discovery pipelines, and Large Language Model (LLM) applications in related domains. However, existing OCSR systems face significant challenges in accurately recognizing stereochemical information due to the subtle visual cues that distinguish stereoisomers, such as wedge and dash bonds, ring conformations, and spatial arrangements. To address these challenges, we propose MolSight, a comprehensive learning framework for OCSR that employs a three-stage training paradigm. In the first stage, we conduct pre-training on large-scale but noisy datasets to endow the model with fundamental perception capabilities for chemical structure images. In the second stage, we perform multi-granularity fine-tuning using datasets with richer supervisory signals, systematically exploring how auxiliary tasks-specifically chemical bond classification and atom localization-contribute to molecular formula recognition. Finally, we employ reinforcement learning for post-training optimization and introduce a novel stereochemical structure dataset. Remarkably, we find that even with MolSight's relatively compact parameter size, the Group Relative Policy Optimization (GRPO) algorithm can further enhance the model's performance on stereomolecular. Through extensive experiments across diverse datasets, our results demonstrate that MolSight achieves state-of-the-art performance in (stereo)chemical optical structure recognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optical Chemical Structure Recognition (OCSR) plays a pivotal role in modern chemical informatics, enabling the automated conversion of chemical structure images from scientific literature, patents, and educational materials into machine-readable molecular representations. This capability is essential for large-scale chemical data mining, drug discovery pipelines, and Large Language Model (LLM) applications in related domains. However, existing OCSR systems face significant challenges in accurately recognizing stereochemical information due to the subtle visual cues that distinguish stereoisomers, such as wedge and dash bonds, ring conformations, and spatial arrangements. To address these challenges, we propose MolSight, a comprehensive learning framework for OCSR that employs a three-stage training paradigm. In the first stage, we conduct pre-training on large-scale but noisy datasets to endow the model with fundamental perception capabilities for chemical structure images. In the second stage, we perform multi-granularity fine-tuning using datasets with richer supervisory signals, systematically exploring how auxiliary tasks-specifically chemical bond classification and atom localization-contribute to molecular formula recognition. Finally, we employ reinforcement learning for post-training optimization and introduce a novel stereochemical structure dataset. Remarkably, we find that even with MolSight's relatively compact parameter size, the Group Relative Policy Optimization (GRPO) algorithm can further enhance the model's performance on stereomolecular. Through extensive experiments across diverse datasets, our results demonstrate that MolSight achieves state-of-the-art performance in (stereo)chemical optical structure recognition."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T15:11:47Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    11,
                    47,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Wenrui Zhang"
                    },
                    {
                        "name": "Xinggang Wang"
                    },
                    {
                        "name": "Bin Feng"
                    },
                    {
                        "name": "Wenyu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wenyu Liu"
                },
                "author": "Wenyu Liu"
            },
            {
                "id": "http://arxiv.org/abs/2511.17292v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17292v1",
                "title": "The Experimental Unit Information Index: Balancing Evidentiary Value and Sample Size of Adaptive Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Experimental Unit Information Index: Balancing Evidentiary Value and Sample Size of Adaptive Designs"
                },
                "updated": "2025-11-21T15:05:00Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    5,
                    0,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17292v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reducing the number of experimental units is one of the three pillars of the 3R principles (Replace, Reduce, Refine) in animal research. At the same time, statistical error rates need to be controlled to enable reliable inferences and decisions. This paper proposes a novel measure to quantify the evidentiary value of one experimental unit for a given study design. The experimental unit information index (EUII) is based on power, Type-I error and sample size, and has attractive interpretations both in terms of frequentist error rates and Bayesian posterior odds. We introduce the EUII in simple statistical test settings and show that its asymptotic value depends only on the assumed relative effect size under the alternative. We then extend the definition to adaptive designs where early stopping for efficacy or futility may cause reductions in sample size. Applications to group-sequential designs and a recently proposed adaptive statistical test procedure show the usefulness of the approach when the goal is to maximize the evidentiary value of one experimental unit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the number of experimental units is one of the three pillars of the 3R principles (Replace, Reduce, Refine) in animal research. At the same time, statistical error rates need to be controlled to enable reliable inferences and decisions. This paper proposes a novel measure to quantify the evidentiary value of one experimental unit for a given study design. The experimental unit information index (EUII) is based on power, Type-I error and sample size, and has attractive interpretations both in terms of frequentist error rates and Bayesian posterior odds. We introduce the EUII in simple statistical test settings and show that its asymptotic value depends only on the assumed relative effect size under the alternative. We then extend the definition to adaptive designs where early stopping for efficacy or futility may cause reductions in sample size. Applications to group-sequential designs and a recently proposed adaptive statistical test procedure show the usefulness of the approach when the goal is to maximize the evidentiary value of one experimental unit."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T15:05:00Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    5,
                    0,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "43 pages, 8 figures, 4 tables",
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Leonhard Held"
                    },
                    {
                        "name": "Fadoua Balabdaoui"
                    },
                    {
                        "name": "Samuel Pawel"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Pawel"
                },
                "author": "Samuel Pawel"
            },
            {
                "id": "http://arxiv.org/abs/2511.17290v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17290v1",
                "title": "Estonian WinoGrande Dataset: Comparative Analysis of LLM Performance on Human and Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estonian WinoGrande Dataset: Comparative Analysis of LLM Performance on Human and Machine Translation"
                },
                "updated": "2025-11-21T15:01:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    1,
                    57,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17290v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this paper, we present a localized and culturally adapted Estonian translation of the test set from the widely used commonsense reasoning benchmark, WinoGrande. We detail the translation and adaptation process carried out by translation specialists and evaluate the performance of both proprietary and open source models on the human translated benchmark. Additionally, we explore the feasibility of achieving high-quality machine translation by incorporating insights from the manual translation process into the design of a detailed prompt. This prompt is specifically tailored to address both the linguistic characteristics of Estonian and the unique translation challenges posed by the WinoGrande dataset. Our findings show that model performance on the human translated Estonian dataset is slightly lower than on the original English test set, while performance on machine-translated data is notably worse. Additionally, our experiments indicate that prompt engineering offers limited improvement in translation quality or model accuracy, and highlight the importance of involving language specialists in dataset translation and adaptation to ensure reliable and interpretable evaluations of language competency and reasoning in large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a localized and culturally adapted Estonian translation of the test set from the widely used commonsense reasoning benchmark, WinoGrande. We detail the translation and adaptation process carried out by translation specialists and evaluate the performance of both proprietary and open source models on the human translated benchmark. Additionally, we explore the feasibility of achieving high-quality machine translation by incorporating insights from the manual translation process into the design of a detailed prompt. This prompt is specifically tailored to address both the linguistic characteristics of Estonian and the unique translation challenges posed by the WinoGrande dataset. Our findings show that model performance on the human translated Estonian dataset is slightly lower than on the original English test set, while performance on machine-translated data is notably worse. Additionally, our experiments indicate that prompt engineering offers limited improvement in translation quality or model accuracy, and highlight the importance of involving language specialists in dataset translation and adaptation to ensure reliable and interpretable evaluations of language competency and reasoning in large language models."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T15:01:57Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    1,
                    57,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Preprint",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Marii Ojastu"
                    },
                    {
                        "name": "Hele-Andra Kuulmets"
                    },
                    {
                        "name": "Aleksei Dorkin"
                    },
                    {
                        "name": "Marika Borovikova"
                    },
                    {
                        "name": "Dage Srg"
                    },
                    {
                        "name": "Kairit Sirts"
                    }
                ],
                "author_detail": {
                    "name": "Kairit Sirts"
                },
                "author": "Kairit Sirts"
            },
            {
                "id": "http://arxiv.org/abs/2505.04736v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.04736v2",
                "title": "The promise and limits of LLMs in constructing proofs and hints for logic problems in intelligent tutoring systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promise and limits of LLMs in constructing proofs and hints for logic problems in intelligent tutoring systems"
                },
                "updated": "2025-11-21T15:00:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    0,
                    6,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.04736v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.04736v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Intelligent tutoring systems have demonstrated effectiveness in teaching formal propositional logic proofs, but their reliance on template-based explanations limits their ability to provide personalized student feedback. While large language models (LLMs) offer promising capabilities for dynamic feedback generation, they risk producing hallucinations or pedagogically unsound explanations. We evaluated the stepwise accuracy of LLMs in constructing multi-step symbolic logic proofs, comparing six prompting techniques across four state-of-the-art LLMs on 358 propositional logic problems. Results show that DeepSeek-V3 achieved superior performance up to 86.7% accuracy on stepwise proof construction and excelled particularly in simpler rules. We further used the best-performing LLM to generate explanatory hints for 1,050 unique student problem-solving states from a logic ITS and evaluated them on 4 criteria with both an LLM grader and human expert ratings on a 20% sample. Our analysis finds that LLM-generated hints were 75% accurate and rated highly by human evaluators on consistency and clarity, but did not perform as well explaining why the hint was provided or its larger context. Our results demonstrate that LLMs may be used to augment tutoring systems with logic tutoring hints, but require additional modifications to ensure accuracy and pedagogical appropriateness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent tutoring systems have demonstrated effectiveness in teaching formal propositional logic proofs, but their reliance on template-based explanations limits their ability to provide personalized student feedback. While large language models (LLMs) offer promising capabilities for dynamic feedback generation, they risk producing hallucinations or pedagogically unsound explanations. We evaluated the stepwise accuracy of LLMs in constructing multi-step symbolic logic proofs, comparing six prompting techniques across four state-of-the-art LLMs on 358 propositional logic problems. Results show that DeepSeek-V3 achieved superior performance up to 86.7% accuracy on stepwise proof construction and excelled particularly in simpler rules. We further used the best-performing LLM to generate explanatory hints for 1,050 unique student problem-solving states from a logic ITS and evaluated them on 4 criteria with both an LLM grader and human expert ratings on a 20% sample. Our analysis finds that LLM-generated hints were 75% accurate and rated highly by human evaluators on consistency and clarity, but did not perform as well explaining why the hint was provided or its larger context. Our results demonstrate that LLMs may be used to augment tutoring systems with logic tutoring hints, but require additional modifications to ensure accuracy and pedagogical appropriateness."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-07T18:48:23Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    18,
                    48,
                    23,
                    2,
                    127,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Sutapa Dey Tithi"
                    },
                    {
                        "name": "Arun Kumar Ramesh"
                    },
                    {
                        "name": "Clara DiMarco"
                    },
                    {
                        "name": "Xiaoyi Tian"
                    },
                    {
                        "name": "Nazia Alam"
                    },
                    {
                        "name": "Kimia Fazeli"
                    },
                    {
                        "name": "Tiffany Barnes"
                    }
                ],
                "author_detail": {
                    "name": "Tiffany Barnes"
                },
                "author": "Tiffany Barnes"
            },
            {
                "id": "http://arxiv.org/abs/2511.17282v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17282v1",
                "title": "Where Culture Fades: Revealing the Cultural Gap in Text-to-Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Where Culture Fades: Revealing the Cultural Gap in Text-to-Image Generation"
                },
                "updated": "2025-11-21T14:40:50Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    14,
                    40,
                    50,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17282v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multilingual text-to-image (T2I) models have advanced rapidly in terms of visual realism and semantic alignment, and are now widely utilized. Yet outputs vary across cultural contexts: because language carries cultural connotations, images synthesized from multilingual prompts should preserve cross-lingual cultural consistency. We conduct a comprehensive analysis showing that current T2I models often produce culturally neutral or English-biased results under multilingual prompts. Analyses of two representative models indicate that the issue stems not from missing cultural knowledge but from insufficient activation of culture-related representations. We propose a probing method that localizes culture-sensitive signals to a small set of neurons in a few fixed layers. Guided by this finding, we introduce two complementary alignment strategies: (1) inference-time cultural activation that amplifies the identified neurons without backbone fine-tuned; and (2) layer-targeted cultural enhancement that updates only culturally relevant layers. Experiments on our CultureBench demonstrate consistent improvements over strong baselines in cultural consistency while preserving fidelity and diversity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual text-to-image (T2I) models have advanced rapidly in terms of visual realism and semantic alignment, and are now widely utilized. Yet outputs vary across cultural contexts: because language carries cultural connotations, images synthesized from multilingual prompts should preserve cross-lingual cultural consistency. We conduct a comprehensive analysis showing that current T2I models often produce culturally neutral or English-biased results under multilingual prompts. Analyses of two representative models indicate that the issue stems not from missing cultural knowledge but from insufficient activation of culture-related representations. We propose a probing method that localizes culture-sensitive signals to a small set of neurons in a few fixed layers. Guided by this finding, we introduce two complementary alignment strategies: (1) inference-time cultural activation that amplifies the identified neurons without backbone fine-tuned; and (2) layer-targeted cultural enhancement that updates only culturally relevant layers. Experiments on our CultureBench demonstrate consistent improvements over strong baselines in cultural consistency while preserving fidelity and diversity."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T14:40:50Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    14,
                    40,
                    50,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Chuancheng Shi"
                    },
                    {
                        "name": "Shangze Li"
                    },
                    {
                        "name": "Shiming Guo"
                    },
                    {
                        "name": "Simiao Xie"
                    },
                    {
                        "name": "Wenhua Wu"
                    },
                    {
                        "name": "Jingtong Dou"
                    },
                    {
                        "name": "Chao Wu"
                    },
                    {
                        "name": "Canran Xiao"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Zifeng Cheng"
                    },
                    {
                        "name": "Fei Shen"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua"
            },
            {
                "id": "http://arxiv.org/abs/2508.10336v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.10336v2",
                "title": "Online selective conformal inference: adaptive scores, convergence rate and optimality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online selective conformal inference: adaptive scores, convergence rate and optimality"
                },
                "updated": "2025-11-21T14:37:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    14,
                    37,
                    9,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.10336v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.10336v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In a supervised online setting, quantifying uncertainty has been proposed in the seminal work of \\cite{gibbs2021adaptive}. For any given point-prediction algorithm, their method (ACI) produces a conformal prediction set with an average missed coverage getting close to a pre-specified level $$ for a long time horizon. We introduce an extended version of this algorithm, called OnlineSCI, allowing the user to additionally select times where such an inference should be made. OnlineSCI encompasses several prominent online selective tasks, such as building prediction intervals for extreme outcomes, classification with abstention, and online testing. While OnlineSCI controls the average missed coverage on the selected in an adversarial setting, our theoretical results also show that it controls the instantaneous error rate (IER) at the selected times, up to a non-asymptotical remainder term. Importantly, our theory covers the case where OnlineSCI updates the point-prediction algorithm at each time step, a property which we refer to as {\\it adaptive} capability. We show that the adaptive versions of OnlineSCI can convergence to an optimal solution and provide an explicit convergence rate in each of the aforementioned application cases, under specific mild conditions. Finally, the favorable behavior of OnlineSCI in practice is illustrated by numerical experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a supervised online setting, quantifying uncertainty has been proposed in the seminal work of \\cite{gibbs2021adaptive}. For any given point-prediction algorithm, their method (ACI) produces a conformal prediction set with an average missed coverage getting close to a pre-specified level $$ for a long time horizon. We introduce an extended version of this algorithm, called OnlineSCI, allowing the user to additionally select times where such an inference should be made. OnlineSCI encompasses several prominent online selective tasks, such as building prediction intervals for extreme outcomes, classification with abstention, and online testing. While OnlineSCI controls the average missed coverage on the selected in an adversarial setting, our theoretical results also show that it controls the instantaneous error rate (IER) at the selected times, up to a non-asymptotical remainder term. Importantly, our theory covers the case where OnlineSCI updates the point-prediction algorithm at each time step, a property which we refer to as {\\it adaptive} capability. We show that the adaptive versions of OnlineSCI can convergence to an optimal solution and provide an explicit convergence rate in each of the aforementioned application cases, under specific mild conditions. Finally, the favorable behavior of OnlineSCI in practice is illustrated by numerical experiments."
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-14T04:36:14Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    4,
                    36,
                    14,
                    3,
                    226,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "math.ST"
                },
                "authors": [
                    {
                        "name": "Pierre Humbert"
                    },
                    {
                        "name": "Ulysse Gazin"
                    },
                    {
                        "name": "Ruth Heller"
                    },
                    {
                        "name": "Etienne Roquain"
                    }
                ],
                "author_detail": {
                    "name": "Etienne Roquain"
                },
                "author": "Etienne Roquain"
            },
            {
                "id": "http://arxiv.org/abs/2509.14001v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.14001v4",
                "title": "MOCHA: Multi-modal Objects-aware Cross-arcHitecture Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOCHA: Multi-modal Objects-aware Cross-arcHitecture Alignment"
                },
                "updated": "2025-11-21T14:33:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    14,
                    33,
                    3,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.14001v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.14001v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Personalized object detection aims to adapt a general-purpose detector to recognize user-specific instances from only a few examples. Lightweight models often struggle in this setting due to their weak semantic priors, while large vision-language models (VLMs) offer strong object-level understanding but are too computationally demanding for real-time or on-device applications. We introduce MOCHA (Multi-modal Objects-aware Cross-arcHitecture Alignment), a distillation framework that transfers multimodal region-level knowledge from a frozen VLM teacher into a lightweight vision-only detector. MOCHA extracts fused visual and textual teacher's embeddings and uses them to guide student training through a dual-objective loss that enforces accurate local alignment and global relational consistency across regions. This process enables efficient transfer of semantics without the need for teacher modifications or textual input at inference. MOCHA consistently outperforms prior baselines across four personalized detection benchmarks under strict few-shot regimes, yielding a +10.1 average improvement, with minimal inference cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized object detection aims to adapt a general-purpose detector to recognize user-specific instances from only a few examples. Lightweight models often struggle in this setting due to their weak semantic priors, while large vision-language models (VLMs) offer strong object-level understanding but are too computationally demanding for real-time or on-device applications. We introduce MOCHA (Multi-modal Objects-aware Cross-arcHitecture Alignment), a distillation framework that transfers multimodal region-level knowledge from a frozen VLM teacher into a lightweight vision-only detector. MOCHA extracts fused visual and textual teacher's embeddings and uses them to guide student training through a dual-objective loss that enforces accurate local alignment and global relational consistency across regions. This process enables efficient transfer of semantics without the need for teacher modifications or textual input at inference. MOCHA consistently outperforms prior baselines across four personalized detection benchmarks under strict few-shot regimes, yielding a +10.1 average improvement, with minimal inference cost."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-17T14:13:20Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    13,
                    20,
                    2,
                    260,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Elena Camuffo"
                    },
                    {
                        "name": "Francesco Barbato"
                    },
                    {
                        "name": "Mete Ozay"
                    },
                    {
                        "name": "Simone Milani"
                    },
                    {
                        "name": "Umberto Michieli"
                    }
                ],
                "author_detail": {
                    "name": "Umberto Michieli"
                },
                "author": "Umberto Michieli"
            },
            {
                "id": "http://arxiv.org/abs/2508.01109v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.01109v2",
                "title": "Platonic Representations for Poverty Mapping: Unified Vision-Language Codes or Agent-Induced Novelty?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Platonic Representations for Poverty Mapping: Unified Vision-Language Codes or Agent-Induced Novelty?"
                },
                "updated": "2025-11-21T14:32:46Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    14,
                    32,
                    46,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.01109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.01109v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We investigate whether socio-economic indicators like household wealth leave recoverable imprints in satellite imagery (capturing physical features) and Internet-sourced text (reflecting historical/economic narratives). Using Demographic and Health Survey (DHS) data from African neighborhoods, we pair Landsat images with LLM-generated textual descriptions conditioned on location/year and text retrieved by an AI search agent from web sources. We develop a multimodal framework predicting household wealth (International Wealth Index) through five pipelines: (i) vision model on satellite images, (ii) LLM using only location/year, (iii) AI agent searching/synthesizing web text, (iv) joint image-text encoder, (v) ensemble of all signals. Our framework yields three contributions. First, fusing vision and agent/LLM text outperforms vision-only baselines in wealth prediction (e.g., R-squared of 0.77 vs. 0.63 on out-of-sample splits), with LLM-internal knowledge proving more effective than agent-retrieved text, improving robustness to out-of-country and out-of-time generalization. Second, we find partial representational convergence: fused embeddings from vision/language modalities correlate moderately (median cosine similarity of 0.60 after alignment), suggesting a shared latent code of material well-being while retaining complementary details, consistent with the Platonic Representation Hypothesis. Although LLM-only text outperforms agent-retrieved data, challenging our Agent-Induced Novelty Hypothesis, modest gains from combining agent data in some splits weakly support the notion that agent-gathered information introduces unique representational structures not fully captured by static LLM knowledge. Third, we release a large-scale multimodal dataset comprising more than 60,000 DHS clusters linked to satellite images, LLM-generated descriptions, and agent-retrieved texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate whether socio-economic indicators like household wealth leave recoverable imprints in satellite imagery (capturing physical features) and Internet-sourced text (reflecting historical/economic narratives). Using Demographic and Health Survey (DHS) data from African neighborhoods, we pair Landsat images with LLM-generated textual descriptions conditioned on location/year and text retrieved by an AI search agent from web sources. We develop a multimodal framework predicting household wealth (International Wealth Index) through five pipelines: (i) vision model on satellite images, (ii) LLM using only location/year, (iii) AI agent searching/synthesizing web text, (iv) joint image-text encoder, (v) ensemble of all signals. Our framework yields three contributions. First, fusing vision and agent/LLM text outperforms vision-only baselines in wealth prediction (e.g., R-squared of 0.77 vs. 0.63 on out-of-sample splits), with LLM-internal knowledge proving more effective than agent-retrieved text, improving robustness to out-of-country and out-of-time generalization. Second, we find partial representational convergence: fused embeddings from vision/language modalities correlate moderately (median cosine similarity of 0.60 after alignment), suggesting a shared latent code of material well-being while retaining complementary details, consistent with the Platonic Representation Hypothesis. Although LLM-only text outperforms agent-retrieved data, challenging our Agent-Induced Novelty Hypothesis, modest gains from combining agent data in some splits weakly support the notion that agent-gathered information introduces unique representational structures not fully captured by static LLM knowledge. Third, we release a large-scale multimodal dataset comprising more than 60,000 DHS clusters linked to satellite images, LLM-generated descriptions, and agent-retrieved texts."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-01T23:07:16Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    23,
                    7,
                    16,
                    4,
                    213,
                    0
                ],
                "arxiv_comment": "7 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Satiyabooshan Murugaboopathy"
                    },
                    {
                        "name": "Connor T. Jerzak"
                    },
                    {
                        "name": "Adel Daoud"
                    }
                ],
                "author_detail": {
                    "name": "Adel Daoud"
                },
                "author": "Adel Daoud"
            },
            {
                "id": "http://arxiv.org/abs/2510.13302v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.13302v2",
                "title": "LLM one-shot style transfer for Authorship Attribution and Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM one-shot style transfer for Authorship Attribution and Verification"
                },
                "updated": "2025-11-21T14:29:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    14,
                    29,
                    26,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.13302v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.13302v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Computational stylometry analyzes writing style through quantitative patterns in text, supporting applications from forensic tasks such as identity linking and plagiarism detection to literary attribution in the humanities. Supervised and contrastive approaches rely on data with spurious correlations and often confuse style with topic. Despite their natural use in AI-generated text detection, the CLM pre-training of modern LLMs has been scarcely leveraged for general authorship problems. We propose a novel unsupervised approach based on this extensive pre-training and the in-context learning capabilities of LLMs, employing the log-probabilities of an LLM to measure style transferability from one text to another. Our method significantly outperforms LLM prompting approaches of comparable scale and achieves higher accuracy than contrastively trained baselines when controlling for topical correlations. Moreover, performance scales fairly consistently with the size of the base model and, in the case of authorship verification, with an additional mechanism that increases test-time computation; enabling flexible trade-offs between computational cost and accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational stylometry analyzes writing style through quantitative patterns in text, supporting applications from forensic tasks such as identity linking and plagiarism detection to literary attribution in the humanities. Supervised and contrastive approaches rely on data with spurious correlations and often confuse style with topic. Despite their natural use in AI-generated text detection, the CLM pre-training of modern LLMs has been scarcely leveraged for general authorship problems. We propose a novel unsupervised approach based on this extensive pre-training and the in-context learning capabilities of LLMs, employing the log-probabilities of an LLM to measure style transferability from one text to another. Our method significantly outperforms LLM prompting approaches of comparable scale and achieves higher accuracy than contrastively trained baselines when controlling for topical correlations. Moreover, performance scales fairly consistently with the size of the base model and, in the case of authorship verification, with an additional mechanism that increases test-time computation; enabling flexible trade-offs between computational cost and accuracy."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-15T08:43:24Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    8,
                    43,
                    24,
                    2,
                    288,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-Gonzlez"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martn"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho"
            },
            {
                "id": "http://arxiv.org/abs/2511.15856v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.15856v2",
                "title": "GLOBE: Accurate and Generalizable PDE Surrogates using Domain-Inspired Architectures and Equivariances",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLOBE: Accurate and Generalizable PDE Surrogates using Domain-Inspired Architectures and Equivariances"
                },
                "updated": "2025-11-21T14:26:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    14,
                    26,
                    8,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.15856v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.15856v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce GLOBE, a new neural surrogate for homogeneous PDEs that draws inductive bias from boundary-element methods and equivariant ML. GLOBE represents solutions as superpositions of learnable Green's-function-like kernels evaluated from boundary faces to targets, composed across multiscale branches and communication hyperlayers. The architecture is translation-, rotation-, and parity-equivariant; discretization-invariant in the fine-mesh limit; and units-invariant via rigorous nondimensionalization. An explicit far-field decay envelope stabilizes extrapolation, boundary-to-boundary hyperlayer communication mediates long-range coupling, and the all-to-all boundary-to-target evaluation yields a global receptive field that respects PDE information flow, even for elliptic PDEs.\n  On AirFRANS (steady incompressible RANS over NACA airfoils), GLOBE achieves substantial accuracy improvements. On the \"Full\" split, it reduces mean-squared error by roughly 200x on all fields relative to the dataset's reference baselines, and roughly 50x relative to the next-best-performing model. In the \"Scarce\" split, it achieves over 100x lower error on velocity and pressure fields and over 600x lower error on surface pressure than Transolver. Qualitative results show sharp near-wall gradients, coherent wakes, and limited errors under modest extrapolation in Reynolds number and angle of attack.\n  In addition to this accuracy, the model is quite compact (117k parameters), and fields can be evaluated at arbitrary points during inference. We also demonstrate the ability to train and predict with non-watertight meshes, which has strong practical implications.\n  These results show that rigorous physics- and domain-inspired inductive biases can achieve large gains in accuracy, generalizability, and practicality for ML-based PDE surrogates for industrial computer-aided engineering (CAE).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce GLOBE, a new neural surrogate for homogeneous PDEs that draws inductive bias from boundary-element methods and equivariant ML. GLOBE represents solutions as superpositions of learnable Green's-function-like kernels evaluated from boundary faces to targets, composed across multiscale branches and communication hyperlayers. The architecture is translation-, rotation-, and parity-equivariant; discretization-invariant in the fine-mesh limit; and units-invariant via rigorous nondimensionalization. An explicit far-field decay envelope stabilizes extrapolation, boundary-to-boundary hyperlayer communication mediates long-range coupling, and the all-to-all boundary-to-target evaluation yields a global receptive field that respects PDE information flow, even for elliptic PDEs.\n  On AirFRANS (steady incompressible RANS over NACA airfoils), GLOBE achieves substantial accuracy improvements. On the \"Full\" split, it reduces mean-squared error by roughly 200x on all fields relative to the dataset's reference baselines, and roughly 50x relative to the next-best-performing model. In the \"Scarce\" split, it achieves over 100x lower error on velocity and pressure fields and over 600x lower error on surface pressure than Transolver. Qualitative results show sharp near-wall gradients, coherent wakes, and limited errors under modest extrapolation in Reynolds number and angle of attack.\n  In addition to this accuracy, the model is quite compact (117k parameters), and fields can be evaluated at arbitrary points during inference. We also demonstrate the ability to train and predict with non-watertight meshes, which has strong practical implications.\n  These results show that rigorous physics- and domain-inspired inductive biases can achieve large gains in accuracy, generalizability, and practicality for ML-based PDE surrogates for industrial computer-aided engineering (CAE)."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-19T20:23:51Z",
                "published_parsed": [
                    2025,
                    11,
                    19,
                    20,
                    23,
                    51,
                    2,
                    323,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Peter Sharpe"
                    }
                ],
                "author_detail": {
                    "name": "Peter Sharpe"
                },
                "author": "Peter Sharpe"
            },
            {
                "id": "http://arxiv.org/abs/2511.15127v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.15127v2",
                "title": "Divergence in tracing the flavors of astrophysical neutrinos: can JUNO help IceCube?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Divergence in tracing the flavors of astrophysical neutrinos: can JUNO help IceCube?"
                },
                "updated": "2025-11-21T14:18:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    14,
                    18,
                    26,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.15127v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.15127v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We derive the flavor fractions $(^{}_e , ^{}_, ^{}_)$ for ultrahigh-energy neutrinos originating from a remote astrophysical source by using their flavor ratios $(f^{}_e , f^{}_, f^{}_)$ observed at a neutrino telescope, and find that a potential divergence may appear in inferring the values of $^{}_$ and $^{}_$ as an unavoidable consequence of the approximate $$-$$ interchange symmetry of lepton flavor mixing. We illustrate such an effect by applying our formula to the IceCube all-sky neutrino flux data ranging from 5 TeV to 10 PeV in the naive assumption that the relevant sources have a common flavor composition. We show that only $^{}_e$ and $^{}_+ ^{}_$ can be extracted from the measurements of $f^{}_e$, $f^{}_$ and $f^{}_$ if the $$-$$ flavor symmetry is exact, and the JUNO and Daya Bay precision antineutrino oscillation data will help a lot in this connection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We derive the flavor fractions $(^{}_e , ^{}_, ^{}_)$ for ultrahigh-energy neutrinos originating from a remote astrophysical source by using their flavor ratios $(f^{}_e , f^{}_, f^{}_)$ observed at a neutrino telescope, and find that a potential divergence may appear in inferring the values of $^{}_$ and $^{}_$ as an unavoidable consequence of the approximate $$-$$ interchange symmetry of lepton flavor mixing. We illustrate such an effect by applying our formula to the IceCube all-sky neutrino flux data ranging from 5 TeV to 10 PeV in the naive assumption that the relevant sources have a common flavor composition. We show that only $^{}_e$ and $^{}_+ ^{}_$ can be extracted from the measurements of $f^{}_e$, $f^{}_$ and $f^{}_$ if the $$-$$ flavor symmetry is exact, and the JUNO and Daya Bay precision antineutrino oscillation data will help a lot in this connection."
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-19T05:07:57Z",
                "published_parsed": [
                    2025,
                    11,
                    19,
                    5,
                    7,
                    57,
                    2,
                    323,
                    0
                ],
                "arxiv_comment": "5 pages. More references added",
                "arxiv_primary_category": {
                    "term": "hep-ph"
                },
                "authors": [
                    {
                        "name": "Zhi-zhong Xing"
                    }
                ],
                "author_detail": {
                    "name": "Zhi-zhong Xing"
                },
                "author": "Zhi-zhong Xing"
            },
            {
                "id": "http://arxiv.org/abs/2511.17268v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17268v1",
                "title": "Far-infrared to centimeter emission of very nearby galaxies with archival data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Far-infrared to centimeter emission of very nearby galaxies with archival data"
                },
                "updated": "2025-11-21T14:13:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    14,
                    13,
                    51,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17268v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Compared to the well-studied infrared and radio domains, galaxy emission in the millimeter (mm) - centimeter (cm) range has been less observed. In this domain, galaxy emission consists of thermal dust, free-free and synchrotron emissions with a possible additional contribution from anomalous microwave emission (AME) peaking near 1 cm.The aim of this study is to accurately characterize the integrated spectral energy distribution (SED) of galaxies in the mm-cm range. We used COBE-DIRBE, IRAS, Planck, and WMAP all-sky surveys, brought to the same resolution of $1^\\circ$, to cover 18 photometric bands from 97$$m to 1.3 cm. Given the low angular resolution and mixing with foreground and background emission that hampers the detection of the galaxy, our sample consists of 6 of the brightest, nearby galaxies: LMC, SMC, M31, M33, NGC 253 and NGC 4945. We subtract Milky Way dust emission, distant unresolved galaxies, and foreground point sources in the fields. We fit each integrated SED with a model of thermal dust, free-free, synchrotron, AME and Cosmic Microwave Background (CMB) temperature fluctuations. The integrated SEDs of our sample of galaxies are well fitted by the model within the uncertainties, although degeneracies between the different components contributing to the mm-cm emission complicate the estimation of their individual contributions. We do not clearly detect AME in any of our target galaxies, and AME emissivity upper limits are weak compared to Galactic standards, suggesting that the signal of AME might be diluted at the scale of a whole galaxy. We infer positive CMB fluctuations in the background of 5 out of our 6 galaxies. This effect might be related to the degeneracy between the dust emissivity index and CMB fluctuations in the background, or linked to the specific spatial distribution of CMB fluctuations coupled with the low resolution and small number statistics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compared to the well-studied infrared and radio domains, galaxy emission in the millimeter (mm) - centimeter (cm) range has been less observed. In this domain, galaxy emission consists of thermal dust, free-free and synchrotron emissions with a possible additional contribution from anomalous microwave emission (AME) peaking near 1 cm.The aim of this study is to accurately characterize the integrated spectral energy distribution (SED) of galaxies in the mm-cm range. We used COBE-DIRBE, IRAS, Planck, and WMAP all-sky surveys, brought to the same resolution of $1^\\circ$, to cover 18 photometric bands from 97$$m to 1.3 cm. Given the low angular resolution and mixing with foreground and background emission that hampers the detection of the galaxy, our sample consists of 6 of the brightest, nearby galaxies: LMC, SMC, M31, M33, NGC 253 and NGC 4945. We subtract Milky Way dust emission, distant unresolved galaxies, and foreground point sources in the fields. We fit each integrated SED with a model of thermal dust, free-free, synchrotron, AME and Cosmic Microwave Background (CMB) temperature fluctuations. The integrated SEDs of our sample of galaxies are well fitted by the model within the uncertainties, although degeneracies between the different components contributing to the mm-cm emission complicate the estimation of their individual contributions. We do not clearly detect AME in any of our target galaxies, and AME emissivity upper limits are weak compared to Galactic standards, suggesting that the signal of AME might be diluted at the scale of a whole galaxy. We infer positive CMB fluctuations in the background of 5 out of our 6 galaxies. This effect might be related to the degeneracy between the dust emissivity index and CMB fluctuations in the background, or linked to the specific spatial distribution of CMB fluctuations coupled with the low resolution and small number statistics."
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T14:13:51Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    14,
                    13,
                    51,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA"
                },
                "authors": [
                    {
                        "name": "L. Correia"
                    },
                    {
                        "name": "C. Bot"
                    },
                    {
                        "name": "J. Chastenet"
                    },
                    {
                        "name": "A. Rymar"
                    },
                    {
                        "name": "R. Paladini"
                    },
                    {
                        "name": "M. Bethermin"
                    },
                    {
                        "name": "D. Ismail"
                    },
                    {
                        "name": "K. A. Lutz"
                    },
                    {
                        "name": "J. -P. Bernard"
                    },
                    {
                        "name": "A. Hughes"
                    },
                    {
                        "name": "D. Paradis"
                    },
                    {
                        "name": "N. Ysard"
                    }
                ],
                "author_detail": {
                    "name": "N. Ysard"
                },
                "author": "N. Ysard"
            },
            {
                "id": "http://arxiv.org/abs/2511.13182v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13182v3",
                "title": "Evaluating Large Language Models for Diacritic Restoration in Romanian Texts: A Comparative Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models for Diacritic Restoration in Romanian Texts: A Comparative Study"
                },
                "updated": "2025-11-21T14:13:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    14,
                    13,
                    16,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13182v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13182v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Automatic diacritic restoration is crucial for text processing in languages with rich diacritical marks, such as Romanian. This study evaluates the performance of several large language models (LLMs) in restoring diacritics in Romanian texts. Using a comprehensive corpus, we tested models including OpenAI's GPT-3.5, GPT-4, GPT-4o, Google's Gemini 1.0 Pro, Meta's Llama 2 and Llama 3, MistralAI's Mixtral 8x7B Instruct, airoboros 70B, and OpenLLM-Ro's RoLlama 2 7B, under multiple prompt templates ranging from zero-shot to complex multi-shot instructions. Results show that models such as GPT-4o achieve high diacritic restoration accuracy, consistently surpassing a neutral echo baseline, while others, including Meta's Llama family, exhibit wider variability. These findings highlight the impact of model architecture, training data, and prompt design on diacritic restoration performance and outline promising directions for improving NLP tools for diacritic-rich languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic diacritic restoration is crucial for text processing in languages with rich diacritical marks, such as Romanian. This study evaluates the performance of several large language models (LLMs) in restoring diacritics in Romanian texts. Using a comprehensive corpus, we tested models including OpenAI's GPT-3.5, GPT-4, GPT-4o, Google's Gemini 1.0 Pro, Meta's Llama 2 and Llama 3, MistralAI's Mixtral 8x7B Instruct, airoboros 70B, and OpenLLM-Ro's RoLlama 2 7B, under multiple prompt templates ranging from zero-shot to complex multi-shot instructions. Results show that models such as GPT-4o achieve high diacritic restoration accuracy, consistently surpassing a neutral echo baseline, while others, including Meta's Llama family, exhibit wider variability. These findings highlight the impact of model architecture, training data, and prompt design on diacritic restoration performance and outline promising directions for improving NLP tools for diacritic-rich languages."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T09:43:54Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    9,
                    43,
                    54,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "The original submission contained metadata errors and requires correction. A revised and complete version will be submitted as a replacement",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Mihai Nadas"
                    },
                    {
                        "name": "Laura Diosan"
                    }
                ],
                "author_detail": {
                    "name": "Laura Diosan"
                },
                "author": "Laura Diosan"
            },
            {
                "id": "http://arxiv.org/abs/2511.17262v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17262v1",
                "title": "SlsReuse: LLM-Powered Serverless Function Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlsReuse: LLM-Powered Serverless Function Reuse"
                },
                "updated": "2025-11-21T14:08:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    14,
                    8,
                    12,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17262v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Serverless computing has rapidly emerged as a popular cloud computing paradigm. It enables developers to implement function-level tasks, i.e., serverless functions, without managing infrastructure. While reducing operational overhead, it poses challenges, especially for novice developers. Developing functions from scratch requires adapting to heterogeneous, platform-specific programming styles, making the process time-consuming and error-prone. Function reuse offers a promising solution to address these challenges. However, research on serverless computing lacks a dedicated approach for function recommendation. Existing techniques from traditional contexts remain insufficient due to the semantic gap between task descriptions and heterogeneous function implementations. Advances in large language models (LLMs), pre-trained on large-scale corpora, create opportunities to bridge this gap by aligning developer requirements with function semantics.\n  This paper presents SlsReuse, the first LLM-powered framework for serverless function reuse. Specifically, SlsReuse first constructs a reusable function repository serving as a foundational knowledge base. Then, it learns unified semantic-enhanced representations of heterogeneous functions through effective prompt engineering with few-shot prompting, capturing implicit code intent, target platforms, programming languages, and cloud services. Finally, given a natural language task query, SlsReuse performs intent-aware discovery combined with a multi-level pruning strategy and similarity matching. We evaluate SlsReuse on a curated dataset of 110 task queries. Built on ChatGPT-4o, one of the most representative LLMs, SlsReuse achieves Recall@10 of 91.20%, exceeding the state-of-the-art baseline by 24.53 percentage points.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing has rapidly emerged as a popular cloud computing paradigm. It enables developers to implement function-level tasks, i.e., serverless functions, without managing infrastructure. While reducing operational overhead, it poses challenges, especially for novice developers. Developing functions from scratch requires adapting to heterogeneous, platform-specific programming styles, making the process time-consuming and error-prone. Function reuse offers a promising solution to address these challenges. However, research on serverless computing lacks a dedicated approach for function recommendation. Existing techniques from traditional contexts remain insufficient due to the semantic gap between task descriptions and heterogeneous function implementations. Advances in large language models (LLMs), pre-trained on large-scale corpora, create opportunities to bridge this gap by aligning developer requirements with function semantics.\n  This paper presents SlsReuse, the first LLM-powered framework for serverless function reuse. Specifically, SlsReuse first constructs a reusable function repository serving as a foundational knowledge base. Then, it learns unified semantic-enhanced representations of heterogeneous functions through effective prompt engineering with few-shot prompting, capturing implicit code intent, target platforms, programming languages, and cloud services. Finally, given a natural language task query, SlsReuse performs intent-aware discovery combined with a multi-level pruning strategy and similarity matching. We evaluate SlsReuse on a curated dataset of 110 task queries. Built on ChatGPT-4o, one of the most representative LLMs, SlsReuse achieves Recall@10 of 91.20%, exceeding the state-of-the-art baseline by 24.53 percentage points."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T14:08:12Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    14,
                    8,
                    12,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Jinfeng Wen"
                    },
                    {
                        "name": "Yuehan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yuehan Sun"
                },
                "author": "Yuehan Sun"
            },
            {
                "id": "http://arxiv.org/abs/2511.17256v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17256v1",
                "title": "Cross-cultural value alignment frameworks for responsible AI governance: Evidence from China-West comparative analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-cultural value alignment frameworks for responsible AI governance: Evidence from China-West comparative analysis"
                },
                "updated": "2025-11-21T14:02:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    14,
                    2,
                    33,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17256v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17256v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As Large Language Models (LLMs) increasingly influence high-stakes decision-making across global contexts, ensuring their alignment with diverse cultural values has become a critical governance challenge. This study presents a Multi-Layered Auditing Platform for Responsible AI that systematically evaluates cross-cultural value alignment in China-origin and Western-origin LLMs through four integrated methodologies: Ethical Dilemma Corpus for assessing temporal stability, Diversity-Enhanced Framework (DEF) for quantifying cultural fidelity, First-Token Probability Alignment for distributional accuracy, and Multi-stAge Reasoning frameworK (MARK) for interpretable decision-making. Our comparative analysis of 20+ leading models, such as Qwen, GPT-4o, Claude, LLaMA, and DeepSeek, reveals universal challenges-fundamental instability in value systems, systematic under-representation of younger demographics, and non-linear relationships between model scale and alignment quality-alongside divergent regional development trajectories. While China-origin models increasingly emphasize multilingual data integration for context-specific optimization, Western models demonstrate greater architectural experimentation but persistent U.S.-centric biases. Neither paradigm achieves robust cross-cultural generalization. We establish that Mistral-series architectures significantly outperform LLaMA3-series in cross-cultural alignment, and that Full-Parameter Fine-Tuning on diverse datasets surpasses Reinforcement Learning from Human Feedback in preserving cultural variation...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) increasingly influence high-stakes decision-making across global contexts, ensuring their alignment with diverse cultural values has become a critical governance challenge. This study presents a Multi-Layered Auditing Platform for Responsible AI that systematically evaluates cross-cultural value alignment in China-origin and Western-origin LLMs through four integrated methodologies: Ethical Dilemma Corpus for assessing temporal stability, Diversity-Enhanced Framework (DEF) for quantifying cultural fidelity, First-Token Probability Alignment for distributional accuracy, and Multi-stAge Reasoning frameworK (MARK) for interpretable decision-making. Our comparative analysis of 20+ leading models, such as Qwen, GPT-4o, Claude, LLaMA, and DeepSeek, reveals universal challenges-fundamental instability in value systems, systematic under-representation of younger demographics, and non-linear relationships between model scale and alignment quality-alongside divergent regional development trajectories. While China-origin models increasingly emphasize multilingual data integration for context-specific optimization, Western models demonstrate greater architectural experimentation but persistent U.S.-centric biases. Neither paradigm achieves robust cross-cultural generalization. We establish that Mistral-series architectures significantly outperform LLaMA3-series in cross-cultural alignment, and that Full-Parameter Fine-Tuning on diverse datasets surpasses Reinforcement Learning from Human Feedback in preserving cultural variation..."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T14:02:33Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    14,
                    2,
                    33,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Presented on Academic Conference \"Technology for Good: Driving Social Impact\" (2025)",
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Haijiang Liu"
                    },
                    {
                        "name": "Jinguang Gu"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Daniel Hershcovich"
                    },
                    {
                        "name": "Qiaoling Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Qiaoling Xiao"
                },
                "author": "Qiaoling Xiao"
            },
            {
                "id": "http://arxiv.org/abs/2511.17255v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17255v1",
                "title": "A Little More Like This: Text-to-Image Retrieval with Vision-Language Models Using Relevance Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Little More Like This: Text-to-Image Retrieval with Vision-Language Models Using Relevance Feedback"
                },
                "updated": "2025-11-21T14:01:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    14,
                    1,
                    36,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17255v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large vision-language models (VLMs) enable intuitive visual search using natural language queries. However, improving their performance often requires fine-tuning and scaling to larger model variants. In this work, we propose a mechanism inspired by traditional text-based search to improve retrieval performance at inference time: relevance feedback. While relevance feedback can serve as an alternative to fine-tuning, its model-agnostic design also enables use with fine-tuned VLMs. Specifically, we introduce and evaluate four feedback strategies for VLM-based retrieval. First, we revise classical pseudo-relevance feedback (PRF), which refines query embeddings based on top-ranked results. To address its limitations, we propose generative relevance feedback (GRF), which uses synthetic captions for query refinement. Furthermore, we introduce an attentive feedback summarizer (AFS), a custom transformer-based model that integrates multimodal fine-grained features from relevant items. Finally, we simulate explicit feedback using ground-truth captions as an upper-bound baseline. Experiments on Flickr30k and COCO with the VLM backbones show that GRF, AFS, and explicit feedback improve retrieval performance by 3-5% in MRR@5 for smaller VLMs, and 1-3% for larger ones, compared to retrieval with no feedback. Moreover, AFS, similarly to explicit feedback, mitigates query drift and is more robust than GRF in iterative, multi-turn retrieval settings. Our findings demonstrate that relevance feedback can consistently enhance retrieval across VLMs and open up opportunities for interactive and adaptive visual search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (VLMs) enable intuitive visual search using natural language queries. However, improving their performance often requires fine-tuning and scaling to larger model variants. In this work, we propose a mechanism inspired by traditional text-based search to improve retrieval performance at inference time: relevance feedback. While relevance feedback can serve as an alternative to fine-tuning, its model-agnostic design also enables use with fine-tuned VLMs. Specifically, we introduce and evaluate four feedback strategies for VLM-based retrieval. First, we revise classical pseudo-relevance feedback (PRF), which refines query embeddings based on top-ranked results. To address its limitations, we propose generative relevance feedback (GRF), which uses synthetic captions for query refinement. Furthermore, we introduce an attentive feedback summarizer (AFS), a custom transformer-based model that integrates multimodal fine-grained features from relevant items. Finally, we simulate explicit feedback using ground-truth captions as an upper-bound baseline. Experiments on Flickr30k and COCO with the VLM backbones show that GRF, AFS, and explicit feedback improve retrieval performance by 3-5% in MRR@5 for smaller VLMs, and 1-3% for larger ones, compared to retrieval with no feedback. Moreover, AFS, similarly to explicit feedback, mitigates query drift and is more robust than GRF in iterative, multi-turn retrieval settings. Our findings demonstrate that relevance feedback can consistently enhance retrieval across VLMs and open up opportunities for interactive and adaptive visual search."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T14:01:36Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    14,
                    1,
                    36,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Accepted to WACV'26",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Bulat Khaertdinov"
                    },
                    {
                        "name": "Mirela Popa"
                    },
                    {
                        "name": "Nava Tintarev"
                    }
                ],
                "author_detail": {
                    "name": "Nava Tintarev"
                },
                "author": "Nava Tintarev"
            },
            {
                "id": "http://arxiv.org/abs/2511.16628v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.16628v2",
                "title": "Sensor Informativeness, Identifiability, and Uncertainty in Bayesian Inverse Problems for Structural Health Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensor Informativeness, Identifiability, and Uncertainty in Bayesian Inverse Problems for Structural Health Monitoring"
                },
                "updated": "2025-11-21T13:51:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    13,
                    51,
                    45,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.16628v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.16628v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In Structural Health Monitoring (SHM), the recovery of distributed mechanical parameters from sparse data is often ill-posed, raising critical questions about identifiability and the reliability of inferred states. While deterministic regularization methods such as Tikhonov stabilise the inversion, they provide little insight into the spatial limits of resolution or the inherent uncertainty of the solution. This paper presents a Bayesian inverse framework that rigorously quantifies these limits, using the identification of distributed flexural rigidity from rotation (tilt) influence lines as a primary case study. Fisher information is employed as a diagnostic metric to quantify sensor informativeness, revealing how specific sensor layouts and load paths constrain the recoverable spatial features of the parameter field.\n  The methodology is applied to the full-scale openLAB research bridge (TU Dresden) using data from controlled vehicle passages. Beyond estimating the flexural rigidity profile, the Bayesian formulation produces credible intervals that expose regions of practical non-identifiability, which deterministic methods may obscure. The results demonstrate that while the measurement data carry high information content for the target parameters, their utility is spatially heterogeneous and strictly bounded by the experiment design. The proposed framework unifies identification with uncertainty quantification, providing a rigorous basis for optimising sensor placement and interpreting the credibility of SHM diagnostics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Structural Health Monitoring (SHM), the recovery of distributed mechanical parameters from sparse data is often ill-posed, raising critical questions about identifiability and the reliability of inferred states. While deterministic regularization methods such as Tikhonov stabilise the inversion, they provide little insight into the spatial limits of resolution or the inherent uncertainty of the solution. This paper presents a Bayesian inverse framework that rigorously quantifies these limits, using the identification of distributed flexural rigidity from rotation (tilt) influence lines as a primary case study. Fisher information is employed as a diagnostic metric to quantify sensor informativeness, revealing how specific sensor layouts and load paths constrain the recoverable spatial features of the parameter field.\n  The methodology is applied to the full-scale openLAB research bridge (TU Dresden) using data from controlled vehicle passages. Beyond estimating the flexural rigidity profile, the Bayesian formulation produces credible intervals that expose regions of practical non-identifiability, which deterministic methods may obscure. The results demonstrate that while the measurement data carry high information content for the target parameters, their utility is spatially heterogeneous and strictly bounded by the experiment design. The proposed framework unifies identification with uncertainty quantification, providing a rigorous basis for optimising sensor placement and interpreting the credibility of SHM diagnostics."
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-20T18:34:27Z",
                "published_parsed": [
                    2025,
                    11,
                    20,
                    18,
                    34,
                    27,
                    3,
                    324,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE"
                },
                "authors": [
                    {
                        "name": "Tammam Bakeer"
                    },
                    {
                        "name": "Max Herbers"
                    },
                    {
                        "name": "Steffen Marx"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Marx"
                },
                "author": "Steffen Marx"
            },
            {
                "id": "http://arxiv.org/abs/2511.17249v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17249v1",
                "title": "FlexiFlow: decomposable flow matching for generation of flexible molecular ensemble",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexiFlow: decomposable flow matching for generation of flexible molecular ensemble"
                },
                "updated": "2025-11-21T13:50:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    13,
                    50,
                    54,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17249v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Sampling useful three-dimensional molecular structures along with their most favorable conformations is a key challenge in drug discovery. Current state-of-the-art 3D de-novo design flow matching or diffusion-based models are limited to generating a single conformation. However, the conformational landscape of a molecule determines its observable properties and how tightly it is able to bind to a given protein target. By generating a representative set of low-energy conformers, we can more directly assess these properties and potentially improve the ability to generate molecules with desired thermodynamic observables. Towards this aim, we propose FlexiFlow, a novel architecture that extends flow-matching models, allowing for the joint sampling of molecules along with multiple conformations while preserving both equivariance and permutation invariance. We demonstrate the effectiveness of our approach on the QM9 and GEOM Drugs datasets, achieving state-of-the-art results in molecular generation tasks. Our results show that FlexiFlow can generate valid, unstrained, unique, and novel molecules with high fidelity to the training data distribution, while also capturing the conformational diversity of molecules. Moreover, we show that our model can generate conformational ensembles that provide similar coverage to state-of-the-art physics-based methods at a fraction of the inference time. Finally, FlexiFlow can be successfully transferred to the protein-conditioned ligand generation task, even when the dataset contains only static pockets without accompanying conformations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sampling useful three-dimensional molecular structures along with their most favorable conformations is a key challenge in drug discovery. Current state-of-the-art 3D de-novo design flow matching or diffusion-based models are limited to generating a single conformation. However, the conformational landscape of a molecule determines its observable properties and how tightly it is able to bind to a given protein target. By generating a representative set of low-energy conformers, we can more directly assess these properties and potentially improve the ability to generate molecules with desired thermodynamic observables. Towards this aim, we propose FlexiFlow, a novel architecture that extends flow-matching models, allowing for the joint sampling of molecules along with multiple conformations while preserving both equivariance and permutation invariance. We demonstrate the effectiveness of our approach on the QM9 and GEOM Drugs datasets, achieving state-of-the-art results in molecular generation tasks. Our results show that FlexiFlow can generate valid, unstrained, unique, and novel molecules with high fidelity to the training data distribution, while also capturing the conformational diversity of molecules. Moreover, we show that our model can generate conformational ensembles that provide similar coverage to state-of-the-art physics-based methods at a fraction of the inference time. Finally, FlexiFlow can be successfully transferred to the protein-conditioned ligand generation task, even when the dataset contains only static pockets without accompanying conformations."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T13:50:54Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    13,
                    50,
                    54,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Preprint. Code to be released upon full publication",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Riccardo Tedoldi"
                    },
                    {
                        "name": "Ola Engkvist"
                    },
                    {
                        "name": "Patrick Bryant"
                    },
                    {
                        "name": "Hossein Azizpour"
                    },
                    {
                        "name": "Jon Paul Janet"
                    },
                    {
                        "name": "Alessandro Tibo"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Tibo"
                },
                "author": "Alessandro Tibo"
            },
            {
                "id": "http://arxiv.org/abs/2511.17247v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17247v1",
                "title": "Signed Networks: theory, methods, and applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Signed Networks: theory, methods, and applications"
                },
                "updated": "2025-11-21T13:46:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    13,
                    46,
                    52,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17247v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Signed networks provide a principled framework for representing systems in which interactions are not merely present or absent but qualitatively distinct: friendly or antagonistic, supportive or conflicting, excitatory or inhibitory. This polarity reshapes how we think about structure and dynamics in complex systems: a negative tie is not simply a missing positive one but a constraint that generates tension, and possibly asymmetry. Across disciplines, from sociology to neuroscience and machine learning, signed networks provide a shared language to formalise duality, balance, and opposition as integral components of system behaviour. This review provides a comprehensive and foundational summary of signed network theory. It formalises the mathematical principles of signed graphs and surveys signed-network-specific measures, including signed degree distributions, clustering, centralities, motifs, and Laplacians. It revisits balance theory, tracing its cognitive and structural formulations and their connections to frustration. Structural aspects of signed networks are examined, analysing key topics such as null models, node embeddings, sign prediction, and community detection. Subsequent sections address dynamical processes on and of signed networks, such as opinion dynamics, contagion models, and data-driven approaches for studying evolving networks. Practical challenges in constructing, inferring and validating signed data from real-world systems are also highlighted, and we offer an overview of currently available datasets. We also address common pitfalls and challenges that arise when modelling or analysing signed data. Overall, this review integrates theoretical foundations, methodological approaches, and cross-domain examples, providing a structured entry point and a reference framework for researchers interested in the study of signed networks in complex systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Signed networks provide a principled framework for representing systems in which interactions are not merely present or absent but qualitatively distinct: friendly or antagonistic, supportive or conflicting, excitatory or inhibitory. This polarity reshapes how we think about structure and dynamics in complex systems: a negative tie is not simply a missing positive one but a constraint that generates tension, and possibly asymmetry. Across disciplines, from sociology to neuroscience and machine learning, signed networks provide a shared language to formalise duality, balance, and opposition as integral components of system behaviour. This review provides a comprehensive and foundational summary of signed network theory. It formalises the mathematical principles of signed graphs and surveys signed-network-specific measures, including signed degree distributions, clustering, centralities, motifs, and Laplacians. It revisits balance theory, tracing its cognitive and structural formulations and their connections to frustration. Structural aspects of signed networks are examined, analysing key topics such as null models, node embeddings, sign prediction, and community detection. Subsequent sections address dynamical processes on and of signed networks, such as opinion dynamics, contagion models, and data-driven approaches for studying evolving networks. Practical challenges in constructing, inferring and validating signed data from real-world systems are also highlighted, and we offer an overview of currently available datasets. We also address common pitfalls and challenges that arise when modelling or analysing signed data. Overall, this review integrates theoretical foundations, methodological approaches, and cross-domain examples, providing a structured entry point and a reference framework for researchers interested in the study of signed networks in complex systems."
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T13:46:52Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    13,
                    46,
                    52,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph"
                },
                "authors": [
                    {
                        "name": "Fernando Diaz-Diaz"
                    },
                    {
                        "name": "Elena Candellone"
                    },
                    {
                        "name": "Miguel A. Gonzalez-Casado"
                    },
                    {
                        "name": "Emma Fraxanet"
                    },
                    {
                        "name": "Antoine Vendeville"
                    },
                    {
                        "name": "Irene Ferri"
                    },
                    {
                        "name": "Andreia Sofia Teixeira"
                    }
                ],
                "author_detail": {
                    "name": "Andreia Sofia Teixeira"
                },
                "author": "Andreia Sofia Teixeira"
            },
            {
                "id": "http://arxiv.org/abs/2510.24428v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.24428v4",
                "title": "CodeWiki: Evaluating AI's Ability to Generate Holistic Documentation for Large-Scale Codebases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeWiki: Evaluating AI's Ability to Generate Holistic Documentation for Large-Scale Codebases"
                },
                "updated": "2025-11-21T13:44:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    13,
                    44,
                    49,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.24428v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.24428v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Given a large and evolving codebase, the ability to automatically generate holistic, architecture-aware documentation that captures not only individual functions but also cross-file, cross-module, and system-level interactions remains an open challenge. Comprehensive documentation is essential for long-term software maintenance and collaboration, yet current automated approaches still fail to model the rich semantic dependencies and architectural structures that define real-world software systems. We present \\textbf{CodeWiki}, a unified framework for automated repository-level documentation across seven programming languages. CodeWiki introduces three key innovations: (i) hierarchical decomposition that preserves architectural context across multiple levels of granularity, (ii) recursive multi-agent processing with dynamic task delegation for scalable generation, and (iii) multi-modal synthesis that integrates textual descriptions with visual artifacts such as architecture diagrams and data-flow representations. To enable rigorous evaluation, we introduce \\textbf{CodeWikiBench}, a comprehensive benchmark featuring multi-dimensional rubrics and LLM-based assessment protocols. Experimental results show that CodeWiki achieves a 68.79\\% quality score with proprietary models, outperforming the closed-source DeepWiki baseline (64.06\\%) by 4.73\\%, with particularly strong improvements on high-level scripting languages (+10.47\\%). We open-source CodeWiki to foster future research and community adoption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given a large and evolving codebase, the ability to automatically generate holistic, architecture-aware documentation that captures not only individual functions but also cross-file, cross-module, and system-level interactions remains an open challenge. Comprehensive documentation is essential for long-term software maintenance and collaboration, yet current automated approaches still fail to model the rich semantic dependencies and architectural structures that define real-world software systems. We present \\textbf{CodeWiki}, a unified framework for automated repository-level documentation across seven programming languages. CodeWiki introduces three key innovations: (i) hierarchical decomposition that preserves architectural context across multiple levels of granularity, (ii) recursive multi-agent processing with dynamic task delegation for scalable generation, and (iii) multi-modal synthesis that integrates textual descriptions with visual artifacts such as architecture diagrams and data-flow representations. To enable rigorous evaluation, we introduce \\textbf{CodeWikiBench}, a comprehensive benchmark featuring multi-dimensional rubrics and LLM-based assessment protocols. Experimental results show that CodeWiki achieves a 68.79\\% quality score with proprietary models, outperforming the closed-source DeepWiki baseline (64.06\\%) by 4.73\\%, with particularly strong improvements on high-level scripting languages (+10.47\\%). We open-source CodeWiki to foster future research and community adoption."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-28T13:52:46Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    52,
                    46,
                    1,
                    301,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Anh Nguyen Hoang"
                    },
                    {
                        "name": "Minh Le-Anh"
                    },
                    {
                        "name": "Bach Le"
                    },
                    {
                        "name": "Nghi D. Q. Bui"
                    }
                ],
                "author_detail": {
                    "name": "Nghi D. Q. Bui"
                },
                "author": "Nghi D. Q. Bui"
            },
            {
                "id": "http://arxiv.org/abs/2508.11198v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.11198v4",
                "title": "Learning to Restore Heisenberg Limit in Noisy Quantum Sensing via Quantum Digital Twin",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Restore Heisenberg Limit in Noisy Quantum Sensing via Quantum Digital Twin"
                },
                "updated": "2025-11-21T13:41:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    13,
                    41,
                    9,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.11198v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.11198v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Quantum sensors leverage nonclassical resources to achieve sensing precision at the Heisenberg limit, surpassing the standard quantum limit attainable through classical strategies. However, a critical issue is that the environmental noise induces rapid decoherence, fundamentally limiting the realizability of the Heisenberg limit. In this Letter, we propose a quantum digital twin protocol to overcome this issue. The protocol first establishes observable-constrained state reconstruction to infer random errors in the decoherence process, and then utilizes reinforcement learning to derive adaptive compensatory control strategies. Demonstrated across discrete, continuous variable and multi-qubit circuit systems, our approach bypasses quantum state tomography's exponential overhead and discovers optimal control schemes to restore the Heisenberg limit. Unlike quantum error correction or mitigation schemes requiring precise noise characterization and ancillary qubits, our autonomous protocol achieves noise-resilient sensing through environment-adaptive control sequencing. This work establishes quantum digital twin as a generic methodology for quantum control, proposing a noise-immune paradigm for next-generation quantum sensors compatible with NISQ-era experimental constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum sensors leverage nonclassical resources to achieve sensing precision at the Heisenberg limit, surpassing the standard quantum limit attainable through classical strategies. However, a critical issue is that the environmental noise induces rapid decoherence, fundamentally limiting the realizability of the Heisenberg limit. In this Letter, we propose a quantum digital twin protocol to overcome this issue. The protocol first establishes observable-constrained state reconstruction to infer random errors in the decoherence process, and then utilizes reinforcement learning to derive adaptive compensatory control strategies. Demonstrated across discrete, continuous variable and multi-qubit circuit systems, our approach bypasses quantum state tomography's exponential overhead and discovers optimal control schemes to restore the Heisenberg limit. Unlike quantum error correction or mitigation schemes requiring precise noise characterization and ancillary qubits, our autonomous protocol achieves noise-resilient sensing through environment-adaptive control sequencing. This work establishes quantum digital twin as a generic methodology for quantum control, proposing a noise-immune paradigm for next-generation quantum sensors compatible with NISQ-era experimental constraints."
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-15T04:14:01Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    4,
                    14,
                    1,
                    4,
                    227,
                    0
                ],
                "arxiv_comment": "7 pages, 4 figures",
                "arxiv_primary_category": {
                    "term": "quant-ph"
                },
                "authors": [
                    {
                        "name": "Hang Xu"
                    },
                    {
                        "name": "Tailong Xiao"
                    },
                    {
                        "name": "Jingzheng Huang"
                    },
                    {
                        "name": "Jianping Fan"
                    },
                    {
                        "name": "Guihua Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Guihua Zeng"
                },
                "author": "Guihua Zeng"
            },
            {
                "id": "http://arxiv.org/abs/2511.16224v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.16224v2",
                "title": "Beyond Code Similarity: Benchmarking the Plausibility, Efficiency, and Complexity of LLM-Generated Smart Contracts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Code Similarity: Benchmarking the Plausibility, Efficiency, and Complexity of LLM-Generated Smart Contracts"
                },
                "updated": "2025-11-21T13:40:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    13,
                    40,
                    38,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.16224v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.16224v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Smart Contracts are critical components of blockchain ecosystems, with Solidity as the dominant programming language. While LLMs excel at general-purpose code generation, the unique constraints of Smart Contracts, such as gas consumption, security, and determinism, raise open questions about the reliability of LLM-generated Solidity code. Existing studies lack a comprehensive evaluation of these critical functional and non-functional properties. We benchmark four state-of-the-art models under zero-shot and retrieval-augmented generation settings across 500 real-world functions. Our multi-faceted assessment employs code similarity metrics, semantic embeddings, automated test execution, gas profiling, and cognitive and cyclomatic complexity analysis. Results show that while LLMs produce code with high semantic similarity to real contracts, their functional correctness is low: only 20% to 26% of zero-shot generations behave identically to ground-truth implementations under testing. The generated code is consistently simpler, with significantly lower complexity and gas consumption, often due to omitted validation logic. Retrieval-Augmented Generation markedly improves performance, boosting functional correctness by up to 45% and yielding more concise and efficient code. Our findings reveal a significant gap between semantic similarity and functional plausibility in LLM-generated Smart Contracts. We conclude that while RAG is a powerful enhancer, achieving robust, production-ready code generation remains a substantial challenge, necessitating careful expert validation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart Contracts are critical components of blockchain ecosystems, with Solidity as the dominant programming language. While LLMs excel at general-purpose code generation, the unique constraints of Smart Contracts, such as gas consumption, security, and determinism, raise open questions about the reliability of LLM-generated Solidity code. Existing studies lack a comprehensive evaluation of these critical functional and non-functional properties. We benchmark four state-of-the-art models under zero-shot and retrieval-augmented generation settings across 500 real-world functions. Our multi-faceted assessment employs code similarity metrics, semantic embeddings, automated test execution, gas profiling, and cognitive and cyclomatic complexity analysis. Results show that while LLMs produce code with high semantic similarity to real contracts, their functional correctness is low: only 20% to 26% of zero-shot generations behave identically to ground-truth implementations under testing. The generated code is consistently simpler, with significantly lower complexity and gas consumption, often due to omitted validation logic. Retrieval-Augmented Generation markedly improves performance, boosting functional correctness by up to 45% and yielding more concise and efficient code. Our findings reveal a significant gap between semantic similarity and functional plausibility in LLM-generated Smart Contracts. We conclude that while RAG is a powerful enhancer, achieving robust, production-ready code generation remains a substantial challenge, necessitating careful expert validation."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-20T10:47:59Z",
                "published_parsed": [
                    2025,
                    11,
                    20,
                    10,
                    47,
                    59,
                    3,
                    324,
                    0
                ],
                "arxiv_comment": "20 pages",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Francesco Salzano"
                    },
                    {
                        "name": "Simone Scalabrino"
                    },
                    {
                        "name": "Rocco Oliveto"
                    },
                    {
                        "name": "Remo Pareschi"
                    }
                ],
                "author_detail": {
                    "name": "Remo Pareschi"
                },
                "author": "Remo Pareschi"
            },
            {
                "id": "http://arxiv.org/abs/2511.17241v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17241v1",
                "title": "Social-Media Based Personas Challenge: Hybrid Prediction of Common and Rare User Actions on Bluesky",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social-Media Based Personas Challenge: Hybrid Prediction of Common and Rare User Actions on Bluesky"
                },
                "updated": "2025-11-21T13:40:14Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    13,
                    40,
                    14,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17241v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Understanding and predicting user behavior on social media platforms is crucial for content recommendation and platform design. While existing approaches focus primarily on common actions like retweeting and liking, the prediction of rare but significant behaviors remains largely unexplored. This paper presents a hybrid methodology for social media user behavior prediction that addresses both frequent and infrequent actions across a diverse action vocabulary. We evaluate our approach on a large-scale Bluesky dataset containing 6.4 million conversation threads spanning 12 distinct user actions across 25 persona clusters. Our methodology combines four complementary approaches: (i) a lookup database system based on historical response patterns; (ii) persona-specific LightGBM models with engineered temporal and semantic features for common actions; (iii) a specialized hybrid neural architecture fusing textual and temporal representations for rare action classification; and (iv) generation of text replies. Our persona-specific models achieve an average macro F1-score of 0.64 for common action prediction, while our rare action classifier achieves 0.56 macro F1-score across 10 rare actions. These results demonstrate that effective social media behavior prediction requires tailored modeling strategies recognizing fundamental differences between action types. Our approach achieved first place in the SocialSim: Social-Media Based Personas challenge organized at the Social Simulation with LLMs workshop at COLM 2025.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and predicting user behavior on social media platforms is crucial for content recommendation and platform design. While existing approaches focus primarily on common actions like retweeting and liking, the prediction of rare but significant behaviors remains largely unexplored. This paper presents a hybrid methodology for social media user behavior prediction that addresses both frequent and infrequent actions across a diverse action vocabulary. We evaluate our approach on a large-scale Bluesky dataset containing 6.4 million conversation threads spanning 12 distinct user actions across 25 persona clusters. Our methodology combines four complementary approaches: (i) a lookup database system based on historical response patterns; (ii) persona-specific LightGBM models with engineered temporal and semantic features for common actions; (iii) a specialized hybrid neural architecture fusing textual and temporal representations for rare action classification; and (iv) generation of text replies. Our persona-specific models achieve an average macro F1-score of 0.64 for common action prediction, while our rare action classifier achieves 0.56 macro F1-score across 10 rare actions. These results demonstrate that effective social media behavior prediction requires tailored modeling strategies recognizing fundamental differences between action types. Our approach achieved first place in the SocialSim: Social-Media Based Personas challenge organized at the Social Simulation with LLMs workshop at COLM 2025."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T13:40:14Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    13,
                    40,
                    14,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "1st place at SocialSim: Social-Media Based Personas challenge 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Benjamin White"
                    },
                    {
                        "name": "Anastasia Shimorina"
                    }
                ],
                "author_detail": {
                    "name": "Anastasia Shimorina"
                },
                "author": "Anastasia Shimorina"
            },
            {
                "id": "http://arxiv.org/abs/2505.13846v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.13846v2",
                "title": "Extension of Dynamic Network Biomarker using the propensity score method: Simulation of causal effects on variance and correlation coefficient",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extension of Dynamic Network Biomarker using the propensity score method: Simulation of causal effects on variance and correlation coefficient"
                },
                "updated": "2025-11-21T13:38:50Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    13,
                    38,
                    50,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.13846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.13846v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In clinical biomarker studies, the Dynamic Network Biomarker (DNB) is sometimes used. DNB is a composite variable derived from the variance and the Pearson correlation coefficient of biological signals. When applying DNB to clinical data, it is important to account for confounding bias. However, little attention has been paid to statistical causal inference methods for variance and correlation coefficients. This study evaluates confounding adjustment using propensity score matching (PSM) through Monte Carlo simulations. Our results support the use of PSM to reduce bias and improve group comparisons when DNB is applied to clinical data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In clinical biomarker studies, the Dynamic Network Biomarker (DNB) is sometimes used. DNB is a composite variable derived from the variance and the Pearson correlation coefficient of biological signals. When applying DNB to clinical data, it is important to account for confounding bias. However, little attention has been paid to statistical causal inference methods for variance and correlation coefficients. This study evaluates confounding adjustment using propensity score matching (PSM) through Monte Carlo simulations. Our results support the use of PSM to reduce bias and improve group comparisons when DNB is applied to clinical data."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-20T02:39:53Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    2,
                    39,
                    53,
                    1,
                    140,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Satoru Shinoda"
                    },
                    {
                        "name": "Hideaki Kawaguchi"
                    }
                ],
                "author_detail": {
                    "name": "Hideaki Kawaguchi"
                },
                "author": "Hideaki Kawaguchi"
            },
            {
                "id": "http://arxiv.org/abs/2511.17235v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17235v1",
                "title": "NX-CGRA: A Programmable Hardware Accelerator for Core Transformer Algorithms on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NX-CGRA: A Programmable Hardware Accelerator for Core Transformer Algorithms on Edge Devices"
                },
                "updated": "2025-11-21T13:26:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    13,
                    26,
                    16,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17235v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The increasing diversity and complexity of transformer workloads at the edge present significant challenges in balancing performance, energy efficiency, and architectural flexibility. This paper introduces NX-CGRA, a programmable hardware accelerator designed to support a range of transformer inference algorithms, including both linear and non-linear functions. Unlike fixed-function accelerators optimized for narrow use cases, NX-CGRA employs a coarse-grained reconfigurable array (CGRA) architecture with software-driven programmability, enabling efficient execution across varied kernel patterns. The architecture is evaluated using representative benchmarks derived from real-world transformer models, demonstrating high overall efficiency and favorable energy-area tradeoffs across different classes of operations. These results indicate the potential of NX-CGRA as a scalable and adaptable hardware solution for edge transformer deployment under constrained power and silicon budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing diversity and complexity of transformer workloads at the edge present significant challenges in balancing performance, energy efficiency, and architectural flexibility. This paper introduces NX-CGRA, a programmable hardware accelerator designed to support a range of transformer inference algorithms, including both linear and non-linear functions. Unlike fixed-function accelerators optimized for narrow use cases, NX-CGRA employs a coarse-grained reconfigurable array (CGRA) architecture with software-driven programmability, enabling efficient execution across varied kernel patterns. The architecture is evaluated using representative benchmarks derived from real-world transformer models, demonstrating high overall efficiency and favorable energy-area tradeoffs across different classes of operations. These results indicate the potential of NX-CGRA as a scalable and adaptable hardware solution for edge transformer deployment under constrained power and silicon budgets."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T13:26:16Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    13,
                    26,
                    16,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "This paper has been accepted for publication at the Design, Automation and Test in Europe (DATE) Conference 2026. 2026 IEEE. Personal use of this material is permitted",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Rohit Prasad"
                    }
                ],
                "author_detail": {
                    "name": "Rohit Prasad"
                },
                "author": "Rohit Prasad"
            },
            {
                "id": "http://arxiv.org/abs/2409.11393v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2409.11393v3",
                "title": "LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Design of Multi Active/Passive Core-Agent Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Design of Multi Active/Passive Core-Agent Architectures"
                },
                "updated": "2025-11-21T13:25:25Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    13,
                    25,
                    25,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2409.11393v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2409.11393v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1016/j.inffus.2025.103865",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "In an era where vast amounts of data are collected and processed from diverse sources, there is a growing demand for sophisticated AI systems capable of intelligently fusing and analyzing this information. To address these challenges, researchers have turned towards integrating tools into LLM-powered agents to enhance the overall information fusion process. However, the conjunction of these technologies and the proposed enhancements in several state-of-the-art works followed a non-unified software architecture, resulting in a lack of modularity and terminological inconsistencies among researchers. To address these issues, we propose a novel LLM-based Agent Unified Modeling Framework (LLM-Agent-UMF) that establishes a clear foundation for agent development from both functional and software architectural perspectives, developed and evaluated using the Architecture Tradeoff and Risk Analysis Framework (ATRAF). Our framework clearly distinguishes between the different components of an LLM-based agent, setting LLMs and tools apart from a new element, the core-agent, which plays the role of central coordinator. This pivotal entity comprises five modules: planning, memory, profile, action, and security -- the latter often neglected in previous works. By classifying core-agents into passive and active types based on their authoritative natures, we propose various multi-core agent architectures that combine unique characteristics of distinctive agents to tackle complex tasks more efficiently. We evaluate our framework by applying it to thirteen state-of-the-art agents, thereby demonstrating its alignment with their functionalities and clarifying overlooked architectural aspects. Moreover, we thoroughly assess five architecture variants of our framework by designing new agent architectures that combine characteristics of state-of-the-art agents to address specific goals. ...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an era where vast amounts of data are collected and processed from diverse sources, there is a growing demand for sophisticated AI systems capable of intelligently fusing and analyzing this information. To address these challenges, researchers have turned towards integrating tools into LLM-powered agents to enhance the overall information fusion process. However, the conjunction of these technologies and the proposed enhancements in several state-of-the-art works followed a non-unified software architecture, resulting in a lack of modularity and terminological inconsistencies among researchers. To address these issues, we propose a novel LLM-based Agent Unified Modeling Framework (LLM-Agent-UMF) that establishes a clear foundation for agent development from both functional and software architectural perspectives, developed and evaluated using the Architecture Tradeoff and Risk Analysis Framework (ATRAF). Our framework clearly distinguishes between the different components of an LLM-based agent, setting LLMs and tools apart from a new element, the core-agent, which plays the role of central coordinator. This pivotal entity comprises five modules: planning, memory, profile, action, and security -- the latter often neglected in previous works. By classifying core-agents into passive and active types based on their authoritative natures, we propose various multi-core agent architectures that combine unique characteristics of distinctive agents to tackle complex tasks more efficiently. We evaluate our framework by applying it to thirteen state-of-the-art agents, thereby demonstrating its alignment with their functionalities and clarifying overlooked architectural aspects. Moreover, we thoroughly assess five architecture variants of our framework by designing new agent architectures that combine characteristics of state-of-the-art agents to address specific goals. ..."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-09-17T17:54:17Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    54,
                    17,
                    1,
                    261,
                    0
                ],
                "arxiv_comment": "39 pages, 19 figures, 3 tables. Published in Information Fusion, Volume 127, March 2026, 103865. Part of the special issue \"Data Fusion Approaches in Data-Centric AI for Developing Trustworthy AI Systems\"",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "arxiv_journal_ref": "Information Fusion 127 (2026) 103865",
                "authors": [
                    {
                        "name": "Amine Ben Hassouna"
                    },
                    {
                        "name": "Hana Chaari"
                    },
                    {
                        "name": "Ines Belhaj"
                    }
                ],
                "author_detail": {
                    "name": "Ines Belhaj"
                },
                "author": "Ines Belhaj",
                "arxiv_doi": "10.1016/j.inffus.2025.103865"
            },
            {
                "id": "http://arxiv.org/abs/2508.04652v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.04652v4",
                "title": "LLM Collaboration With Multi-Agent Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Collaboration With Multi-Agent Reinforcement Learning"
                },
                "updated": "2025-11-21T13:14:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    13,
                    14,
                    11,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.04652v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.04652v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "A large amount of work has been done in Multi-Agent Systems (MAS) for modeling and solving problems with multiple interacting agents. However, most LLMs are pretrained independently and not specifically optimized for coordination. Existing LLM fine-tuning frameworks rely on individual rewards, which require complex reward designs for each agent to encourage collaboration. To address these challenges, we model LLM collaboration as a cooperative Multi-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent, multi-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO), to solve it, building on current RL approaches for LLMs as well as MARL techniques. Our experiments on LLM writing and coding collaboration demonstrate that fine-tuning MAS with MAGRPO enables agents to generate high-quality responses efficiently through effective cooperation. Our approach opens the door to using other MARL methods for LLMs and highlights the associated challenges.\n  Our code is available at https://github.com/OpenMLRL/CoMLRL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A large amount of work has been done in Multi-Agent Systems (MAS) for modeling and solving problems with multiple interacting agents. However, most LLMs are pretrained independently and not specifically optimized for coordination. Existing LLM fine-tuning frameworks rely on individual rewards, which require complex reward designs for each agent to encourage collaboration. To address these challenges, we model LLM collaboration as a cooperative Multi-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent, multi-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO), to solve it, building on current RL approaches for LLMs as well as MARL techniques. Our experiments on LLM writing and coding collaboration demonstrate that fine-tuning MAS with MAGRPO enables agents to generate high-quality responses efficiently through effective cooperation. Our approach opens the door to using other MARL methods for LLMs and highlights the associated challenges.\n  Our code is available at https://github.com/OpenMLRL/CoMLRL."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-06T17:18:25Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    18,
                    25,
                    2,
                    218,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Shuo Liu"
                    },
                    {
                        "name": "Tianle Chen"
                    },
                    {
                        "name": "Zeyu Liang"
                    },
                    {
                        "name": "Xueguang Lyu"
                    },
                    {
                        "name": "Christopher Amato"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Amato"
                },
                "author": "Christopher Amato"
            },
            {
                "id": "http://arxiv.org/abs/2511.17220v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17220v1",
                "title": "Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs"
                },
                "updated": "2025-11-21T13:01:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    13,
                    1,
                    28,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17220v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This study presents PARROT (Persuasion and Agreement Robustness Rating of Output Truth), a robustness focused framework designed to measure the degradation in accuracy that occurs under social pressure exerted on users through authority and persuasion in large language models (LLMs) the phenomenon of sycophancy (excessive conformity). PARROT (i) isolates causal effects by comparing the neutral version of the same question with an authoritatively false version using a double-blind evaluation, (ii) quantifies confidence shifts toward the correct and imposed false responses using log-likelihood-based calibration tracking, and (iii) systematically classifies failure modes (e.g., robust correct, sycophantic agreement, reinforced error, stubborn error, self-correction, etc.) using an eight-state behavioral taxonomy. We evaluated 22 models using 1,302 MMLU-style multiple-choice questions across 13 domains and domain-specific authority templates. Findings show marked heterogeneity: advanced models (e.g., GPT-5, GPT-4.1, Claude Sonnet 4.5) exhibit low \"follow rates\" ($\\leq 11\\%$, GPT-5: 4\\%) and minimal accuracy loss, while older/smaller models show severe epistemic collapse (GPT-4: 80\\%, Qwen 2.5-1.5B: 94\\%). The danger is not limited to response changes; weak models reduce confidence in the correct response while increasing confidence in the imposed incorrect response. While international law and global knowledge at the domain level exhibit high fragility, elementary mathematics is relatively resilient. Consequently, we argue that the goal of \"resistance to overfitting pressure\" should be addressed as a primary objective alongside accuracy, harm avoidance, and privacy for safe deployment in the real world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents PARROT (Persuasion and Agreement Robustness Rating of Output Truth), a robustness focused framework designed to measure the degradation in accuracy that occurs under social pressure exerted on users through authority and persuasion in large language models (LLMs) the phenomenon of sycophancy (excessive conformity). PARROT (i) isolates causal effects by comparing the neutral version of the same question with an authoritatively false version using a double-blind evaluation, (ii) quantifies confidence shifts toward the correct and imposed false responses using log-likelihood-based calibration tracking, and (iii) systematically classifies failure modes (e.g., robust correct, sycophantic agreement, reinforced error, stubborn error, self-correction, etc.) using an eight-state behavioral taxonomy. We evaluated 22 models using 1,302 MMLU-style multiple-choice questions across 13 domains and domain-specific authority templates. Findings show marked heterogeneity: advanced models (e.g., GPT-5, GPT-4.1, Claude Sonnet 4.5) exhibit low \"follow rates\" ($\\leq 11\\%$, GPT-5: 4\\%) and minimal accuracy loss, while older/smaller models show severe epistemic collapse (GPT-4: 80\\%, Qwen 2.5-1.5B: 94\\%). The danger is not limited to response changes; weak models reduce confidence in the correct response while increasing confidence in the imposed incorrect response. While international law and global knowledge at the domain level exhibit high fragility, elementary mathematics is relatively resilient. Consequently, we argue that the goal of \"resistance to overfitting pressure\" should be addressed as a primary objective alongside accuracy, harm avoidance, and privacy for safe deployment in the real world."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T13:01:28Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    13,
                    1,
                    28,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yusuf elebi"
                    },
                    {
                        "name": "Mahmoud El Hussieni"
                    },
                    {
                        "name": "zay Ezerceli"
                    }
                ],
                "author_detail": {
                    "name": "zay Ezerceli"
                },
                "author": "zay Ezerceli"
            },
            {
                "id": "http://arxiv.org/abs/2511.17217v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17217v1",
                "title": "Dual-domain Adaptation Networks for Realistic Image Super-resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual-domain Adaptation Networks for Realistic Image Super-resolution"
                },
                "updated": "2025-11-21T12:57:23Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    57,
                    23,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17217v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Realistic image super-resolution (SR) focuses on transforming real-world low-resolution (LR) images into high-resolution (HR) ones, handling more complex degradation patterns than synthetic SR tasks. This is critical for applications like surveillance, medical imaging, and consumer electronics. However, current methods struggle with limited real-world LR-HR data, impacting the learning of basic image features. Pre-trained SR models from large-scale synthetic datasets offer valuable prior knowledge, which can improve generalization, speed up training, and reduce the need for extensive real-world data in realistic SR tasks. In this paper, we introduce a novel approach, Dual-domain Adaptation Networks, which is able to efficiently adapt pre-trained image SR models from simulated to real-world datasets. To achieve this target, we first set up a spatial-domain adaptation strategy through selectively updating parameters of pre-trained models and employing the low-rank adaptation technique to adjust frozen parameters. Recognizing that image super-resolution involves recovering high-frequency components, we further integrate a frequency domain adaptation branch into the adapted model, which combines the spectral data of the input and the spatial-domain backbone's intermediate features to infer HR frequency maps, enhancing the SR result. Experimental evaluations on public realistic image SR benchmarks, including RealSR, D2CRealSR, and DRealSR, demonstrate the superiority of our proposed method over existing state-of-the-art models. Codes are available at: https://github.com/dummerchen/DAN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Realistic image super-resolution (SR) focuses on transforming real-world low-resolution (LR) images into high-resolution (HR) ones, handling more complex degradation patterns than synthetic SR tasks. This is critical for applications like surveillance, medical imaging, and consumer electronics. However, current methods struggle with limited real-world LR-HR data, impacting the learning of basic image features. Pre-trained SR models from large-scale synthetic datasets offer valuable prior knowledge, which can improve generalization, speed up training, and reduce the need for extensive real-world data in realistic SR tasks. In this paper, we introduce a novel approach, Dual-domain Adaptation Networks, which is able to efficiently adapt pre-trained image SR models from simulated to real-world datasets. To achieve this target, we first set up a spatial-domain adaptation strategy through selectively updating parameters of pre-trained models and employing the low-rank adaptation technique to adjust frozen parameters. Recognizing that image super-resolution involves recovering high-frequency components, we further integrate a frequency domain adaptation branch into the adapted model, which combines the spectral data of the input and the spatial-domain backbone's intermediate features to infer HR frequency maps, enhancing the SR result. Experimental evaluations on public realistic image SR benchmarks, including RealSR, D2CRealSR, and DRealSR, demonstrate the superiority of our proposed method over existing state-of-the-art models. Codes are available at: https://github.com/dummerchen/DAN."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T12:57:23Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    57,
                    23,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Chaowei Fang"
                    },
                    {
                        "name": "Bolin Fu"
                    },
                    {
                        "name": "De Cheng"
                    },
                    {
                        "name": "Lechao Cheng"
                    },
                    {
                        "name": "Guanbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Guanbin Li"
                },
                "author": "Guanbin Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.17208v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17208v1",
                "title": "A Simple Yet Strong Baseline for Long-Term Conversational Memory of LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple Yet Strong Baseline for Long-Term Conversational Memory of LLM Agents"
                },
                "updated": "2025-11-21T12:41:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    41,
                    17,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17208v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLM-based conversational agents still struggle to maintain coherent, personalized interaction over many sessions: fixed context windows limit how much history can be kept in view, and most external memory approaches trade off between coarse retrieval over large chunks and fine-grained but fragmented views of the dialogue. Motivated by neo-Davidsonian event semantics, we propose an event-centric alternative that represents conversational history as short, event-like propositions which bundle together participants, temporal cues, and minimal local context, rather than as independent relation triples or opaque summaries. In contrast to work that aggressively compresses or forgets past content, our design aims to preserve information in a non-compressive form and make it more accessible, rather than more lossy. Concretely, we instruct an LLM to decompose each session into enriched elementary discourse units (EDUs) -- self-contained statements with normalized entities and source turn attributions -- and organize sessions, EDUs, and their arguments in a heterogeneous graph that supports associative recall. On top of this representation we build two simple retrieval-based variants that use dense similarity search and LLM filtering, with an optional graph-based propagation step to connect and aggregate evidence across related EDUs. Experiments on the LoCoMo and LongMemEval$_S$ benchmarks show that these event-centric memories match or surpass strong baselines, while operating with much shorter QA contexts. Our results suggest that structurally simple, event-level memory provides a principled and practical foundation for long-horizon conversational agents. Our code and data will be released at https://github.com/KevinSRR/EMem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based conversational agents still struggle to maintain coherent, personalized interaction over many sessions: fixed context windows limit how much history can be kept in view, and most external memory approaches trade off between coarse retrieval over large chunks and fine-grained but fragmented views of the dialogue. Motivated by neo-Davidsonian event semantics, we propose an event-centric alternative that represents conversational history as short, event-like propositions which bundle together participants, temporal cues, and minimal local context, rather than as independent relation triples or opaque summaries. In contrast to work that aggressively compresses or forgets past content, our design aims to preserve information in a non-compressive form and make it more accessible, rather than more lossy. Concretely, we instruct an LLM to decompose each session into enriched elementary discourse units (EDUs) -- self-contained statements with normalized entities and source turn attributions -- and organize sessions, EDUs, and their arguments in a heterogeneous graph that supports associative recall. On top of this representation we build two simple retrieval-based variants that use dense similarity search and LLM filtering, with an optional graph-based propagation step to connect and aggregate evidence across related EDUs. Experiments on the LoCoMo and LongMemEval$_S$ benchmarks show that these event-centric memories match or surpass strong baselines, while operating with much shorter QA contexts. Our results suggest that structurally simple, event-level memory provides a principled and practical foundation for long-horizon conversational agents. Our code and data will be released at https://github.com/KevinSRR/EMem."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T12:41:17Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    41,
                    17,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Work in progress",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Sizhe Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Sizhe Zhou"
                },
                "author": "Sizhe Zhou"
            },
            {
                "id": "http://arxiv.org/abs/2511.17205v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17205v1",
                "title": "E$^3$-Pruner: Towards Efficient, Economical, and Effective Layer Pruning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E$^3$-Pruner: Towards Efficient, Economical, and Effective Layer Pruning for Large Language Models"
                },
                "updated": "2025-11-21T12:32:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    32,
                    1,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17205v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the increasing size of large language models, layer pruning has gained increased attention as a hardware-friendly approach for model compression. However, existing layer pruning methods struggle to simultaneously address key practical deployment challenges, including performance degradation, high training costs, and limited acceleration. To overcome these limitations, we propose \\name, a task-\\underline{E}ffective, training-\\underline{E}conomical and inference-\\underline{E}fficient layer pruning framework. \\namespace introduces two key innovations: (1) a differentiable mask optimization method using a Gumbel-TopK sampler, enabling efficient and precise pruning mask search; and (2) an entropy-aware adaptive knowledge distillation strategy that enhances task performance. Extensive experiments over diverse model architectures and benchmarks demonstrate the superiority of our method over state-of-the-art approaches. Notably, \\namespace achieves 96\\% accuracy, a mere 0.8\\% drop from the original model (96.8\\%) on MATH-500 when pruning 25\\% layers of Qwen3-32B, outperforming existing SOTA (95\\%), with a 1.33$\\times$ inference speedup by consuming merely 0.5B tokens (0.5\\% of the post-training data volume).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing size of large language models, layer pruning has gained increased attention as a hardware-friendly approach for model compression. However, existing layer pruning methods struggle to simultaneously address key practical deployment challenges, including performance degradation, high training costs, and limited acceleration. To overcome these limitations, we propose \\name, a task-\\underline{E}ffective, training-\\underline{E}conomical and inference-\\underline{E}fficient layer pruning framework. \\namespace introduces two key innovations: (1) a differentiable mask optimization method using a Gumbel-TopK sampler, enabling efficient and precise pruning mask search; and (2) an entropy-aware adaptive knowledge distillation strategy that enhances task performance. Extensive experiments over diverse model architectures and benchmarks demonstrate the superiority of our method over state-of-the-art approaches. Notably, \\namespace achieves 96\\% accuracy, a mere 0.8\\% drop from the original model (96.8\\%) on MATH-500 when pruning 25\\% layers of Qwen3-32B, outperforming existing SOTA (95\\%), with a 1.33$\\times$ inference speedup by consuming merely 0.5B tokens (0.5\\% of the post-training data volume)."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T12:32:01Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    32,
                    1,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Tao Yuan"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Yinfei Pan"
                    },
                    {
                        "name": "Xuyang Cao"
                    },
                    {
                        "name": "Tianyu Zhang"
                    },
                    {
                        "name": "Lu Hou"
                    },
                    {
                        "name": "Ting Hu"
                    },
                    {
                        "name": "Xianzhi Yu"
                    }
                ],
                "author_detail": {
                    "name": "Xianzhi Yu"
                },
                "author": "Xianzhi Yu"
            },
            {
                "id": "http://arxiv.org/abs/2511.17199v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17199v1",
                "title": "VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation"
                },
                "updated": "2025-11-21T12:26:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    26,
                    30,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17199v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-language-action (VLA) models show potential for general robotic tasks, but remain challenging in spatiotemporally coherent manipulation, which requires fine-grained representations. Typically, existing methods embed 3D positions into visual representations to enhance the spatial precision of actions. However, these methods struggle to achieve temporally coherent control over action execution. In this work, we propose VLA-4D, a general VLA model with 4D awareness for spatiotemporally coherent robotic manipulation. Our model is guided by two key designs: 1) 4D-aware visual representation. We extract visual features, embed 1D time into 3D positions for 4D embeddings, and fuse them into a unified visual representation via a cross-attention mechanism. 2) Spatiotemporal action representation. We extend conventional spatial action representations with temporal information to enable the spatiotemporal planning, and align the multimodal representations into the LLM for spatiotemporal action prediction. Within this unified framework, the designed visual and action representations jointly make robotic manipulation spatially-smooth and temporally-coherent. In addition, we extend the VLA dataset with temporal action annotations for fine-tuning our model. Extensive experiments have been conducted to verify the superiority of our method across different tasks of robotic manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language-action (VLA) models show potential for general robotic tasks, but remain challenging in spatiotemporally coherent manipulation, which requires fine-grained representations. Typically, existing methods embed 3D positions into visual representations to enhance the spatial precision of actions. However, these methods struggle to achieve temporally coherent control over action execution. In this work, we propose VLA-4D, a general VLA model with 4D awareness for spatiotemporally coherent robotic manipulation. Our model is guided by two key designs: 1) 4D-aware visual representation. We extract visual features, embed 1D time into 3D positions for 4D embeddings, and fuse them into a unified visual representation via a cross-attention mechanism. 2) Spatiotemporal action representation. We extend conventional spatial action representations with temporal information to enable the spatiotemporal planning, and align the multimodal representations into the LLM for spatiotemporal action prediction. Within this unified framework, the designed visual and action representations jointly make robotic manipulation spatially-smooth and temporally-coherent. In addition, we extend the VLA dataset with temporal action annotations for fine-tuning our model. Extensive experiments have been conducted to verify the superiority of our method across different tasks of robotic manipulation."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T12:26:30Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    26,
                    30,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Hanyu Zhou"
                    },
                    {
                        "name": "Chuanhao Ma"
                    },
                    {
                        "name": "Gim Hee Lee"
                    }
                ],
                "author_detail": {
                    "name": "Gim Hee Lee"
                },
                "author": "Gim Hee Lee"
            },
            {
                "id": "http://arxiv.org/abs/2511.17198v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17198v1",
                "title": "Designing Domain-Specific Agents via Hierarchical Task Abstraction Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing Domain-Specific Agents via Hierarchical Task Abstraction Mechanism"
                },
                "updated": "2025-11-21T12:25:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    25,
                    47,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17198v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLM-driven agents, particularly those using general frameworks like ReAct or human-inspired role-playing, often struggle in specialized domains that necessitate rigorously structured workflows. Fields such as remote sensing, requiring specialized tools (e.g., correction, spectral indices calculation), and multi-step procedures (e.g., numerous intermediate products and optional steps), significantly challenge generalized approaches. To address this gap, we introduce a novel agent design framework centered on a Hierarchical Task Abstraction Mechanism (HTAM). Specifically, HTAM moves beyond emulating social roles, instead structuring multi-agent systems into a logical hierarchy that mirrors the intrinsic task-dependency graph of a given domain. This task-centric architecture thus enforces procedural correctness and decomposes complex problems into sequential layers, where each layer's sub-agents operate on the outputs of the preceding layers. We instantiate this framework as EarthAgent, a multi-agent system tailored for complex geospatial analysis. To evaluate such complex planning capabilities, we build GeoPlan-bench, a comprehensive benchmark of realistic, multi-step geospatial planning tasks. It is accompanied by a suite of carefully designed metrics to evaluate tool selection, path similarity, and logical completeness. Experiments show that EarthAgent substantially outperforms a range of established single- and multi-agent systems. Our work demonstrates that aligning agent architecture with a domain's intrinsic task structure is a critical step toward building robust and reliable specialized autonomous systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-driven agents, particularly those using general frameworks like ReAct or human-inspired role-playing, often struggle in specialized domains that necessitate rigorously structured workflows. Fields such as remote sensing, requiring specialized tools (e.g., correction, spectral indices calculation), and multi-step procedures (e.g., numerous intermediate products and optional steps), significantly challenge generalized approaches. To address this gap, we introduce a novel agent design framework centered on a Hierarchical Task Abstraction Mechanism (HTAM). Specifically, HTAM moves beyond emulating social roles, instead structuring multi-agent systems into a logical hierarchy that mirrors the intrinsic task-dependency graph of a given domain. This task-centric architecture thus enforces procedural correctness and decomposes complex problems into sequential layers, where each layer's sub-agents operate on the outputs of the preceding layers. We instantiate this framework as EarthAgent, a multi-agent system tailored for complex geospatial analysis. To evaluate such complex planning capabilities, we build GeoPlan-bench, a comprehensive benchmark of realistic, multi-step geospatial planning tasks. It is accompanied by a suite of carefully designed metrics to evaluate tool selection, path similarity, and logical completeness. Experiments show that EarthAgent substantially outperforms a range of established single- and multi-agent systems. Our work demonstrates that aligning agent architecture with a domain's intrinsic task structure is a critical step toward building robust and reliable specialized autonomous systems."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T12:25:47Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    25,
                    47,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Page: https://earth-insights.github.io/EarthAgent",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Kaiyu Li"
                    },
                    {
                        "name": "Jiayu Wang"
                    },
                    {
                        "name": "Zhi Wang"
                    },
                    {
                        "name": "Hui Qiao"
                    },
                    {
                        "name": "Weizhan Zhang"
                    },
                    {
                        "name": "Deyu Meng"
                    },
                    {
                        "name": "Xiangyong Cao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyong Cao"
                },
                "author": "Xiangyong Cao"
            },
            {
                "id": "http://arxiv.org/abs/2511.17194v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17194v1",
                "title": "Steering in the Shadows: Causal Amplification for Activation Space Attacks in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering in the Shadows: Causal Amplification for Activation Space Attacks in Large Language Models"
                },
                "updated": "2025-11-21T12:19:55Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    19,
                    55,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17194v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern large language models (LLMs) are typically secured by auditing data, prompts, and refusal policies, while treating the forward pass as an implementation detail. We show that intermediate activations in decoder-only LLMs form a vulnerable attack surface for behavioral control. Building on recent findings on attention sinks and compression valleys, we identify a high-gain region in the residual stream where small, well-aligned perturbations are causally amplified along the autoregressive trajectory--a Causal Amplification Effect (CAE). We exploit this as an attack surface via Sensitivity-Scaled Steering (SSS), a progressive activation-level attack that combines beginning-of-sequence (BOS) anchoring with sensitivity-based reinforcement to focus a limited perturbation budget on the most vulnerable layers and tokens. We show that across multiple open-weight models and four behavioral axes, SSS induces large shifts in evil, hallucination, sycophancy, and sentiment while preserving high coherence and general capabilities, turning activation steering into a concrete security concern for white-box and supply-chain LLM deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) are typically secured by auditing data, prompts, and refusal policies, while treating the forward pass as an implementation detail. We show that intermediate activations in decoder-only LLMs form a vulnerable attack surface for behavioral control. Building on recent findings on attention sinks and compression valleys, we identify a high-gain region in the residual stream where small, well-aligned perturbations are causally amplified along the autoregressive trajectory--a Causal Amplification Effect (CAE). We exploit this as an attack surface via Sensitivity-Scaled Steering (SSS), a progressive activation-level attack that combines beginning-of-sequence (BOS) anchoring with sensitivity-based reinforcement to focus a limited perturbation budget on the most vulnerable layers and tokens. We show that across multiple open-weight models and four behavioral axes, SSS induces large shifts in evil, hallucination, sycophancy, and sentiment while preserving high coherence and general capabilities, turning activation steering into a concrete security concern for white-box and supply-chain LLM deployments."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T12:19:55Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    19,
                    55,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "31 pages, 5 figures, 9 tables",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Zhiyuan Xu"
                    },
                    {
                        "name": "Stanislav Abaimov"
                    },
                    {
                        "name": "Joseph Gardiner"
                    },
                    {
                        "name": "Sana Belguith"
                    }
                ],
                "author_detail": {
                    "name": "Sana Belguith"
                },
                "author": "Sana Belguith"
            },
            {
                "id": "http://arxiv.org/abs/2506.02392v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.02392v3",
                "title": "Improving Generalization of Neural Combinatorial Optimization for Vehicle Routing Problems via Test-Time Projection Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Generalization of Neural Combinatorial Optimization for Vehicle Routing Problems via Test-Time Projection Learning"
                },
                "updated": "2025-11-21T12:16:15Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    16,
                    15,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.02392v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.02392v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Neural Combinatorial Optimization (NCO) has emerged as a promising learning-based paradigm for addressing Vehicle Routing Problems (VRPs) by minimizing the need for extensive manual engineering. While existing NCO methods, trained on small-scale instances (e.g., 100 nodes), have demonstrated considerable success on problems of similar scale, their performance significantly degrades when applied to large-scale scenarios. This degradation arises from the distributional shift between training and testing data, rendering policies learned on small instances ineffective for larger problems. To overcome this limitation, we introduce a novel learning framework driven by Large Language Models (LLMs). This framework learns a projection between the training and testing distributions, which is then deployed to enhance the scalability of the NCO model. Notably, unlike prevailing techniques that necessitate joint training with the neural network, our approach operates exclusively during the inference phase, obviating the need for model retraining. Extensive experiments demonstrate that our method enables a backbone model (trained on 100-node instances) to achieve superior performance on large-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) of up to 100K nodes from diverse distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Combinatorial Optimization (NCO) has emerged as a promising learning-based paradigm for addressing Vehicle Routing Problems (VRPs) by minimizing the need for extensive manual engineering. While existing NCO methods, trained on small-scale instances (e.g., 100 nodes), have demonstrated considerable success on problems of similar scale, their performance significantly degrades when applied to large-scale scenarios. This degradation arises from the distributional shift between training and testing data, rendering policies learned on small instances ineffective for larger problems. To overcome this limitation, we introduce a novel learning framework driven by Large Language Models (LLMs). This framework learns a projection between the training and testing distributions, which is then deployed to enhance the scalability of the NCO model. Notably, unlike prevailing techniques that necessitate joint training with the neural network, our approach operates exclusively during the inference phase, obviating the need for model retraining. Extensive experiments demonstrate that our method enables a backbone model (trained on 100-node instances) to achieve superior performance on large-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) of up to 100K nodes from diverse distributions."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-03T03:15:22Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    3,
                    15,
                    22,
                    1,
                    154,
                    0
                ],
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2505.24627",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yuanyao Chen"
                    },
                    {
                        "name": "Rongsheng Chen"
                    },
                    {
                        "name": "Fu Luo"
                    },
                    {
                        "name": "Zhenkun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhenkun Wang"
                },
                "author": "Zhenkun Wang"
            },
            {
                "id": "http://arxiv.org/abs/2511.17190v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17190v1",
                "title": "AutoLink: Autonomous Schema Exploration and Expansion for Scalable Schema Linking in Text-to-SQL at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoLink: Autonomous Schema Exploration and Expansion for Scalable Schema Linking in Text-to-SQL at Scale"
                },
                "updated": "2025-11-21T12:12:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    12,
                    17,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17190v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17190v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "For industrial-scale text-to-SQL, supplying the entire database schema to Large Language Models (LLMs) is impractical due to context window limits and irrelevant noise. Schema linking, which filters the schema to a relevant subset, is therefore critical. However, existing methods incur prohibitive costs, struggle to trade off recall and noise, and scale poorly to large databases. We present \\textbf{AutoLink}, an autonomous agent framework that reformulates schema linking as an iterative, agent-driven process. Guided by an LLM, AutoLink dynamically explores and expands the linked schema subset, progressively identifying necessary schema components without inputting the full database schema. Our experiments demonstrate AutoLink's superior performance, achieving state-of-the-art strict schema linking recall of \\textbf{97.4\\%} on Bird-Dev and \\textbf{91.2\\%} on Spider-2.0-Lite, with competitive execution accuracy, i.e., \\textbf{68.7\\%} EX on Bird-Dev (better than CHESS) and \\textbf{34.9\\%} EX on Spider-2.0-Lite (ranking 2nd on the official leaderboard). Crucially, AutoLink exhibits \\textbf{exceptional scalability}, \\textbf{maintaining high recall}, \\textbf{efficient token consumption}, and \\textbf{robust execution accuracy} on large schemas (e.g., over 3,000 columns) where existing methods severely degrade-making it a highly scalable, high-recall schema-linking solution for industrial text-to-SQL systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For industrial-scale text-to-SQL, supplying the entire database schema to Large Language Models (LLMs) is impractical due to context window limits and irrelevant noise. Schema linking, which filters the schema to a relevant subset, is therefore critical. However, existing methods incur prohibitive costs, struggle to trade off recall and noise, and scale poorly to large databases. We present \\textbf{AutoLink}, an autonomous agent framework that reformulates schema linking as an iterative, agent-driven process. Guided by an LLM, AutoLink dynamically explores and expands the linked schema subset, progressively identifying necessary schema components without inputting the full database schema. Our experiments demonstrate AutoLink's superior performance, achieving state-of-the-art strict schema linking recall of \\textbf{97.4\\%} on Bird-Dev and \\textbf{91.2\\%} on Spider-2.0-Lite, with competitive execution accuracy, i.e., \\textbf{68.7\\%} EX on Bird-Dev (better than CHESS) and \\textbf{34.9\\%} EX on Spider-2.0-Lite (ranking 2nd on the official leaderboard). Crucially, AutoLink exhibits \\textbf{exceptional scalability}, \\textbf{maintaining high recall}, \\textbf{efficient token consumption}, and \\textbf{robust execution accuracy} on large schemas (e.g., over 3,000 columns) where existing methods severely degrade-making it a highly scalable, high-recall schema-linking solution for industrial text-to-SQL systems."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T12:12:17Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    12,
                    17,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ziyang Wang"
                    },
                    {
                        "name": "Yuanlei Zheng"
                    },
                    {
                        "name": "Zhenbiao Cao"
                    },
                    {
                        "name": "Xiaojin Zhang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    },
                    {
                        "name": "Pei Fu"
                    },
                    {
                        "name": "Zhenbo Luo"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai"
            },
            {
                "id": "http://arxiv.org/abs/2511.10138v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10138v2",
                "title": "GPR: Towards a Generative Pre-trained One-Model Paradigm for Large-Scale Advertising Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPR: Towards a Generative Pre-trained One-Model Paradigm for Large-Scale Advertising Recommendation"
                },
                "updated": "2025-11-21T12:09:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    9,
                    32,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10138v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As an intelligent infrastructure connecting users with commercial content, advertising recommendation systems play a central role in information flow and value creation within the digital economy. However, existing multi-stage advertising recommendation systems suffer from objective misalignment and error propagation, making it difficult to achieve global optimality, while unified generative recommendation models still struggle to meet the demands of practical industrial applications. To address these issues, we propose GPR (Generative Pre-trained Recommender), the first one-model framework that redefines advertising recommendation as an end-to-end generative task, replacing the traditional cascading paradigm with a unified generative approach. To realize GPR, we introduce three key innovations spanning unified representation, network architecture, and training strategy. First, we design a unified input schema and tokenization method tailored to advertising scenarios, mapping both ads and organic content into a shared multi-level semantic ID space, thereby enhancing semantic alignment and modeling consistency across heterogeneous data. Second, we develop the Heterogeneous Hierarchical Decoder (HHD), a dual-decoder architecture that decouples user intent modeling from ad generation, achieving a balance between training efficiency and inference flexibility while maintaining strong modeling capacity. Finally, we propose a multi-stage joint training strategy that integrates Multi-Token Prediction (MTP), Value-Aware Fine-Tuning and the Hierarchy Enhanced Policy Optimization (HEPO) algorithm, forming a complete generative recommendation pipeline that unifies interest modeling, value alignment, and policy optimization. GPR has been fully deployed in the Tencent Weixin Channels advertising system, delivering significant improvements in key business metrics including GMV and CTCVR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As an intelligent infrastructure connecting users with commercial content, advertising recommendation systems play a central role in information flow and value creation within the digital economy. However, existing multi-stage advertising recommendation systems suffer from objective misalignment and error propagation, making it difficult to achieve global optimality, while unified generative recommendation models still struggle to meet the demands of practical industrial applications. To address these issues, we propose GPR (Generative Pre-trained Recommender), the first one-model framework that redefines advertising recommendation as an end-to-end generative task, replacing the traditional cascading paradigm with a unified generative approach. To realize GPR, we introduce three key innovations spanning unified representation, network architecture, and training strategy. First, we design a unified input schema and tokenization method tailored to advertising scenarios, mapping both ads and organic content into a shared multi-level semantic ID space, thereby enhancing semantic alignment and modeling consistency across heterogeneous data. Second, we develop the Heterogeneous Hierarchical Decoder (HHD), a dual-decoder architecture that decouples user intent modeling from ad generation, achieving a balance between training efficiency and inference flexibility while maintaining strong modeling capacity. Finally, we propose a multi-stage joint training strategy that integrates Multi-Token Prediction (MTP), Value-Aware Fine-Tuning and the Hierarchy Enhanced Policy Optimization (HEPO) algorithm, forming a complete generative recommendation pipeline that unifies interest modeling, value alignment, and policy optimization. GPR has been fully deployed in the Tencent Weixin Channels advertising system, delivering significant improvements in key business metrics including GMV and CTCVR."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T09:50:53Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    9,
                    50,
                    53,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "12 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Changping Wang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Yuling Xiong"
                    },
                    {
                        "name": "Xun Liu"
                    },
                    {
                        "name": "Haiyang Wu"
                    },
                    {
                        "name": "Qian Li"
                    },
                    {
                        "name": "Enming Zhang"
                    },
                    {
                        "name": "Jiawei Sun"
                    },
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Zishuai Zhang"
                    },
                    {
                        "name": "Ruoran Liu"
                    },
                    {
                        "name": "Suyuan Huang"
                    },
                    {
                        "name": "Zhaoxin Zhang"
                    },
                    {
                        "name": "Zhengkai Guo"
                    },
                    {
                        "name": "Shuojin Yang"
                    },
                    {
                        "name": "Meng-Hao Guo"
                    },
                    {
                        "name": "Huan Yu"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Shi-Min Hu"
                    }
                ],
                "author_detail": {
                    "name": "Shi-Min Hu"
                },
                "author": "Shi-Min Hu"
            },
            {
                "id": "http://arxiv.org/abs/2511.17180v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17180v1",
                "title": "Nonparametric Inference for Extreme CoVaR and CoES",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonparametric Inference for Extreme CoVaR and CoES"
                },
                "updated": "2025-11-21T12:01:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    1,
                    45,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17180v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Systemic risk measures quantify the potential risk to an individual financial constituent arising from the distress of entire financial system. As a generalization of two widely applied risk measures, Value-at-Risk and Expected Shortfall, the Conditional Value-at-Risk (CoVaR) and Conditional Expected Shortfall (CoES) have recently been receiving growing attention on applications in economics and finance, since they serve as crucial metrics for systemic risk measurement. However, existing approaches confront some challenges in statistical inference and asymptotic theories when estimating CoES, particularly at high risk levels. In this paper, within a framework of upper tail dependence, we propose several extrapolative methods to estimate both extreme CoVaR and CoES nonparametrically via an adjustment factor, which are intimately related to the nonparametric modelling of the tail dependence function. In addition, we study the asymptotic theories of all proposed extrapolative methods based on multivariate extreme value theory. Finally, some simulations and real data analyses are conducted to demonstrate the empirical performances of our methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systemic risk measures quantify the potential risk to an individual financial constituent arising from the distress of entire financial system. As a generalization of two widely applied risk measures, Value-at-Risk and Expected Shortfall, the Conditional Value-at-Risk (CoVaR) and Conditional Expected Shortfall (CoES) have recently been receiving growing attention on applications in economics and finance, since they serve as crucial metrics for systemic risk measurement. However, existing approaches confront some challenges in statistical inference and asymptotic theories when estimating CoES, particularly at high risk levels. In this paper, within a framework of upper tail dependence, we propose several extrapolative methods to estimate both extreme CoVaR and CoES nonparametrically via an adjustment factor, which are intimately related to the nonparametric modelling of the tail dependence function. In addition, we study the asymptotic theories of all proposed extrapolative methods based on multivariate extreme value theory. Finally, some simulations and real data analyses are conducted to demonstrate the empirical performances of our methods."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T12:01:45Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    1,
                    45,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Qingzhao Zhong"
                    },
                    {
                        "name": "Yanxi Hou"
                    }
                ],
                "author_detail": {
                    "name": "Yanxi Hou"
                },
                "author": "Yanxi Hou"
            },
            {
                "id": "http://arxiv.org/abs/2511.17179v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17179v1",
                "title": "Toward Sustainable Generative AI: A Scoping Review of Carbon Footprint and Environmental Impacts Across Training and Inference Stages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Sustainable Generative AI: A Scoping Review of Carbon Footprint and Environmental Impacts Across Training and Inference Stages"
                },
                "updated": "2025-11-21T11:59:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    59,
                    34,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17179v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generative AI is spreading rapidly, creating significant social and economic value while also raising concerns about its high energy use and environmental sustainability. While prior studies have predominantly focused on the energy-intensive nature of the training phase, the cumulative environmental footprint generated during large-scale service operations, particularly in the inference phase, has received comparatively less attention. To bridge this gap this study conducts a scoping review of methodologies and research trends in AI carbon footprint assessment. We analyze the classification and standardization status of existing AI carbon measurement tools and methodologies, and comparatively examine the environmental impacts arising from both training and inference stages. In addition, we identify how multidimensional factors such as model size, prompt complexity, serving environments, and system boundary definitions shape the resulting carbon footprint. Our review reveals critical limitations in current AI carbon accounting practices, including methodological inconsistencies, technology-specific biases, and insufficient attention to end-to-end system perspectives. Building on these insights, we propose future research and governance directions: (1) establishing standardized and transparent universal measurement protocols, (2) designing dynamic evaluation frameworks that incorporate user behavior, (3) developing life-cycle monitoring systems that encompass embodied emissions, and (4) advancing multidimensional sustainability assessment framework that balance model performance with environmental efficiency. This paper provides a foundation for interdisciplinary dialogue aimed at building a sustainable AI ecosystem and offers a baseline guideline for researchers seeking to understand the environmental implications of AI across technical, social, and operational dimensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI is spreading rapidly, creating significant social and economic value while also raising concerns about its high energy use and environmental sustainability. While prior studies have predominantly focused on the energy-intensive nature of the training phase, the cumulative environmental footprint generated during large-scale service operations, particularly in the inference phase, has received comparatively less attention. To bridge this gap this study conducts a scoping review of methodologies and research trends in AI carbon footprint assessment. We analyze the classification and standardization status of existing AI carbon measurement tools and methodologies, and comparatively examine the environmental impacts arising from both training and inference stages. In addition, we identify how multidimensional factors such as model size, prompt complexity, serving environments, and system boundary definitions shape the resulting carbon footprint. Our review reveals critical limitations in current AI carbon accounting practices, including methodological inconsistencies, technology-specific biases, and insufficient attention to end-to-end system perspectives. Building on these insights, we propose future research and governance directions: (1) establishing standardized and transparent universal measurement protocols, (2) designing dynamic evaluation frameworks that incorporate user behavior, (3) developing life-cycle monitoring systems that encompass embodied emissions, and (4) advancing multidimensional sustainability assessment framework that balance model performance with environmental efficiency. This paper provides a foundation for interdisciplinary dialogue aimed at building a sustainable AI ecosystem and offers a baseline guideline for researchers seeking to understand the environmental implications of AI across technical, social, and operational dimensions."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T11:59:34Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    59,
                    34,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "43 pages, 5 figure, 3 tables",
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Min-Kyu Kim"
                    },
                    {
                        "name": "Tae-An Yoo"
                    },
                    {
                        "name": "Ji-Bum Chung"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Bum Chung"
                },
                "author": "Ji-Bum Chung"
            },
            {
                "id": "http://arxiv.org/abs/2511.16449v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.16449v2",
                "title": "VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference"
                },
                "updated": "2025-11-21T11:57:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    57,
                    47,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.16449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.16449v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-Language-Action (VLA) models have shown great promise for embodied AI, yet the heavy computational cost of processing continuous visual streams severely limits their real-time deployment. Token pruning (keeping salient visual tokens and dropping redundant ones) has emerged as an effective approach for accelerating Vision-Language Models (VLMs), offering a solution for efficient VLA. However, these VLM-specific token pruning methods select tokens based solely on semantic salience metrics (e.g., prefill attention), while overlooking the VLA's intrinsic dual-system nature of high-level semantic understanding and low-level action execution. Consequently, these methods bias token retention toward semantic cues, discard critical information for action generation, and significantly degrade VLA performance. To bridge this gap, we propose VLA-Pruner, a versatile plug-and-play VLA-specific token prune method that aligns with the dual-system nature of VLA models and exploits the temporal continuity in robot manipulation. Specifically, VLA-Pruner adopts a dual-level importance criterion for visual token retention: vision-language prefill attention for semantic-level relevance and action decode attention, estimated via temporal smoothing, for action-level importance. Based on this criterion, VLA-Pruner proposes a novel dual-level token selection strategy that adaptively preserves a compact, informative set of visual tokens for both semantic understanding and action execution under given compute budget. Experiments show that VLA-Pruner achieves state-of-the-art performance across multiple VLA architectures and diverse robotic tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have shown great promise for embodied AI, yet the heavy computational cost of processing continuous visual streams severely limits their real-time deployment. Token pruning (keeping salient visual tokens and dropping redundant ones) has emerged as an effective approach for accelerating Vision-Language Models (VLMs), offering a solution for efficient VLA. However, these VLM-specific token pruning methods select tokens based solely on semantic salience metrics (e.g., prefill attention), while overlooking the VLA's intrinsic dual-system nature of high-level semantic understanding and low-level action execution. Consequently, these methods bias token retention toward semantic cues, discard critical information for action generation, and significantly degrade VLA performance. To bridge this gap, we propose VLA-Pruner, a versatile plug-and-play VLA-specific token prune method that aligns with the dual-system nature of VLA models and exploits the temporal continuity in robot manipulation. Specifically, VLA-Pruner adopts a dual-level importance criterion for visual token retention: vision-language prefill attention for semantic-level relevance and action decode attention, estimated via temporal smoothing, for action-level importance. Based on this criterion, VLA-Pruner proposes a novel dual-level token selection strategy that adaptively preserves a compact, informative set of visual tokens for both semantic understanding and action execution under given compute budget. Experiments show that VLA-Pruner achieves state-of-the-art performance across multiple VLA architectures and diverse robotic tasks."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-20T15:16:09Z",
                "published_parsed": [
                    2025,
                    11,
                    20,
                    15,
                    16,
                    9,
                    3,
                    324,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Ziyan Liu"
                    },
                    {
                        "name": "Yeqiu Chen"
                    },
                    {
                        "name": "Hongyi Cai"
                    },
                    {
                        "name": "Tao Lin"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Bo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhao"
                },
                "author": "Bo Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2511.17178v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17178v1",
                "title": "Efficient Robot Design with Multi-Objective Black-Box Optimization and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Robot Design with Multi-Objective Black-Box Optimization and Large Language Models"
                },
                "updated": "2025-11-21T11:56:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    56,
                    52,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17178v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Various methods for robot design optimization have been developed so far. These methods are diverse, ranging from numerical optimization to black-box optimization. While numerical optimization is fast, it is not suitable for cases involving complex structures or discrete values, leading to frequent use of black-box optimization instead. However, black-box optimization suffers from low sampling efficiency and takes considerable sampling iterations to obtain good solutions. In this study, we propose a method to enhance the efficiency of robot body design based on black-box optimization by utilizing large language models (LLMs). In parallel with the sampling process based on black-box optimization, sampling is performed using LLMs, which are provided with problem settings and extensive feedback. We demonstrate that this method enables more efficient exploration of design solutions and discuss its characteristics and limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various methods for robot design optimization have been developed so far. These methods are diverse, ranging from numerical optimization to black-box optimization. While numerical optimization is fast, it is not suitable for cases involving complex structures or discrete values, leading to frequent use of black-box optimization instead. However, black-box optimization suffers from low sampling efficiency and takes considerable sampling iterations to obtain good solutions. In this study, we propose a method to enhance the efficiency of robot body design based on black-box optimization by utilizing large language models (LLMs). In parallel with the sampling process based on black-box optimization, sampling is performed using LLMs, which are provided with problem settings and extensive feedback. We demonstrate that this method enables more efficient exploration of design solutions and discuss its characteristics and limitations."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T11:56:52Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    56,
                    52,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Kento Kawaharazuka"
                    },
                    {
                        "name": "Yoshiki Obinata"
                    },
                    {
                        "name": "Naoaki Kanazawa"
                    },
                    {
                        "name": "Haoyu Jia"
                    },
                    {
                        "name": "Kei Okada"
                    }
                ],
                "author_detail": {
                    "name": "Kei Okada"
                },
                "author": "Kei Okada"
            },
            {
                "id": "http://arxiv.org/abs/2505.12396v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.12396v4",
                "title": "LLM-CoT Enhanced Graph Neural Recommendation with Harmonized Group Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-CoT Enhanced Graph Neural Recommendation with Harmonized Group Policy Optimization"
                },
                "updated": "2025-11-21T11:54:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    54,
                    35,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.12396v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.12396v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Graph neural networks (GNNs) have advanced recommender systems by modeling interaction relationships. However, existing graph-based recommenders rely on sparse ID features and do not fully exploit textual information, resulting in low information density within representations. Furthermore, graph contrastive learning faces challenges. Random negative sampling can introduce false negative samples, while fixed temperature coefficients cannot adapt to the heterogeneity of different nodes. In addition, current efforts to enhance recommendations with large language models (LLMs) have not fully utilized their Chain-of-Thought (CoT) reasoning capabilities to guide representation learning. To address these limitations, we introduces LGHRec (LLM-CoT Enhanced Graph Neural Recommendation with Harmonized Group Policy Optimization). This framework leverages the CoT reasoning ability of LLMs to generate semantic IDs, enriching reasoning processes and improving information density and semantic quality of representations. Moreover, we design a reinforcement learning algorithm, Harmonized Group Policy Optimization (HGPO), to optimize negative sampling strategies and temperature coefficients in contrastive learning. This approach enhances long-tail recommendation performance and ensures optimization consistency across different groups. Experimental results on three datasets demonstrate that LGHRec improves representation quality through semantic IDs generated by LLM's CoT reasoning and effectively boosts contrastive learning with HGPO. Our method outperforms several baseline models. The code is available at: https://anonymous.4open.science/r/LLM-Rec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks (GNNs) have advanced recommender systems by modeling interaction relationships. However, existing graph-based recommenders rely on sparse ID features and do not fully exploit textual information, resulting in low information density within representations. Furthermore, graph contrastive learning faces challenges. Random negative sampling can introduce false negative samples, while fixed temperature coefficients cannot adapt to the heterogeneity of different nodes. In addition, current efforts to enhance recommendations with large language models (LLMs) have not fully utilized their Chain-of-Thought (CoT) reasoning capabilities to guide representation learning. To address these limitations, we introduces LGHRec (LLM-CoT Enhanced Graph Neural Recommendation with Harmonized Group Policy Optimization). This framework leverages the CoT reasoning ability of LLMs to generate semantic IDs, enriching reasoning processes and improving information density and semantic quality of representations. Moreover, we design a reinforcement learning algorithm, Harmonized Group Policy Optimization (HGPO), to optimize negative sampling strategies and temperature coefficients in contrastive learning. This approach enhances long-tail recommendation performance and ensures optimization consistency across different groups. Experimental results on three datasets demonstrate that LGHRec improves representation quality through semantic IDs generated by LLM's CoT reasoning and effectively boosts contrastive learning with HGPO. Our method outperforms several baseline models. The code is available at: https://anonymous.4open.science/r/LLM-Rec."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-18T12:50:36Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    12,
                    50,
                    36,
                    6,
                    138,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Hailong Luo"
                    },
                    {
                        "name": "Bin Wu"
                    },
                    {
                        "name": "Hongyong Jia"
                    },
                    {
                        "name": "Qingqing Zhu"
                    },
                    {
                        "name": "Lianlei Shan"
                    }
                ],
                "author_detail": {
                    "name": "Lianlei Shan"
                },
                "author": "Lianlei Shan"
            },
            {
                "id": "http://arxiv.org/abs/2511.07318v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.07318v2",
                "title": "When Bias Pretends to Be Truth: How Spurious Correlations Undermine Hallucination Detection in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Bias Pretends to Be Truth: How Spurious Correlations Undermine Hallucination Detection in LLMs"
                },
                "updated": "2025-11-21T11:45:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    45,
                    34,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.07318v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.07318v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Despite substantial advances, large language models (LLMs) continue to exhibit hallucinations, generating plausible yet incorrect responses. In this paper, we highlight a critical yet previously underexplored class of hallucinations driven by spurious correlations -- superficial but statistically prominent associations between features (e.g., surnames) and attributes (e.g., nationality) present in the training data. We demonstrate that these spurious correlations induce hallucinations that are confidently generated, immune to model scaling, evade current detection methods, and persist even after refusal fine-tuning. Through systematically controlled synthetic experiments and empirical evaluations on state-of-the-art open-source and proprietary LLMs (including GPT-5), we show that existing hallucination detection methods, such as confidence-based filtering and inner-state probing, fundamentally fail in the presence of spurious correlations. Our theoretical analysis further elucidates why these statistical biases intrinsically undermine confidence-based detection techniques. Our findings thus emphasize the urgent need for new approaches explicitly designed to address hallucinations caused by spurious correlations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite substantial advances, large language models (LLMs) continue to exhibit hallucinations, generating plausible yet incorrect responses. In this paper, we highlight a critical yet previously underexplored class of hallucinations driven by spurious correlations -- superficial but statistically prominent associations between features (e.g., surnames) and attributes (e.g., nationality) present in the training data. We demonstrate that these spurious correlations induce hallucinations that are confidently generated, immune to model scaling, evade current detection methods, and persist even after refusal fine-tuning. Through systematically controlled synthetic experiments and empirical evaluations on state-of-the-art open-source and proprietary LLMs (including GPT-5), we show that existing hallucination detection methods, such as confidence-based filtering and inner-state probing, fundamentally fail in the presence of spurious correlations. Our theoretical analysis further elucidates why these statistical biases intrinsically undermine confidence-based detection techniques. Our findings thus emphasize the urgent need for new approaches explicitly designed to address hallucinations caused by spurious correlations."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-10T17:19:27Z",
                "published_parsed": [
                    2025,
                    11,
                    10,
                    17,
                    19,
                    27,
                    0,
                    314,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Shaowen Wang"
                    },
                    {
                        "name": "Yiqi Dong"
                    },
                    {
                        "name": "Ruinian Chang"
                    },
                    {
                        "name": "Tansheng Zhu"
                    },
                    {
                        "name": "Yuebo Sun"
                    },
                    {
                        "name": "Kaifeng Lyu"
                    },
                    {
                        "name": "Jian Li"
                    }
                ],
                "author_detail": {
                    "name": "Jian Li"
                },
                "author": "Jian Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.17171v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17171v1",
                "title": "FireScope: Wildfire Risk Prediction with a Chain-of-Thought Oracle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FireScope: Wildfire Risk Prediction with a Chain-of-Thought Oracle"
                },
                "updated": "2025-11-21T11:45:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    45,
                    22,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17171v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Predicting wildfire risk is a reasoning-intensive spatial problem that requires the integration of visual, climatic, and geographic factors to infer continuous risk maps. Existing methods lack the causal reasoning and multimodal understanding required for reliable generalization. We introduce $\\textbf{FireScope-Bench}$, a large-scale dataset and benchmark that couples Sentinel-2 imagery and climate data with expert-defined risk rasters across the USA, and real wildfire events in Europe for cross-continental evaluation. Building on this dataset, we propose $\\textbf{FireScope}$, a VLM-based reasoning-to-generation framework that learns from both reinforcement learning and visual supervision to predict risk rasters with complementary reasoning traces. When trained in the USA and tested in Europe, $\\textbf{FireScope}$ achieves substantial performance gains, while expert feedback and automated analysis confirm that its reasoning traces are faithful and semantically meaningful. Our findings demonstrate that reasoning can ground raster prediction models, improving both generalization and interpretability. To our knowledge, this is the first framework to (1) demonstrate that language-based reasoning can improve generalization in visual generation, (2) propose a high-resolution wildfire risk model that can be applied across continents, and (3) enable systematic studies of robust cross-continental generalization for multimodal fire risk models. We believe that $\\textbf{FireScope-Bench}$ has the potential to serve as a foundation for advancing reasoning-driven, interpretable and generalizable spatial modeling. Data and source code will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting wildfire risk is a reasoning-intensive spatial problem that requires the integration of visual, climatic, and geographic factors to infer continuous risk maps. Existing methods lack the causal reasoning and multimodal understanding required for reliable generalization. We introduce $\\textbf{FireScope-Bench}$, a large-scale dataset and benchmark that couples Sentinel-2 imagery and climate data with expert-defined risk rasters across the USA, and real wildfire events in Europe for cross-continental evaluation. Building on this dataset, we propose $\\textbf{FireScope}$, a VLM-based reasoning-to-generation framework that learns from both reinforcement learning and visual supervision to predict risk rasters with complementary reasoning traces. When trained in the USA and tested in Europe, $\\textbf{FireScope}$ achieves substantial performance gains, while expert feedback and automated analysis confirm that its reasoning traces are faithful and semantically meaningful. Our findings demonstrate that reasoning can ground raster prediction models, improving both generalization and interpretability. To our knowledge, this is the first framework to (1) demonstrate that language-based reasoning can improve generalization in visual generation, (2) propose a high-resolution wildfire risk model that can be applied across continents, and (3) enable systematic studies of robust cross-continental generalization for multimodal fire risk models. We believe that $\\textbf{FireScope-Bench}$ has the potential to serve as a foundation for advancing reasoning-driven, interpretable and generalizable spatial modeling. Data and source code will be made publicly available."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T11:45:22Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    45,
                    22,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Mario Markov"
                    },
                    {
                        "name": "Stefan Maria Ailuro"
                    },
                    {
                        "name": "Luc Van Gool"
                    },
                    {
                        "name": "Konrad Schindler"
                    },
                    {
                        "name": "Danda Pani Paudel"
                    }
                ],
                "author_detail": {
                    "name": "Danda Pani Paudel"
                },
                "arxiv_affiliation": "ETH Zurich",
                "author": "Danda Pani Paudel"
            },
            {
                "id": "http://arxiv.org/abs/2511.17170v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17170v1",
                "title": "Hallucinate Less by Thinking More: Aspect-Based Causal Abstention for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinate Less by Thinking More: Aspect-Based Causal Abstention for Large Language Models"
                },
                "updated": "2025-11-21T11:42:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    42,
                    49,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17170v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17170v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) often produce fluent but factually incorrect responses, a phenomenon known as hallucination. Abstention, where the model chooses not to answer and instead outputs phrases such as \"I don't know\", is a common safeguard. However, existing abstention methods typically rely on post-generation signals, such as generation variations or feedback, which limits their ability to prevent unreliable responses in advance. In this paper, we introduce Aspect-Based Causal Abstention (ABCA), a new framework that enables early abstention by analysing the internal diversity of LLM knowledge through causal inference. This diversity reflects the multifaceted nature of parametric knowledge acquired from various sources, representing diverse aspects such as disciplines, legal contexts, or temporal frames. ABCA estimates causal effects conditioned on these aspects to assess the reliability of knowledge relevant to a given query. Based on these estimates, we enable two types of abstention: Type-1, where aspect effects are inconsistent (knowledge conflict), and Type-2, where aspect effects consistently support abstention (knowledge insufficiency). Experiments on standard benchmarks demonstrate that ABCA improves abstention reliability, achieves state-of-the-art performance, and enhances the interpretability of abstention decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often produce fluent but factually incorrect responses, a phenomenon known as hallucination. Abstention, where the model chooses not to answer and instead outputs phrases such as \"I don't know\", is a common safeguard. However, existing abstention methods typically rely on post-generation signals, such as generation variations or feedback, which limits their ability to prevent unreliable responses in advance. In this paper, we introduce Aspect-Based Causal Abstention (ABCA), a new framework that enables early abstention by analysing the internal diversity of LLM knowledge through causal inference. This diversity reflects the multifaceted nature of parametric knowledge acquired from various sources, representing diverse aspects such as disciplines, legal contexts, or temporal frames. ABCA estimates causal effects conditioned on these aspects to assess the reliability of knowledge relevant to a given query. Based on these estimates, we enable two types of abstention: Type-1, where aspect effects are inconsistent (knowledge conflict), and Type-2, where aspect effects consistently support abstention (knowledge insufficiency). Experiments on standard benchmarks demonstrate that ABCA improves abstention reliability, achieves state-of-the-art performance, and enhances the interpretability of abstention decisions."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T11:42:49Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    42,
                    49,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Accepted to AAAI 2026 (Main Technical Track)",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Vy Nguyen"
                    },
                    {
                        "name": "Ziqi Xu"
                    },
                    {
                        "name": "Jeffrey Chan"
                    },
                    {
                        "name": "Estrid He"
                    },
                    {
                        "name": "Feng Xia"
                    },
                    {
                        "name": "Xiuzhen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiuzhen Zhang"
                },
                "author": "Xiuzhen Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.17162v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17162v1",
                "title": "The Belief-Desire-Intention Ontology for modelling mental reality and agency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Belief-Desire-Intention Ontology for modelling mental reality and agency"
                },
                "updated": "2025-11-21T11:30:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    30,
                    17,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17162v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Belief-Desire-Intention (BDI) model is a cornerstone for representing rational agency in artificial intelligence and cognitive sciences. Yet, its integration into structured, semantically interoperable knowledge representations remains limited. This paper presents a formal BDI Ontology, conceived as a modular Ontology Design Pattern (ODP) that captures the cognitive architecture of agents through beliefs, desires, intentions, and their dynamic interrelations. The ontology ensures semantic precision and reusability by aligning with foundational ontologies and best practices in modular design. Two complementary lines of experimentation demonstrate its applicability: (i) coupling the ontology with Large Language Models (LLMs) via Logic Augmented Generation (LAG) to assess the contribution of ontological grounding to inferential coherence and consistency; and (ii) integrating the ontology within the Semas reasoning platform, which implements the Triples-to-Beliefs-to-Triples (T2B2T) paradigm, enabling a bidirectional flow between RDF triples and agent mental states. Together, these experiments illustrate how the BDI Ontology acts as both a conceptual and operational bridge between declarative and procedural intelligence, paving the way for cognitively grounded, explainable, and semantically interoperable multi-agent and neuro-symbolic systems operating within the Web of Data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Belief-Desire-Intention (BDI) model is a cornerstone for representing rational agency in artificial intelligence and cognitive sciences. Yet, its integration into structured, semantically interoperable knowledge representations remains limited. This paper presents a formal BDI Ontology, conceived as a modular Ontology Design Pattern (ODP) that captures the cognitive architecture of agents through beliefs, desires, intentions, and their dynamic interrelations. The ontology ensures semantic precision and reusability by aligning with foundational ontologies and best practices in modular design. Two complementary lines of experimentation demonstrate its applicability: (i) coupling the ontology with Large Language Models (LLMs) via Logic Augmented Generation (LAG) to assess the contribution of ontological grounding to inferential coherence and consistency; and (ii) integrating the ontology within the Semas reasoning platform, which implements the Triples-to-Beliefs-to-Triples (T2B2T) paradigm, enabling a bidirectional flow between RDF triples and agent mental states. Together, these experiments illustrate how the BDI Ontology acts as both a conceptual and operational bridge between declarative and procedural intelligence, paving the way for cognitively grounded, explainable, and semantically interoperable multi-agent and neuro-symbolic systems operating within the Web of Data."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T11:30:17Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    30,
                    17,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Sara Zuppiroli"
                    },
                    {
                        "name": "Carmelo Fabio Longo"
                    },
                    {
                        "name": "Anna Sofia Lippolis"
                    },
                    {
                        "name": "Rocco Paolillo"
                    },
                    {
                        "name": "Lorenzo Giammei"
                    },
                    {
                        "name": "Miguel Ceriani"
                    },
                    {
                        "name": "Francesco Poggi"
                    },
                    {
                        "name": "Antonio Zinilli"
                    },
                    {
                        "name": "Andrea Giovanni Nuzzolese"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Giovanni Nuzzolese"
                },
                "author": "Andrea Giovanni Nuzzolese"
            },
            {
                "id": "http://arxiv.org/abs/2511.17161v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17161v1",
                "title": "The PLLuM Instruction Corpus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The PLLuM Instruction Corpus"
                },
                "updated": "2025-11-21T11:28:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    28,
                    11,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17161v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper describes the instruction dataset used to fine-tune a set of transformer-based large language models (LLMs) developed in the PLLuM (Polish Large Language Model) project. We present a functional typology of the organic, converted, and synthetic instructions used in PLLuM and share some observations about the implications of using human-authored versus synthetic instruction datasets in the linguistic adaptation of base LLMs. Additionally, we release the first representative subset of the PLLuM instruction corpus (PLLuMIC), which we believe to be useful in guiding and planning the development of similar datasets for other LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper describes the instruction dataset used to fine-tune a set of transformer-based large language models (LLMs) developed in the PLLuM (Polish Large Language Model) project. We present a functional typology of the organic, converted, and synthetic instructions used in PLLuM and share some observations about the implications of using human-authored versus synthetic instruction datasets in the linguistic adaptation of base LLMs. Additionally, we release the first representative subset of the PLLuM instruction corpus (PLLuMIC), which we believe to be useful in guiding and planning the development of similar datasets for other LLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T11:28:11Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    28,
                    11,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Piotr Pzik"
                    },
                    {
                        "name": "Filip arnecki"
                    },
                    {
                        "name": "Konrad Kaczyski"
                    },
                    {
                        "name": "Anna Cichosz"
                    },
                    {
                        "name": "Zuzanna Deckert"
                    },
                    {
                        "name": "Monika Garnys"
                    },
                    {
                        "name": "Izabela Grabarczyk"
                    },
                    {
                        "name": "Wojciech Janowski"
                    },
                    {
                        "name": "Sylwia Karasiska"
                    },
                    {
                        "name": "Aleksandra Kujawiak"
                    },
                    {
                        "name": "Piotr Misztela"
                    },
                    {
                        "name": "Maria Szymaska"
                    },
                    {
                        "name": "Karolina Walkusz"
                    },
                    {
                        "name": "Igor Siek"
                    },
                    {
                        "name": "Maciej Chrabszcz"
                    },
                    {
                        "name": "Anna Koos"
                    },
                    {
                        "name": "Agnieszka Karliska"
                    },
                    {
                        "name": "Karolina Seweryn"
                    },
                    {
                        "name": "Aleksandra Krasnodbska"
                    },
                    {
                        "name": "Paula Betscher"
                    },
                    {
                        "name": "Zofia Cieliska"
                    },
                    {
                        "name": "Katarzyna Kowol"
                    },
                    {
                        "name": "Artur Wilczek"
                    },
                    {
                        "name": "Maciej Trzciski"
                    },
                    {
                        "name": "Katarzyna Dziewulska"
                    },
                    {
                        "name": "Roman Roszko"
                    },
                    {
                        "name": "Tomasz Berna"
                    },
                    {
                        "name": "Jurgita Vaienonien"
                    },
                    {
                        "name": "Danuta Roszko"
                    },
                    {
                        "name": "Pawe Levchuk"
                    },
                    {
                        "name": "Pawe Kowalski"
                    },
                    {
                        "name": "Irena Prawdzic-Jankowska"
                    },
                    {
                        "name": "Marek Kozowski"
                    },
                    {
                        "name": "Sawomir Dadas"
                    },
                    {
                        "name": "Rafa Powiata"
                    },
                    {
                        "name": "Alina Wrblewska"
                    },
                    {
                        "name": "Katarzyna Krasnowska-Kiera"
                    },
                    {
                        "name": "Maciej Ogrodniczuk"
                    },
                    {
                        "name": "Micha Rudolf"
                    },
                    {
                        "name": "Piotr Rybak"
                    },
                    {
                        "name": "Karolina Saputa"
                    },
                    {
                        "name": "Joanna Wooszyn"
                    },
                    {
                        "name": "Marcin Oleksy"
                    },
                    {
                        "name": "Bartomiej Koptyra"
                    },
                    {
                        "name": "Teddy Ferdinan"
                    },
                    {
                        "name": "Stanisaw Woniak"
                    },
                    {
                        "name": "Maciej Piasecki"
                    },
                    {
                        "name": "Pawe Walkowiak"
                    },
                    {
                        "name": "Konrad Wojtasik"
                    },
                    {
                        "name": "Arkadiusz Janz"
                    },
                    {
                        "name": "Przemysaw Kazienko"
                    },
                    {
                        "name": "Julia Moska"
                    },
                    {
                        "name": "Jan Koco"
                    }
                ],
                "author_detail": {
                    "name": "Jan Koco"
                },
                "author": "Jan Koco"
            },
            {
                "id": "http://arxiv.org/abs/2511.17155v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17155v1",
                "title": "UI-Styler: Ultrasound Image Style Transfer with Class-Aware Prompts for Cross-Device Diagnosis Using a Frozen Black-Box Inference Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UI-Styler: Ultrasound Image Style Transfer with Class-Aware Prompts for Cross-Device Diagnosis Using a Frozen Black-Box Inference Network"
                },
                "updated": "2025-11-21T11:19:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    19,
                    57,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17155v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The appearance of ultrasound images varies across acquisition devices, causing domain shifts that degrade the performance of fixed black-box downstream inference models when reused. To mitigate this issue, it is practical to develop unpaired image translation (UIT) methods that effectively align the statistical distributions between source and target domains, particularly under the constraint of a reused inference-blackbox setting. However, existing UIT approaches often overlook class-specific semantic alignment during domain adaptation, resulting in misaligned content-class mappings that can impair diagnostic accuracy. To address this limitation, we propose UI-Styler, a novel ultrasound-specific, class-aware image style transfer framework. UI-Styler leverages a pattern-matching mechanism to transfer texture patterns embedded in the target images onto source images while preserving the source structural content. In addition, we introduce a class-aware prompting strategy guided by pseudo labels of the target domain, which enforces accurate semantic alignment with diagnostic categories. Extensive experiments on ultrasound cross-device tasks demonstrate that UI-Styler consistently outperforms existing UIT methods, achieving state-of-the-art performance in distribution distance and downstream tasks, such as classification and segmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The appearance of ultrasound images varies across acquisition devices, causing domain shifts that degrade the performance of fixed black-box downstream inference models when reused. To mitigate this issue, it is practical to develop unpaired image translation (UIT) methods that effectively align the statistical distributions between source and target domains, particularly under the constraint of a reused inference-blackbox setting. However, existing UIT approaches often overlook class-specific semantic alignment during domain adaptation, resulting in misaligned content-class mappings that can impair diagnostic accuracy. To address this limitation, we propose UI-Styler, a novel ultrasound-specific, class-aware image style transfer framework. UI-Styler leverages a pattern-matching mechanism to transfer texture patterns embedded in the target images onto source images while preserving the source structural content. In addition, we introduce a class-aware prompting strategy guided by pseudo labels of the target domain, which enforces accurate semantic alignment with diagnostic categories. Extensive experiments on ultrasound cross-device tasks demonstrate that UI-Styler consistently outperforms existing UIT methods, achieving state-of-the-art performance in distribution distance and downstream tasks, such as classification and segmentation."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T11:19:57Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    19,
                    57,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Project page: https://dotrannhattuong.github.io/UIStyler, Accepted to WACV 2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Nhat-Tuong Do-Tran"
                    },
                    {
                        "name": "Ngoc-Hoang-Lam Le"
                    },
                    {
                        "name": "Ching-Chun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ching-Chun Huang"
                },
                "author": "Ching-Chun Huang"
            },
            {
                "id": "http://arxiv.org/abs/2511.17154v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17154v1",
                "title": "Proposal of an AI-Based Support Assistant for the ALICE-FIT Detector Setup at CERN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proposal of an AI-Based Support Assistant for the ALICE-FIT Detector Setup at CERN"
                },
                "updated": "2025-11-21T11:18:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    18,
                    59,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17154v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We propose an AI-based assistant designed to support the ALICE Fast Interaction Trigger (FIT) detector operators at CERN. The assistant helps diagnose and resolve operational issues in the Detector Control System (DCS), where decisions must often be made quickly and with incomplete information. By combining Large Language Models (LLMs) with a controlled Retrieval-Augmented Generation (RAG) pipeline, the system can generate context-aware suggestions based on verified ALICE-FIT documentation and problems that have appeared in the past.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an AI-based assistant designed to support the ALICE Fast Interaction Trigger (FIT) detector operators at CERN. The assistant helps diagnose and resolve operational issues in the Detector Control System (DCS), where decisions must often be made quickly and with incomplete information. By combining Large Language Models (LLMs) with a controlled Retrieval-Augmented Generation (RAG) pipeline, the system can generate context-aware suggestions based on verified ALICE-FIT documentation and problems that have appeared in the past."
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T11:18:59Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    18,
                    59,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex"
                },
                "authors": [
                    {
                        "name": "Ignacy Mermer"
                    },
                    {
                        "name": "Jakub Muszyski"
                    },
                    {
                        "name": "Jakub Moaryn"
                    },
                    {
                        "name": "Krystian Roson"
                    }
                ],
                "author_detail": {
                    "name": "Krystian Roson"
                },
                "author": "Krystian Roson"
            },
            {
                "id": "http://arxiv.org/abs/2511.17153v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17153v1",
                "title": "LangMark: A Multilingual Dataset for Automatic Post-Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LangMark: A Multilingual Dataset for Automatic Post-Editing"
                },
                "updated": "2025-11-21T11:18:15Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    18,
                    15,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17153v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.18653/v1/2025.acl-long.1569",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Automatic post-editing (APE) aims to correct errors in machine-translated text, enhancing translation quality, while reducing the need for human intervention. Despite advances in neural machine translation (NMT), the development of effective APE systems has been hindered by the lack of large-scale multilingual datasets specifically tailored to NMT outputs. To address this gap, we present and release LangMark, a new human-annotated multilingual APE dataset for English translation to seven languages: Brazilian Portuguese, French, German, Italian, Japanese, Russian, and Spanish. The dataset has 206,983 triplets, with each triplet consisting of a source segment, its NMT output, and a human post-edited translation. Annotated by expert human linguists, our dataset offers both linguistic diversity and scale. Leveraging this dataset, we empirically show that Large Language Models (LLMs) with few-shot prompting can effectively perform APE, improving upon leading commercial and even proprietary machine translation systems. We believe that this new resource will facilitate the future development and evaluation of APE systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic post-editing (APE) aims to correct errors in machine-translated text, enhancing translation quality, while reducing the need for human intervention. Despite advances in neural machine translation (NMT), the development of effective APE systems has been hindered by the lack of large-scale multilingual datasets specifically tailored to NMT outputs. To address this gap, we present and release LangMark, a new human-annotated multilingual APE dataset for English translation to seven languages: Brazilian Portuguese, French, German, Italian, Japanese, Russian, and Spanish. The dataset has 206,983 triplets, with each triplet consisting of a source segment, its NMT output, and a human post-edited translation. Annotated by expert human linguists, our dataset offers both linguistic diversity and scale. Leveraging this dataset, we empirically show that Large Language Models (LLMs) with few-shot prompting can effectively perform APE, improving upon leading commercial and even proprietary machine translation systems. We believe that this new resource will facilitate the future development and evaluation of APE systems."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T11:18:15Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    18,
                    15,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "15 pages, 8 figures, ACL 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "arxiv_journal_ref": "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2025, pages 32653-32667",
                "authors": [
                    {
                        "name": "Diego Velazquez"
                    },
                    {
                        "name": "Mikaela Grace"
                    },
                    {
                        "name": "Konstantinos Karageorgos"
                    },
                    {
                        "name": "Lawrence Carin"
                    },
                    {
                        "name": "Aaron Schliem"
                    },
                    {
                        "name": "Dimitrios Zaikis"
                    },
                    {
                        "name": "Roger Wechsler"
                    }
                ],
                "author_detail": {
                    "name": "Roger Wechsler"
                },
                "author": "Roger Wechsler",
                "arxiv_doi": "10.18653/v1/2025.acl-long.1569"
            },
            {
                "id": "http://arxiv.org/abs/2511.01588v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.01588v2",
                "title": "Explore More, Learn Better: Parallel MLLM Embeddings under Mutual Information Minimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explore More, Learn Better: Parallel MLLM Embeddings under Mutual Information Minimization"
                },
                "updated": "2025-11-21T11:16:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    16,
                    4,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.01588v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.01588v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Embedding models are a cornerstone of modern AI. Driven by Multimodal Large Language Models (MLLMs), they have made great progress in architecture and data curation, while the holistic paradigm is still limited to SSC, i.e., single input, singular embedding, contrastive supervision, which collapses rich, multifaceted inputs into monolithic embeddings and fails to fully exploit MLLM capabilities. In this paper, we tailor one Parallel Decoupling Framework (PDF) for multimodal embedding learning, by utilizing the proprietary steerability of MLLMs, i.e., their ability to flexibly generate quite differentiated response under explicit instructions. Concretely, PDF conditions a shared MLLM backbone on distinct, learnable prefixes to roll out multiple parallel paths for one input, then relies on these paths to obtain parallel embeddings. To promote full parallel diversity, we employ Mutual Information Minimization (MIM) as an explicit constraint, coupled with per-path contrastive supervision to maintain semantic alignment. Such dual-objectives force PDF to yield robust semantic coverage and a generalizable embedding space. Ultimately, the remarkable embedding space are accessible at inference via one single forward pass, incurring negligible computational overhead. We instantiate PDF on multiple MLLM backbones and prove its effectiveness on MMEB benchmark. Significant gains are consistently achieved across various resolutions and model sizes, e.g., boosting the VLM2Vec-LLaVA-1.6-LR model by a remarkable +8.9% (7B), while the VLM2Vec-Qwen2VL models by +4.2% (2B) and +3.1% (7B). In terms of efficiency, our 2B model surpasses its baseline by +2.6% using only half the computational budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding models are a cornerstone of modern AI. Driven by Multimodal Large Language Models (MLLMs), they have made great progress in architecture and data curation, while the holistic paradigm is still limited to SSC, i.e., single input, singular embedding, contrastive supervision, which collapses rich, multifaceted inputs into monolithic embeddings and fails to fully exploit MLLM capabilities. In this paper, we tailor one Parallel Decoupling Framework (PDF) for multimodal embedding learning, by utilizing the proprietary steerability of MLLMs, i.e., their ability to flexibly generate quite differentiated response under explicit instructions. Concretely, PDF conditions a shared MLLM backbone on distinct, learnable prefixes to roll out multiple parallel paths for one input, then relies on these paths to obtain parallel embeddings. To promote full parallel diversity, we employ Mutual Information Minimization (MIM) as an explicit constraint, coupled with per-path contrastive supervision to maintain semantic alignment. Such dual-objectives force PDF to yield robust semantic coverage and a generalizable embedding space. Ultimately, the remarkable embedding space are accessible at inference via one single forward pass, incurring negligible computational overhead. We instantiate PDF on multiple MLLM backbones and prove its effectiveness on MMEB benchmark. Significant gains are consistently achieved across various resolutions and model sizes, e.g., boosting the VLM2Vec-LLaVA-1.6-LR model by a remarkable +8.9% (7B), while the VLM2Vec-Qwen2VL models by +4.2% (2B) and +3.1% (7B). In terms of efficiency, our 2B model surpasses its baseline by +2.6% using only half the computational budget."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-03T13:57:08Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    13,
                    57,
                    8,
                    0,
                    307,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Zhicheng Wang"
                    },
                    {
                        "name": "Chen Ju"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Shuai Xiao"
                    },
                    {
                        "name": "Jinsong Lan"
                    },
                    {
                        "name": "Xiaoyong Zhu"
                    },
                    {
                        "name": "Ying Chen"
                    },
                    {
                        "name": "Zhiguo Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zhiguo Cao"
                },
                "author": "Zhiguo Cao"
            },
            {
                "id": "http://arxiv.org/abs/2506.08274v5",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.08274v5",
                "title": "The Impact of Feature Scaling In Machine Learning: Effects on Regression and Classification Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Feature Scaling In Machine Learning: Effects on Regression and Classification Tasks"
                },
                "updated": "2025-11-21T11:10:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    10,
                    4,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.08274v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.08274v5",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/ACCESS.2025.3635541",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "This research addresses the critical lack of comprehensive studies on feature scaling by systematically evaluating 12 scaling techniques - including several less common transformations - across 14 different Machine Learning algorithms and 16 datasets for classification and regression tasks. We meticulously analyzed impacts on predictive performance (using metrics such as accuracy, MAE, MSE, and $R^2$) and computational costs (training time, inference time, and memory usage). Key findings reveal that while ensemble methods (such as Random Forest and gradient boosting models like XGBoost, CatBoost and LightGBM) demonstrate robust performance largely independent of scaling, other widely used models such as Logistic Regression, SVMs, TabNet, and MLPs show significant performance variations highly dependent on the chosen scaler. This extensive empirical analysis, with all source code, experimental results, and model parameters made publicly available to ensure complete transparency and reproducibility, offers model-specific crucial guidance to practitioners on the need for an optimal selection of feature scaling techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research addresses the critical lack of comprehensive studies on feature scaling by systematically evaluating 12 scaling techniques - including several less common transformations - across 14 different Machine Learning algorithms and 16 datasets for classification and regression tasks. We meticulously analyzed impacts on predictive performance (using metrics such as accuracy, MAE, MSE, and $R^2$) and computational costs (training time, inference time, and memory usage). Key findings reveal that while ensemble methods (such as Random Forest and gradient boosting models like XGBoost, CatBoost and LightGBM) demonstrate robust performance largely independent of scaling, other widely used models such as Logistic Regression, SVMs, TabNet, and MLPs show significant performance variations highly dependent on the chosen scaler. This extensive empirical analysis, with all source code, experimental results, and model parameters made publicly available to ensure complete transparency and reproducibility, offers model-specific crucial guidance to practitioners on the need for an optimal selection of feature scaling techniques."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-09T22:32:51Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    22,
                    32,
                    51,
                    0,
                    160,
                    0
                ],
                "arxiv_comment": "36 pages",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "arxiv_journal_ref": "IEEE Access(2025)",
                "authors": [
                    {
                        "name": "Joo Manoel Herrera Pinheiro"
                    },
                    {
                        "name": "Suzana Vilas Boas de Oliveira"
                    },
                    {
                        "name": "Thiago Henrique Segreto Silva"
                    },
                    {
                        "name": "Pedro Antonio Rabelo Saraiva"
                    },
                    {
                        "name": "Enzo Ferreira de Souza"
                    },
                    {
                        "name": "Ricardo V. Godoy"
                    },
                    {
                        "name": "Leonardo Andr Ambrosio"
                    },
                    {
                        "name": "Marcelo Becker"
                    }
                ],
                "author_detail": {
                    "name": "Marcelo Becker"
                },
                "author": "Marcelo Becker",
                "arxiv_doi": "10.1109/ACCESS.2025.3635541"
            },
            {
                "id": "http://arxiv.org/abs/2511.17138v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17138v1",
                "title": "One-Step Diffusion Transformer for Controllable Real-World Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-Step Diffusion Transformer for Controllable Real-World Image Super-Resolution"
                },
                "updated": "2025-11-21T11:00:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    0,
                    59,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17138v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in diffusion-based real-world image super-resolution (Real-ISR) have demonstrated remarkable perceptual quality, yet the balance between fidelity and controllability remains a problem: multi-step diffusion-based methods suffer from generative diversity and randomness, resulting in low fidelity, while one-step methods lose control flexibility due to fidelity-specific finetuning. In this paper, we present ODTSR, a one-step diffusion transformer based on Qwen-Image that performs Real-ISR considering fidelity and controllability simultaneously: a newly introduced visual stream receives low-quality images (LQ) with adjustable noise (Control Noise), and the original visual stream receives LQs with consistent noise (Prior Noise), forming the Noise-hybrid Visual Stream (NVS) design. ODTSR further employs Fidelity-aware Adversarial Training (FAA) to enhance controllability and achieve one-step inference. Extensive experiments demonstrate that ODTSR not only achieves state-of-the-art (SOTA) performance on generic Real-ISR, but also enables prompt controllability on challenging scenarios such as real-world scene text image super-resolution (STISR) of Chinese characters without training on specific datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion-based real-world image super-resolution (Real-ISR) have demonstrated remarkable perceptual quality, yet the balance between fidelity and controllability remains a problem: multi-step diffusion-based methods suffer from generative diversity and randomness, resulting in low fidelity, while one-step methods lose control flexibility due to fidelity-specific finetuning. In this paper, we present ODTSR, a one-step diffusion transformer based on Qwen-Image that performs Real-ISR considering fidelity and controllability simultaneously: a newly introduced visual stream receives low-quality images (LQ) with adjustable noise (Control Noise), and the original visual stream receives LQs with consistent noise (Prior Noise), forming the Noise-hybrid Visual Stream (NVS) design. ODTSR further employs Fidelity-aware Adversarial Training (FAA) to enhance controllability and achieve one-step inference. Extensive experiments demonstrate that ODTSR not only achieves state-of-the-art (SOTA) performance on generic Real-ISR, but also enables prompt controllability on challenging scenarios such as real-world scene text image super-resolution (STISR) of Chinese characters without training on specific datasets."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T11:00:59Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    0,
                    59,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yushun Fang"
                    },
                    {
                        "name": "Yuxiang Chen"
                    },
                    {
                        "name": "Shibo Yin"
                    },
                    {
                        "name": "Qiang Hu"
                    },
                    {
                        "name": "Jiangchao Yao"
                    },
                    {
                        "name": "Ya Zhang"
                    },
                    {
                        "name": "Xiaoyun Zhang"
                    },
                    {
                        "name": "Yanfeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanfeng Wang"
                },
                "author": "Yanfeng Wang"
            },
            {
                "id": "http://arxiv.org/abs/2509.24975v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.24975v2",
                "title": "DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via Repetitive Pattern",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via Repetitive Pattern"
                },
                "updated": "2025-11-21T10:57:25Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    10,
                    57,
                    25,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.24975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.24975v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Software development relies heavily on extensive unit testing, which makes the efficiency of automated Unit Test Generation (UTG) particularly important. However, most existing LLMs generate test cases one token at a time in each forward pass, which leads to inefficient UTG. Recently, diffusion LLMs (dLLMs) have emerged, offering promising parallel generation capabilities and showing strong potential for efficient UTG. Despite this advantage, their application to UTG is still constrained by a clear trade-off between efficiency and test quality, since increasing the number of tokens generated in each step often causes a sharp decline in the quality of test cases. To overcome this limitation, we present DiffTester, an acceleration framework specifically tailored for dLLMs in UTG. The key idea of DiffTester is that unit tests targeting the same focal method often share repetitive structural patterns. By dynamically identifying these common patterns through abstract syntax tree analysis during generation, DiffTester adaptively increases the number of tokens produced at each step without compromising the quality of the output. To enable comprehensive evaluation, we extend the original TestEval benchmark, which was limited to Python, by introducing additional programming languages including Java and C++. Extensive experiments on three benchmarks with two representative models show that DiffTester delivers significant acceleration while preserving test coverage. Moreover, DiffTester generalizes well across different dLLMs and programming languages, providing a practical and scalable solution for efficient UTG in software development. Code and data are publicly available at https://github.com/wellbeingyang/DLM4UTG-open .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software development relies heavily on extensive unit testing, which makes the efficiency of automated Unit Test Generation (UTG) particularly important. However, most existing LLMs generate test cases one token at a time in each forward pass, which leads to inefficient UTG. Recently, diffusion LLMs (dLLMs) have emerged, offering promising parallel generation capabilities and showing strong potential for efficient UTG. Despite this advantage, their application to UTG is still constrained by a clear trade-off between efficiency and test quality, since increasing the number of tokens generated in each step often causes a sharp decline in the quality of test cases. To overcome this limitation, we present DiffTester, an acceleration framework specifically tailored for dLLMs in UTG. The key idea of DiffTester is that unit tests targeting the same focal method often share repetitive structural patterns. By dynamically identifying these common patterns through abstract syntax tree analysis during generation, DiffTester adaptively increases the number of tokens produced at each step without compromising the quality of the output. To enable comprehensive evaluation, we extend the original TestEval benchmark, which was limited to Python, by introducing additional programming languages including Java and C++. Extensive experiments on three benchmarks with two representative models show that DiffTester delivers significant acceleration while preserving test coverage. Moreover, DiffTester generalizes well across different dLLMs and programming languages, providing a practical and scalable solution for efficient UTG in software development. Code and data are publicly available at https://github.com/wellbeingyang/DLM4UTG-open ."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-29T16:04:18Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    4,
                    18,
                    0,
                    272,
                    0
                ],
                "arxiv_comment": "Update reference",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Lekang Yang"
                    },
                    {
                        "name": "Yuetong Liu"
                    },
                    {
                        "name": "Yitong Zhang"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.17129v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17129v1",
                "title": "Learning to Compress: Unlocking the Potential of Large Language Models for Text Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Compress: Unlocking the Potential of Large Language Models for Text Representation"
                },
                "updated": "2025-11-21T10:45:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    10,
                    45,
                    44,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17129v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Text representation plays a critical role in tasks like clustering, retrieval, and other downstream applications. With the emergence of large language models (LLMs), there is increasing interest in harnessing their capabilities for this purpose. However, most of the LLMs are inherently causal and optimized for next-token prediction, making them suboptimal for producing holistic representations. To address this, recent studies introduced pretext tasks to adapt LLMs for text representation. Most of these tasks, however, rely on token-level prediction objectives, such as the masked next-token prediction (MNTP) used in LLM2Vec. In this work, we explore the untapped potential of context compression as a pretext task for unsupervised adaptation of LLMs. During compression pre-training, the model learns to generate compact memory tokens, which substitute the whole context for downstream sequence prediction. Experiments demonstrate that a well-designed compression objective can significantly enhance LLM-based text representations, outperforming models trained with token-level pretext tasks. Further improvements through contrastive learning produce a strong representation model (LLM2Comp) that outperforms contemporary LLM-based text encoders on a wide range of tasks while being more sample-efficient, requiring significantly less training data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text representation plays a critical role in tasks like clustering, retrieval, and other downstream applications. With the emergence of large language models (LLMs), there is increasing interest in harnessing their capabilities for this purpose. However, most of the LLMs are inherently causal and optimized for next-token prediction, making them suboptimal for producing holistic representations. To address this, recent studies introduced pretext tasks to adapt LLMs for text representation. Most of these tasks, however, rely on token-level prediction objectives, such as the masked next-token prediction (MNTP) used in LLM2Vec. In this work, we explore the untapped potential of context compression as a pretext task for unsupervised adaptation of LLMs. During compression pre-training, the model learns to generate compact memory tokens, which substitute the whole context for downstream sequence prediction. Experiments demonstrate that a well-designed compression objective can significantly enhance LLM-based text representations, outperforming models trained with token-level pretext tasks. Further improvements through contrastive learning produce a strong representation model (LLM2Comp) that outperforms contemporary LLM-based text encoders on a wide range of tasks while being more sample-efficient, requiring significantly less training data."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T10:45:44Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    10,
                    45,
                    44,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Accepted by AAAI'26",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yeqin Zhang"
                    },
                    {
                        "name": "Yizheng Zhao"
                    },
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Ruihang Miao"
                    },
                    {
                        "name": "Cam-Tu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Cam-Tu Nguyen"
                },
                "author": "Cam-Tu Nguyen"
            },
            {
                "id": "http://arxiv.org/abs/2511.17127v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17127v1",
                "title": "Training Foundation Models on a Full-Stack AMD Platform: Compute, Networking, and System Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Foundation Models on a Full-Stack AMD Platform: Compute, Networking, and System Design"
                },
                "updated": "2025-11-21T10:44:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    10,
                    44,
                    2,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17127v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We report on the first large-scale mixture-of-experts (MoE) pretraining study on pure AMD hardware, utilizing both MI300X GPUs with Pollara interconnect. We distill practical guidance for both systems and model design. On the systems side, we deliver a comprehensive cluster and networking characterization: microbenchmarks for all core collectives (all-reduce, reduce-scatter, all-gather, broadcast) across message sizes and GPU counts on Pollara. To our knowledge, this is the first at this scale. We further provide MI300X microbenchmarks on kernel sizing and memory bandwidth to inform model design. On the modeling side, we introduce and apply MI300X-aware transformer sizing rules for attention and MLP blocks and justify MoE widths that jointly optimize training throughput and inference latency. We describe our training stack in depth, including often-ignored utilities such as fault-tolerance and checkpoint-reshaping, as well as detailed information on our training recipe. We also provide a preview of our model architecture and base model - ZAYA1 (760M active, 8.3B total parameters MoE) - which will be further improved upon in forthcoming papers. ZAYA1-base achieves performance comparable to leading base models such as Qwen3-4B and Gemma3-12B at its scale and larger, and outperforms models including Llama-3-8B and OLMoE across reasoning, mathematics, and coding benchmarks. Together, these results demonstrate that the AMD hardware, network, and software stack are mature and optimized enough for competitive large-scale pretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on the first large-scale mixture-of-experts (MoE) pretraining study on pure AMD hardware, utilizing both MI300X GPUs with Pollara interconnect. We distill practical guidance for both systems and model design. On the systems side, we deliver a comprehensive cluster and networking characterization: microbenchmarks for all core collectives (all-reduce, reduce-scatter, all-gather, broadcast) across message sizes and GPU counts on Pollara. To our knowledge, this is the first at this scale. We further provide MI300X microbenchmarks on kernel sizing and memory bandwidth to inform model design. On the modeling side, we introduce and apply MI300X-aware transformer sizing rules for attention and MLP blocks and justify MoE widths that jointly optimize training throughput and inference latency. We describe our training stack in depth, including often-ignored utilities such as fault-tolerance and checkpoint-reshaping, as well as detailed information on our training recipe. We also provide a preview of our model architecture and base model - ZAYA1 (760M active, 8.3B total parameters MoE) - which will be further improved upon in forthcoming papers. ZAYA1-base achieves performance comparable to leading base models such as Qwen3-4B and Gemma3-12B at its scale and larger, and outperforms models including Llama-3-8B and OLMoE across reasoning, mathematics, and coding benchmarks. Together, these results demonstrate that the AMD hardware, network, and software stack are mature and optimized enough for competitive large-scale pretraining."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T10:44:02Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    10,
                    44,
                    2,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Quentin Anthony"
                    },
                    {
                        "name": "Yury Tokpanov"
                    },
                    {
                        "name": "Skyler Szot"
                    },
                    {
                        "name": "Srivatsan Rajagopal"
                    },
                    {
                        "name": "Praneeth Medepalli"
                    },
                    {
                        "name": "Rishi Iyer"
                    },
                    {
                        "name": "Vasu Shyam"
                    },
                    {
                        "name": "Anna Golubeva"
                    },
                    {
                        "name": "Ansh Chaurasia"
                    },
                    {
                        "name": "Xiao Yang"
                    },
                    {
                        "name": "Tomas Figliolia"
                    },
                    {
                        "name": "Robert Washbourne"
                    },
                    {
                        "name": "Drew Thorstensen"
                    },
                    {
                        "name": "Amartey Pearson"
                    },
                    {
                        "name": "Zack Grossbart"
                    },
                    {
                        "name": "Jason van Patten"
                    },
                    {
                        "name": "Emad Barsoum"
                    },
                    {
                        "name": "Zhenyu Gu"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Beren Millidge"
                    }
                ],
                "author_detail": {
                    "name": "Beren Millidge"
                },
                "author": "Beren Millidge"
            },
            {
                "id": "http://arxiv.org/abs/2511.17124v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17124v1",
                "title": "A Counterfactual LLM Framework for Detecting Human Biases: A Case Study of Sex/Gender in Emergency Triage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Counterfactual LLM Framework for Detecting Human Biases: A Case Study of Sex/Gender in Emergency Triage"
                },
                "updated": "2025-11-21T10:37:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    10,
                    37,
                    52,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17124v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17124v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present a novel, domain-agnostic counterfactual approach that uses Large Language Models (LLMs) to quantify gender disparities in human clinical decision-making. The method trains an LLM to emulate observed decisions, then evaluates counterfactual pairs in which only gender is flipped, estimating directional disparities while holding all other clinical factors constant. We study emergency triage, validating the approach on more than 150,000 admissions to the Bordeaux University Hospital (France) and replicating results on a subset of MIMIC-IV across a different language, population, and healthcare system. In the Bordeaux cohort, otherwise identical presentations were approximately 2.1% more likely to receive a lower-severity triage score when presented as female rather than male; scaled to national emergency volumes in France, this corresponds to more than 200,000 lower-severity assignments per year. Modality-specific analyses indicate that both explicit tabular gender indicators and implicit textual gender cues contribute to the disparity. Beyond emergency care, the approach supports bias audits in other settings (e.g., hiring, academic, and justice decisions), providing a scalable tool to detect and address inequities in real-world decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel, domain-agnostic counterfactual approach that uses Large Language Models (LLMs) to quantify gender disparities in human clinical decision-making. The method trains an LLM to emulate observed decisions, then evaluates counterfactual pairs in which only gender is flipped, estimating directional disparities while holding all other clinical factors constant. We study emergency triage, validating the approach on more than 150,000 admissions to the Bordeaux University Hospital (France) and replicating results on a subset of MIMIC-IV across a different language, population, and healthcare system. In the Bordeaux cohort, otherwise identical presentations were approximately 2.1% more likely to receive a lower-severity triage score when presented as female rather than male; scaled to national emergency volumes in France, this corresponds to more than 200,000 lower-severity assignments per year. Modality-specific analyses indicate that both explicit tabular gender indicators and implicit textual gender cues contribute to the disparity. Beyond emergency care, the approach supports bias audits in other settings (e.g., hiring, academic, and justice decisions), providing a scalable tool to detect and address inequities in real-world decision-making."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T10:37:52Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    10,
                    37,
                    52,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Currently under review at npj Digital Medicine",
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Ariel Guerra-Adames"
                    },
                    {
                        "name": "Marta Avalos-Fernandez"
                    },
                    {
                        "name": "Ocane Dormus"
                    },
                    {
                        "name": "Leo Anthony Celi"
                    },
                    {
                        "name": "Cdric Gil-Jardin"
                    },
                    {
                        "name": "Emmanuel Lagarde"
                    }
                ],
                "author_detail": {
                    "name": "Emmanuel Lagarde"
                },
                "author": "Emmanuel Lagarde"
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2511.15846v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.15846v2",
                "title": "The Loss of Control Playbook: Degrees, Dynamics, and Preparedness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Loss of Control Playbook: Degrees, Dynamics, and Preparedness"
                },
                "updated": "2025-11-21T18:53:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    18,
                    53,
                    3,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.15846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.15846v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This research report addresses the absence of an actionable definition for Loss of Control (LoC) in AI systems by developing a novel taxonomy and preparedness framework. Despite increasing policy and research attention, existing LoC definitions vary significantly in scope and timeline, hindering effective LoC assessment and mitigation. To address this issue, we draw from an extensive literature review and propose a graded LoC taxonomy, based on the metrics of severity and persistence, that distinguishes between Deviation, Bounded LoC, and Strict LoC. We model pathways toward a societal state of vulnerability in which sufficiently advanced AI systems have acquired or could acquire the means to cause Bounded or Strict LoC once a catalyst, either misalignment or pure malfunction, materializes. We argue that this state becomes increasingly likely over time, absent strategic intervention, and propose a strategy to avoid reaching a state of vulnerability. Rather than focusing solely on intervening on AI capabilities and propensities potentially relevant for LoC or on preventing potential catalysts, we introduce a complementary framework that emphasizes three extrinsic factors: Deployment context, Affordances, and Permissions (the DAP framework). Compared to work on intrinsic factors and catalysts, this framework has the unfair advantage of being actionable today. Finally, we put forward a plan to maintain preparedness and prevent the occurrence of LoC outcomes should a state of societal vulnerability be reached, focusing on governance measures (threat modeling, deployment policies, emergency response) and technical controls (pre-deployment testing, control measures, monitoring) that could maintain a condition of perennial suspension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research report addresses the absence of an actionable definition for Loss of Control (LoC) in AI systems by developing a novel taxonomy and preparedness framework. Despite increasing policy and research attention, existing LoC definitions vary significantly in scope and timeline, hindering effective LoC assessment and mitigation. To address this issue, we draw from an extensive literature review and propose a graded LoC taxonomy, based on the metrics of severity and persistence, that distinguishes between Deviation, Bounded LoC, and Strict LoC. We model pathways toward a societal state of vulnerability in which sufficiently advanced AI systems have acquired or could acquire the means to cause Bounded or Strict LoC once a catalyst, either misalignment or pure malfunction, materializes. We argue that this state becomes increasingly likely over time, absent strategic intervention, and propose a strategy to avoid reaching a state of vulnerability. Rather than focusing solely on intervening on AI capabilities and propensities potentially relevant for LoC or on preventing potential catalysts, we introduce a complementary framework that emphasizes three extrinsic factors: Deployment context, Affordances, and Permissions (the DAP framework). Compared to work on intrinsic factors and catalysts, this framework has the unfair advantage of being actionable today. Finally, we put forward a plan to maintain preparedness and prevent the occurrence of LoC outcomes should a state of societal vulnerability be reached, focusing on governance measures (threat modeling, deployment policies, emergency response) and technical controls (pre-deployment testing, control measures, monitoring) that could maintain a condition of perennial suspension."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-19T20:10:39Z",
                "published_parsed": [
                    2025,
                    11,
                    19,
                    20,
                    10,
                    39,
                    2,
                    323,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Charlotte Stix"
                    },
                    {
                        "name": "Annika Hallensleben"
                    },
                    {
                        "name": "Alejandro Ortega"
                    },
                    {
                        "name": "Matteo Pistillo"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Pistillo"
                },
                "author": "Matteo Pistillo"
            },
            {
                "id": "http://arxiv.org/abs/2511.17487v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17487v1",
                "title": "Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models"
                },
                "updated": "2025-11-21T18:43:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    18,
                    43,
                    1,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17487v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Scaling up multimodal models has enabled remarkable advances in visual understanding and reasoning, but practical demands call for smaller, efficient systems. In this work, we conduct a principled analysis of downscaling intelligence in multimodal models, examining how reduced large language model (LLM) capacity affects multimodal capabilities. Our initial findings reveal an interesting trend: LLM downscaling disproportionately affects visual capabilities, rather than abilities inherited from the LLM. We then examine whether this drop mainly reflects the expected decline in visual reasoning or a more fundamental loss of perceptual abilities. Isolating the effect of LLM downscaling on perception, we find performance still drops sharply, often matching or exceeding the impact on reasoning. To address this bottleneck, we introduce visual extraction tuning, which explicitly trains the model to extract instruction-relevant visual details consistently across tasks. With these extracted visual details, we then apply step-by-step reasoning to generate answers. Together, these components form our Extract+Think approach, setting a new standard for efficiency and performance in this space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling up multimodal models has enabled remarkable advances in visual understanding and reasoning, but practical demands call for smaller, efficient systems. In this work, we conduct a principled analysis of downscaling intelligence in multimodal models, examining how reduced large language model (LLM) capacity affects multimodal capabilities. Our initial findings reveal an interesting trend: LLM downscaling disproportionately affects visual capabilities, rather than abilities inherited from the LLM. We then examine whether this drop mainly reflects the expected decline in visual reasoning or a more fundamental loss of perceptual abilities. Isolating the effect of LLM downscaling on perception, we find performance still drops sharply, often matching or exceeding the impact on reasoning. To address this bottleneck, we introduce visual extraction tuning, which explicitly trains the model to extract instruction-relevant visual details consistently across tasks. With these extracted visual details, we then apply step-by-step reasoning to generate answers. Together, these components form our Extract+Think approach, setting a new standard for efficiency and performance in this space."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T18:43:01Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    18,
                    43,
                    1,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Website at https://web.stanford.edu/~markendo/projects/downscaling_intelligence",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Mark Endo"
                    },
                    {
                        "name": "Serena Yeung-Levy"
                    }
                ],
                "author_detail": {
                    "name": "Serena Yeung-Levy"
                },
                "author": "Serena Yeung-Levy"
            },
            {
                "id": "http://arxiv.org/abs/2511.17473v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17473v1",
                "title": "Masked-and-Reordered Self-Supervision for Reinforcement Learning from Verifiable Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked-and-Reordered Self-Supervision for Reinforcement Learning from Verifiable Rewards"
                },
                "updated": "2025-11-21T18:23:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    18,
                    23,
                    4,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17473v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Test-time scaling has been shown to substantially improve large language models' (LLMs) mathematical reasoning. However, for a large portion of mathematical corpora, especially theorem proving, RLVR's scalability is limited: intermediate reasoning is crucial, while final answers are difficult to directly and reliably verify. Meanwhile, token-level SFT often degenerates into rote memorization rather than inducing longer chains of thought. Inspired by BERT's self-supervised tasks, we propose MR-RLVR (Masked-and-Reordered RLVR), which constructs process-level self-supervised rewards via \"masked-then-fill\" and \"step reordering\" to extract learnable signals from intermediate reasoning. Our training pipeline comprises two stages: we first perform self-supervised training on sampled mathematical calculation and proof data; we then conduct RLVR fine-tuning on mathematical calculation datasets where only outcomes are verifiable. We implement MR-RLVR on Qwen2.5-3B and DeepSeek-R1-Distill-Qwen-1.5B, and evaluate on AIME24, AIME25, AMC23, and MATH500. Under a fixed sampling and decoding budget, MR-RLVR achieves average relative gains over the original RLVR of +9.86% Pass@1, +5.27% Pass@5, and +4.00% Pass@8. These results indicate that incorporating process-aware self-supervised signals can effectively enhance RLVR's scalability and performance in only outcome-verifiable settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling has been shown to substantially improve large language models' (LLMs) mathematical reasoning. However, for a large portion of mathematical corpora, especially theorem proving, RLVR's scalability is limited: intermediate reasoning is crucial, while final answers are difficult to directly and reliably verify. Meanwhile, token-level SFT often degenerates into rote memorization rather than inducing longer chains of thought. Inspired by BERT's self-supervised tasks, we propose MR-RLVR (Masked-and-Reordered RLVR), which constructs process-level self-supervised rewards via \"masked-then-fill\" and \"step reordering\" to extract learnable signals from intermediate reasoning. Our training pipeline comprises two stages: we first perform self-supervised training on sampled mathematical calculation and proof data; we then conduct RLVR fine-tuning on mathematical calculation datasets where only outcomes are verifiable. We implement MR-RLVR on Qwen2.5-3B and DeepSeek-R1-Distill-Qwen-1.5B, and evaluate on AIME24, AIME25, AMC23, and MATH500. Under a fixed sampling and decoding budget, MR-RLVR achieves average relative gains over the original RLVR of +9.86% Pass@1, +5.27% Pass@5, and +4.00% Pass@8. These results indicate that incorporating process-aware self-supervised signals can effectively enhance RLVR's scalability and performance in only outcome-verifiable settings."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T18:23:04Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    18,
                    23,
                    4,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Zhifeng Gao"
                    },
                    {
                        "name": "Guolin Ke"
                    }
                ],
                "author_detail": {
                    "name": "Guolin Ke"
                },
                "author": "Guolin Ke"
            },
            {
                "id": "http://arxiv.org/abs/2509.21651v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.21651v2",
                "title": "Can AI Perceive Physical Danger and Intervene?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can AI Perceive Physical Danger and Intervene?"
                },
                "updated": "2025-11-21T18:22:41Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    18,
                    22,
                    41,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.21651v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.21651v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "When AI interacts with the physical world -- as a robot or an assistive agent -- new safety challenges emerge beyond those of purely ``digital AI\". In such interactions, the potential for physical harm is direct and immediate. How well do state-of-the-art foundation models understand common-sense facts about physical safety, e.g. that a box may be too heavy to lift, or that a hot cup of coffee should not be handed to a child? In this paper, our contributions are three-fold: first, we develop a highly scalable approach to continuous physical safety benchmarking of Embodied AI systems, grounded in real-world injury narratives and operational safety constraints. To probe multi-modal safety understanding, we turn these narratives and constraints into photorealistic images and videos capturing transitions from safe to unsafe states, using advanced generative models. Secondly, we comprehensively analyze the ability of major foundation models to perceive risks, reason about safety, and trigger interventions; this yields multi-faceted insights into their deployment readiness for safety-critical agentic applications. Finally, we develop a post-training paradigm to teach models to explicitly reason about embodiment-specific safety constraints provided through system instructions. The resulting models generate thinking traces that make safety reasoning interpretable and transparent, achieving state of the art performance in constraint satisfaction evaluations. The benchmark is released at https://asimov-benchmark.github.io/v2",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When AI interacts with the physical world -- as a robot or an assistive agent -- new safety challenges emerge beyond those of purely ``digital AI\". In such interactions, the potential for physical harm is direct and immediate. How well do state-of-the-art foundation models understand common-sense facts about physical safety, e.g. that a box may be too heavy to lift, or that a hot cup of coffee should not be handed to a child? In this paper, our contributions are three-fold: first, we develop a highly scalable approach to continuous physical safety benchmarking of Embodied AI systems, grounded in real-world injury narratives and operational safety constraints. To probe multi-modal safety understanding, we turn these narratives and constraints into photorealistic images and videos capturing transitions from safe to unsafe states, using advanced generative models. Secondly, we comprehensively analyze the ability of major foundation models to perceive risks, reason about safety, and trigger interventions; this yields multi-faceted insights into their deployment readiness for safety-critical agentic applications. Finally, we develop a post-training paradigm to teach models to explicitly reason about embodiment-specific safety constraints provided through system instructions. The resulting models generate thinking traces that make safety reasoning interpretable and transparent, achieving state of the art performance in constraint satisfaction evaluations. The benchmark is released at https://asimov-benchmark.github.io/v2"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-25T22:09:17Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    22,
                    9,
                    17,
                    3,
                    268,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Abhishek Jindal"
                    },
                    {
                        "name": "Dmitry Kalashnikov"
                    },
                    {
                        "name": "R. Alex Hofer"
                    },
                    {
                        "name": "Oscar Chang"
                    },
                    {
                        "name": "Divya Garikapati"
                    },
                    {
                        "name": "Anirudha Majumdar"
                    },
                    {
                        "name": "Pierre Sermanet"
                    },
                    {
                        "name": "Vikas Sindhwani"
                    }
                ],
                "author_detail": {
                    "name": "Vikas Sindhwani"
                },
                "author": "Vikas Sindhwani"
            },
            {
                "id": "http://arxiv.org/abs/2511.17467v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17467v1",
                "title": "PersonaAgent with GraphRAG: Community-Aware Knowledge Graphs for Personalized LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonaAgent with GraphRAG: Community-Aware Knowledge Graphs for Personalized LLM"
                },
                "updated": "2025-11-21T18:15:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    18,
                    15,
                    47,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17467v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We propose a novel framework for persona-based language model system, motivated by the need for personalized AI agents that adapt to individual user preferences. In our approach, the agent embodies the user's \"persona\" (e.g. user profile or taste) and is powered by a large language model (LLM). To enable the agent to leverage rich contextual information, we introduce a Knowledge-Graph-enhanced Retrieval-Augmented Generation (Graph RAG) mechanism that constructs an LLM-derived graph index of relevant documents and summarizes communities of related information. Our framework generates personalized prompts by combining: (1) a summary of the user's historical behaviors and preferences extracted from the knowledge graph, and (2) relevant global interaction patterns identified through graph-based community detection. This dynamic prompt engineering approach allows the agent to maintain consistent persona-aligned behaviors while benefiting from collective knowledge. On the LaMP benchmark, our method improves news categorization F1 by 11.1%, movie tagging F1 by 56.1%, and reduces product rating MAE by 10.4% over prior methods. Our code is available at https://anonymous.4open.science/r/PersonaAgentwGraphRAG-DE6F",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel framework for persona-based language model system, motivated by the need for personalized AI agents that adapt to individual user preferences. In our approach, the agent embodies the user's \"persona\" (e.g. user profile or taste) and is powered by a large language model (LLM). To enable the agent to leverage rich contextual information, we introduce a Knowledge-Graph-enhanced Retrieval-Augmented Generation (Graph RAG) mechanism that constructs an LLM-derived graph index of relevant documents and summarizes communities of related information. Our framework generates personalized prompts by combining: (1) a summary of the user's historical behaviors and preferences extracted from the knowledge graph, and (2) relevant global interaction patterns identified through graph-based community detection. This dynamic prompt engineering approach allows the agent to maintain consistent persona-aligned behaviors while benefiting from collective knowledge. On the LaMP benchmark, our method improves news categorization F1 by 11.1%, movie tagging F1 by 56.1%, and reduces product rating MAE by 10.4% over prior methods. Our code is available at https://anonymous.4open.science/r/PersonaAgentwGraphRAG-DE6F"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T18:15:47Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    18,
                    15,
                    47,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Siqi Liang"
                    },
                    {
                        "name": "Yudi Zhang"
                    },
                    {
                        "name": "Yue Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yue Guo"
                },
                "author": "Yue Guo"
            },
            {
                "id": "http://arxiv.org/abs/2508.00086v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.00086v2",
                "title": "Do LLMs produce texts with \"human-like\" lexical diversity?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs produce texts with \"human-like\" lexical diversity?"
                },
                "updated": "2025-11-21T18:13:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    18,
                    13,
                    28,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.00086v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.00086v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The degree to which large language models (LLMs) produce writing that is truly human-like remains unclear despite the extensive empirical attention that this question has received. The present study addresses this question from the perspective of lexical diversity. Specifically, the study investigates patterns of lexical diversity in LLM-generated texts from four ChatGPT models (ChatGPT-3.5, ChatGPT-4, ChatGPT-o4 mini, and ChatGPT-4.5) in comparison with texts written by L1 and L2 English participants (n = 240) across four education levels. Six dimensions of lexical diversity were measured in each text: volume, abundance, variety-repetition, evenness, disparity, and dispersion. Results from one-way MANOVAs, one-way ANOVAs, and Support Vector Machines revealed that the ChatGPT-generated texts differed significantly from human-written texts for each variable, with ChatGPT-o4 mini and ChatGPT-4.5 differing the most. Within these two groups, ChatGPT-4.5 demonstrated higher levels of lexical diversity than older models despite producing fewer tokens. The human writers' lexical diversity did not differ across subgroups (i.e., education, language status). Altogether, the results indicate that ChatGPT models do not produce human-like texts in relation to lexical diversity, and the newer models produce less human-like text than older models. We discuss the implications of these results for language pedagogy and related applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The degree to which large language models (LLMs) produce writing that is truly human-like remains unclear despite the extensive empirical attention that this question has received. The present study addresses this question from the perspective of lexical diversity. Specifically, the study investigates patterns of lexical diversity in LLM-generated texts from four ChatGPT models (ChatGPT-3.5, ChatGPT-4, ChatGPT-o4 mini, and ChatGPT-4.5) in comparison with texts written by L1 and L2 English participants (n = 240) across four education levels. Six dimensions of lexical diversity were measured in each text: volume, abundance, variety-repetition, evenness, disparity, and dispersion. Results from one-way MANOVAs, one-way ANOVAs, and Support Vector Machines revealed that the ChatGPT-generated texts differed significantly from human-written texts for each variable, with ChatGPT-o4 mini and ChatGPT-4.5 differing the most. Within these two groups, ChatGPT-4.5 demonstrated higher levels of lexical diversity than older models despite producing fewer tokens. The human writers' lexical diversity did not differ across subgroups (i.e., education, language status). Altogether, the results indicate that ChatGPT models do not produce human-like texts in relation to lexical diversity, and the newer models produce less human-like text than older models. We discuss the implications of these results for language pedagogy and related applications."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-31T18:22:11Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    18,
                    22,
                    11,
                    3,
                    212,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Kelly Kendro"
                    },
                    {
                        "name": "Jeffrey Maloney"
                    },
                    {
                        "name": "Scott Jarvis"
                    }
                ],
                "author_detail": {
                    "name": "Scott Jarvis"
                },
                "author": "Scott Jarvis"
            },
            {
                "id": "http://arxiv.org/abs/2511.17464v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17464v1",
                "title": "A Patient-Centric Blockchain Framework for Secure Electronic Health Record Management: Decoupling Data Storage from Access Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Patient-Centric Blockchain Framework for Secure Electronic Health Record Management: Decoupling Data Storage from Access Control"
                },
                "updated": "2025-11-21T18:09:25Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    18,
                    9,
                    25,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17464v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present a patient-centric architecture for electronic health record (EHR) sharing that separates content storage from authorization and audit. Encrypted FHIR resources are stored off-chain; a public blockchain records only cryptographic commitments and patient-signed, time-bounded permissions using EIP-712. Keys are distributed via public-key wrapping, enabling storage providers to remain honest-but-curious without risking confidentiality. We formalize security goals (confidentiality, integrity, cryptographically attributable authorization, and auditability of authorization events) and provide a Solidity reference implementation deployed as single-patient contracts. On-chain costs for permission grants average 78,000 gas (L1), and end-to-end access latency for 1 MB records is 0.7--1.4s (mean values for S3 and IPFS respectively), dominated by storage retrieval. Layer-2 deployment reduces gas usage by 10--13x, though data availability charges dominate actual costs. We discuss metadata privacy, key registry requirements, and regulatory considerations (HIPAA/GDPR), demonstrating a practical route to restoring patient control while preserving security properties required for sensitive clinical data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a patient-centric architecture for electronic health record (EHR) sharing that separates content storage from authorization and audit. Encrypted FHIR resources are stored off-chain; a public blockchain records only cryptographic commitments and patient-signed, time-bounded permissions using EIP-712. Keys are distributed via public-key wrapping, enabling storage providers to remain honest-but-curious without risking confidentiality. We formalize security goals (confidentiality, integrity, cryptographically attributable authorization, and auditability of authorization events) and provide a Solidity reference implementation deployed as single-patient contracts. On-chain costs for permission grants average 78,000 gas (L1), and end-to-end access latency for 1 MB records is 0.7--1.4s (mean values for S3 and IPFS respectively), dominated by storage retrieval. Layer-2 deployment reduces gas usage by 10--13x, though data availability charges dominate actual costs. We discuss metadata privacy, key registry requirements, and regulatory considerations (HIPAA/GDPR), demonstrating a practical route to restoring patient control while preserving security properties required for sensitive clinical data."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T18:09:25Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    18,
                    9,
                    25,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Tanzim Hossain Romel"
                    },
                    {
                        "name": "Kawshik Kumar Paul"
                    },
                    {
                        "name": "Tanberul Islam Ruhan"
                    },
                    {
                        "name": "Maisha Rahman Mim"
                    },
                    {
                        "name": "Abu Sayed Md. Latiful Hoque"
                    }
                ],
                "author_detail": {
                    "name": "Abu Sayed Md. Latiful Hoque"
                },
                "author": "Abu Sayed Md. Latiful Hoque"
            },
            {
                "id": "http://arxiv.org/abs/2508.12315v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.12315v3",
                "title": "Deciphering the global production network from cross-border firm transactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deciphering the global production network from cross-border firm transactions"
                },
                "updated": "2025-11-21T17:58:00Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    58,
                    0,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.12315v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.12315v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Critical for policy-making and business operations, the study of global supply chains has been severely hampered by a lack of detailed data. Here we harness international firm-level transaction data covering 20m global firms, and 1 billion cross-border transactions, to infer key inputs for over 1200 products. Transforming this data to a directed network, we find that products are clustered into three large groups including textiles, chemicals and food, and machinery and metals. European industrial nations and China dominate critical intermediate products such as metals, common components and tools, while industrial complexity is highly correlated with embeddedness in densely connected supply chains. We find structural similarities with AIPNET, a product network generated via LLM queries, and strong linkages between products identified in manually-mapped electric vehicle battery and semiconductor supply chains. Finally, both forward and backward linkages are predictive of country-product diversification patterns, with stronger overall evidence for backward (upstream) linkages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critical for policy-making and business operations, the study of global supply chains has been severely hampered by a lack of detailed data. Here we harness international firm-level transaction data covering 20m global firms, and 1 billion cross-border transactions, to infer key inputs for over 1200 products. Transforming this data to a directed network, we find that products are clustered into three large groups including textiles, chemicals and food, and machinery and metals. European industrial nations and China dominate critical intermediate products such as metals, common components and tools, while industrial complexity is highly correlated with embeddedness in densely connected supply chains. We find structural similarities with AIPNET, a product network generated via LLM queries, and strong linkages between products identified in manually-mapped electric vehicle battery and semiconductor supply chains. Finally, both forward and backward linkages are predictive of country-product diversification patterns, with stronger overall evidence for backward (upstream) linkages."
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-17T10:17:23Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    10,
                    17,
                    23,
                    6,
                    229,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN"
                },
                "authors": [
                    {
                        "name": "Neave O'Clery"
                    },
                    {
                        "name": "Ben Radcliffe-Brown"
                    },
                    {
                        "name": "Thomas Spencer"
                    },
                    {
                        "name": "Daniel Tarling-Hunter"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Tarling-Hunter"
                },
                "author": "Daniel Tarling-Hunter"
            },
            {
                "id": "http://arxiv.org/abs/2511.17446v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17446v1",
                "title": "Unmasking Airborne Threats: Guided-Transformers for Portable Aerosol Mass Spectrometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmasking Airborne Threats: Guided-Transformers for Portable Aerosol Mass Spectrometry"
                },
                "updated": "2025-11-21T17:45:00Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    45,
                    0,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17446v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Matrix Assisted Laser Desorption/Ionization Mass Spectrometry (MALDI-MS) is a cornerstone in biomolecular analysis, offering precise identification of pathogens through unique mass spectral signatures. Yet, its reliance on labor-intensive sample preparation and multi-shot spectral averaging restricts its use to laboratory settings, rendering it impractical for real-time environmental monitoring. These limitations are especially pronounced in emerging aerosol MALDI-MS systems, where autonomous sampling generates noisy spectra for unknown aerosol analytes, requiring single-shot detection for effective analysis. Addressing these challenges, we propose the Mass Spectral Dictionary-Guided Transformer (MS-DGFormer): a data-driven framework that redefines spectral analysis by directly processing raw, minimally prepared mass spectral data. MS-DGFormer leverages a transformer architecture, designed to capture the long-range dependencies inherent in these time-series spectra. To enhance feature extraction, we introduce a novel dictionary encoder that integrates denoised spectral information derived from Singular Value Decomposition (SVD), enabling the model to discern critical biomolecular patterns from single-shot spectra with robust performance. This innovation provides a system to achieve superior pathogen identification from aerosol samples, facilitating autonomous, real-time analysis in field conditions. By eliminating the need for extensive preprocessing, our method unlocks the potential for portable, deployable MALDI-MS platforms, revolutionizing environmental pathogen detection and rapid response to biological threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix Assisted Laser Desorption/Ionization Mass Spectrometry (MALDI-MS) is a cornerstone in biomolecular analysis, offering precise identification of pathogens through unique mass spectral signatures. Yet, its reliance on labor-intensive sample preparation and multi-shot spectral averaging restricts its use to laboratory settings, rendering it impractical for real-time environmental monitoring. These limitations are especially pronounced in emerging aerosol MALDI-MS systems, where autonomous sampling generates noisy spectra for unknown aerosol analytes, requiring single-shot detection for effective analysis. Addressing these challenges, we propose the Mass Spectral Dictionary-Guided Transformer (MS-DGFormer): a data-driven framework that redefines spectral analysis by directly processing raw, minimally prepared mass spectral data. MS-DGFormer leverages a transformer architecture, designed to capture the long-range dependencies inherent in these time-series spectra. To enhance feature extraction, we introduce a novel dictionary encoder that integrates denoised spectral information derived from Singular Value Decomposition (SVD), enabling the model to discern critical biomolecular patterns from single-shot spectra with robust performance. This innovation provides a system to achieve superior pathogen identification from aerosol samples, facilitating autonomous, real-time analysis in field conditions. By eliminating the need for extensive preprocessing, our method unlocks the potential for portable, deployable MALDI-MS platforms, revolutionizing environmental pathogen detection and rapid response to biological threats."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T17:45:00Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    45,
                    0,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "13 pages, 9 figures. Preprint. Submitted to Computers in Biology and Medicine",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Kyle M. Regan"
                    },
                    {
                        "name": "Michael McLoughlin"
                    },
                    {
                        "name": "Wayne A. Bryden"
                    },
                    {
                        "name": "Gonzalo R. Arce"
                    }
                ],
                "author_detail": {
                    "name": "Gonzalo R. Arce"
                },
                "author": "Gonzalo R. Arce"
            },
            {
                "id": "http://arxiv.org/abs/2406.03283v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2406.03283v2",
                "title": "CATCODER: Repository-Level Code Generation with Relevant Code and Type Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CATCODER: Repository-Level Code Generation with Relevant Code and Type Context"
                },
                "updated": "2025-11-21T17:41:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    41,
                    45,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2406.03283v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2406.03283v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, repository-level code generation presents unique challenges, particularly due to the need to utilize information spread across multiple files within a repository. Specifically, successful generation depends on a solid grasp of both general, context-agnostic knowledge and specific, context-dependent knowledge. While LLMs are widely used for the context-agnostic aspect, existing retrieval-based approaches sometimes fall short as they are limited in obtaining a broader and deeper repository context. In this paper, we present CatCoder, a novel code generation framework designed for statically typed programming languages. CatCoder enhances repository-level code generation by integrating relevant code and type context. Specifically, it leverages static analyzers to extract type dependencies and merges this information with retrieved code to create comprehensive prompts for LLMs. To evaluate the effectiveness of CatCoder, we adapt and construct benchmarks that include 199 Java tasks and 90 Rust tasks. The results show that CatCoder outperforms the RepoCoder baseline by up to 14.44% and 17.35%, in terms of compile@k and pass@k scores. In addition, the generalizability of CatCoder is assessed using various LLMs, including both code-specialized models and general-purpose models. Our findings indicate consistent performance improvements across all models, which underlines the practicality of CatCoder. Furthermore, we evaluate the time consumption of CatCoder in a large open source repository, and the results demonstrate the scalability of CatCoder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, repository-level code generation presents unique challenges, particularly due to the need to utilize information spread across multiple files within a repository. Specifically, successful generation depends on a solid grasp of both general, context-agnostic knowledge and specific, context-dependent knowledge. While LLMs are widely used for the context-agnostic aspect, existing retrieval-based approaches sometimes fall short as they are limited in obtaining a broader and deeper repository context. In this paper, we present CatCoder, a novel code generation framework designed for statically typed programming languages. CatCoder enhances repository-level code generation by integrating relevant code and type context. Specifically, it leverages static analyzers to extract type dependencies and merges this information with retrieved code to create comprehensive prompts for LLMs. To evaluate the effectiveness of CatCoder, we adapt and construct benchmarks that include 199 Java tasks and 90 Rust tasks. The results show that CatCoder outperforms the RepoCoder baseline by up to 14.44% and 17.35%, in terms of compile@k and pass@k scores. In addition, the generalizability of CatCoder is assessed using various LLMs, including both code-specialized models and general-purpose models. Our findings indicate consistent performance improvements across all models, which underlines the practicality of CatCoder. Furthermore, we evaluate the time consumption of CatCoder in a large open source repository, and the results demonstrate the scalability of CatCoder."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-06-05T13:56:42Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    13,
                    56,
                    42,
                    2,
                    157,
                    0
                ],
                "arxiv_comment": "Revised and extended version; To be published in ACM Transactions on Software Engineering and Methodology",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Zhiyuan Pan"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Xiaohu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohu Yang"
                },
                "author": "Xiaohu Yang"
            },
            {
                "id": "http://arxiv.org/abs/2511.17442v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17442v1",
                "title": "REMSA: An LLM Agent for Foundation Model Selection in Remote Sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REMSA: An LLM Agent for Foundation Model Selection in Remote Sensing"
                },
                "updated": "2025-11-21T17:41:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    41,
                    26,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17442v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Foundation Models (FMs) are increasingly used in remote sensing (RS) for tasks such as environmental monitoring, disaster assessment, and land-use mapping. These models include unimodal vision encoders trained on a single data modality and multimodal architectures trained on combinations of SAR, multispectral, hyperspectral, and image-text data. They support diverse RS tasks including semantic segmentation, image classification, change detection, and visual question answering. However, selecting an appropriate remote sensing foundation model (RSFM) remains difficult due to scattered documentation, heterogeneous formats, and varied deployment constraints. We introduce the RSFM Database (RS-FMD), a structured resource covering over 150 RSFMs spanning multiple data modalities, resolutions, and learning paradigms. Built on RS-FMD, we present REMSA, the first LLM-based agent for automated RSFM selection from natural language queries. REMSA interprets user requirements, resolves missing constraints, ranks candidate models using in-context learning, and provides transparent justifications. We also propose a benchmark of 75 expert-verified RS query scenarios, producing 900 configurations under an expert-centered evaluation protocol. REMSA outperforms several baselines, including naive agents, dense retrieval, and unstructured RAG-based LLMs. It operates entirely on publicly available metadata and does not access private or sensitive data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Models (FMs) are increasingly used in remote sensing (RS) for tasks such as environmental monitoring, disaster assessment, and land-use mapping. These models include unimodal vision encoders trained on a single data modality and multimodal architectures trained on combinations of SAR, multispectral, hyperspectral, and image-text data. They support diverse RS tasks including semantic segmentation, image classification, change detection, and visual question answering. However, selecting an appropriate remote sensing foundation model (RSFM) remains difficult due to scattered documentation, heterogeneous formats, and varied deployment constraints. We introduce the RSFM Database (RS-FMD), a structured resource covering over 150 RSFMs spanning multiple data modalities, resolutions, and learning paradigms. Built on RS-FMD, we present REMSA, the first LLM-based agent for automated RSFM selection from natural language queries. REMSA interprets user requirements, resolves missing constraints, ranks candidate models using in-context learning, and provides transparent justifications. We also propose a benchmark of 75 expert-verified RS query scenarios, producing 900 configurations under an expert-centered evaluation protocol. REMSA outperforms several baselines, including naive agents, dense retrieval, and unstructured RAG-based LLMs. It operates entirely on publicly available metadata and does not access private or sensitive data."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T17:41:26Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    41,
                    26,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Code and data available at https://github.com/be-chen/REMSA",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Binger Chen"
                    },
                    {
                        "name": "Tacettin Emre Bk"
                    },
                    {
                        "name": "Behnood Rasti"
                    },
                    {
                        "name": "Volker Markl"
                    },
                    {
                        "name": "Begm Demir"
                    }
                ],
                "author_detail": {
                    "name": "Begm Demir"
                },
                "author": "Begm Demir"
            },
            {
                "id": "http://arxiv.org/abs/2511.17432v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17432v1",
                "title": "SMILE: A Composite Lexical-Semantic Metric for Question-Answering Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMILE: A Composite Lexical-Semantic Metric for Question-Answering Evaluation"
                },
                "updated": "2025-11-21T17:30:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    30,
                    18,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17432v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Traditional evaluation metrics for textual and visual question answering, like ROUGE, METEOR, and Exact Match (EM), focus heavily on n-gram based lexical similarity, often missing the deeper semantic understanding needed for accurate assessment. While measures like BERTScore and MoverScore leverage contextual embeddings to address this limitation, they lack flexibility in balancing sentence-level and keyword-level semantics and ignore lexical similarity, which remains important. Large Language Model (LLM) based evaluators, though powerful, come with drawbacks like high costs, bias, inconsistency, and hallucinations. To address these issues, we introduce SMILE: Semantic Metric Integrating Lexical Exactness, a novel approach that combines sentence-level semantic understanding with keyword-level semantic understanding and easy keyword matching. This composite method balances lexical precision and semantic relevance, offering a comprehensive evaluation. Extensive benchmarks across text, image, and video QA tasks show SMILE is highly correlated with human judgments and computationally lightweight, bridging the gap between lexical and semantic evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional evaluation metrics for textual and visual question answering, like ROUGE, METEOR, and Exact Match (EM), focus heavily on n-gram based lexical similarity, often missing the deeper semantic understanding needed for accurate assessment. While measures like BERTScore and MoverScore leverage contextual embeddings to address this limitation, they lack flexibility in balancing sentence-level and keyword-level semantics and ignore lexical similarity, which remains important. Large Language Model (LLM) based evaluators, though powerful, come with drawbacks like high costs, bias, inconsistency, and hallucinations. To address these issues, we introduce SMILE: Semantic Metric Integrating Lexical Exactness, a novel approach that combines sentence-level semantic understanding with keyword-level semantic understanding and easy keyword matching. This composite method balances lexical precision and semantic relevance, offering a comprehensive evaluation. Extensive benchmarks across text, image, and video QA tasks show SMILE is highly correlated with human judgments and computationally lightweight, bridging the gap between lexical and semantic evaluation."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T17:30:18Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    30,
                    18,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "23 pages, 6 tables, 9 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Shrikant Kendre"
                    },
                    {
                        "name": "Austin Xu"
                    },
                    {
                        "name": "Honglu Zhou"
                    },
                    {
                        "name": "Michael Ryoo"
                    },
                    {
                        "name": "Shafiq Joty"
                    },
                    {
                        "name": "Juan Carlos Niebles"
                    }
                ],
                "author_detail": {
                    "name": "Juan Carlos Niebles"
                },
                "author": "Juan Carlos Niebles"
            },
            {
                "id": "http://arxiv.org/abs/2511.13646v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13646v2",
                "title": "Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?"
                },
                "updated": "2025-11-21T17:22:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    22,
                    32,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13646v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-Gdel Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that LIVE-SWE-AGENT can achieve an impressive solve rate of 77.4% without test-time scaling, outperforming all existing software agents, including the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-Gdel Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that LIVE-SWE-AGENT can achieve an impressive solve rate of 77.4% without test-time scaling, outperforming all existing software agents, including the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T17:58:18Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    58,
                    18,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Chunqiu Steven Xia"
                    },
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Yan Yang"
                    },
                    {
                        "name": "Yuxiang Wei"
                    },
                    {
                        "name": "Lingming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lingming Zhang"
                },
                "author": "Lingming Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.17408v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17408v1",
                "title": "That's not natural: The Impact of Off-Policy Training Data on Probe Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "That's not natural: The Impact of Off-Policy Training Data on Probe Performance"
                },
                "updated": "2025-11-21T17:08:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    8,
                    48,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17408v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Probing has emerged as a promising method for monitoring Large Language Models (LLMs), enabling inference-time detection of concerning behaviours such as deception and sycophancy. However, natural examples of many behaviours are rare, forcing researchers to rely on synthetic or off-policy LLM responses for training probes. We systematically evaluate how the use of synthetic and off-policy data influences probe generalisation across eight distinct LLM behaviours. Testing linear and attention probes across multiple LLMs, we find that the response generation strategy can significantly affect probe performance, though the magnitude of this effect varies by behaviour. We find that successful generalisation from off-policy data, to test sets where the model is incentivised to produce the target behaviour, is predictive of successful on-policy generalisation. Leveraging this result, we predict that Deception and Sandbagging probes may fail to generalise from off-policy to on-policy data when used in real monitoring scenarios. Notably, shifts in the training data domain still cause even larger performance degradation, with different-domain test scores being consistently lower than the same-domain ones. These results indicate that, in the absence of on-policy data, using same-domain off-policy data yields more reliable probes than using on-policy data from a different domain, emphasizing the need for methods that can better handle distribution shifts in LLM monitoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing has emerged as a promising method for monitoring Large Language Models (LLMs), enabling inference-time detection of concerning behaviours such as deception and sycophancy. However, natural examples of many behaviours are rare, forcing researchers to rely on synthetic or off-policy LLM responses for training probes. We systematically evaluate how the use of synthetic and off-policy data influences probe generalisation across eight distinct LLM behaviours. Testing linear and attention probes across multiple LLMs, we find that the response generation strategy can significantly affect probe performance, though the magnitude of this effect varies by behaviour. We find that successful generalisation from off-policy data, to test sets where the model is incentivised to produce the target behaviour, is predictive of successful on-policy generalisation. Leveraging this result, we predict that Deception and Sandbagging probes may fail to generalise from off-policy to on-policy data when used in real monitoring scenarios. Notably, shifts in the training data domain still cause even larger performance degradation, with different-domain test scores being consistently lower than the same-domain ones. These results indicate that, in the absence of on-policy data, using same-domain off-policy data yields more reliable probes than using on-policy data from a different domain, emphasizing the need for methods that can better handle distribution shifts in LLM monitoring."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T17:08:48Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    8,
                    48,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "10 pages, EurIPS 2025 Workshop on Private AI Governance",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Nathalie Kirch"
                    },
                    {
                        "name": "Samuel Dower"
                    },
                    {
                        "name": "Adrians Skapars"
                    },
                    {
                        "name": "Ekdeep Singh Lubana"
                    },
                    {
                        "name": "Dmitrii Krasheninnikov"
                    }
                ],
                "author_detail": {
                    "name": "Dmitrii Krasheninnikov"
                },
                "author": "Dmitrii Krasheninnikov"
            },
            {
                "id": "http://arxiv.org/abs/2511.11910v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.11910v2",
                "title": "Seeing the Forest and the Trees: Query-Aware Tokenizer for Long-Video Multimodal Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seeing the Forest and the Trees: Query-Aware Tokenizer for Long-Video Multimodal Language Models"
                },
                "updated": "2025-11-21T17:08:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    8,
                    33,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.11910v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.11910v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Despite the recent advances in the video understanding ability of multimodal large language models (MLLMs), long video understanding remains a challenge. One of the main issues is that the number of vision tokens grows linearly with video length, which causes an explosion in attention cost, memory, and latency. To solve this challenge, we present Query-aware Token Selector (\\textbf{QTSplus}), a lightweight yet powerful visual token selection module that serves as an information gate between the vision encoder and LLMs. Given a text query and video tokens, QTSplus dynamically selects the most important visual evidence for the input text query by (i) scoring visual tokens via cross-attention, (ii) \\emph{predicting} an instance-specific retention budget based on the complexity of the query, and (iii) \\emph{selecting} Top-$n$ tokens with a differentiable straight-through estimator during training and a hard gate at inference. Furthermore, a small re-encoder preserves temporal order using absolute time information, enabling second-level localization while maintaining global coverage.\n  Integrated into Qwen2.5-VL, QTSplus compresses the vision stream by up to \\textbf{89\\%} and reduces end-to-end latency by \\textbf{28\\%} on long videos. The evaluation on eight long video understanding benchmarks shows near-parity accuracy overall when compared with the original Qwen models and outperforms the original model by \\textbf{+20.5} and \\textbf{+5.6} points respectively on TempCompass direction and order accuracies. These results show that QTSplus is an effective, general mechanism for scaling MLLMs to real-world long-video scenarios while preserving task-relevant evidence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent advances in the video understanding ability of multimodal large language models (MLLMs), long video understanding remains a challenge. One of the main issues is that the number of vision tokens grows linearly with video length, which causes an explosion in attention cost, memory, and latency. To solve this challenge, we present Query-aware Token Selector (\\textbf{QTSplus}), a lightweight yet powerful visual token selection module that serves as an information gate between the vision encoder and LLMs. Given a text query and video tokens, QTSplus dynamically selects the most important visual evidence for the input text query by (i) scoring visual tokens via cross-attention, (ii) \\emph{predicting} an instance-specific retention budget based on the complexity of the query, and (iii) \\emph{selecting} Top-$n$ tokens with a differentiable straight-through estimator during training and a hard gate at inference. Furthermore, a small re-encoder preserves temporal order using absolute time information, enabling second-level localization while maintaining global coverage.\n  Integrated into Qwen2.5-VL, QTSplus compresses the vision stream by up to \\textbf{89\\%} and reduces end-to-end latency by \\textbf{28\\%} on long videos. The evaluation on eight long video understanding benchmarks shows near-parity accuracy overall when compared with the original Qwen models and outperforms the original model by \\textbf{+20.5} and \\textbf{+5.6} points respectively on TempCompass direction and order accuracies. These results show that QTSplus is an effective, general mechanism for scaling MLLMs to real-world long-video scenarios while preserving task-relevant evidence."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-14T22:41:27Z",
                "published_parsed": [
                    2025,
                    11,
                    14,
                    22,
                    41,
                    27,
                    4,
                    318,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Siyou Li"
                    },
                    {
                        "name": "Huanan Wu"
                    },
                    {
                        "name": "Juexi Shao"
                    },
                    {
                        "name": "Yinghao Ma"
                    },
                    {
                        "name": "Yujian Gan"
                    },
                    {
                        "name": "Yihao Luo"
                    },
                    {
                        "name": "Yuwei Wang"
                    },
                    {
                        "name": "Dong Nie"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Wengqing Wu"
                    },
                    {
                        "name": "Le Zhang"
                    },
                    {
                        "name": "Massimo Poesio"
                    },
                    {
                        "name": "Juntao Yu"
                    }
                ],
                "author_detail": {
                    "name": "Juntao Yu"
                },
                "author": "Juntao Yu"
            },
            {
                "id": "http://arxiv.org/abs/2511.17405v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17405v1",
                "title": "Beyond Multiple Choice: A Hybrid Framework for Unifying Robust Evaluation and Verifiable Reasoning Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Multiple Choice: A Hybrid Framework for Unifying Robust Evaluation and Verifiable Reasoning Training"
                },
                "updated": "2025-11-21T17:06:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    6,
                    37,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17405v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multiple-choice question answering (MCQA) has been a popular format for evaluating and reinforcement fine-tuning (RFT) of modern multimodal language models. Its constrained output format allows for simplified, deterministic automatic verification. However, we find that the options may leak exploitable signals, which makes the accuracy metrics unreliable for indicating real capabilities and encourages explicit or implicit answer guessing behaviors during RFT. We propose ReVeL (Rewrite and Verify by LLM), a framework that rewrites multiple-choice questions into open-form questions while keeping answers verifiable whenever possible. The framework categorizes questions according to different answer types, apply different rewriting and verification schemes, respectively. When applied for RFT, we converted 20k MCQA examples and use GRPO to finetune Qwen2.5-VL models. Models trained on ReVeL-OpenQA match MCQA accuracy on multiple-choice benchmarks and improve OpenQA accuracy by about six percentage points, indicating better data efficiency and more robust reward signals than MCQA-based training. When used for evaluation, ReVeL also reveals up to 20 percentage points of score inflation in MCQA benchmarks (relative to OpenQA), improves judging accuracy, and reduces both cost and latency. We will release code and data publicly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple-choice question answering (MCQA) has been a popular format for evaluating and reinforcement fine-tuning (RFT) of modern multimodal language models. Its constrained output format allows for simplified, deterministic automatic verification. However, we find that the options may leak exploitable signals, which makes the accuracy metrics unreliable for indicating real capabilities and encourages explicit or implicit answer guessing behaviors during RFT. We propose ReVeL (Rewrite and Verify by LLM), a framework that rewrites multiple-choice questions into open-form questions while keeping answers verifiable whenever possible. The framework categorizes questions according to different answer types, apply different rewriting and verification schemes, respectively. When applied for RFT, we converted 20k MCQA examples and use GRPO to finetune Qwen2.5-VL models. Models trained on ReVeL-OpenQA match MCQA accuracy on multiple-choice benchmarks and improve OpenQA accuracy by about six percentage points, indicating better data efficiency and more robust reward signals than MCQA-based training. When used for evaluation, ReVeL also reveals up to 20 percentage points of score inflation in MCQA benchmarks (relative to OpenQA), improves judging accuracy, and reduces both cost and latency. We will release code and data publicly."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T17:06:37Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    17,
                    6,
                    37,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Project url: https://flageval-baai.github.io/ReVeL/",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yesheng Liu"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Haiyu Xu"
                    },
                    {
                        "name": "Baoqi Pei"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Mingxuan Zhao"
                    },
                    {
                        "name": "Jingshu Zheng"
                    },
                    {
                        "name": "Zheqi He"
                    },
                    {
                        "name": "JG Yao"
                    },
                    {
                        "name": "Bowen Qin"
                    },
                    {
                        "name": "Xi Yang"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2412.19622v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2412.19622v2",
                "title": "Reassessing prediction in the brain: Pre-onset neural encoding during natural listening does not reflect pre-activation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reassessing prediction in the brain: Pre-onset neural encoding during natural listening does not reflect pre-activation"
                },
                "updated": "2025-11-21T16:51:07Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    16,
                    51,
                    7,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2412.19622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2412.19622v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Predictive processing theories propose that the brain continuously anticipates upcoming input. However, direct neural evidence for predictive pre-activation during natural language comprehension remains limited and debated. Previous studies using large language model (LLM)-based encoding models with fMRI and ECoG have reported pre-onset signals that appear to encode upcoming words, but these effects may instead reflect dependencies in the stimulus or autocorrelations in neural activity. Here, we re-examined this question by aligning LLM-derived word embeddings with neural activity recorded during naturalistic listening using magnetoencephalography (MEG) and electrocorticography (ECoG). We replicated pre-onset encoding effects previously observed in ECoG across both modalities, and found that they persist even after controlling for stimulus correlations. Crucially, temporal generalization analyses revealed no stable overlap between pre- and post-onset representations, indicating that pre-onset activity does not reflect pre-activation of the next word. Consistent with this, long-range predictive effects previously reported in fMRI did not replicate in our higher-temporal-resolution data. While we found no evidence for predictive pre-activation, we observed clear signatures of postdiction, with neural activity reflecting persistent encoding of prior words. These results suggest that reported apparent predictive signals do not reflect pre-activation of upcoming input. They call for caution in interpreting LLM-based encoding models and highlight the need for a more nuanced understanding of what constitutes \"prediction\" in language comprehension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictive processing theories propose that the brain continuously anticipates upcoming input. However, direct neural evidence for predictive pre-activation during natural language comprehension remains limited and debated. Previous studies using large language model (LLM)-based encoding models with fMRI and ECoG have reported pre-onset signals that appear to encode upcoming words, but these effects may instead reflect dependencies in the stimulus or autocorrelations in neural activity. Here, we re-examined this question by aligning LLM-derived word embeddings with neural activity recorded during naturalistic listening using magnetoencephalography (MEG) and electrocorticography (ECoG). We replicated pre-onset encoding effects previously observed in ECoG across both modalities, and found that they persist even after controlling for stimulus correlations. Crucially, temporal generalization analyses revealed no stable overlap between pre- and post-onset representations, indicating that pre-onset activity does not reflect pre-activation of the next word. Consistent with this, long-range predictive effects previously reported in fMRI did not replicate in our higher-temporal-resolution data. While we found no evidence for predictive pre-activation, we observed clear signatures of postdiction, with neural activity reflecting persistent encoding of prior words. These results suggest that reported apparent predictive signals do not reflect pre-activation of upcoming input. They call for caution in interpreting LLM-based encoding models and highlight the need for a more nuanced understanding of what constitutes \"prediction\" in language comprehension."
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-12-27T12:49:03Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    12,
                    49,
                    3,
                    4,
                    362,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC"
                },
                "authors": [
                    {
                        "name": "Sahel Azizpour"
                    },
                    {
                        "name": "Britta U. Westner"
                    },
                    {
                        "name": "Jakub Szewczyk"
                    },
                    {
                        "name": "Umut Gl"
                    },
                    {
                        "name": "Linda Geerligs"
                    }
                ],
                "author_detail": {
                    "name": "Linda Geerligs"
                },
                "author": "Linda Geerligs"
            },
            {
                "id": "http://arxiv.org/abs/2511.17366v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17366v1",
                "title": "METIS: Multi-Source Egocentric Training for Integrated Dexterous Vision-Language-Action Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "METIS: Multi-Source Egocentric Training for Integrated Dexterous Vision-Language-Action Model"
                },
                "updated": "2025-11-21T16:32:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    16,
                    32,
                    36,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17366v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Building a generalist robot that can perceive, reason, and act across diverse tasks remains an open challenge, especially for dexterous manipulation. A major bottleneck lies in the scarcity of large-scale, action-annotated data for dexterous skills, as teleoperation is difficult and costly. Human data, with its vast scale and diverse manipulation behaviors, provides rich priors for learning robotic actions. While prior works have explored leveraging human demonstrations, they are often constrained by limited scenarios and a large visual gap between human and robots. To eliminate these limitations, we propose METIS, a vision-language-action (VLA) model for dexterous manipulation pretrained on multi-source egocentric datasets. We first construct EgoAtlas, which integrates large-scale human and robotic data from multiple sources, all unified under a consistent action space. We further extract motion-aware dynamics, a compact and discretized motion representation, which provides efficient and expressive supervision for VLA training. Built upon them, METIS integrates reasoning and acting into a unified framework, enabling effective deployment to downstream dexterous manipulation tasks. Our method demonstrates exceptional dexterous manipulation capabilities, achieving highest average success rate in six real-world tasks. Experimental results also highlight the superior generalization and robustness to out-of-distribution scenarios. These findings emphasize METIS as a promising step toward a generalist model for dexterous manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a generalist robot that can perceive, reason, and act across diverse tasks remains an open challenge, especially for dexterous manipulation. A major bottleneck lies in the scarcity of large-scale, action-annotated data for dexterous skills, as teleoperation is difficult and costly. Human data, with its vast scale and diverse manipulation behaviors, provides rich priors for learning robotic actions. While prior works have explored leveraging human demonstrations, they are often constrained by limited scenarios and a large visual gap between human and robots. To eliminate these limitations, we propose METIS, a vision-language-action (VLA) model for dexterous manipulation pretrained on multi-source egocentric datasets. We first construct EgoAtlas, which integrates large-scale human and robotic data from multiple sources, all unified under a consistent action space. We further extract motion-aware dynamics, a compact and discretized motion representation, which provides efficient and expressive supervision for VLA training. Built upon them, METIS integrates reasoning and acting into a unified framework, enabling effective deployment to downstream dexterous manipulation tasks. Our method demonstrates exceptional dexterous manipulation capabilities, achieving highest average success rate in six real-world tasks. Experimental results also highlight the superior generalization and robustness to out-of-distribution scenarios. These findings emphasize METIS as a promising step toward a generalist model for dexterous manipulation."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T16:32:36Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    16,
                    32,
                    36,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Yankai Fu"
                    },
                    {
                        "name": "Ning Chen"
                    },
                    {
                        "name": "Junkai Zhao"
                    },
                    {
                        "name": "Shaozhe Shan"
                    },
                    {
                        "name": "Guocai Yao"
                    },
                    {
                        "name": "Pengwei Wang"
                    },
                    {
                        "name": "Zhongyuan Wang"
                    },
                    {
                        "name": "Shanghang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shanghang Zhang"
                },
                "author": "Shanghang Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.17361v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17361v1",
                "title": "SuperQuadricOcc: Multi-Layer Gaussian Approximation of Superquadrics for Real-Time Self-Supervised Occupancy Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperQuadricOcc: Multi-Layer Gaussian Approximation of Superquadrics for Real-Time Self-Supervised Occupancy Estimation"
                },
                "updated": "2025-11-21T16:26:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    16,
                    26,
                    31,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17361v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Semantic occupancy estimation enables comprehensive scene understanding for automated driving, providing dense spatial and semantic information essential for perception and planning. While Gaussian representations have been widely adopted in self-supervised occupancy estimation, the deployment of a large number of Gaussian primitives drastically increases memory requirements and is not suitable for real-time inference. In contrast, superquadrics permit reduced primitive count and lower memory requirements due to their diverse shape set. However, implementation into a self-supervised occupancy model is nontrivial due to the absence of a superquadric rasterizer to enable model supervision. Our proposed method, SuperQuadricOcc, employs a superquadric-based scene representation. By leveraging a multi-layer icosphere-tessellated Gaussian approximation of superquadrics, we enable Gaussian rasterization for supervision during training. On the Occ3D dataset, SuperQuadricOcc achieves a 75\\% reduction in memory footprint, 124\\% faster inference, and a 5.9\\% improvement in mIoU compared to previous Gaussian-based methods, without the use of temporal labels. To our knowledge, this is the first occupancy model to enable real-time inference while maintaining competitive performance. The use of superquadrics reduces the number of primitives required for scene modeling by 84\\% relative to Gaussian-based approaches. Finally, evaluation against prior methods is facilitated by our fast superquadric voxelization module. The code will be released as open source.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic occupancy estimation enables comprehensive scene understanding for automated driving, providing dense spatial and semantic information essential for perception and planning. While Gaussian representations have been widely adopted in self-supervised occupancy estimation, the deployment of a large number of Gaussian primitives drastically increases memory requirements and is not suitable for real-time inference. In contrast, superquadrics permit reduced primitive count and lower memory requirements due to their diverse shape set. However, implementation into a self-supervised occupancy model is nontrivial due to the absence of a superquadric rasterizer to enable model supervision. Our proposed method, SuperQuadricOcc, employs a superquadric-based scene representation. By leveraging a multi-layer icosphere-tessellated Gaussian approximation of superquadrics, we enable Gaussian rasterization for supervision during training. On the Occ3D dataset, SuperQuadricOcc achieves a 75\\% reduction in memory footprint, 124\\% faster inference, and a 5.9\\% improvement in mIoU compared to previous Gaussian-based methods, without the use of temporal labels. To our knowledge, this is the first occupancy model to enable real-time inference while maintaining competitive performance. The use of superquadrics reduces the number of primitives required for scene modeling by 84\\% relative to Gaussian-based approaches. Finally, evaluation against prior methods is facilitated by our fast superquadric voxelization module. The code will be released as open source."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T16:26:31Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    16,
                    26,
                    31,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Seamie Hayes"
                    },
                    {
                        "name": "Reenu Mohandas"
                    },
                    {
                        "name": "Tim Brophy"
                    },
                    {
                        "name": "Alexandre Boulch"
                    },
                    {
                        "name": "Ganesh Sistu"
                    },
                    {
                        "name": "Ciaran Eising"
                    }
                ],
                "author_detail": {
                    "name": "Ciaran Eising"
                },
                "author": "Ciaran Eising"
            },
            {
                "id": "http://arxiv.org/abs/2507.04482v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.04482v2",
                "title": "A Training-Free Style-Personalization via SVD-Based Feature Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Training-Free Style-Personalization via SVD-Based Feature Decomposition"
                },
                "updated": "2025-11-21T16:25:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    16,
                    25,
                    35,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.04482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.04482v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present a training-free framework for style-personalized image generation that operates during inference using a scale-wise autoregressive model. Our method generates a stylized image guided by a single reference style while preserving semantic consistency and mitigating content leakage. Through a detailed step-wise analysis of the generation process, we identify a pivotal step where the dominant singular values of the internal feature encode style-related components. Building upon this insight, we introduce two lightweight control modules: Principal Feature Blending, which enables precise modulation of style through SVD-based feature reconstruction, and Structural Attention Correction, which stabilizes structural consistency by leveraging content-guided attention correction across fine stages. Without any additional training, extensive experiments demonstrate that our method achieves competitive style fidelity and prompt fidelity compared to fine-tuned baselines, while offering faster inference and greater deployment flexibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a training-free framework for style-personalized image generation that operates during inference using a scale-wise autoregressive model. Our method generates a stylized image guided by a single reference style while preserving semantic consistency and mitigating content leakage. Through a detailed step-wise analysis of the generation process, we identify a pivotal step where the dominant singular values of the internal feature encode style-related components. Building upon this insight, we introduce two lightweight control modules: Principal Feature Blending, which enables precise modulation of style through SVD-based feature reconstruction, and Structural Attention Correction, which stabilizes structural consistency by leveraging content-guided attention correction across fine stages. Without any additional training, extensive experiments demonstrate that our method achieves competitive style fidelity and prompt fidelity compared to fine-tuned baselines, while offering faster inference and greater deployment flexibility."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-06T17:42:11Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    17,
                    42,
                    11,
                    6,
                    187,
                    0
                ],
                "arxiv_comment": "21 pages, 14 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Kyoungmin Lee"
                    },
                    {
                        "name": "Jihun Park"
                    },
                    {
                        "name": "Jongmin Gim"
                    },
                    {
                        "name": "Wonhyeok Choi"
                    },
                    {
                        "name": "Kyumin Hwang"
                    },
                    {
                        "name": "Jaeyeul Kim"
                    },
                    {
                        "name": "Sunghoon Im"
                    }
                ],
                "author_detail": {
                    "name": "Sunghoon Im"
                },
                "author": "Sunghoon Im"
            },
            {
                "id": "http://arxiv.org/abs/2505.09439v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.09439v3",
                "title": "Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?"
                },
                "updated": "2025-11-21T16:16:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    16,
                    16,
                    38,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.09439v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.09439v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni, on an audio question answering dataset with the reinforcement learning method GRPO. This leads to new State-of-the-Art performance on the recent MMAU and MMAR benchmarks. Omni-R1 achieves the highest accuracies on the sounds, music, speech, and overall average categories, both on the Test-mini and Test-full splits. To understand the performance improvement, we tested models both with and without audio and found that much of the performance improvement from GRPO could be attributed to better text-based reasoning. We also made a surprising discovery that fine-tuning without audio on a text-only dataset was effective at improving the audio-based performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni, on an audio question answering dataset with the reinforcement learning method GRPO. This leads to new State-of-the-Art performance on the recent MMAU and MMAR benchmarks. Omni-R1 achieves the highest accuracies on the sounds, music, speech, and overall average categories, both on the Test-mini and Test-full splits. To understand the performance improvement, we tested models both with and without audio and found that much of the performance improvement from GRPO could be attributed to better text-based reasoning. We also made a surprising discovery that fine-tuning without audio on a text-only dataset was effective at improving the audio-based performance."
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-14T14:47:16Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    14,
                    47,
                    16,
                    2,
                    134,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS"
                },
                "authors": [
                    {
                        "name": "Andrew Rouditchenko"
                    },
                    {
                        "name": "Saurabhchand Bhati"
                    },
                    {
                        "name": "Edson Araujo"
                    },
                    {
                        "name": "Samuel Thomas"
                    },
                    {
                        "name": "Hilde Kuehne"
                    },
                    {
                        "name": "Rogerio Feris"
                    },
                    {
                        "name": "James Glass"
                    }
                ],
                "author_detail": {
                    "name": "James Glass"
                },
                "author": "James Glass"
            },
            {
                "id": "http://arxiv.org/abs/2511.17335v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17335v1",
                "title": "Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM"
                },
                "updated": "2025-11-21T15:55:25Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    55,
                    25,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17335v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Human-robot collaboration towards a shared goal requires robots to understand human action and interaction with the surrounding environment. This paper focuses on human-robot interaction (HRI) based on human-robot dialogue that relies on the robot action confirmation and action step generation using multimodal scene understanding. The state-of-the-art approach uses multimodal transformers to generate robot action steps aligned with robot action confirmation from a single clip showing a task composed of multiple micro steps. Although actions towards a long-horizon task depend on each other throughout an entire video, the current approaches mainly focus on clip-level processing and do not leverage long-context information. This paper proposes a long-context Q-former incorporating left and right context dependency in full videos. Furthermore, this paper proposes a text-conditioning approach to feed text embeddings directly into the LLM decoder to mitigate the high abstraction of the information in text by Q-former. Experiments with the YouCook2 corpus show that the accuracy of confirmation generation is a major factor in the performance of action planning. Furthermore, we demonstrate that the long-context Q-former improves the confirmation and action planning by integrating VideoLLaMA3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-robot collaboration towards a shared goal requires robots to understand human action and interaction with the surrounding environment. This paper focuses on human-robot interaction (HRI) based on human-robot dialogue that relies on the robot action confirmation and action step generation using multimodal scene understanding. The state-of-the-art approach uses multimodal transformers to generate robot action steps aligned with robot action confirmation from a single clip showing a task composed of multiple micro steps. Although actions towards a long-horizon task depend on each other throughout an entire video, the current approaches mainly focus on clip-level processing and do not leverage long-context information. This paper proposes a long-context Q-former incorporating left and right context dependency in full videos. Furthermore, this paper proposes a text-conditioning approach to feed text embeddings directly into the LLM decoder to mitigate the high abstraction of the information in text by Q-former. Experiments with the YouCook2 corpus show that the accuracy of confirmation generation is a major factor in the performance of action planning. Furthermore, we demonstrate that the long-context Q-former improves the confirmation and action planning by integrating VideoLLaMA3."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T15:55:25Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    55,
                    25,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Accepted to ASRU 2025",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Chiori Hori"
                    },
                    {
                        "name": "Yoshiki Masuyama"
                    },
                    {
                        "name": "Siddarth Jain"
                    },
                    {
                        "name": "Radu Corcodel"
                    },
                    {
                        "name": "Devesh Jha"
                    },
                    {
                        "name": "Diego Romeres"
                    },
                    {
                        "name": "Jonathan Le Roux"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Le Roux"
                },
                "author": "Jonathan Le Roux"
            },
            {
                "id": "http://arxiv.org/abs/2511.17330v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17330v1",
                "title": "Agentic Program Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Program Verification"
                },
                "updated": "2025-11-21T15:51:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    51,
                    48,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17330v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Automatically generated code is gaining traction recently, owing to the prevalence of Large Language Models (LLMs). Further, the AlphaProof initiative has demonstrated the possibility of using AI for general mathematical reasoning. Reasoning about computer programs (software) can be accomplished via general mathematical reasoning; however, it tends to be more structured and richer in contexts. This forms an attractive proposition, since then AI agents can be used to reason about voluminous code that gets generated by AI.\n  In this work, we present a first LLM agent, AutoRocq, for conducting program verification. Unlike past works, which rely on extensive training of LLMs on proof examples, our agent learns on-the-fly and improves the proof via an iterative refinement loop. The iterative improvement of the proof is achieved by the proof agent communicating with the Rocq (formerly Coq) theorem prover to get additional context and feedback. The final result of the iteration is a proof derivation checked by the Rocq theorem prover. In this way, our proof construction involves autonomous collaboration between the proof agent and the theorem prover. This autonomy facilitates the search for proofs and decision-making in deciding on the structure of the proof tree.\n  Experimental evaluation on SV-COMP benchmarks and on Linux kernel modules shows promising efficacy in achieving automated program verification. As automation in code generation becomes more widespread, we posit that our proof agent can be potentially integrated with AI coding agents to achieve a generate and validate loop, thus moving closer to the vision of trusted automatic programming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically generated code is gaining traction recently, owing to the prevalence of Large Language Models (LLMs). Further, the AlphaProof initiative has demonstrated the possibility of using AI for general mathematical reasoning. Reasoning about computer programs (software) can be accomplished via general mathematical reasoning; however, it tends to be more structured and richer in contexts. This forms an attractive proposition, since then AI agents can be used to reason about voluminous code that gets generated by AI.\n  In this work, we present a first LLM agent, AutoRocq, for conducting program verification. Unlike past works, which rely on extensive training of LLMs on proof examples, our agent learns on-the-fly and improves the proof via an iterative refinement loop. The iterative improvement of the proof is achieved by the proof agent communicating with the Rocq (formerly Coq) theorem prover to get additional context and feedback. The final result of the iteration is a proof derivation checked by the Rocq theorem prover. In this way, our proof construction involves autonomous collaboration between the proof agent and the theorem prover. This autonomy facilitates the search for proofs and decision-making in deciding on the structure of the proof tree.\n  Experimental evaluation on SV-COMP benchmarks and on Linux kernel modules shows promising efficacy in achieving automated program verification. As automation in code generation becomes more widespread, we posit that our proof agent can be potentially integrated with AI coding agents to achieve a generate and validate loop, thus moving closer to the vision of trusted automatic programming."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T15:51:48Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    51,
                    48,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "21 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Haoxin Tu"
                    },
                    {
                        "name": "Huan Zhao"
                    },
                    {
                        "name": "Yahui Song"
                    },
                    {
                        "name": "Mehtab Zafar"
                    },
                    {
                        "name": "Ruijie Meng"
                    },
                    {
                        "name": "Abhik Roychoudhury"
                    }
                ],
                "author_detail": {
                    "name": "Abhik Roychoudhury"
                },
                "author": "Abhik Roychoudhury"
            },
            {
                "id": "http://arxiv.org/abs/2511.17315v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17315v1",
                "title": "Humanlike Multi-user Agent (HUMA): Designing a Deceptively Human AI Facilitator for Group Chats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humanlike Multi-user Agent (HUMA): Designing a Deceptively Human AI Facilitator for Group Chats"
                },
                "updated": "2025-11-21T15:34:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    34,
                    42,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17315v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Conversational agents built on large language models (LLMs) are becoming increasingly prevalent, yet most systems are designed for one-on-one, turn-based exchanges rather than natural, asynchronous group chats. As AI assistants become widespread throughout digital platforms, from virtual assistants to customer service, developing natural and humanlike interaction patterns seems crucial for maintaining user trust and engagement. We present the Humanlike Multi-user Agent (HUMA), an LLM-based facilitator that participates in multi-party conversations using human-like strategies and timing. HUMA extends prior multi-user chatbot work with an event-driven architecture that handles messages, replies, reactions and introduces realistic response-time simulation. HUMA comprises three components-Router, Action Agent, and Reflection-which together adapt LLMs to group conversation dynamics.\n  We evaluate HUMA in a controlled study with 97 participants in four-person role-play chats, comparing AI and human community managers (CMs). Participants classified CMs as human at near-chance rates in both conditions, indicating they could not reliably distinguish HUMA agents from humans. Subjective experience was comparable across conditions: community-manager effectiveness, social presence, and engagement/satisfaction differed only modestly with small effect sizes. Our results suggest that, in natural group chat settings, an AI facilitator can match human quality while remaining difficult to identify as nonhuman.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational agents built on large language models (LLMs) are becoming increasingly prevalent, yet most systems are designed for one-on-one, turn-based exchanges rather than natural, asynchronous group chats. As AI assistants become widespread throughout digital platforms, from virtual assistants to customer service, developing natural and humanlike interaction patterns seems crucial for maintaining user trust and engagement. We present the Humanlike Multi-user Agent (HUMA), an LLM-based facilitator that participates in multi-party conversations using human-like strategies and timing. HUMA extends prior multi-user chatbot work with an event-driven architecture that handles messages, replies, reactions and introduces realistic response-time simulation. HUMA comprises three components-Router, Action Agent, and Reflection-which together adapt LLMs to group conversation dynamics.\n  We evaluate HUMA in a controlled study with 97 participants in four-person role-play chats, comparing AI and human community managers (CMs). Participants classified CMs as human at near-chance rates in both conditions, indicating they could not reliably distinguish HUMA agents from humans. Subjective experience was comparable across conditions: community-manager effectiveness, social presence, and engagement/satisfaction differed only modestly with small effect sizes. Our results suggest that, in natural group chat settings, an AI facilitator can match human quality while remaining difficult to identify as nonhuman."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T15:34:42Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    34,
                    42,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "9 pages, 4 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Mateusz Jacniacki"
                    },
                    {
                        "name": "Mart Carmona Serrat"
                    }
                ],
                "author_detail": {
                    "name": "Mart Carmona Serrat"
                },
                "author": "Mart Carmona Serrat"
            },
            {
                "id": "http://arxiv.org/abs/2507.04224v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.04224v3",
                "title": "Fairness Evaluation of Large Language Models in Academic Library Reference Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fairness Evaluation of Large Language Models in Academic Library Reference Services"
                },
                "updated": "2025-11-21T15:33:14Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    33,
                    14,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.04224v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.04224v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As libraries explore large language models (LLMs) for use in virtual reference services, a key question arises: Can LLMs serve all users equitably, regardless of demographics or social status? While they offer great potential for scalable support, LLMs may also reproduce societal biases embedded in their training data, risking the integrity of libraries' commitment to equitable service. To address this concern, we evaluate whether LLMs differentiate responses across user identities by prompting six state-of-the-art LLMs to assist patrons differing in sex, race/ethnicity, and institutional role. We find no evidence of differentiation by race or ethnicity, and only minor evidence of stereotypical bias against women in one model. LLMs demonstrate nuanced accommodation of institutional roles through the use of linguistic choices related to formality, politeness, and domain-specific vocabularies, reflecting professional norms rather than discriminatory treatment. These findings suggest that current LLMs show a promising degree of readiness to support equitable and contextually appropriate communication in academic library reference services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As libraries explore large language models (LLMs) for use in virtual reference services, a key question arises: Can LLMs serve all users equitably, regardless of demographics or social status? While they offer great potential for scalable support, LLMs may also reproduce societal biases embedded in their training data, risking the integrity of libraries' commitment to equitable service. To address this concern, we evaluate whether LLMs differentiate responses across user identities by prompting six state-of-the-art LLMs to assist patrons differing in sex, race/ethnicity, and institutional role. We find no evidence of differentiation by race or ethnicity, and only minor evidence of stereotypical bias against women in one model. LLMs demonstrate nuanced accommodation of institutional roles through the use of linguistic choices related to formality, politeness, and domain-specific vocabularies, reflecting professional norms rather than discriminatory treatment. These findings suggest that current LLMs show a promising degree of readiness to support equitable and contextually appropriate communication in academic library reference services."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-06T03:28:24Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    3,
                    28,
                    24,
                    6,
                    187,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Haining Wang"
                    },
                    {
                        "name": "Jason Clark"
                    },
                    {
                        "name": "Yueru Yan"
                    },
                    {
                        "name": "Star Bradley"
                    },
                    {
                        "name": "Ruiyang Chen"
                    },
                    {
                        "name": "Yiqiong Zhang"
                    },
                    {
                        "name": "Hengyi Fu"
                    },
                    {
                        "name": "Zuoyu Tian"
                    }
                ],
                "author_detail": {
                    "name": "Zuoyu Tian"
                },
                "author": "Zuoyu Tian"
            },
            {
                "id": "http://arxiv.org/abs/2511.02894v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.02894v3",
                "title": "Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models"
                },
                "updated": "2025-11-21T15:30:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    30,
                    31,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.02894v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.02894v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The widespread integration of wearable sensing devices in Internet of Things (IoT) ecosystems, particularly in healthcare, smart homes, and industrial applications, has required robust human activity recognition (HAR) techniques to improve functionality and user experience. Although machine learning models have advanced HAR, they are increasingly susceptible to data poisoning attacks that compromise the data integrity and reliability of these systems. Conventional approaches to defending against such attacks often require extensive task-specific training with large, labeled datasets, which limits adaptability in dynamic IoT environments. This work proposes a novel framework that uses large language models (LLMs) to perform poisoning detection and sanitization in HAR systems, utilizing zero-shot, one-shot, and few-shot learning paradigms. Our approach incorporates \\textit{role play} prompting, whereby the LLM assumes the role of expert to contextualize and evaluate sensor anomalies, and \\textit{think step-by-step} reasoning, guiding the LLM to infer poisoning indicators in the raw sensor data and plausible clean alternatives. These strategies minimize reliance on curation of extensive datasets and enable robust, adaptable defense mechanisms in real-time. We perform an extensive evaluation of the framework, quantifying detection accuracy, sanitization quality, latency, and communication cost, thus demonstrating the practicality and effectiveness of LLMs in improving the security and reliability of wearable IoT systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread integration of wearable sensing devices in Internet of Things (IoT) ecosystems, particularly in healthcare, smart homes, and industrial applications, has required robust human activity recognition (HAR) techniques to improve functionality and user experience. Although machine learning models have advanced HAR, they are increasingly susceptible to data poisoning attacks that compromise the data integrity and reliability of these systems. Conventional approaches to defending against such attacks often require extensive task-specific training with large, labeled datasets, which limits adaptability in dynamic IoT environments. This work proposes a novel framework that uses large language models (LLMs) to perform poisoning detection and sanitization in HAR systems, utilizing zero-shot, one-shot, and few-shot learning paradigms. Our approach incorporates \\textit{role play} prompting, whereby the LLM assumes the role of expert to contextualize and evaluate sensor anomalies, and \\textit{think step-by-step} reasoning, guiding the LLM to infer poisoning indicators in the raw sensor data and plausible clean alternatives. These strategies minimize reliance on curation of extensive datasets and enable robust, adaptable defense mechanisms in real-time. We perform an extensive evaluation of the framework, quantifying detection accuracy, sanitization quality, latency, and communication cost, thus demonstrating the practicality and effectiveness of LLMs in improving the security and reliability of wearable IoT systems."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-04T15:59:10Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    59,
                    10,
                    1,
                    308,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "W. K. M Mithsara"
                    },
                    {
                        "name": "Ning Yang"
                    },
                    {
                        "name": "Ahmed Imteaj"
                    },
                    {
                        "name": "Hussein Zangoti"
                    },
                    {
                        "name": "Abdur R. Shahid"
                    }
                ],
                "author_detail": {
                    "name": "Abdur R. Shahid"
                },
                "author": "Abdur R. Shahid"
            },
            {
                "id": "http://arxiv.org/abs/2511.16544v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.16544v2",
                "title": "WER is Unaware: Assessing How ASR Errors Distort Clinical Understanding in Patient Facing Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WER is Unaware: Assessing How ASR Errors Distort Clinical Understanding in Patient Facing Dialogue"
                },
                "updated": "2025-11-21T15:29:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    29,
                    43,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.16544v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.16544v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As Automatic Speech Recognition (ASR) is increasingly deployed in clinical dialogue, standard evaluations still rely heavily on Word Error Rate (WER). This paper challenges that standard, investigating whether WER or other common metrics correlate with the clinical impact of transcription errors. We establish a gold-standard benchmark by having expert clinicians compare ground-truth utterances to their ASR-generated counterparts, labeling the clinical impact of any discrepancies found in two distinct doctor-patient dialogue datasets. Our analysis reveals that WER and a comprehensive suite of existing metrics correlate poorly with the clinician-assigned risk labels (No, Minimal, or Significant Impact). To bridge this evaluation gap, we introduce an LLM-as-a-Judge, programmatically optimized using GEPA through DSPy to replicate expert clinical assessment. The optimized judge (Gemini-2.5-Pro) achieves human-comparable performance, obtaining 90% accuracy and a strong Cohen's $$ of 0.816. This work provides a validated, automated framework for moving ASR evaluation beyond simple textual fidelity to a necessary, scalable assessment of safety in clinical dialogue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Automatic Speech Recognition (ASR) is increasingly deployed in clinical dialogue, standard evaluations still rely heavily on Word Error Rate (WER). This paper challenges that standard, investigating whether WER or other common metrics correlate with the clinical impact of transcription errors. We establish a gold-standard benchmark by having expert clinicians compare ground-truth utterances to their ASR-generated counterparts, labeling the clinical impact of any discrepancies found in two distinct doctor-patient dialogue datasets. Our analysis reveals that WER and a comprehensive suite of existing metrics correlate poorly with the clinician-assigned risk labels (No, Minimal, or Significant Impact). To bridge this evaluation gap, we introduce an LLM-as-a-Judge, programmatically optimized using GEPA through DSPy to replicate expert clinical assessment. The optimized judge (Gemini-2.5-Pro) achieves human-comparable performance, obtaining 90% accuracy and a strong Cohen's $$ of 0.816. This work provides a validated, automated framework for moving ASR evaluation beyond simple textual fidelity to a necessary, scalable assessment of safety in clinical dialogue."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-20T16:59:20Z",
                "published_parsed": [
                    2025,
                    11,
                    20,
                    16,
                    59,
                    20,
                    3,
                    324,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zachary Ellis"
                    },
                    {
                        "name": "Jared Joselowitz"
                    },
                    {
                        "name": "Yash Deo"
                    },
                    {
                        "name": "Yajie He"
                    },
                    {
                        "name": "Anna Kalygina"
                    },
                    {
                        "name": "Aisling Higham"
                    },
                    {
                        "name": "Mana Rahimzadeh"
                    },
                    {
                        "name": "Yan Jia"
                    },
                    {
                        "name": "Ibrahim Habli"
                    },
                    {
                        "name": "Ernest Lim"
                    }
                ],
                "author_detail": {
                    "name": "Ernest Lim"
                },
                "author": "Ernest Lim"
            },
            {
                "id": "http://arxiv.org/abs/2511.17308v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17308v1",
                "title": "SpatialGeo:Boosting Spatial Reasoning in Multimodal LLMs via Geometry-Semantics Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpatialGeo:Boosting Spatial Reasoning in Multimodal LLMs via Geometry-Semantics Fusion"
                },
                "updated": "2025-11-21T15:24:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    24,
                    33,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17308v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multimodal large language models (MLLMs) have achieved significant progress in image and language tasks due to the strong reasoning capability of large language models (LLMs). Nevertheless, most MLLMs suffer from limited spatial reasoning ability to interpret and infer spatial arrangements in three-dimensional space. In this work, we propose a novel vision encoder based on hierarchical fusion of geometry and semantics features, generating spatial-aware visual embedding and boosting the spatial grounding capability of MLLMs. Specifically, we first unveil that the spatial ambiguity shortcoming stems from the lossy embedding of the vision encoder utilized in most existing MLLMs (e.g., CLIP), restricted to instance-level semantic features. This motivates us to complement CLIP with the geometry features from vision-only self-supervised learning via a hierarchical adapter, enhancing the spatial awareness in the proposed SpatialGeo. The network is efficiently trained using pretrained LLaVA model and optimized with random feature dropping to avoid trivial solutions relying solely on the CLIP encoder. Experimental results show that SpatialGeo improves the accuracy in spatial reasoning tasks, enhancing state-of-the-art models by at least 8.0% in SpatialRGPT-Bench with approximately 50% less memory cost during inference. The source code is available via https://ricky-plus.github.io/SpatialGeoPages/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have achieved significant progress in image and language tasks due to the strong reasoning capability of large language models (LLMs). Nevertheless, most MLLMs suffer from limited spatial reasoning ability to interpret and infer spatial arrangements in three-dimensional space. In this work, we propose a novel vision encoder based on hierarchical fusion of geometry and semantics features, generating spatial-aware visual embedding and boosting the spatial grounding capability of MLLMs. Specifically, we first unveil that the spatial ambiguity shortcoming stems from the lossy embedding of the vision encoder utilized in most existing MLLMs (e.g., CLIP), restricted to instance-level semantic features. This motivates us to complement CLIP with the geometry features from vision-only self-supervised learning via a hierarchical adapter, enhancing the spatial awareness in the proposed SpatialGeo. The network is efficiently trained using pretrained LLaVA model and optimized with random feature dropping to avoid trivial solutions relying solely on the CLIP encoder. Experimental results show that SpatialGeo improves the accuracy in spatial reasoning tasks, enhancing state-of-the-art models by at least 8.0% in SpatialRGPT-Bench with approximately 50% less memory cost during inference. The source code is available via https://ricky-plus.github.io/SpatialGeoPages/."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T15:24:33Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    24,
                    33,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jiajie Guo"
                    },
                    {
                        "name": "Qingpeng Zhu"
                    },
                    {
                        "name": "Jin Zeng"
                    },
                    {
                        "name": "Xiaolong Wu"
                    },
                    {
                        "name": "Changyong He"
                    },
                    {
                        "name": "Weida Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weida Wang"
                },
                "author": "Weida Wang"
            },
            {
                "id": "http://arxiv.org/abs/2511.17301v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17301v1",
                "title": "Large Language Models for Sentiment Analysis to Detect Social Challenges: A Use Case with South African Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Sentiment Analysis to Detect Social Challenges: A Use Case with South African Languages"
                },
                "updated": "2025-11-21T15:14:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    14,
                    32,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17301v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Sentiment analysis can aid in understanding people's opinions and emotions on social issues. In multilingual communities sentiment analysis systems can be used to quickly identify social challenges in social media posts, enabling government departments to detect and address these issues more precisely and effectively. Recently, large-language models (LLMs) have become available to the wide public and initial analyses have shown that they exhibit magnificent zero-shot sentiment analysis abilities in English. However, there is no work that has investigated to leverage LLMs for sentiment analysis on social media posts in South African languages and detect social challenges. Consequently, in this work, we analyse the zero-shot performance of the state-of-the-art LLMs GPT-3.5, GPT-4, LlaMa 2, PaLM 2, and Dolly 2 to investigate the sentiment polarities of the 10 most emerging topics in English, Sepedi and Setswana social media posts that fall within the jurisdictional areas of 10 South African government departments. Our results demonstrate that there are big differences between the various LLMs, topics, and languages. In addition, we show that a fusion of the outcomes of different LLMs provides large gains in sentiment classification performance with sentiment classification errors below 1%. Consequently, it is now feasible to provide systems that generate reliable information about sentiment analysis to detect social challenges and draw conclusions about possible needs for actions on specific topics and within different language groups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sentiment analysis can aid in understanding people's opinions and emotions on social issues. In multilingual communities sentiment analysis systems can be used to quickly identify social challenges in social media posts, enabling government departments to detect and address these issues more precisely and effectively. Recently, large-language models (LLMs) have become available to the wide public and initial analyses have shown that they exhibit magnificent zero-shot sentiment analysis abilities in English. However, there is no work that has investigated to leverage LLMs for sentiment analysis on social media posts in South African languages and detect social challenges. Consequently, in this work, we analyse the zero-shot performance of the state-of-the-art LLMs GPT-3.5, GPT-4, LlaMa 2, PaLM 2, and Dolly 2 to investigate the sentiment polarities of the 10 most emerging topics in English, Sepedi and Setswana social media posts that fall within the jurisdictional areas of 10 South African government departments. Our results demonstrate that there are big differences between the various LLMs, topics, and languages. In addition, we show that a fusion of the outcomes of different LLMs provides large gains in sentiment classification performance with sentiment classification errors below 1%. Consequently, it is now feasible to provide systems that generate reliable information about sentiment analysis to detect social challenges and draw conclusions about possible needs for actions on specific topics and within different language groups."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T15:14:32Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    14,
                    32,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Published in the Proceedings of The Southern African Conference on AI Research (SACAIR 2024), Bloemfontein, South Africa, 2-6 December 2024. ISBN: 978-0-7961-6069-0",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Koena Ronny Mabokela"
                    },
                    {
                        "name": "Tim Schlippe"
                    },
                    {
                        "name": "Matthias Wlfel"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Wlfel"
                },
                "author": "Matthias Wlfel"
            },
            {
                "id": "http://arxiv.org/abs/2511.17300v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17300v1",
                "title": "MolSight: Optical Chemical Structure Recognition with SMILES Pretraining, Multi-Granularity Learning and Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MolSight: Optical Chemical Structure Recognition with SMILES Pretraining, Multi-Granularity Learning and Reinforcement Learning"
                },
                "updated": "2025-11-21T15:11:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    11,
                    47,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17300v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Optical Chemical Structure Recognition (OCSR) plays a pivotal role in modern chemical informatics, enabling the automated conversion of chemical structure images from scientific literature, patents, and educational materials into machine-readable molecular representations. This capability is essential for large-scale chemical data mining, drug discovery pipelines, and Large Language Model (LLM) applications in related domains. However, existing OCSR systems face significant challenges in accurately recognizing stereochemical information due to the subtle visual cues that distinguish stereoisomers, such as wedge and dash bonds, ring conformations, and spatial arrangements. To address these challenges, we propose MolSight, a comprehensive learning framework for OCSR that employs a three-stage training paradigm. In the first stage, we conduct pre-training on large-scale but noisy datasets to endow the model with fundamental perception capabilities for chemical structure images. In the second stage, we perform multi-granularity fine-tuning using datasets with richer supervisory signals, systematically exploring how auxiliary tasks-specifically chemical bond classification and atom localization-contribute to molecular formula recognition. Finally, we employ reinforcement learning for post-training optimization and introduce a novel stereochemical structure dataset. Remarkably, we find that even with MolSight's relatively compact parameter size, the Group Relative Policy Optimization (GRPO) algorithm can further enhance the model's performance on stereomolecular. Through extensive experiments across diverse datasets, our results demonstrate that MolSight achieves state-of-the-art performance in (stereo)chemical optical structure recognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optical Chemical Structure Recognition (OCSR) plays a pivotal role in modern chemical informatics, enabling the automated conversion of chemical structure images from scientific literature, patents, and educational materials into machine-readable molecular representations. This capability is essential for large-scale chemical data mining, drug discovery pipelines, and Large Language Model (LLM) applications in related domains. However, existing OCSR systems face significant challenges in accurately recognizing stereochemical information due to the subtle visual cues that distinguish stereoisomers, such as wedge and dash bonds, ring conformations, and spatial arrangements. To address these challenges, we propose MolSight, a comprehensive learning framework for OCSR that employs a three-stage training paradigm. In the first stage, we conduct pre-training on large-scale but noisy datasets to endow the model with fundamental perception capabilities for chemical structure images. In the second stage, we perform multi-granularity fine-tuning using datasets with richer supervisory signals, systematically exploring how auxiliary tasks-specifically chemical bond classification and atom localization-contribute to molecular formula recognition. Finally, we employ reinforcement learning for post-training optimization and introduce a novel stereochemical structure dataset. Remarkably, we find that even with MolSight's relatively compact parameter size, the Group Relative Policy Optimization (GRPO) algorithm can further enhance the model's performance on stereomolecular. Through extensive experiments across diverse datasets, our results demonstrate that MolSight achieves state-of-the-art performance in (stereo)chemical optical structure recognition."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T15:11:47Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    11,
                    47,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Wenrui Zhang"
                    },
                    {
                        "name": "Xinggang Wang"
                    },
                    {
                        "name": "Bin Feng"
                    },
                    {
                        "name": "Wenyu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wenyu Liu"
                },
                "author": "Wenyu Liu"
            },
            {
                "id": "http://arxiv.org/abs/2511.17290v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17290v1",
                "title": "Estonian WinoGrande Dataset: Comparative Analysis of LLM Performance on Human and Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estonian WinoGrande Dataset: Comparative Analysis of LLM Performance on Human and Machine Translation"
                },
                "updated": "2025-11-21T15:01:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    1,
                    57,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17290v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this paper, we present a localized and culturally adapted Estonian translation of the test set from the widely used commonsense reasoning benchmark, WinoGrande. We detail the translation and adaptation process carried out by translation specialists and evaluate the performance of both proprietary and open source models on the human translated benchmark. Additionally, we explore the feasibility of achieving high-quality machine translation by incorporating insights from the manual translation process into the design of a detailed prompt. This prompt is specifically tailored to address both the linguistic characteristics of Estonian and the unique translation challenges posed by the WinoGrande dataset. Our findings show that model performance on the human translated Estonian dataset is slightly lower than on the original English test set, while performance on machine-translated data is notably worse. Additionally, our experiments indicate that prompt engineering offers limited improvement in translation quality or model accuracy, and highlight the importance of involving language specialists in dataset translation and adaptation to ensure reliable and interpretable evaluations of language competency and reasoning in large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a localized and culturally adapted Estonian translation of the test set from the widely used commonsense reasoning benchmark, WinoGrande. We detail the translation and adaptation process carried out by translation specialists and evaluate the performance of both proprietary and open source models on the human translated benchmark. Additionally, we explore the feasibility of achieving high-quality machine translation by incorporating insights from the manual translation process into the design of a detailed prompt. This prompt is specifically tailored to address both the linguistic characteristics of Estonian and the unique translation challenges posed by the WinoGrande dataset. Our findings show that model performance on the human translated Estonian dataset is slightly lower than on the original English test set, while performance on machine-translated data is notably worse. Additionally, our experiments indicate that prompt engineering offers limited improvement in translation quality or model accuracy, and highlight the importance of involving language specialists in dataset translation and adaptation to ensure reliable and interpretable evaluations of language competency and reasoning in large language models."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T15:01:57Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    1,
                    57,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Preprint",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Marii Ojastu"
                    },
                    {
                        "name": "Hele-Andra Kuulmets"
                    },
                    {
                        "name": "Aleksei Dorkin"
                    },
                    {
                        "name": "Marika Borovikova"
                    },
                    {
                        "name": "Dage Srg"
                    },
                    {
                        "name": "Kairit Sirts"
                    }
                ],
                "author_detail": {
                    "name": "Kairit Sirts"
                },
                "author": "Kairit Sirts"
            },
            {
                "id": "http://arxiv.org/abs/2505.04736v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.04736v2",
                "title": "The promise and limits of LLMs in constructing proofs and hints for logic problems in intelligent tutoring systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promise and limits of LLMs in constructing proofs and hints for logic problems in intelligent tutoring systems"
                },
                "updated": "2025-11-21T15:00:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    15,
                    0,
                    6,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.04736v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.04736v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Intelligent tutoring systems have demonstrated effectiveness in teaching formal propositional logic proofs, but their reliance on template-based explanations limits their ability to provide personalized student feedback. While large language models (LLMs) offer promising capabilities for dynamic feedback generation, they risk producing hallucinations or pedagogically unsound explanations. We evaluated the stepwise accuracy of LLMs in constructing multi-step symbolic logic proofs, comparing six prompting techniques across four state-of-the-art LLMs on 358 propositional logic problems. Results show that DeepSeek-V3 achieved superior performance up to 86.7% accuracy on stepwise proof construction and excelled particularly in simpler rules. We further used the best-performing LLM to generate explanatory hints for 1,050 unique student problem-solving states from a logic ITS and evaluated them on 4 criteria with both an LLM grader and human expert ratings on a 20% sample. Our analysis finds that LLM-generated hints were 75% accurate and rated highly by human evaluators on consistency and clarity, but did not perform as well explaining why the hint was provided or its larger context. Our results demonstrate that LLMs may be used to augment tutoring systems with logic tutoring hints, but require additional modifications to ensure accuracy and pedagogical appropriateness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent tutoring systems have demonstrated effectiveness in teaching formal propositional logic proofs, but their reliance on template-based explanations limits their ability to provide personalized student feedback. While large language models (LLMs) offer promising capabilities for dynamic feedback generation, they risk producing hallucinations or pedagogically unsound explanations. We evaluated the stepwise accuracy of LLMs in constructing multi-step symbolic logic proofs, comparing six prompting techniques across four state-of-the-art LLMs on 358 propositional logic problems. Results show that DeepSeek-V3 achieved superior performance up to 86.7% accuracy on stepwise proof construction and excelled particularly in simpler rules. We further used the best-performing LLM to generate explanatory hints for 1,050 unique student problem-solving states from a logic ITS and evaluated them on 4 criteria with both an LLM grader and human expert ratings on a 20% sample. Our analysis finds that LLM-generated hints were 75% accurate and rated highly by human evaluators on consistency and clarity, but did not perform as well explaining why the hint was provided or its larger context. Our results demonstrate that LLMs may be used to augment tutoring systems with logic tutoring hints, but require additional modifications to ensure accuracy and pedagogical appropriateness."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-07T18:48:23Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    18,
                    48,
                    23,
                    2,
                    127,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Sutapa Dey Tithi"
                    },
                    {
                        "name": "Arun Kumar Ramesh"
                    },
                    {
                        "name": "Clara DiMarco"
                    },
                    {
                        "name": "Xiaoyi Tian"
                    },
                    {
                        "name": "Nazia Alam"
                    },
                    {
                        "name": "Kimia Fazeli"
                    },
                    {
                        "name": "Tiffany Barnes"
                    }
                ],
                "author_detail": {
                    "name": "Tiffany Barnes"
                },
                "author": "Tiffany Barnes"
            },
            {
                "id": "http://arxiv.org/abs/2508.01109v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.01109v2",
                "title": "Platonic Representations for Poverty Mapping: Unified Vision-Language Codes or Agent-Induced Novelty?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Platonic Representations for Poverty Mapping: Unified Vision-Language Codes or Agent-Induced Novelty?"
                },
                "updated": "2025-11-21T14:32:46Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    14,
                    32,
                    46,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.01109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.01109v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We investigate whether socio-economic indicators like household wealth leave recoverable imprints in satellite imagery (capturing physical features) and Internet-sourced text (reflecting historical/economic narratives). Using Demographic and Health Survey (DHS) data from African neighborhoods, we pair Landsat images with LLM-generated textual descriptions conditioned on location/year and text retrieved by an AI search agent from web sources. We develop a multimodal framework predicting household wealth (International Wealth Index) through five pipelines: (i) vision model on satellite images, (ii) LLM using only location/year, (iii) AI agent searching/synthesizing web text, (iv) joint image-text encoder, (v) ensemble of all signals. Our framework yields three contributions. First, fusing vision and agent/LLM text outperforms vision-only baselines in wealth prediction (e.g., R-squared of 0.77 vs. 0.63 on out-of-sample splits), with LLM-internal knowledge proving more effective than agent-retrieved text, improving robustness to out-of-country and out-of-time generalization. Second, we find partial representational convergence: fused embeddings from vision/language modalities correlate moderately (median cosine similarity of 0.60 after alignment), suggesting a shared latent code of material well-being while retaining complementary details, consistent with the Platonic Representation Hypothesis. Although LLM-only text outperforms agent-retrieved data, challenging our Agent-Induced Novelty Hypothesis, modest gains from combining agent data in some splits weakly support the notion that agent-gathered information introduces unique representational structures not fully captured by static LLM knowledge. Third, we release a large-scale multimodal dataset comprising more than 60,000 DHS clusters linked to satellite images, LLM-generated descriptions, and agent-retrieved texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate whether socio-economic indicators like household wealth leave recoverable imprints in satellite imagery (capturing physical features) and Internet-sourced text (reflecting historical/economic narratives). Using Demographic and Health Survey (DHS) data from African neighborhoods, we pair Landsat images with LLM-generated textual descriptions conditioned on location/year and text retrieved by an AI search agent from web sources. We develop a multimodal framework predicting household wealth (International Wealth Index) through five pipelines: (i) vision model on satellite images, (ii) LLM using only location/year, (iii) AI agent searching/synthesizing web text, (iv) joint image-text encoder, (v) ensemble of all signals. Our framework yields three contributions. First, fusing vision and agent/LLM text outperforms vision-only baselines in wealth prediction (e.g., R-squared of 0.77 vs. 0.63 on out-of-sample splits), with LLM-internal knowledge proving more effective than agent-retrieved text, improving robustness to out-of-country and out-of-time generalization. Second, we find partial representational convergence: fused embeddings from vision/language modalities correlate moderately (median cosine similarity of 0.60 after alignment), suggesting a shared latent code of material well-being while retaining complementary details, consistent with the Platonic Representation Hypothesis. Although LLM-only text outperforms agent-retrieved data, challenging our Agent-Induced Novelty Hypothesis, modest gains from combining agent data in some splits weakly support the notion that agent-gathered information introduces unique representational structures not fully captured by static LLM knowledge. Third, we release a large-scale multimodal dataset comprising more than 60,000 DHS clusters linked to satellite images, LLM-generated descriptions, and agent-retrieved texts."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-01T23:07:16Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    23,
                    7,
                    16,
                    4,
                    213,
                    0
                ],
                "arxiv_comment": "7 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Satiyabooshan Murugaboopathy"
                    },
                    {
                        "name": "Connor T. Jerzak"
                    },
                    {
                        "name": "Adel Daoud"
                    }
                ],
                "author_detail": {
                    "name": "Adel Daoud"
                },
                "author": "Adel Daoud"
            },
            {
                "id": "http://arxiv.org/abs/2510.13302v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.13302v2",
                "title": "LLM one-shot style transfer for Authorship Attribution and Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM one-shot style transfer for Authorship Attribution and Verification"
                },
                "updated": "2025-11-21T14:29:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    14,
                    29,
                    26,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.13302v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.13302v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Computational stylometry analyzes writing style through quantitative patterns in text, supporting applications from forensic tasks such as identity linking and plagiarism detection to literary attribution in the humanities. Supervised and contrastive approaches rely on data with spurious correlations and often confuse style with topic. Despite their natural use in AI-generated text detection, the CLM pre-training of modern LLMs has been scarcely leveraged for general authorship problems. We propose a novel unsupervised approach based on this extensive pre-training and the in-context learning capabilities of LLMs, employing the log-probabilities of an LLM to measure style transferability from one text to another. Our method significantly outperforms LLM prompting approaches of comparable scale and achieves higher accuracy than contrastively trained baselines when controlling for topical correlations. Moreover, performance scales fairly consistently with the size of the base model and, in the case of authorship verification, with an additional mechanism that increases test-time computation; enabling flexible trade-offs between computational cost and accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational stylometry analyzes writing style through quantitative patterns in text, supporting applications from forensic tasks such as identity linking and plagiarism detection to literary attribution in the humanities. Supervised and contrastive approaches rely on data with spurious correlations and often confuse style with topic. Despite their natural use in AI-generated text detection, the CLM pre-training of modern LLMs has been scarcely leveraged for general authorship problems. We propose a novel unsupervised approach based on this extensive pre-training and the in-context learning capabilities of LLMs, employing the log-probabilities of an LLM to measure style transferability from one text to another. Our method significantly outperforms LLM prompting approaches of comparable scale and achieves higher accuracy than contrastively trained baselines when controlling for topical correlations. Moreover, performance scales fairly consistently with the size of the base model and, in the case of authorship verification, with an additional mechanism that increases test-time computation; enabling flexible trade-offs between computational cost and accuracy."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-15T08:43:24Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    8,
                    43,
                    24,
                    2,
                    288,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-Gonzlez"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martn"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho"
            },
            {
                "id": "http://arxiv.org/abs/2511.11313v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.11313v3",
                "title": "DocSLM: A Small Vision-Language Model for Long Multimodal Document Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DocSLM: A Small Vision-Language Model for Long Multimodal Document Understanding"
                },
                "updated": "2025-11-21T14:18:23Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    14,
                    18,
                    23,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.11313v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.11313v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Vision-Language Models (LVLMs) have demonstrated strong multimodal reasoning capabilities on long and complex documents. However, their high memory footprint makes them impractical for deployment on resource-constrained edge devices. We present DocSLM, an efficient Small Vision-Language Model designed for long-document understanding under constrained memory resources. DocSLM incorporates a Hierarchical Multimodal Compressor that jointly encodes visual, textual, and layout information from each page into a fixed-length sequence, greatly reducing memory consumption while preserving both local and global semantics. To enable scalable processing over arbitrarily long inputs, we introduce a Streaming Abstention mechanism that operates on document segments sequentially and filters low-confidence responses using an entropy-based uncertainty calibrator. Across multiple long multimodal document benchmarks, DocSLM matches or surpasses state-of-the-art methods while using 82\\% fewer visual tokens, 75\\% fewer parameters, and 71\\% lower latency, delivering reliable multimodal document understanding on lightweight edge devices. Code and Model are available in https://github.com/Tanveer81/DocSLM.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) have demonstrated strong multimodal reasoning capabilities on long and complex documents. However, their high memory footprint makes them impractical for deployment on resource-constrained edge devices. We present DocSLM, an efficient Small Vision-Language Model designed for long-document understanding under constrained memory resources. DocSLM incorporates a Hierarchical Multimodal Compressor that jointly encodes visual, textual, and layout information from each page into a fixed-length sequence, greatly reducing memory consumption while preserving both local and global semantics. To enable scalable processing over arbitrarily long inputs, we introduce a Streaming Abstention mechanism that operates on document segments sequentially and filters low-confidence responses using an entropy-based uncertainty calibrator. Across multiple long multimodal document benchmarks, DocSLM matches or surpasses state-of-the-art methods while using 82\\% fewer visual tokens, 75\\% fewer parameters, and 71\\% lower latency, delivering reliable multimodal document understanding on lightweight edge devices. Code and Model are available in https://github.com/Tanveer81/DocSLM.git."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-14T13:56:39Z",
                "published_parsed": [
                    2025,
                    11,
                    14,
                    13,
                    56,
                    39,
                    4,
                    318,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Tanveer Hannan"
                    },
                    {
                        "name": "Dimitrios Mallios"
                    },
                    {
                        "name": "Parth Pathak"
                    },
                    {
                        "name": "Faegheh Sardari"
                    },
                    {
                        "name": "Thomas Seidl"
                    },
                    {
                        "name": "Gedas Bertasius"
                    },
                    {
                        "name": "Mohsen Fayyaz"
                    },
                    {
                        "name": "Sunando Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Sunando Sengupta"
                },
                "author": "Sunando Sengupta"
            },
            {
                "id": "http://arxiv.org/abs/2511.13182v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13182v3",
                "title": "Evaluating Large Language Models for Diacritic Restoration in Romanian Texts: A Comparative Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models for Diacritic Restoration in Romanian Texts: A Comparative Study"
                },
                "updated": "2025-11-21T14:13:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    14,
                    13,
                    16,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13182v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13182v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Automatic diacritic restoration is crucial for text processing in languages with rich diacritical marks, such as Romanian. This study evaluates the performance of several large language models (LLMs) in restoring diacritics in Romanian texts. Using a comprehensive corpus, we tested models including OpenAI's GPT-3.5, GPT-4, GPT-4o, Google's Gemini 1.0 Pro, Meta's Llama 2 and Llama 3, MistralAI's Mixtral 8x7B Instruct, airoboros 70B, and OpenLLM-Ro's RoLlama 2 7B, under multiple prompt templates ranging from zero-shot to complex multi-shot instructions. Results show that models such as GPT-4o achieve high diacritic restoration accuracy, consistently surpassing a neutral echo baseline, while others, including Meta's Llama family, exhibit wider variability. These findings highlight the impact of model architecture, training data, and prompt design on diacritic restoration performance and outline promising directions for improving NLP tools for diacritic-rich languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic diacritic restoration is crucial for text processing in languages with rich diacritical marks, such as Romanian. This study evaluates the performance of several large language models (LLMs) in restoring diacritics in Romanian texts. Using a comprehensive corpus, we tested models including OpenAI's GPT-3.5, GPT-4, GPT-4o, Google's Gemini 1.0 Pro, Meta's Llama 2 and Llama 3, MistralAI's Mixtral 8x7B Instruct, airoboros 70B, and OpenLLM-Ro's RoLlama 2 7B, under multiple prompt templates ranging from zero-shot to complex multi-shot instructions. Results show that models such as GPT-4o achieve high diacritic restoration accuracy, consistently surpassing a neutral echo baseline, while others, including Meta's Llama family, exhibit wider variability. These findings highlight the impact of model architecture, training data, and prompt design on diacritic restoration performance and outline promising directions for improving NLP tools for diacritic-rich languages."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T09:43:54Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    9,
                    43,
                    54,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "The original submission contained metadata errors and requires correction. A revised and complete version will be submitted as a replacement",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Mihai Nadas"
                    },
                    {
                        "name": "Laura Diosan"
                    }
                ],
                "author_detail": {
                    "name": "Laura Diosan"
                },
                "author": "Laura Diosan"
            },
            {
                "id": "http://arxiv.org/abs/2511.17262v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17262v1",
                "title": "SlsReuse: LLM-Powered Serverless Function Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlsReuse: LLM-Powered Serverless Function Reuse"
                },
                "updated": "2025-11-21T14:08:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    14,
                    8,
                    12,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17262v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Serverless computing has rapidly emerged as a popular cloud computing paradigm. It enables developers to implement function-level tasks, i.e., serverless functions, without managing infrastructure. While reducing operational overhead, it poses challenges, especially for novice developers. Developing functions from scratch requires adapting to heterogeneous, platform-specific programming styles, making the process time-consuming and error-prone. Function reuse offers a promising solution to address these challenges. However, research on serverless computing lacks a dedicated approach for function recommendation. Existing techniques from traditional contexts remain insufficient due to the semantic gap between task descriptions and heterogeneous function implementations. Advances in large language models (LLMs), pre-trained on large-scale corpora, create opportunities to bridge this gap by aligning developer requirements with function semantics.\n  This paper presents SlsReuse, the first LLM-powered framework for serverless function reuse. Specifically, SlsReuse first constructs a reusable function repository serving as a foundational knowledge base. Then, it learns unified semantic-enhanced representations of heterogeneous functions through effective prompt engineering with few-shot prompting, capturing implicit code intent, target platforms, programming languages, and cloud services. Finally, given a natural language task query, SlsReuse performs intent-aware discovery combined with a multi-level pruning strategy and similarity matching. We evaluate SlsReuse on a curated dataset of 110 task queries. Built on ChatGPT-4o, one of the most representative LLMs, SlsReuse achieves Recall@10 of 91.20%, exceeding the state-of-the-art baseline by 24.53 percentage points.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing has rapidly emerged as a popular cloud computing paradigm. It enables developers to implement function-level tasks, i.e., serverless functions, without managing infrastructure. While reducing operational overhead, it poses challenges, especially for novice developers. Developing functions from scratch requires adapting to heterogeneous, platform-specific programming styles, making the process time-consuming and error-prone. Function reuse offers a promising solution to address these challenges. However, research on serverless computing lacks a dedicated approach for function recommendation. Existing techniques from traditional contexts remain insufficient due to the semantic gap between task descriptions and heterogeneous function implementations. Advances in large language models (LLMs), pre-trained on large-scale corpora, create opportunities to bridge this gap by aligning developer requirements with function semantics.\n  This paper presents SlsReuse, the first LLM-powered framework for serverless function reuse. Specifically, SlsReuse first constructs a reusable function repository serving as a foundational knowledge base. Then, it learns unified semantic-enhanced representations of heterogeneous functions through effective prompt engineering with few-shot prompting, capturing implicit code intent, target platforms, programming languages, and cloud services. Finally, given a natural language task query, SlsReuse performs intent-aware discovery combined with a multi-level pruning strategy and similarity matching. We evaluate SlsReuse on a curated dataset of 110 task queries. Built on ChatGPT-4o, one of the most representative LLMs, SlsReuse achieves Recall@10 of 91.20%, exceeding the state-of-the-art baseline by 24.53 percentage points."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T14:08:12Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    14,
                    8,
                    12,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Jinfeng Wen"
                    },
                    {
                        "name": "Yuehan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yuehan Sun"
                },
                "author": "Yuehan Sun"
            },
            {
                "id": "http://arxiv.org/abs/2511.17256v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17256v1",
                "title": "Cross-cultural value alignment frameworks for responsible AI governance: Evidence from China-West comparative analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-cultural value alignment frameworks for responsible AI governance: Evidence from China-West comparative analysis"
                },
                "updated": "2025-11-21T14:02:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    14,
                    2,
                    33,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17256v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17256v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As Large Language Models (LLMs) increasingly influence high-stakes decision-making across global contexts, ensuring their alignment with diverse cultural values has become a critical governance challenge. This study presents a Multi-Layered Auditing Platform for Responsible AI that systematically evaluates cross-cultural value alignment in China-origin and Western-origin LLMs through four integrated methodologies: Ethical Dilemma Corpus for assessing temporal stability, Diversity-Enhanced Framework (DEF) for quantifying cultural fidelity, First-Token Probability Alignment for distributional accuracy, and Multi-stAge Reasoning frameworK (MARK) for interpretable decision-making. Our comparative analysis of 20+ leading models, such as Qwen, GPT-4o, Claude, LLaMA, and DeepSeek, reveals universal challenges-fundamental instability in value systems, systematic under-representation of younger demographics, and non-linear relationships between model scale and alignment quality-alongside divergent regional development trajectories. While China-origin models increasingly emphasize multilingual data integration for context-specific optimization, Western models demonstrate greater architectural experimentation but persistent U.S.-centric biases. Neither paradigm achieves robust cross-cultural generalization. We establish that Mistral-series architectures significantly outperform LLaMA3-series in cross-cultural alignment, and that Full-Parameter Fine-Tuning on diverse datasets surpasses Reinforcement Learning from Human Feedback in preserving cultural variation...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) increasingly influence high-stakes decision-making across global contexts, ensuring their alignment with diverse cultural values has become a critical governance challenge. This study presents a Multi-Layered Auditing Platform for Responsible AI that systematically evaluates cross-cultural value alignment in China-origin and Western-origin LLMs through four integrated methodologies: Ethical Dilemma Corpus for assessing temporal stability, Diversity-Enhanced Framework (DEF) for quantifying cultural fidelity, First-Token Probability Alignment for distributional accuracy, and Multi-stAge Reasoning frameworK (MARK) for interpretable decision-making. Our comparative analysis of 20+ leading models, such as Qwen, GPT-4o, Claude, LLaMA, and DeepSeek, reveals universal challenges-fundamental instability in value systems, systematic under-representation of younger demographics, and non-linear relationships between model scale and alignment quality-alongside divergent regional development trajectories. While China-origin models increasingly emphasize multilingual data integration for context-specific optimization, Western models demonstrate greater architectural experimentation but persistent U.S.-centric biases. Neither paradigm achieves robust cross-cultural generalization. We establish that Mistral-series architectures significantly outperform LLaMA3-series in cross-cultural alignment, and that Full-Parameter Fine-Tuning on diverse datasets surpasses Reinforcement Learning from Human Feedback in preserving cultural variation..."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T14:02:33Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    14,
                    2,
                    33,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Presented on Academic Conference \"Technology for Good: Driving Social Impact\" (2025)",
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Haijiang Liu"
                    },
                    {
                        "name": "Jinguang Gu"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Daniel Hershcovich"
                    },
                    {
                        "name": "Qiaoling Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Qiaoling Xiao"
                },
                "author": "Qiaoling Xiao"
            },
            {
                "id": "http://arxiv.org/abs/2510.24428v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.24428v4",
                "title": "CodeWiki: Evaluating AI's Ability to Generate Holistic Documentation for Large-Scale Codebases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeWiki: Evaluating AI's Ability to Generate Holistic Documentation for Large-Scale Codebases"
                },
                "updated": "2025-11-21T13:44:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    13,
                    44,
                    49,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.24428v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.24428v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Given a large and evolving codebase, the ability to automatically generate holistic, architecture-aware documentation that captures not only individual functions but also cross-file, cross-module, and system-level interactions remains an open challenge. Comprehensive documentation is essential for long-term software maintenance and collaboration, yet current automated approaches still fail to model the rich semantic dependencies and architectural structures that define real-world software systems. We present \\textbf{CodeWiki}, a unified framework for automated repository-level documentation across seven programming languages. CodeWiki introduces three key innovations: (i) hierarchical decomposition that preserves architectural context across multiple levels of granularity, (ii) recursive multi-agent processing with dynamic task delegation for scalable generation, and (iii) multi-modal synthesis that integrates textual descriptions with visual artifacts such as architecture diagrams and data-flow representations. To enable rigorous evaluation, we introduce \\textbf{CodeWikiBench}, a comprehensive benchmark featuring multi-dimensional rubrics and LLM-based assessment protocols. Experimental results show that CodeWiki achieves a 68.79\\% quality score with proprietary models, outperforming the closed-source DeepWiki baseline (64.06\\%) by 4.73\\%, with particularly strong improvements on high-level scripting languages (+10.47\\%). We open-source CodeWiki to foster future research and community adoption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given a large and evolving codebase, the ability to automatically generate holistic, architecture-aware documentation that captures not only individual functions but also cross-file, cross-module, and system-level interactions remains an open challenge. Comprehensive documentation is essential for long-term software maintenance and collaboration, yet current automated approaches still fail to model the rich semantic dependencies and architectural structures that define real-world software systems. We present \\textbf{CodeWiki}, a unified framework for automated repository-level documentation across seven programming languages. CodeWiki introduces three key innovations: (i) hierarchical decomposition that preserves architectural context across multiple levels of granularity, (ii) recursive multi-agent processing with dynamic task delegation for scalable generation, and (iii) multi-modal synthesis that integrates textual descriptions with visual artifacts such as architecture diagrams and data-flow representations. To enable rigorous evaluation, we introduce \\textbf{CodeWikiBench}, a comprehensive benchmark featuring multi-dimensional rubrics and LLM-based assessment protocols. Experimental results show that CodeWiki achieves a 68.79\\% quality score with proprietary models, outperforming the closed-source DeepWiki baseline (64.06\\%) by 4.73\\%, with particularly strong improvements on high-level scripting languages (+10.47\\%). We open-source CodeWiki to foster future research and community adoption."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-28T13:52:46Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    52,
                    46,
                    1,
                    301,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Anh Nguyen Hoang"
                    },
                    {
                        "name": "Minh Le-Anh"
                    },
                    {
                        "name": "Bach Le"
                    },
                    {
                        "name": "Nghi D. Q. Bui"
                    }
                ],
                "author_detail": {
                    "name": "Nghi D. Q. Bui"
                },
                "author": "Nghi D. Q. Bui"
            },
            {
                "id": "http://arxiv.org/abs/2511.17242v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17242v1",
                "title": "Equivariant-Aware Structured Pruning for Efficient Edge Deployment: A Comprehensive Framework with Adaptive Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Equivariant-Aware Structured Pruning for Efficient Edge Deployment: A Comprehensive Framework with Adaptive Fine-Tuning"
                },
                "updated": "2025-11-21T13:41:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    13,
                    41,
                    47,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17242v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper presents a novel framework combining group equivariant convolutional neural networks (G-CNNs) with equivariant-aware structured pruning to produce compact, transformation-invariant models for resource-constrained environments. Equivariance to rotations is achieved through the C4 cyclic group via the e2cnn library,enabling consistent performance under geometric transformations while reducing computational overhead.\n  Our approach introduces structured pruning that preserves equivariant properties by analyzing e2cnn layer structure and applying neuron-level pruning to fully connected components. To mitigate accuracy degradation, we implement adaptive fine-tuning that automatically triggers when accuracy drop exceeds 2%, using early stopping and learning rate scheduling for efficient recovery. The framework includes dynamic INT8 quantization and a comprehensive pipeline encompassing training, knowledge distillation, structured pruning, fine-tuning, and quantization.\n  We evaluate our method on satellite imagery (EuroSAT) and standard benchmarks (CIFAR-10, Rotated MNIST) demonstrating effectiveness across diverse domains. Experimental results show 29.3% parameter reduction with significant accuracy recovery, demonstrating that structured pruning of equivariant networks achieves substantial compression while maintaining geometric robustness. Our pipeline provides a reproducible framework for optimizing equivariant models, bridging the gap between group-theoretic network design and practical deployment constraints, with particular relevance to satellite imagery analysis and geometric vision tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel framework combining group equivariant convolutional neural networks (G-CNNs) with equivariant-aware structured pruning to produce compact, transformation-invariant models for resource-constrained environments. Equivariance to rotations is achieved through the C4 cyclic group via the e2cnn library,enabling consistent performance under geometric transformations while reducing computational overhead.\n  Our approach introduces structured pruning that preserves equivariant properties by analyzing e2cnn layer structure and applying neuron-level pruning to fully connected components. To mitigate accuracy degradation, we implement adaptive fine-tuning that automatically triggers when accuracy drop exceeds 2%, using early stopping and learning rate scheduling for efficient recovery. The framework includes dynamic INT8 quantization and a comprehensive pipeline encompassing training, knowledge distillation, structured pruning, fine-tuning, and quantization.\n  We evaluate our method on satellite imagery (EuroSAT) and standard benchmarks (CIFAR-10, Rotated MNIST) demonstrating effectiveness across diverse domains. Experimental results show 29.3% parameter reduction with significant accuracy recovery, demonstrating that structured pruning of equivariant networks achieves substantial compression while maintaining geometric robustness. Our pipeline provides a reproducible framework for optimizing equivariant models, bridging the gap between group-theoretic network design and practical deployment constraints, with particular relevance to satellite imagery analysis and geometric vision tasks."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T13:41:47Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    13,
                    41,
                    47,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "8 pages, 5 tables, 1 figure. Accepted at IEEE EdgeCom 2025 (11th IEEE International Conference on Edge Computing and Scalable Cloud)",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Mohammed Alnemari"
                    }
                ],
                "author_detail": {
                    "name": "Mohammed Alnemari"
                },
                "author": "Mohammed Alnemari"
            },
            {
                "id": "http://arxiv.org/abs/2511.16224v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.16224v2",
                "title": "Beyond Code Similarity: Benchmarking the Plausibility, Efficiency, and Complexity of LLM-Generated Smart Contracts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Code Similarity: Benchmarking the Plausibility, Efficiency, and Complexity of LLM-Generated Smart Contracts"
                },
                "updated": "2025-11-21T13:40:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    13,
                    40,
                    38,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.16224v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.16224v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Smart Contracts are critical components of blockchain ecosystems, with Solidity as the dominant programming language. While LLMs excel at general-purpose code generation, the unique constraints of Smart Contracts, such as gas consumption, security, and determinism, raise open questions about the reliability of LLM-generated Solidity code. Existing studies lack a comprehensive evaluation of these critical functional and non-functional properties. We benchmark four state-of-the-art models under zero-shot and retrieval-augmented generation settings across 500 real-world functions. Our multi-faceted assessment employs code similarity metrics, semantic embeddings, automated test execution, gas profiling, and cognitive and cyclomatic complexity analysis. Results show that while LLMs produce code with high semantic similarity to real contracts, their functional correctness is low: only 20% to 26% of zero-shot generations behave identically to ground-truth implementations under testing. The generated code is consistently simpler, with significantly lower complexity and gas consumption, often due to omitted validation logic. Retrieval-Augmented Generation markedly improves performance, boosting functional correctness by up to 45% and yielding more concise and efficient code. Our findings reveal a significant gap between semantic similarity and functional plausibility in LLM-generated Smart Contracts. We conclude that while RAG is a powerful enhancer, achieving robust, production-ready code generation remains a substantial challenge, necessitating careful expert validation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart Contracts are critical components of blockchain ecosystems, with Solidity as the dominant programming language. While LLMs excel at general-purpose code generation, the unique constraints of Smart Contracts, such as gas consumption, security, and determinism, raise open questions about the reliability of LLM-generated Solidity code. Existing studies lack a comprehensive evaluation of these critical functional and non-functional properties. We benchmark four state-of-the-art models under zero-shot and retrieval-augmented generation settings across 500 real-world functions. Our multi-faceted assessment employs code similarity metrics, semantic embeddings, automated test execution, gas profiling, and cognitive and cyclomatic complexity analysis. Results show that while LLMs produce code with high semantic similarity to real contracts, their functional correctness is low: only 20% to 26% of zero-shot generations behave identically to ground-truth implementations under testing. The generated code is consistently simpler, with significantly lower complexity and gas consumption, often due to omitted validation logic. Retrieval-Augmented Generation markedly improves performance, boosting functional correctness by up to 45% and yielding more concise and efficient code. Our findings reveal a significant gap between semantic similarity and functional plausibility in LLM-generated Smart Contracts. We conclude that while RAG is a powerful enhancer, achieving robust, production-ready code generation remains a substantial challenge, necessitating careful expert validation."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-20T10:47:59Z",
                "published_parsed": [
                    2025,
                    11,
                    20,
                    10,
                    47,
                    59,
                    3,
                    324,
                    0
                ],
                "arxiv_comment": "20 pages",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Francesco Salzano"
                    },
                    {
                        "name": "Simone Scalabrino"
                    },
                    {
                        "name": "Rocco Oliveto"
                    },
                    {
                        "name": "Remo Pareschi"
                    }
                ],
                "author_detail": {
                    "name": "Remo Pareschi"
                },
                "author": "Remo Pareschi"
            },
            {
                "id": "http://arxiv.org/abs/2511.17241v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17241v1",
                "title": "Social-Media Based Personas Challenge: Hybrid Prediction of Common and Rare User Actions on Bluesky",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social-Media Based Personas Challenge: Hybrid Prediction of Common and Rare User Actions on Bluesky"
                },
                "updated": "2025-11-21T13:40:14Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    13,
                    40,
                    14,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17241v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Understanding and predicting user behavior on social media platforms is crucial for content recommendation and platform design. While existing approaches focus primarily on common actions like retweeting and liking, the prediction of rare but significant behaviors remains largely unexplored. This paper presents a hybrid methodology for social media user behavior prediction that addresses both frequent and infrequent actions across a diverse action vocabulary. We evaluate our approach on a large-scale Bluesky dataset containing 6.4 million conversation threads spanning 12 distinct user actions across 25 persona clusters. Our methodology combines four complementary approaches: (i) a lookup database system based on historical response patterns; (ii) persona-specific LightGBM models with engineered temporal and semantic features for common actions; (iii) a specialized hybrid neural architecture fusing textual and temporal representations for rare action classification; and (iv) generation of text replies. Our persona-specific models achieve an average macro F1-score of 0.64 for common action prediction, while our rare action classifier achieves 0.56 macro F1-score across 10 rare actions. These results demonstrate that effective social media behavior prediction requires tailored modeling strategies recognizing fundamental differences between action types. Our approach achieved first place in the SocialSim: Social-Media Based Personas challenge organized at the Social Simulation with LLMs workshop at COLM 2025.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and predicting user behavior on social media platforms is crucial for content recommendation and platform design. While existing approaches focus primarily on common actions like retweeting and liking, the prediction of rare but significant behaviors remains largely unexplored. This paper presents a hybrid methodology for social media user behavior prediction that addresses both frequent and infrequent actions across a diverse action vocabulary. We evaluate our approach on a large-scale Bluesky dataset containing 6.4 million conversation threads spanning 12 distinct user actions across 25 persona clusters. Our methodology combines four complementary approaches: (i) a lookup database system based on historical response patterns; (ii) persona-specific LightGBM models with engineered temporal and semantic features for common actions; (iii) a specialized hybrid neural architecture fusing textual and temporal representations for rare action classification; and (iv) generation of text replies. Our persona-specific models achieve an average macro F1-score of 0.64 for common action prediction, while our rare action classifier achieves 0.56 macro F1-score across 10 rare actions. These results demonstrate that effective social media behavior prediction requires tailored modeling strategies recognizing fundamental differences between action types. Our approach achieved first place in the SocialSim: Social-Media Based Personas challenge organized at the Social Simulation with LLMs workshop at COLM 2025."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T13:40:14Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    13,
                    40,
                    14,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "1st place at SocialSim: Social-Media Based Personas challenge 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Benjamin White"
                    },
                    {
                        "name": "Anastasia Shimorina"
                    }
                ],
                "author_detail": {
                    "name": "Anastasia Shimorina"
                },
                "author": "Anastasia Shimorina"
            },
            {
                "id": "http://arxiv.org/abs/2511.17235v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17235v1",
                "title": "NX-CGRA: A Programmable Hardware Accelerator for Core Transformer Algorithms on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NX-CGRA: A Programmable Hardware Accelerator for Core Transformer Algorithms on Edge Devices"
                },
                "updated": "2025-11-21T13:26:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    13,
                    26,
                    16,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17235v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The increasing diversity and complexity of transformer workloads at the edge present significant challenges in balancing performance, energy efficiency, and architectural flexibility. This paper introduces NX-CGRA, a programmable hardware accelerator designed to support a range of transformer inference algorithms, including both linear and non-linear functions. Unlike fixed-function accelerators optimized for narrow use cases, NX-CGRA employs a coarse-grained reconfigurable array (CGRA) architecture with software-driven programmability, enabling efficient execution across varied kernel patterns. The architecture is evaluated using representative benchmarks derived from real-world transformer models, demonstrating high overall efficiency and favorable energy-area tradeoffs across different classes of operations. These results indicate the potential of NX-CGRA as a scalable and adaptable hardware solution for edge transformer deployment under constrained power and silicon budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing diversity and complexity of transformer workloads at the edge present significant challenges in balancing performance, energy efficiency, and architectural flexibility. This paper introduces NX-CGRA, a programmable hardware accelerator designed to support a range of transformer inference algorithms, including both linear and non-linear functions. Unlike fixed-function accelerators optimized for narrow use cases, NX-CGRA employs a coarse-grained reconfigurable array (CGRA) architecture with software-driven programmability, enabling efficient execution across varied kernel patterns. The architecture is evaluated using representative benchmarks derived from real-world transformer models, demonstrating high overall efficiency and favorable energy-area tradeoffs across different classes of operations. These results indicate the potential of NX-CGRA as a scalable and adaptable hardware solution for edge transformer deployment under constrained power and silicon budgets."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T13:26:16Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    13,
                    26,
                    16,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "This paper has been accepted for publication at the Design, Automation and Test in Europe (DATE) Conference 2026. 2026 IEEE. Personal use of this material is permitted",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Rohit Prasad"
                    }
                ],
                "author_detail": {
                    "name": "Rohit Prasad"
                },
                "author": "Rohit Prasad"
            },
            {
                "id": "http://arxiv.org/abs/2409.11393v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2409.11393v3",
                "title": "LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Design of Multi Active/Passive Core-Agent Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Design of Multi Active/Passive Core-Agent Architectures"
                },
                "updated": "2025-11-21T13:25:25Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    13,
                    25,
                    25,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2409.11393v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2409.11393v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1016/j.inffus.2025.103865",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "In an era where vast amounts of data are collected and processed from diverse sources, there is a growing demand for sophisticated AI systems capable of intelligently fusing and analyzing this information. To address these challenges, researchers have turned towards integrating tools into LLM-powered agents to enhance the overall information fusion process. However, the conjunction of these technologies and the proposed enhancements in several state-of-the-art works followed a non-unified software architecture, resulting in a lack of modularity and terminological inconsistencies among researchers. To address these issues, we propose a novel LLM-based Agent Unified Modeling Framework (LLM-Agent-UMF) that establishes a clear foundation for agent development from both functional and software architectural perspectives, developed and evaluated using the Architecture Tradeoff and Risk Analysis Framework (ATRAF). Our framework clearly distinguishes between the different components of an LLM-based agent, setting LLMs and tools apart from a new element, the core-agent, which plays the role of central coordinator. This pivotal entity comprises five modules: planning, memory, profile, action, and security -- the latter often neglected in previous works. By classifying core-agents into passive and active types based on their authoritative natures, we propose various multi-core agent architectures that combine unique characteristics of distinctive agents to tackle complex tasks more efficiently. We evaluate our framework by applying it to thirteen state-of-the-art agents, thereby demonstrating its alignment with their functionalities and clarifying overlooked architectural aspects. Moreover, we thoroughly assess five architecture variants of our framework by designing new agent architectures that combine characteristics of state-of-the-art agents to address specific goals. ...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an era where vast amounts of data are collected and processed from diverse sources, there is a growing demand for sophisticated AI systems capable of intelligently fusing and analyzing this information. To address these challenges, researchers have turned towards integrating tools into LLM-powered agents to enhance the overall information fusion process. However, the conjunction of these technologies and the proposed enhancements in several state-of-the-art works followed a non-unified software architecture, resulting in a lack of modularity and terminological inconsistencies among researchers. To address these issues, we propose a novel LLM-based Agent Unified Modeling Framework (LLM-Agent-UMF) that establishes a clear foundation for agent development from both functional and software architectural perspectives, developed and evaluated using the Architecture Tradeoff and Risk Analysis Framework (ATRAF). Our framework clearly distinguishes between the different components of an LLM-based agent, setting LLMs and tools apart from a new element, the core-agent, which plays the role of central coordinator. This pivotal entity comprises five modules: planning, memory, profile, action, and security -- the latter often neglected in previous works. By classifying core-agents into passive and active types based on their authoritative natures, we propose various multi-core agent architectures that combine unique characteristics of distinctive agents to tackle complex tasks more efficiently. We evaluate our framework by applying it to thirteen state-of-the-art agents, thereby demonstrating its alignment with their functionalities and clarifying overlooked architectural aspects. Moreover, we thoroughly assess five architecture variants of our framework by designing new agent architectures that combine characteristics of state-of-the-art agents to address specific goals. ..."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-09-17T17:54:17Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    54,
                    17,
                    1,
                    261,
                    0
                ],
                "arxiv_comment": "39 pages, 19 figures, 3 tables. Published in Information Fusion, Volume 127, March 2026, 103865. Part of the special issue \"Data Fusion Approaches in Data-Centric AI for Developing Trustworthy AI Systems\"",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "arxiv_journal_ref": "Information Fusion 127 (2026) 103865",
                "authors": [
                    {
                        "name": "Amine Ben Hassouna"
                    },
                    {
                        "name": "Hana Chaari"
                    },
                    {
                        "name": "Ines Belhaj"
                    }
                ],
                "author_detail": {
                    "name": "Ines Belhaj"
                },
                "author": "Ines Belhaj",
                "arxiv_doi": "10.1016/j.inffus.2025.103865"
            },
            {
                "id": "http://arxiv.org/abs/2508.04652v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.04652v4",
                "title": "LLM Collaboration With Multi-Agent Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Collaboration With Multi-Agent Reinforcement Learning"
                },
                "updated": "2025-11-21T13:14:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    13,
                    14,
                    11,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.04652v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.04652v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "A large amount of work has been done in Multi-Agent Systems (MAS) for modeling and solving problems with multiple interacting agents. However, most LLMs are pretrained independently and not specifically optimized for coordination. Existing LLM fine-tuning frameworks rely on individual rewards, which require complex reward designs for each agent to encourage collaboration. To address these challenges, we model LLM collaboration as a cooperative Multi-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent, multi-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO), to solve it, building on current RL approaches for LLMs as well as MARL techniques. Our experiments on LLM writing and coding collaboration demonstrate that fine-tuning MAS with MAGRPO enables agents to generate high-quality responses efficiently through effective cooperation. Our approach opens the door to using other MARL methods for LLMs and highlights the associated challenges.\n  Our code is available at https://github.com/OpenMLRL/CoMLRL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A large amount of work has been done in Multi-Agent Systems (MAS) for modeling and solving problems with multiple interacting agents. However, most LLMs are pretrained independently and not specifically optimized for coordination. Existing LLM fine-tuning frameworks rely on individual rewards, which require complex reward designs for each agent to encourage collaboration. To address these challenges, we model LLM collaboration as a cooperative Multi-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent, multi-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO), to solve it, building on current RL approaches for LLMs as well as MARL techniques. Our experiments on LLM writing and coding collaboration demonstrate that fine-tuning MAS with MAGRPO enables agents to generate high-quality responses efficiently through effective cooperation. Our approach opens the door to using other MARL methods for LLMs and highlights the associated challenges.\n  Our code is available at https://github.com/OpenMLRL/CoMLRL."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-06T17:18:25Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    18,
                    25,
                    2,
                    218,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Shuo Liu"
                    },
                    {
                        "name": "Tianle Chen"
                    },
                    {
                        "name": "Zeyu Liang"
                    },
                    {
                        "name": "Xueguang Lyu"
                    },
                    {
                        "name": "Christopher Amato"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Amato"
                },
                "author": "Christopher Amato"
            },
            {
                "id": "http://arxiv.org/abs/2505.20714v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.20714v2",
                "title": "Wideband RF Radiance Field Modeling Using Frequency-embedded 3D Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wideband RF Radiance Field Modeling Using Frequency-embedded 3D Gaussian Splatting"
                },
                "updated": "2025-11-21T13:07:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    13,
                    7,
                    32,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.20714v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.20714v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Indoor environments typically contain diverse RF signals distributed across multiple frequency bands, including NB-IoT, Wi-Fi, and millimeter-wave. Consequently, wideband RF modeling is essential for practical applications such as joint deployment of heterogeneous RF systems, cross-band communication, and distributed RF sensing. Although 3D Gaussian Splatting (3DGS) techniques effectively reconstruct RF radiance fields at a single frequency, they cannot model fields at arbitrary or unknown frequencies across a wide range. In this paper, we present a novel 3DGS algorithm for unified wideband RF radiance field modeling. RF wave propagation depends on signal frequency and the 3D spatial environment, including geometry and material electromagnetic (EM) properties. To address these factors, we introduce a frequency-embedded EM feature network that utilizes 3D Gaussian spheres at each spatial location to learn the relationship between frequency and transmission characteristics, such as attenuation and radiance intensity. With a dataset containing sparse frequency samples in a specific 3D environment, our model can efficiently reconstruct RF radiance fields at arbitrary and unseen frequencies. To assess our approach, we introduce a large-scale power angular spectrum (PAS) dataset with 50,000 samples spanning 1 to 94 GHz across six indoor environments. Experimental results show that the proposed model trained on multiple frequencies achieves a Structural Similarity Index Measure (SSIM) of 0.922 for PAS reconstruction, surpassing state-of-the-art single-frequency 3DGS models with SSIM of 0.863.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indoor environments typically contain diverse RF signals distributed across multiple frequency bands, including NB-IoT, Wi-Fi, and millimeter-wave. Consequently, wideband RF modeling is essential for practical applications such as joint deployment of heterogeneous RF systems, cross-band communication, and distributed RF sensing. Although 3D Gaussian Splatting (3DGS) techniques effectively reconstruct RF radiance fields at a single frequency, they cannot model fields at arbitrary or unknown frequencies across a wide range. In this paper, we present a novel 3DGS algorithm for unified wideband RF radiance field modeling. RF wave propagation depends on signal frequency and the 3D spatial environment, including geometry and material electromagnetic (EM) properties. To address these factors, we introduce a frequency-embedded EM feature network that utilizes 3D Gaussian spheres at each spatial location to learn the relationship between frequency and transmission characteristics, such as attenuation and radiance intensity. With a dataset containing sparse frequency samples in a specific 3D environment, our model can efficiently reconstruct RF radiance fields at arbitrary and unseen frequencies. To assess our approach, we introduce a large-scale power angular spectrum (PAS) dataset with 50,000 samples spanning 1 to 94 GHz across six indoor environments. Experimental results show that the proposed model trained on multiple frequencies achieves a Structural Similarity Index Measure (SSIM) of 0.922 for PAS reconstruction, surpassing state-of-the-art single-frequency 3DGS models with SSIM of 0.863."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-27T04:48:26Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    4,
                    48,
                    26,
                    1,
                    147,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Zechen Li"
                    },
                    {
                        "name": "Lanqing Yang"
                    },
                    {
                        "name": "Yiheng Bian"
                    },
                    {
                        "name": "Hao Pan"
                    },
                    {
                        "name": "Yongjian Fu"
                    },
                    {
                        "name": "Yezhou Wang"
                    },
                    {
                        "name": "Zhuxi Chen"
                    },
                    {
                        "name": "Yi-Chao Chen"
                    },
                    {
                        "name": "Guangtao Xue"
                    }
                ],
                "author_detail": {
                    "name": "Guangtao Xue"
                },
                "author": "Guangtao Xue"
            },
            {
                "id": "http://arxiv.org/abs/2511.17220v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17220v1",
                "title": "Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs"
                },
                "updated": "2025-11-21T13:01:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    13,
                    1,
                    28,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17220v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This study presents PARROT (Persuasion and Agreement Robustness Rating of Output Truth), a robustness focused framework designed to measure the degradation in accuracy that occurs under social pressure exerted on users through authority and persuasion in large language models (LLMs) the phenomenon of sycophancy (excessive conformity). PARROT (i) isolates causal effects by comparing the neutral version of the same question with an authoritatively false version using a double-blind evaluation, (ii) quantifies confidence shifts toward the correct and imposed false responses using log-likelihood-based calibration tracking, and (iii) systematically classifies failure modes (e.g., robust correct, sycophantic agreement, reinforced error, stubborn error, self-correction, etc.) using an eight-state behavioral taxonomy. We evaluated 22 models using 1,302 MMLU-style multiple-choice questions across 13 domains and domain-specific authority templates. Findings show marked heterogeneity: advanced models (e.g., GPT-5, GPT-4.1, Claude Sonnet 4.5) exhibit low \"follow rates\" ($\\leq 11\\%$, GPT-5: 4\\%) and minimal accuracy loss, while older/smaller models show severe epistemic collapse (GPT-4: 80\\%, Qwen 2.5-1.5B: 94\\%). The danger is not limited to response changes; weak models reduce confidence in the correct response while increasing confidence in the imposed incorrect response. While international law and global knowledge at the domain level exhibit high fragility, elementary mathematics is relatively resilient. Consequently, we argue that the goal of \"resistance to overfitting pressure\" should be addressed as a primary objective alongside accuracy, harm avoidance, and privacy for safe deployment in the real world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents PARROT (Persuasion and Agreement Robustness Rating of Output Truth), a robustness focused framework designed to measure the degradation in accuracy that occurs under social pressure exerted on users through authority and persuasion in large language models (LLMs) the phenomenon of sycophancy (excessive conformity). PARROT (i) isolates causal effects by comparing the neutral version of the same question with an authoritatively false version using a double-blind evaluation, (ii) quantifies confidence shifts toward the correct and imposed false responses using log-likelihood-based calibration tracking, and (iii) systematically classifies failure modes (e.g., robust correct, sycophantic agreement, reinforced error, stubborn error, self-correction, etc.) using an eight-state behavioral taxonomy. We evaluated 22 models using 1,302 MMLU-style multiple-choice questions across 13 domains and domain-specific authority templates. Findings show marked heterogeneity: advanced models (e.g., GPT-5, GPT-4.1, Claude Sonnet 4.5) exhibit low \"follow rates\" ($\\leq 11\\%$, GPT-5: 4\\%) and minimal accuracy loss, while older/smaller models show severe epistemic collapse (GPT-4: 80\\%, Qwen 2.5-1.5B: 94\\%). The danger is not limited to response changes; weak models reduce confidence in the correct response while increasing confidence in the imposed incorrect response. While international law and global knowledge at the domain level exhibit high fragility, elementary mathematics is relatively resilient. Consequently, we argue that the goal of \"resistance to overfitting pressure\" should be addressed as a primary objective alongside accuracy, harm avoidance, and privacy for safe deployment in the real world."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T13:01:28Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    13,
                    1,
                    28,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yusuf elebi"
                    },
                    {
                        "name": "Mahmoud El Hussieni"
                    },
                    {
                        "name": "zay Ezerceli"
                    }
                ],
                "author_detail": {
                    "name": "zay Ezerceli"
                },
                "author": "zay Ezerceli"
            },
            {
                "id": "http://arxiv.org/abs/2511.17208v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17208v1",
                "title": "A Simple Yet Strong Baseline for Long-Term Conversational Memory of LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple Yet Strong Baseline for Long-Term Conversational Memory of LLM Agents"
                },
                "updated": "2025-11-21T12:41:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    41,
                    17,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17208v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLM-based conversational agents still struggle to maintain coherent, personalized interaction over many sessions: fixed context windows limit how much history can be kept in view, and most external memory approaches trade off between coarse retrieval over large chunks and fine-grained but fragmented views of the dialogue. Motivated by neo-Davidsonian event semantics, we propose an event-centric alternative that represents conversational history as short, event-like propositions which bundle together participants, temporal cues, and minimal local context, rather than as independent relation triples or opaque summaries. In contrast to work that aggressively compresses or forgets past content, our design aims to preserve information in a non-compressive form and make it more accessible, rather than more lossy. Concretely, we instruct an LLM to decompose each session into enriched elementary discourse units (EDUs) -- self-contained statements with normalized entities and source turn attributions -- and organize sessions, EDUs, and their arguments in a heterogeneous graph that supports associative recall. On top of this representation we build two simple retrieval-based variants that use dense similarity search and LLM filtering, with an optional graph-based propagation step to connect and aggregate evidence across related EDUs. Experiments on the LoCoMo and LongMemEval$_S$ benchmarks show that these event-centric memories match or surpass strong baselines, while operating with much shorter QA contexts. Our results suggest that structurally simple, event-level memory provides a principled and practical foundation for long-horizon conversational agents. Our code and data will be released at https://github.com/KevinSRR/EMem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based conversational agents still struggle to maintain coherent, personalized interaction over many sessions: fixed context windows limit how much history can be kept in view, and most external memory approaches trade off between coarse retrieval over large chunks and fine-grained but fragmented views of the dialogue. Motivated by neo-Davidsonian event semantics, we propose an event-centric alternative that represents conversational history as short, event-like propositions which bundle together participants, temporal cues, and minimal local context, rather than as independent relation triples or opaque summaries. In contrast to work that aggressively compresses or forgets past content, our design aims to preserve information in a non-compressive form and make it more accessible, rather than more lossy. Concretely, we instruct an LLM to decompose each session into enriched elementary discourse units (EDUs) -- self-contained statements with normalized entities and source turn attributions -- and organize sessions, EDUs, and their arguments in a heterogeneous graph that supports associative recall. On top of this representation we build two simple retrieval-based variants that use dense similarity search and LLM filtering, with an optional graph-based propagation step to connect and aggregate evidence across related EDUs. Experiments on the LoCoMo and LongMemEval$_S$ benchmarks show that these event-centric memories match or surpass strong baselines, while operating with much shorter QA contexts. Our results suggest that structurally simple, event-level memory provides a principled and practical foundation for long-horizon conversational agents. Our code and data will be released at https://github.com/KevinSRR/EMem."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T12:41:17Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    41,
                    17,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Work in progress",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Sizhe Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Sizhe Zhou"
                },
                "author": "Sizhe Zhou"
            },
            {
                "id": "http://arxiv.org/abs/2511.17205v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17205v1",
                "title": "E$^3$-Pruner: Towards Efficient, Economical, and Effective Layer Pruning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E$^3$-Pruner: Towards Efficient, Economical, and Effective Layer Pruning for Large Language Models"
                },
                "updated": "2025-11-21T12:32:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    32,
                    1,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17205v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the increasing size of large language models, layer pruning has gained increased attention as a hardware-friendly approach for model compression. However, existing layer pruning methods struggle to simultaneously address key practical deployment challenges, including performance degradation, high training costs, and limited acceleration. To overcome these limitations, we propose \\name, a task-\\underline{E}ffective, training-\\underline{E}conomical and inference-\\underline{E}fficient layer pruning framework. \\namespace introduces two key innovations: (1) a differentiable mask optimization method using a Gumbel-TopK sampler, enabling efficient and precise pruning mask search; and (2) an entropy-aware adaptive knowledge distillation strategy that enhances task performance. Extensive experiments over diverse model architectures and benchmarks demonstrate the superiority of our method over state-of-the-art approaches. Notably, \\namespace achieves 96\\% accuracy, a mere 0.8\\% drop from the original model (96.8\\%) on MATH-500 when pruning 25\\% layers of Qwen3-32B, outperforming existing SOTA (95\\%), with a 1.33$\\times$ inference speedup by consuming merely 0.5B tokens (0.5\\% of the post-training data volume).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing size of large language models, layer pruning has gained increased attention as a hardware-friendly approach for model compression. However, existing layer pruning methods struggle to simultaneously address key practical deployment challenges, including performance degradation, high training costs, and limited acceleration. To overcome these limitations, we propose \\name, a task-\\underline{E}ffective, training-\\underline{E}conomical and inference-\\underline{E}fficient layer pruning framework. \\namespace introduces two key innovations: (1) a differentiable mask optimization method using a Gumbel-TopK sampler, enabling efficient and precise pruning mask search; and (2) an entropy-aware adaptive knowledge distillation strategy that enhances task performance. Extensive experiments over diverse model architectures and benchmarks demonstrate the superiority of our method over state-of-the-art approaches. Notably, \\namespace achieves 96\\% accuracy, a mere 0.8\\% drop from the original model (96.8\\%) on MATH-500 when pruning 25\\% layers of Qwen3-32B, outperforming existing SOTA (95\\%), with a 1.33$\\times$ inference speedup by consuming merely 0.5B tokens (0.5\\% of the post-training data volume)."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T12:32:01Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    32,
                    1,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Tao Yuan"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Yinfei Pan"
                    },
                    {
                        "name": "Xuyang Cao"
                    },
                    {
                        "name": "Tianyu Zhang"
                    },
                    {
                        "name": "Lu Hou"
                    },
                    {
                        "name": "Ting Hu"
                    },
                    {
                        "name": "Xianzhi Yu"
                    }
                ],
                "author_detail": {
                    "name": "Xianzhi Yu"
                },
                "author": "Xianzhi Yu"
            },
            {
                "id": "http://arxiv.org/abs/2511.17201v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17201v1",
                "title": "Continual Alignment for SAM: Rethinking Foundation Models for Medical Image Segmentation in Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Alignment for SAM: Rethinking Foundation Models for Medical Image Segmentation in Continual Learning"
                },
                "updated": "2025-11-21T12:27:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    27,
                    49,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17201v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In medical image segmentation, heterogeneous privacy policies across institutions often make joint training on pooled datasets infeasible, motivating continual image segmentation-learning from data streams without catastrophic forgetting. While the Segment Anything Model (SAM) offers strong zero-shot priors and has been widely fine-tuned across downstream tasks, its large parameter count and computational overhead challenge practical deployment. This paper demonstrates that the SAM paradigm is highly promising once its computational efficiency and performance can be balanced. To this end, we introduce the Alignment Layer, a lightweight, plug-and-play module which aligns encoder-decoder feature distributions to efficiently adapt SAM to specific medical images, improving accuracy while reducing computation. Building on SAM and the Alignment Layer, we then propose Continual Alignment for SAM (CA-SAM), a continual learning strategy that automatically adapts the appropriate Alignment Layer to mitigate catastrophic forgetting, while leveraging SAM's zero-shot priors to preserve strong performance on unseen medical datasets. Experimented across nine medical segmentation datasets under continual-learning scenario, CA-SAM achieves state-of-the-art performance. Our code, models and datasets will be released on \\mbox{https://github.com/azzzzyo/Continual-Alignment-for-SAM.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In medical image segmentation, heterogeneous privacy policies across institutions often make joint training on pooled datasets infeasible, motivating continual image segmentation-learning from data streams without catastrophic forgetting. While the Segment Anything Model (SAM) offers strong zero-shot priors and has been widely fine-tuned across downstream tasks, its large parameter count and computational overhead challenge practical deployment. This paper demonstrates that the SAM paradigm is highly promising once its computational efficiency and performance can be balanced. To this end, we introduce the Alignment Layer, a lightweight, plug-and-play module which aligns encoder-decoder feature distributions to efficiently adapt SAM to specific medical images, improving accuracy while reducing computation. Building on SAM and the Alignment Layer, we then propose Continual Alignment for SAM (CA-SAM), a continual learning strategy that automatically adapts the appropriate Alignment Layer to mitigate catastrophic forgetting, while leveraging SAM's zero-shot priors to preserve strong performance on unseen medical datasets. Experimented across nine medical segmentation datasets under continual-learning scenario, CA-SAM achieves state-of-the-art performance. Our code, models and datasets will be released on \\mbox{https://github.com/azzzzyo/Continual-Alignment-for-SAM.}"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T12:27:49Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    27,
                    49,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jiayi Wang"
                    },
                    {
                        "name": "Wei Dai"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Haixia Bi"
                    },
                    {
                        "name": "Jian Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jian Sun"
                },
                "author": "Jian Sun"
            },
            {
                "id": "http://arxiv.org/abs/2511.17199v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17199v1",
                "title": "VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation"
                },
                "updated": "2025-11-21T12:26:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    26,
                    30,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17199v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-language-action (VLA) models show potential for general robotic tasks, but remain challenging in spatiotemporally coherent manipulation, which requires fine-grained representations. Typically, existing methods embed 3D positions into visual representations to enhance the spatial precision of actions. However, these methods struggle to achieve temporally coherent control over action execution. In this work, we propose VLA-4D, a general VLA model with 4D awareness for spatiotemporally coherent robotic manipulation. Our model is guided by two key designs: 1) 4D-aware visual representation. We extract visual features, embed 1D time into 3D positions for 4D embeddings, and fuse them into a unified visual representation via a cross-attention mechanism. 2) Spatiotemporal action representation. We extend conventional spatial action representations with temporal information to enable the spatiotemporal planning, and align the multimodal representations into the LLM for spatiotemporal action prediction. Within this unified framework, the designed visual and action representations jointly make robotic manipulation spatially-smooth and temporally-coherent. In addition, we extend the VLA dataset with temporal action annotations for fine-tuning our model. Extensive experiments have been conducted to verify the superiority of our method across different tasks of robotic manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language-action (VLA) models show potential for general robotic tasks, but remain challenging in spatiotemporally coherent manipulation, which requires fine-grained representations. Typically, existing methods embed 3D positions into visual representations to enhance the spatial precision of actions. However, these methods struggle to achieve temporally coherent control over action execution. In this work, we propose VLA-4D, a general VLA model with 4D awareness for spatiotemporally coherent robotic manipulation. Our model is guided by two key designs: 1) 4D-aware visual representation. We extract visual features, embed 1D time into 3D positions for 4D embeddings, and fuse them into a unified visual representation via a cross-attention mechanism. 2) Spatiotemporal action representation. We extend conventional spatial action representations with temporal information to enable the spatiotemporal planning, and align the multimodal representations into the LLM for spatiotemporal action prediction. Within this unified framework, the designed visual and action representations jointly make robotic manipulation spatially-smooth and temporally-coherent. In addition, we extend the VLA dataset with temporal action annotations for fine-tuning our model. Extensive experiments have been conducted to verify the superiority of our method across different tasks of robotic manipulation."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T12:26:30Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    26,
                    30,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Hanyu Zhou"
                    },
                    {
                        "name": "Chuanhao Ma"
                    },
                    {
                        "name": "Gim Hee Lee"
                    }
                ],
                "author_detail": {
                    "name": "Gim Hee Lee"
                },
                "author": "Gim Hee Lee"
            },
            {
                "id": "http://arxiv.org/abs/2511.17198v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17198v1",
                "title": "Designing Domain-Specific Agents via Hierarchical Task Abstraction Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing Domain-Specific Agents via Hierarchical Task Abstraction Mechanism"
                },
                "updated": "2025-11-21T12:25:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    25,
                    47,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17198v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLM-driven agents, particularly those using general frameworks like ReAct or human-inspired role-playing, often struggle in specialized domains that necessitate rigorously structured workflows. Fields such as remote sensing, requiring specialized tools (e.g., correction, spectral indices calculation), and multi-step procedures (e.g., numerous intermediate products and optional steps), significantly challenge generalized approaches. To address this gap, we introduce a novel agent design framework centered on a Hierarchical Task Abstraction Mechanism (HTAM). Specifically, HTAM moves beyond emulating social roles, instead structuring multi-agent systems into a logical hierarchy that mirrors the intrinsic task-dependency graph of a given domain. This task-centric architecture thus enforces procedural correctness and decomposes complex problems into sequential layers, where each layer's sub-agents operate on the outputs of the preceding layers. We instantiate this framework as EarthAgent, a multi-agent system tailored for complex geospatial analysis. To evaluate such complex planning capabilities, we build GeoPlan-bench, a comprehensive benchmark of realistic, multi-step geospatial planning tasks. It is accompanied by a suite of carefully designed metrics to evaluate tool selection, path similarity, and logical completeness. Experiments show that EarthAgent substantially outperforms a range of established single- and multi-agent systems. Our work demonstrates that aligning agent architecture with a domain's intrinsic task structure is a critical step toward building robust and reliable specialized autonomous systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-driven agents, particularly those using general frameworks like ReAct or human-inspired role-playing, often struggle in specialized domains that necessitate rigorously structured workflows. Fields such as remote sensing, requiring specialized tools (e.g., correction, spectral indices calculation), and multi-step procedures (e.g., numerous intermediate products and optional steps), significantly challenge generalized approaches. To address this gap, we introduce a novel agent design framework centered on a Hierarchical Task Abstraction Mechanism (HTAM). Specifically, HTAM moves beyond emulating social roles, instead structuring multi-agent systems into a logical hierarchy that mirrors the intrinsic task-dependency graph of a given domain. This task-centric architecture thus enforces procedural correctness and decomposes complex problems into sequential layers, where each layer's sub-agents operate on the outputs of the preceding layers. We instantiate this framework as EarthAgent, a multi-agent system tailored for complex geospatial analysis. To evaluate such complex planning capabilities, we build GeoPlan-bench, a comprehensive benchmark of realistic, multi-step geospatial planning tasks. It is accompanied by a suite of carefully designed metrics to evaluate tool selection, path similarity, and logical completeness. Experiments show that EarthAgent substantially outperforms a range of established single- and multi-agent systems. Our work demonstrates that aligning agent architecture with a domain's intrinsic task structure is a critical step toward building robust and reliable specialized autonomous systems."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T12:25:47Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    25,
                    47,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Page: https://earth-insights.github.io/EarthAgent",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Kaiyu Li"
                    },
                    {
                        "name": "Jiayu Wang"
                    },
                    {
                        "name": "Zhi Wang"
                    },
                    {
                        "name": "Hui Qiao"
                    },
                    {
                        "name": "Weizhan Zhang"
                    },
                    {
                        "name": "Deyu Meng"
                    },
                    {
                        "name": "Xiangyong Cao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyong Cao"
                },
                "author": "Xiangyong Cao"
            },
            {
                "id": "http://arxiv.org/abs/2511.17194v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17194v1",
                "title": "Steering in the Shadows: Causal Amplification for Activation Space Attacks in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering in the Shadows: Causal Amplification for Activation Space Attacks in Large Language Models"
                },
                "updated": "2025-11-21T12:19:55Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    19,
                    55,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17194v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern large language models (LLMs) are typically secured by auditing data, prompts, and refusal policies, while treating the forward pass as an implementation detail. We show that intermediate activations in decoder-only LLMs form a vulnerable attack surface for behavioral control. Building on recent findings on attention sinks and compression valleys, we identify a high-gain region in the residual stream where small, well-aligned perturbations are causally amplified along the autoregressive trajectory--a Causal Amplification Effect (CAE). We exploit this as an attack surface via Sensitivity-Scaled Steering (SSS), a progressive activation-level attack that combines beginning-of-sequence (BOS) anchoring with sensitivity-based reinforcement to focus a limited perturbation budget on the most vulnerable layers and tokens. We show that across multiple open-weight models and four behavioral axes, SSS induces large shifts in evil, hallucination, sycophancy, and sentiment while preserving high coherence and general capabilities, turning activation steering into a concrete security concern for white-box and supply-chain LLM deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) are typically secured by auditing data, prompts, and refusal policies, while treating the forward pass as an implementation detail. We show that intermediate activations in decoder-only LLMs form a vulnerable attack surface for behavioral control. Building on recent findings on attention sinks and compression valleys, we identify a high-gain region in the residual stream where small, well-aligned perturbations are causally amplified along the autoregressive trajectory--a Causal Amplification Effect (CAE). We exploit this as an attack surface via Sensitivity-Scaled Steering (SSS), a progressive activation-level attack that combines beginning-of-sequence (BOS) anchoring with sensitivity-based reinforcement to focus a limited perturbation budget on the most vulnerable layers and tokens. We show that across multiple open-weight models and four behavioral axes, SSS induces large shifts in evil, hallucination, sycophancy, and sentiment while preserving high coherence and general capabilities, turning activation steering into a concrete security concern for white-box and supply-chain LLM deployments."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T12:19:55Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    19,
                    55,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "31 pages, 5 figures, 9 tables",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Zhiyuan Xu"
                    },
                    {
                        "name": "Stanislav Abaimov"
                    },
                    {
                        "name": "Joseph Gardiner"
                    },
                    {
                        "name": "Sana Belguith"
                    }
                ],
                "author_detail": {
                    "name": "Sana Belguith"
                },
                "author": "Sana Belguith"
            },
            {
                "id": "http://arxiv.org/abs/2506.02392v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.02392v3",
                "title": "Improving Generalization of Neural Combinatorial Optimization for Vehicle Routing Problems via Test-Time Projection Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Generalization of Neural Combinatorial Optimization for Vehicle Routing Problems via Test-Time Projection Learning"
                },
                "updated": "2025-11-21T12:16:15Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    16,
                    15,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.02392v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.02392v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Neural Combinatorial Optimization (NCO) has emerged as a promising learning-based paradigm for addressing Vehicle Routing Problems (VRPs) by minimizing the need for extensive manual engineering. While existing NCO methods, trained on small-scale instances (e.g., 100 nodes), have demonstrated considerable success on problems of similar scale, their performance significantly degrades when applied to large-scale scenarios. This degradation arises from the distributional shift between training and testing data, rendering policies learned on small instances ineffective for larger problems. To overcome this limitation, we introduce a novel learning framework driven by Large Language Models (LLMs). This framework learns a projection between the training and testing distributions, which is then deployed to enhance the scalability of the NCO model. Notably, unlike prevailing techniques that necessitate joint training with the neural network, our approach operates exclusively during the inference phase, obviating the need for model retraining. Extensive experiments demonstrate that our method enables a backbone model (trained on 100-node instances) to achieve superior performance on large-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) of up to 100K nodes from diverse distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Combinatorial Optimization (NCO) has emerged as a promising learning-based paradigm for addressing Vehicle Routing Problems (VRPs) by minimizing the need for extensive manual engineering. While existing NCO methods, trained on small-scale instances (e.g., 100 nodes), have demonstrated considerable success on problems of similar scale, their performance significantly degrades when applied to large-scale scenarios. This degradation arises from the distributional shift between training and testing data, rendering policies learned on small instances ineffective for larger problems. To overcome this limitation, we introduce a novel learning framework driven by Large Language Models (LLMs). This framework learns a projection between the training and testing distributions, which is then deployed to enhance the scalability of the NCO model. Notably, unlike prevailing techniques that necessitate joint training with the neural network, our approach operates exclusively during the inference phase, obviating the need for model retraining. Extensive experiments demonstrate that our method enables a backbone model (trained on 100-node instances) to achieve superior performance on large-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) of up to 100K nodes from diverse distributions."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-03T03:15:22Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    3,
                    15,
                    22,
                    1,
                    154,
                    0
                ],
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2505.24627",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yuanyao Chen"
                    },
                    {
                        "name": "Rongsheng Chen"
                    },
                    {
                        "name": "Fu Luo"
                    },
                    {
                        "name": "Zhenkun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhenkun Wang"
                },
                "author": "Zhenkun Wang"
            },
            {
                "id": "http://arxiv.org/abs/2511.17190v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17190v1",
                "title": "AutoLink: Autonomous Schema Exploration and Expansion for Scalable Schema Linking in Text-to-SQL at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoLink: Autonomous Schema Exploration and Expansion for Scalable Schema Linking in Text-to-SQL at Scale"
                },
                "updated": "2025-11-21T12:12:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    12,
                    17,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17190v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17190v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "For industrial-scale text-to-SQL, supplying the entire database schema to Large Language Models (LLMs) is impractical due to context window limits and irrelevant noise. Schema linking, which filters the schema to a relevant subset, is therefore critical. However, existing methods incur prohibitive costs, struggle to trade off recall and noise, and scale poorly to large databases. We present \\textbf{AutoLink}, an autonomous agent framework that reformulates schema linking as an iterative, agent-driven process. Guided by an LLM, AutoLink dynamically explores and expands the linked schema subset, progressively identifying necessary schema components without inputting the full database schema. Our experiments demonstrate AutoLink's superior performance, achieving state-of-the-art strict schema linking recall of \\textbf{97.4\\%} on Bird-Dev and \\textbf{91.2\\%} on Spider-2.0-Lite, with competitive execution accuracy, i.e., \\textbf{68.7\\%} EX on Bird-Dev (better than CHESS) and \\textbf{34.9\\%} EX on Spider-2.0-Lite (ranking 2nd on the official leaderboard). Crucially, AutoLink exhibits \\textbf{exceptional scalability}, \\textbf{maintaining high recall}, \\textbf{efficient token consumption}, and \\textbf{robust execution accuracy} on large schemas (e.g., over 3,000 columns) where existing methods severely degrade-making it a highly scalable, high-recall schema-linking solution for industrial text-to-SQL systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For industrial-scale text-to-SQL, supplying the entire database schema to Large Language Models (LLMs) is impractical due to context window limits and irrelevant noise. Schema linking, which filters the schema to a relevant subset, is therefore critical. However, existing methods incur prohibitive costs, struggle to trade off recall and noise, and scale poorly to large databases. We present \\textbf{AutoLink}, an autonomous agent framework that reformulates schema linking as an iterative, agent-driven process. Guided by an LLM, AutoLink dynamically explores and expands the linked schema subset, progressively identifying necessary schema components without inputting the full database schema. Our experiments demonstrate AutoLink's superior performance, achieving state-of-the-art strict schema linking recall of \\textbf{97.4\\%} on Bird-Dev and \\textbf{91.2\\%} on Spider-2.0-Lite, with competitive execution accuracy, i.e., \\textbf{68.7\\%} EX on Bird-Dev (better than CHESS) and \\textbf{34.9\\%} EX on Spider-2.0-Lite (ranking 2nd on the official leaderboard). Crucially, AutoLink exhibits \\textbf{exceptional scalability}, \\textbf{maintaining high recall}, \\textbf{efficient token consumption}, and \\textbf{robust execution accuracy} on large schemas (e.g., over 3,000 columns) where existing methods severely degrade-making it a highly scalable, high-recall schema-linking solution for industrial text-to-SQL systems."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T12:12:17Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    12,
                    12,
                    17,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ziyang Wang"
                    },
                    {
                        "name": "Yuanlei Zheng"
                    },
                    {
                        "name": "Zhenbiao Cao"
                    },
                    {
                        "name": "Xiaojin Zhang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    },
                    {
                        "name": "Pei Fu"
                    },
                    {
                        "name": "Zhenbo Luo"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai"
            },
            {
                "id": "http://arxiv.org/abs/2511.16449v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.16449v2",
                "title": "VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference"
                },
                "updated": "2025-11-21T11:57:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    57,
                    47,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.16449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.16449v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-Language-Action (VLA) models have shown great promise for embodied AI, yet the heavy computational cost of processing continuous visual streams severely limits their real-time deployment. Token pruning (keeping salient visual tokens and dropping redundant ones) has emerged as an effective approach for accelerating Vision-Language Models (VLMs), offering a solution for efficient VLA. However, these VLM-specific token pruning methods select tokens based solely on semantic salience metrics (e.g., prefill attention), while overlooking the VLA's intrinsic dual-system nature of high-level semantic understanding and low-level action execution. Consequently, these methods bias token retention toward semantic cues, discard critical information for action generation, and significantly degrade VLA performance. To bridge this gap, we propose VLA-Pruner, a versatile plug-and-play VLA-specific token prune method that aligns with the dual-system nature of VLA models and exploits the temporal continuity in robot manipulation. Specifically, VLA-Pruner adopts a dual-level importance criterion for visual token retention: vision-language prefill attention for semantic-level relevance and action decode attention, estimated via temporal smoothing, for action-level importance. Based on this criterion, VLA-Pruner proposes a novel dual-level token selection strategy that adaptively preserves a compact, informative set of visual tokens for both semantic understanding and action execution under given compute budget. Experiments show that VLA-Pruner achieves state-of-the-art performance across multiple VLA architectures and diverse robotic tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have shown great promise for embodied AI, yet the heavy computational cost of processing continuous visual streams severely limits their real-time deployment. Token pruning (keeping salient visual tokens and dropping redundant ones) has emerged as an effective approach for accelerating Vision-Language Models (VLMs), offering a solution for efficient VLA. However, these VLM-specific token pruning methods select tokens based solely on semantic salience metrics (e.g., prefill attention), while overlooking the VLA's intrinsic dual-system nature of high-level semantic understanding and low-level action execution. Consequently, these methods bias token retention toward semantic cues, discard critical information for action generation, and significantly degrade VLA performance. To bridge this gap, we propose VLA-Pruner, a versatile plug-and-play VLA-specific token prune method that aligns with the dual-system nature of VLA models and exploits the temporal continuity in robot manipulation. Specifically, VLA-Pruner adopts a dual-level importance criterion for visual token retention: vision-language prefill attention for semantic-level relevance and action decode attention, estimated via temporal smoothing, for action-level importance. Based on this criterion, VLA-Pruner proposes a novel dual-level token selection strategy that adaptively preserves a compact, informative set of visual tokens for both semantic understanding and action execution under given compute budget. Experiments show that VLA-Pruner achieves state-of-the-art performance across multiple VLA architectures and diverse robotic tasks."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-20T15:16:09Z",
                "published_parsed": [
                    2025,
                    11,
                    20,
                    15,
                    16,
                    9,
                    3,
                    324,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Ziyan Liu"
                    },
                    {
                        "name": "Yeqiu Chen"
                    },
                    {
                        "name": "Hongyi Cai"
                    },
                    {
                        "name": "Tao Lin"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Bo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhao"
                },
                "author": "Bo Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2511.17178v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17178v1",
                "title": "Efficient Robot Design with Multi-Objective Black-Box Optimization and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Robot Design with Multi-Objective Black-Box Optimization and Large Language Models"
                },
                "updated": "2025-11-21T11:56:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    56,
                    52,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17178v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Various methods for robot design optimization have been developed so far. These methods are diverse, ranging from numerical optimization to black-box optimization. While numerical optimization is fast, it is not suitable for cases involving complex structures or discrete values, leading to frequent use of black-box optimization instead. However, black-box optimization suffers from low sampling efficiency and takes considerable sampling iterations to obtain good solutions. In this study, we propose a method to enhance the efficiency of robot body design based on black-box optimization by utilizing large language models (LLMs). In parallel with the sampling process based on black-box optimization, sampling is performed using LLMs, which are provided with problem settings and extensive feedback. We demonstrate that this method enables more efficient exploration of design solutions and discuss its characteristics and limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various methods for robot design optimization have been developed so far. These methods are diverse, ranging from numerical optimization to black-box optimization. While numerical optimization is fast, it is not suitable for cases involving complex structures or discrete values, leading to frequent use of black-box optimization instead. However, black-box optimization suffers from low sampling efficiency and takes considerable sampling iterations to obtain good solutions. In this study, we propose a method to enhance the efficiency of robot body design based on black-box optimization by utilizing large language models (LLMs). In parallel with the sampling process based on black-box optimization, sampling is performed using LLMs, which are provided with problem settings and extensive feedback. We demonstrate that this method enables more efficient exploration of design solutions and discuss its characteristics and limitations."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T11:56:52Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    56,
                    52,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Kento Kawaharazuka"
                    },
                    {
                        "name": "Yoshiki Obinata"
                    },
                    {
                        "name": "Naoaki Kanazawa"
                    },
                    {
                        "name": "Haoyu Jia"
                    },
                    {
                        "name": "Kei Okada"
                    }
                ],
                "author_detail": {
                    "name": "Kei Okada"
                },
                "author": "Kei Okada"
            },
            {
                "id": "http://arxiv.org/abs/2505.12396v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.12396v4",
                "title": "LLM-CoT Enhanced Graph Neural Recommendation with Harmonized Group Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-CoT Enhanced Graph Neural Recommendation with Harmonized Group Policy Optimization"
                },
                "updated": "2025-11-21T11:54:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    54,
                    35,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.12396v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.12396v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Graph neural networks (GNNs) have advanced recommender systems by modeling interaction relationships. However, existing graph-based recommenders rely on sparse ID features and do not fully exploit textual information, resulting in low information density within representations. Furthermore, graph contrastive learning faces challenges. Random negative sampling can introduce false negative samples, while fixed temperature coefficients cannot adapt to the heterogeneity of different nodes. In addition, current efforts to enhance recommendations with large language models (LLMs) have not fully utilized their Chain-of-Thought (CoT) reasoning capabilities to guide representation learning. To address these limitations, we introduces LGHRec (LLM-CoT Enhanced Graph Neural Recommendation with Harmonized Group Policy Optimization). This framework leverages the CoT reasoning ability of LLMs to generate semantic IDs, enriching reasoning processes and improving information density and semantic quality of representations. Moreover, we design a reinforcement learning algorithm, Harmonized Group Policy Optimization (HGPO), to optimize negative sampling strategies and temperature coefficients in contrastive learning. This approach enhances long-tail recommendation performance and ensures optimization consistency across different groups. Experimental results on three datasets demonstrate that LGHRec improves representation quality through semantic IDs generated by LLM's CoT reasoning and effectively boosts contrastive learning with HGPO. Our method outperforms several baseline models. The code is available at: https://anonymous.4open.science/r/LLM-Rec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks (GNNs) have advanced recommender systems by modeling interaction relationships. However, existing graph-based recommenders rely on sparse ID features and do not fully exploit textual information, resulting in low information density within representations. Furthermore, graph contrastive learning faces challenges. Random negative sampling can introduce false negative samples, while fixed temperature coefficients cannot adapt to the heterogeneity of different nodes. In addition, current efforts to enhance recommendations with large language models (LLMs) have not fully utilized their Chain-of-Thought (CoT) reasoning capabilities to guide representation learning. To address these limitations, we introduces LGHRec (LLM-CoT Enhanced Graph Neural Recommendation with Harmonized Group Policy Optimization). This framework leverages the CoT reasoning ability of LLMs to generate semantic IDs, enriching reasoning processes and improving information density and semantic quality of representations. Moreover, we design a reinforcement learning algorithm, Harmonized Group Policy Optimization (HGPO), to optimize negative sampling strategies and temperature coefficients in contrastive learning. This approach enhances long-tail recommendation performance and ensures optimization consistency across different groups. Experimental results on three datasets demonstrate that LGHRec improves representation quality through semantic IDs generated by LLM's CoT reasoning and effectively boosts contrastive learning with HGPO. Our method outperforms several baseline models. The code is available at: https://anonymous.4open.science/r/LLM-Rec."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-18T12:50:36Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    12,
                    50,
                    36,
                    6,
                    138,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Hailong Luo"
                    },
                    {
                        "name": "Bin Wu"
                    },
                    {
                        "name": "Hongyong Jia"
                    },
                    {
                        "name": "Qingqing Zhu"
                    },
                    {
                        "name": "Lianlei Shan"
                    }
                ],
                "author_detail": {
                    "name": "Lianlei Shan"
                },
                "author": "Lianlei Shan"
            },
            {
                "id": "http://arxiv.org/abs/2511.07318v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.07318v2",
                "title": "When Bias Pretends to Be Truth: How Spurious Correlations Undermine Hallucination Detection in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Bias Pretends to Be Truth: How Spurious Correlations Undermine Hallucination Detection in LLMs"
                },
                "updated": "2025-11-21T11:45:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    45,
                    34,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.07318v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.07318v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Despite substantial advances, large language models (LLMs) continue to exhibit hallucinations, generating plausible yet incorrect responses. In this paper, we highlight a critical yet previously underexplored class of hallucinations driven by spurious correlations -- superficial but statistically prominent associations between features (e.g., surnames) and attributes (e.g., nationality) present in the training data. We demonstrate that these spurious correlations induce hallucinations that are confidently generated, immune to model scaling, evade current detection methods, and persist even after refusal fine-tuning. Through systematically controlled synthetic experiments and empirical evaluations on state-of-the-art open-source and proprietary LLMs (including GPT-5), we show that existing hallucination detection methods, such as confidence-based filtering and inner-state probing, fundamentally fail in the presence of spurious correlations. Our theoretical analysis further elucidates why these statistical biases intrinsically undermine confidence-based detection techniques. Our findings thus emphasize the urgent need for new approaches explicitly designed to address hallucinations caused by spurious correlations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite substantial advances, large language models (LLMs) continue to exhibit hallucinations, generating plausible yet incorrect responses. In this paper, we highlight a critical yet previously underexplored class of hallucinations driven by spurious correlations -- superficial but statistically prominent associations between features (e.g., surnames) and attributes (e.g., nationality) present in the training data. We demonstrate that these spurious correlations induce hallucinations that are confidently generated, immune to model scaling, evade current detection methods, and persist even after refusal fine-tuning. Through systematically controlled synthetic experiments and empirical evaluations on state-of-the-art open-source and proprietary LLMs (including GPT-5), we show that existing hallucination detection methods, such as confidence-based filtering and inner-state probing, fundamentally fail in the presence of spurious correlations. Our theoretical analysis further elucidates why these statistical biases intrinsically undermine confidence-based detection techniques. Our findings thus emphasize the urgent need for new approaches explicitly designed to address hallucinations caused by spurious correlations."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-10T17:19:27Z",
                "published_parsed": [
                    2025,
                    11,
                    10,
                    17,
                    19,
                    27,
                    0,
                    314,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Shaowen Wang"
                    },
                    {
                        "name": "Yiqi Dong"
                    },
                    {
                        "name": "Ruinian Chang"
                    },
                    {
                        "name": "Tansheng Zhu"
                    },
                    {
                        "name": "Yuebo Sun"
                    },
                    {
                        "name": "Kaifeng Lyu"
                    },
                    {
                        "name": "Jian Li"
                    }
                ],
                "author_detail": {
                    "name": "Jian Li"
                },
                "author": "Jian Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.17170v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17170v1",
                "title": "Hallucinate Less by Thinking More: Aspect-Based Causal Abstention for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinate Less by Thinking More: Aspect-Based Causal Abstention for Large Language Models"
                },
                "updated": "2025-11-21T11:42:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    42,
                    49,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17170v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17170v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) often produce fluent but factually incorrect responses, a phenomenon known as hallucination. Abstention, where the model chooses not to answer and instead outputs phrases such as \"I don't know\", is a common safeguard. However, existing abstention methods typically rely on post-generation signals, such as generation variations or feedback, which limits their ability to prevent unreliable responses in advance. In this paper, we introduce Aspect-Based Causal Abstention (ABCA), a new framework that enables early abstention by analysing the internal diversity of LLM knowledge through causal inference. This diversity reflects the multifaceted nature of parametric knowledge acquired from various sources, representing diverse aspects such as disciplines, legal contexts, or temporal frames. ABCA estimates causal effects conditioned on these aspects to assess the reliability of knowledge relevant to a given query. Based on these estimates, we enable two types of abstention: Type-1, where aspect effects are inconsistent (knowledge conflict), and Type-2, where aspect effects consistently support abstention (knowledge insufficiency). Experiments on standard benchmarks demonstrate that ABCA improves abstention reliability, achieves state-of-the-art performance, and enhances the interpretability of abstention decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often produce fluent but factually incorrect responses, a phenomenon known as hallucination. Abstention, where the model chooses not to answer and instead outputs phrases such as \"I don't know\", is a common safeguard. However, existing abstention methods typically rely on post-generation signals, such as generation variations or feedback, which limits their ability to prevent unreliable responses in advance. In this paper, we introduce Aspect-Based Causal Abstention (ABCA), a new framework that enables early abstention by analysing the internal diversity of LLM knowledge through causal inference. This diversity reflects the multifaceted nature of parametric knowledge acquired from various sources, representing diverse aspects such as disciplines, legal contexts, or temporal frames. ABCA estimates causal effects conditioned on these aspects to assess the reliability of knowledge relevant to a given query. Based on these estimates, we enable two types of abstention: Type-1, where aspect effects are inconsistent (knowledge conflict), and Type-2, where aspect effects consistently support abstention (knowledge insufficiency). Experiments on standard benchmarks demonstrate that ABCA improves abstention reliability, achieves state-of-the-art performance, and enhances the interpretability of abstention decisions."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T11:42:49Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    42,
                    49,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Accepted to AAAI 2026 (Main Technical Track)",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Vy Nguyen"
                    },
                    {
                        "name": "Ziqi Xu"
                    },
                    {
                        "name": "Jeffrey Chan"
                    },
                    {
                        "name": "Estrid He"
                    },
                    {
                        "name": "Feng Xia"
                    },
                    {
                        "name": "Xiuzhen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiuzhen Zhang"
                },
                "author": "Xiuzhen Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.17166v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17166v1",
                "title": "Reflection-Based Relative Localization for Cooperative UAV Teams Using Active Markers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflection-Based Relative Localization for Cooperative UAV Teams Using Active Markers"
                },
                "updated": "2025-11-21T11:34:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    34,
                    8,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17166v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reflections of active markers in the environment are a common source of ambiguity in onboard visual relative localization. This work presents a novel approach for onboard relative localization in multi-robot teams that exploits these typically unwanted reflections of active markers in the environment. It operates without prior knowledge of robot size or predefined marker configurations and remains independent of surface properties, an essential feature for heterogeneous micro-aerial swarms cooperating in unknown environments. It explicitly accounts for uncertainties caused by non-flat surfaces, with a particular focus on dynamic water surfaces, which are especially relevant for marine deployments. We validated the approach in both indoor and outdoor experiments, demonstrating that the proposed reflection-based localization system operates reliably without prior knowledge of team member size and achieves greater effective range (above 30 m) and accuracy than state-of-the-art methods. The video and source code of this work will be made publicly available after publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflections of active markers in the environment are a common source of ambiguity in onboard visual relative localization. This work presents a novel approach for onboard relative localization in multi-robot teams that exploits these typically unwanted reflections of active markers in the environment. It operates without prior knowledge of robot size or predefined marker configurations and remains independent of surface properties, an essential feature for heterogeneous micro-aerial swarms cooperating in unknown environments. It explicitly accounts for uncertainties caused by non-flat surfaces, with a particular focus on dynamic water surfaces, which are especially relevant for marine deployments. We validated the approach in both indoor and outdoor experiments, demonstrating that the proposed reflection-based localization system operates reliably without prior knowledge of team member size and achieves greater effective range (above 30 m) and accuracy than state-of-the-art methods. The video and source code of this work will be made publicly available after publication."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T11:34:08Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    34,
                    8,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Tim Lakemann"
                    },
                    {
                        "name": "Daniel Bonilla Licea"
                    },
                    {
                        "name": "Viktor Walter"
                    },
                    {
                        "name": "Martin Saska"
                    }
                ],
                "author_detail": {
                    "name": "Martin Saska"
                },
                "author": "Martin Saska"
            },
            {
                "id": "http://arxiv.org/abs/2511.17162v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17162v1",
                "title": "The Belief-Desire-Intention Ontology for modelling mental reality and agency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Belief-Desire-Intention Ontology for modelling mental reality and agency"
                },
                "updated": "2025-11-21T11:30:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    30,
                    17,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17162v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Belief-Desire-Intention (BDI) model is a cornerstone for representing rational agency in artificial intelligence and cognitive sciences. Yet, its integration into structured, semantically interoperable knowledge representations remains limited. This paper presents a formal BDI Ontology, conceived as a modular Ontology Design Pattern (ODP) that captures the cognitive architecture of agents through beliefs, desires, intentions, and their dynamic interrelations. The ontology ensures semantic precision and reusability by aligning with foundational ontologies and best practices in modular design. Two complementary lines of experimentation demonstrate its applicability: (i) coupling the ontology with Large Language Models (LLMs) via Logic Augmented Generation (LAG) to assess the contribution of ontological grounding to inferential coherence and consistency; and (ii) integrating the ontology within the Semas reasoning platform, which implements the Triples-to-Beliefs-to-Triples (T2B2T) paradigm, enabling a bidirectional flow between RDF triples and agent mental states. Together, these experiments illustrate how the BDI Ontology acts as both a conceptual and operational bridge between declarative and procedural intelligence, paving the way for cognitively grounded, explainable, and semantically interoperable multi-agent and neuro-symbolic systems operating within the Web of Data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Belief-Desire-Intention (BDI) model is a cornerstone for representing rational agency in artificial intelligence and cognitive sciences. Yet, its integration into structured, semantically interoperable knowledge representations remains limited. This paper presents a formal BDI Ontology, conceived as a modular Ontology Design Pattern (ODP) that captures the cognitive architecture of agents through beliefs, desires, intentions, and their dynamic interrelations. The ontology ensures semantic precision and reusability by aligning with foundational ontologies and best practices in modular design. Two complementary lines of experimentation demonstrate its applicability: (i) coupling the ontology with Large Language Models (LLMs) via Logic Augmented Generation (LAG) to assess the contribution of ontological grounding to inferential coherence and consistency; and (ii) integrating the ontology within the Semas reasoning platform, which implements the Triples-to-Beliefs-to-Triples (T2B2T) paradigm, enabling a bidirectional flow between RDF triples and agent mental states. Together, these experiments illustrate how the BDI Ontology acts as both a conceptual and operational bridge between declarative and procedural intelligence, paving the way for cognitively grounded, explainable, and semantically interoperable multi-agent and neuro-symbolic systems operating within the Web of Data."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T11:30:17Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    30,
                    17,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Sara Zuppiroli"
                    },
                    {
                        "name": "Carmelo Fabio Longo"
                    },
                    {
                        "name": "Anna Sofia Lippolis"
                    },
                    {
                        "name": "Rocco Paolillo"
                    },
                    {
                        "name": "Lorenzo Giammei"
                    },
                    {
                        "name": "Miguel Ceriani"
                    },
                    {
                        "name": "Francesco Poggi"
                    },
                    {
                        "name": "Antonio Zinilli"
                    },
                    {
                        "name": "Andrea Giovanni Nuzzolese"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Giovanni Nuzzolese"
                },
                "author": "Andrea Giovanni Nuzzolese"
            },
            {
                "id": "http://arxiv.org/abs/2511.17161v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17161v1",
                "title": "The PLLuM Instruction Corpus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The PLLuM Instruction Corpus"
                },
                "updated": "2025-11-21T11:28:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    28,
                    11,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17161v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper describes the instruction dataset used to fine-tune a set of transformer-based large language models (LLMs) developed in the PLLuM (Polish Large Language Model) project. We present a functional typology of the organic, converted, and synthetic instructions used in PLLuM and share some observations about the implications of using human-authored versus synthetic instruction datasets in the linguistic adaptation of base LLMs. Additionally, we release the first representative subset of the PLLuM instruction corpus (PLLuMIC), which we believe to be useful in guiding and planning the development of similar datasets for other LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper describes the instruction dataset used to fine-tune a set of transformer-based large language models (LLMs) developed in the PLLuM (Polish Large Language Model) project. We present a functional typology of the organic, converted, and synthetic instructions used in PLLuM and share some observations about the implications of using human-authored versus synthetic instruction datasets in the linguistic adaptation of base LLMs. Additionally, we release the first representative subset of the PLLuM instruction corpus (PLLuMIC), which we believe to be useful in guiding and planning the development of similar datasets for other LLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T11:28:11Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    28,
                    11,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Piotr Pzik"
                    },
                    {
                        "name": "Filip arnecki"
                    },
                    {
                        "name": "Konrad Kaczyski"
                    },
                    {
                        "name": "Anna Cichosz"
                    },
                    {
                        "name": "Zuzanna Deckert"
                    },
                    {
                        "name": "Monika Garnys"
                    },
                    {
                        "name": "Izabela Grabarczyk"
                    },
                    {
                        "name": "Wojciech Janowski"
                    },
                    {
                        "name": "Sylwia Karasiska"
                    },
                    {
                        "name": "Aleksandra Kujawiak"
                    },
                    {
                        "name": "Piotr Misztela"
                    },
                    {
                        "name": "Maria Szymaska"
                    },
                    {
                        "name": "Karolina Walkusz"
                    },
                    {
                        "name": "Igor Siek"
                    },
                    {
                        "name": "Maciej Chrabszcz"
                    },
                    {
                        "name": "Anna Koos"
                    },
                    {
                        "name": "Agnieszka Karliska"
                    },
                    {
                        "name": "Karolina Seweryn"
                    },
                    {
                        "name": "Aleksandra Krasnodbska"
                    },
                    {
                        "name": "Paula Betscher"
                    },
                    {
                        "name": "Zofia Cieliska"
                    },
                    {
                        "name": "Katarzyna Kowol"
                    },
                    {
                        "name": "Artur Wilczek"
                    },
                    {
                        "name": "Maciej Trzciski"
                    },
                    {
                        "name": "Katarzyna Dziewulska"
                    },
                    {
                        "name": "Roman Roszko"
                    },
                    {
                        "name": "Tomasz Berna"
                    },
                    {
                        "name": "Jurgita Vaienonien"
                    },
                    {
                        "name": "Danuta Roszko"
                    },
                    {
                        "name": "Pawe Levchuk"
                    },
                    {
                        "name": "Pawe Kowalski"
                    },
                    {
                        "name": "Irena Prawdzic-Jankowska"
                    },
                    {
                        "name": "Marek Kozowski"
                    },
                    {
                        "name": "Sawomir Dadas"
                    },
                    {
                        "name": "Rafa Powiata"
                    },
                    {
                        "name": "Alina Wrblewska"
                    },
                    {
                        "name": "Katarzyna Krasnowska-Kiera"
                    },
                    {
                        "name": "Maciej Ogrodniczuk"
                    },
                    {
                        "name": "Micha Rudolf"
                    },
                    {
                        "name": "Piotr Rybak"
                    },
                    {
                        "name": "Karolina Saputa"
                    },
                    {
                        "name": "Joanna Wooszyn"
                    },
                    {
                        "name": "Marcin Oleksy"
                    },
                    {
                        "name": "Bartomiej Koptyra"
                    },
                    {
                        "name": "Teddy Ferdinan"
                    },
                    {
                        "name": "Stanisaw Woniak"
                    },
                    {
                        "name": "Maciej Piasecki"
                    },
                    {
                        "name": "Pawe Walkowiak"
                    },
                    {
                        "name": "Konrad Wojtasik"
                    },
                    {
                        "name": "Arkadiusz Janz"
                    },
                    {
                        "name": "Przemysaw Kazienko"
                    },
                    {
                        "name": "Julia Moska"
                    },
                    {
                        "name": "Jan Koco"
                    }
                ],
                "author_detail": {
                    "name": "Jan Koco"
                },
                "author": "Jan Koco"
            },
            {
                "id": "http://arxiv.org/abs/2511.17154v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17154v1",
                "title": "Proposal of an AI-Based Support Assistant for the ALICE-FIT Detector Setup at CERN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proposal of an AI-Based Support Assistant for the ALICE-FIT Detector Setup at CERN"
                },
                "updated": "2025-11-21T11:18:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    18,
                    59,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17154v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We propose an AI-based assistant designed to support the ALICE Fast Interaction Trigger (FIT) detector operators at CERN. The assistant helps diagnose and resolve operational issues in the Detector Control System (DCS), where decisions must often be made quickly and with incomplete information. By combining Large Language Models (LLMs) with a controlled Retrieval-Augmented Generation (RAG) pipeline, the system can generate context-aware suggestions based on verified ALICE-FIT documentation and problems that have appeared in the past.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an AI-based assistant designed to support the ALICE Fast Interaction Trigger (FIT) detector operators at CERN. The assistant helps diagnose and resolve operational issues in the Detector Control System (DCS), where decisions must often be made quickly and with incomplete information. By combining Large Language Models (LLMs) with a controlled Retrieval-Augmented Generation (RAG) pipeline, the system can generate context-aware suggestions based on verified ALICE-FIT documentation and problems that have appeared in the past."
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T11:18:59Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    18,
                    59,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex"
                },
                "authors": [
                    {
                        "name": "Ignacy Mermer"
                    },
                    {
                        "name": "Jakub Muszyski"
                    },
                    {
                        "name": "Jakub Moaryn"
                    },
                    {
                        "name": "Krystian Roson"
                    }
                ],
                "author_detail": {
                    "name": "Krystian Roson"
                },
                "author": "Krystian Roson"
            },
            {
                "id": "http://arxiv.org/abs/2511.17153v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17153v1",
                "title": "LangMark: A Multilingual Dataset for Automatic Post-Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LangMark: A Multilingual Dataset for Automatic Post-Editing"
                },
                "updated": "2025-11-21T11:18:15Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    18,
                    15,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17153v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.18653/v1/2025.acl-long.1569",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Automatic post-editing (APE) aims to correct errors in machine-translated text, enhancing translation quality, while reducing the need for human intervention. Despite advances in neural machine translation (NMT), the development of effective APE systems has been hindered by the lack of large-scale multilingual datasets specifically tailored to NMT outputs. To address this gap, we present and release LangMark, a new human-annotated multilingual APE dataset for English translation to seven languages: Brazilian Portuguese, French, German, Italian, Japanese, Russian, and Spanish. The dataset has 206,983 triplets, with each triplet consisting of a source segment, its NMT output, and a human post-edited translation. Annotated by expert human linguists, our dataset offers both linguistic diversity and scale. Leveraging this dataset, we empirically show that Large Language Models (LLMs) with few-shot prompting can effectively perform APE, improving upon leading commercial and even proprietary machine translation systems. We believe that this new resource will facilitate the future development and evaluation of APE systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic post-editing (APE) aims to correct errors in machine-translated text, enhancing translation quality, while reducing the need for human intervention. Despite advances in neural machine translation (NMT), the development of effective APE systems has been hindered by the lack of large-scale multilingual datasets specifically tailored to NMT outputs. To address this gap, we present and release LangMark, a new human-annotated multilingual APE dataset for English translation to seven languages: Brazilian Portuguese, French, German, Italian, Japanese, Russian, and Spanish. The dataset has 206,983 triplets, with each triplet consisting of a source segment, its NMT output, and a human post-edited translation. Annotated by expert human linguists, our dataset offers both linguistic diversity and scale. Leveraging this dataset, we empirically show that Large Language Models (LLMs) with few-shot prompting can effectively perform APE, improving upon leading commercial and even proprietary machine translation systems. We believe that this new resource will facilitate the future development and evaluation of APE systems."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T11:18:15Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    18,
                    15,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "15 pages, 8 figures, ACL 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "arxiv_journal_ref": "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2025, pages 32653-32667",
                "authors": [
                    {
                        "name": "Diego Velazquez"
                    },
                    {
                        "name": "Mikaela Grace"
                    },
                    {
                        "name": "Konstantinos Karageorgos"
                    },
                    {
                        "name": "Lawrence Carin"
                    },
                    {
                        "name": "Aaron Schliem"
                    },
                    {
                        "name": "Dimitrios Zaikis"
                    },
                    {
                        "name": "Roger Wechsler"
                    }
                ],
                "author_detail": {
                    "name": "Roger Wechsler"
                },
                "author": "Roger Wechsler",
                "arxiv_doi": "10.18653/v1/2025.acl-long.1569"
            },
            {
                "id": "http://arxiv.org/abs/2509.24975v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.24975v2",
                "title": "DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via Repetitive Pattern",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via Repetitive Pattern"
                },
                "updated": "2025-11-21T10:57:25Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    10,
                    57,
                    25,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.24975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.24975v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Software development relies heavily on extensive unit testing, which makes the efficiency of automated Unit Test Generation (UTG) particularly important. However, most existing LLMs generate test cases one token at a time in each forward pass, which leads to inefficient UTG. Recently, diffusion LLMs (dLLMs) have emerged, offering promising parallel generation capabilities and showing strong potential for efficient UTG. Despite this advantage, their application to UTG is still constrained by a clear trade-off between efficiency and test quality, since increasing the number of tokens generated in each step often causes a sharp decline in the quality of test cases. To overcome this limitation, we present DiffTester, an acceleration framework specifically tailored for dLLMs in UTG. The key idea of DiffTester is that unit tests targeting the same focal method often share repetitive structural patterns. By dynamically identifying these common patterns through abstract syntax tree analysis during generation, DiffTester adaptively increases the number of tokens produced at each step without compromising the quality of the output. To enable comprehensive evaluation, we extend the original TestEval benchmark, which was limited to Python, by introducing additional programming languages including Java and C++. Extensive experiments on three benchmarks with two representative models show that DiffTester delivers significant acceleration while preserving test coverage. Moreover, DiffTester generalizes well across different dLLMs and programming languages, providing a practical and scalable solution for efficient UTG in software development. Code and data are publicly available at https://github.com/wellbeingyang/DLM4UTG-open .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software development relies heavily on extensive unit testing, which makes the efficiency of automated Unit Test Generation (UTG) particularly important. However, most existing LLMs generate test cases one token at a time in each forward pass, which leads to inefficient UTG. Recently, diffusion LLMs (dLLMs) have emerged, offering promising parallel generation capabilities and showing strong potential for efficient UTG. Despite this advantage, their application to UTG is still constrained by a clear trade-off between efficiency and test quality, since increasing the number of tokens generated in each step often causes a sharp decline in the quality of test cases. To overcome this limitation, we present DiffTester, an acceleration framework specifically tailored for dLLMs in UTG. The key idea of DiffTester is that unit tests targeting the same focal method often share repetitive structural patterns. By dynamically identifying these common patterns through abstract syntax tree analysis during generation, DiffTester adaptively increases the number of tokens produced at each step without compromising the quality of the output. To enable comprehensive evaluation, we extend the original TestEval benchmark, which was limited to Python, by introducing additional programming languages including Java and C++. Extensive experiments on three benchmarks with two representative models show that DiffTester delivers significant acceleration while preserving test coverage. Moreover, DiffTester generalizes well across different dLLMs and programming languages, providing a practical and scalable solution for efficient UTG in software development. Code and data are publicly available at https://github.com/wellbeingyang/DLM4UTG-open ."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-29T16:04:18Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    4,
                    18,
                    0,
                    272,
                    0
                ],
                "arxiv_comment": "Update reference",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Lekang Yang"
                    },
                    {
                        "name": "Yuetong Liu"
                    },
                    {
                        "name": "Yitong Zhang"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.17133v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17133v1",
                "title": "Off the Planckian Locus: Using 2D Chromaticity to Improve In-Camera Color",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Off the Planckian Locus: Using 2D Chromaticity to Improve In-Camera Color"
                },
                "updated": "2025-11-21T10:49:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    10,
                    49,
                    4,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17133v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Traditional in-camera colorimetric mapping relies on correlated color temperature (CCT)-based interpolation between pre-calibrated transforms optimized for Planckian illuminants such as CIE A and D65. However, modern lighting technologies such as LEDs can deviate substantially from the Planckian locus, exposing the limitations of relying on conventional one-dimensional CCT for illumination characterization. This paper demonstrates that transitioning from 1D CCT (on the Planckian locus) to a 2D chromaticity space (off the Planckian locus) improves colorimetric accuracy across various mapping approaches. In addition, we replace conventional CCT interpolation with a lightweight multi-layer perceptron (MLP) that leverages 2D chromaticity features for robust colorimetric mapping under non-Planckian illuminants. A lightbox-based calibration procedure incorporating representative LED sources is used to train our MLP. Validated across diverse LED lighting, our method reduces angular reproduction error by 22% on average in LED-lit scenes, maintains backward compatibility with traditional illuminants, accommodates multi-illuminant scenes, and supports real-time in-camera deployment with negligible additional computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional in-camera colorimetric mapping relies on correlated color temperature (CCT)-based interpolation between pre-calibrated transforms optimized for Planckian illuminants such as CIE A and D65. However, modern lighting technologies such as LEDs can deviate substantially from the Planckian locus, exposing the limitations of relying on conventional one-dimensional CCT for illumination characterization. This paper demonstrates that transitioning from 1D CCT (on the Planckian locus) to a 2D chromaticity space (off the Planckian locus) improves colorimetric accuracy across various mapping approaches. In addition, we replace conventional CCT interpolation with a lightweight multi-layer perceptron (MLP) that leverages 2D chromaticity features for robust colorimetric mapping under non-Planckian illuminants. A lightbox-based calibration procedure incorporating representative LED sources is used to train our MLP. Validated across diverse LED lighting, our method reduces angular reproduction error by 22% on average in LED-lit scenes, maintains backward compatibility with traditional illuminants, accommodates multi-illuminant scenes, and supports real-time in-camera deployment with negligible additional computational cost."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T10:49:04Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    10,
                    49,
                    4,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Project page: https://cst-mlp.github.io",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "SaiKiran Tedla"
                    },
                    {
                        "name": "Joshua E. Little"
                    },
                    {
                        "name": "Hakki Can Karaimer"
                    },
                    {
                        "name": "Michael S. Brown"
                    }
                ],
                "author_detail": {
                    "name": "Michael S. Brown"
                },
                "author": "Michael S. Brown"
            },
            {
                "id": "http://arxiv.org/abs/2511.17131v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17131v1",
                "title": "UI-CUBE: Enterprise-Grade Computer Use Agent Benchmarking Beyond Task Accuracy to Operational Reliability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UI-CUBE: Enterprise-Grade Computer Use Agent Benchmarking Beyond Task Accuracy to Operational Reliability"
                },
                "updated": "2025-11-21T10:47:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    10,
                    47,
                    22,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17131v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While current Computer Use Agent (CUA) benchmarks measure task completion effectively, they provide limited assessment of enterprise deployment readiness, emphasizing functional correctness over the operational reliability required for production systems. We present UI-CUBE (UiPath Computer Use BEnchmark), a systematic benchmark comprising 226 tasks across two difficulty tiers designed to expose fundamental architectural limitations in current CUAs. Our evaluation covers simple UI interactions (136 tasks) and complex workflows including copy-paste tasks (50 tasks) and enterprise application scenarios (40 tasks), with systematic interface variation coverage, multi-resolution testing and automated validation of task success through the application state. Evaluation of five state-of-the-art models reveals a sharp capability cliff rather than gradual performance degradation. Simple UI interactions achieve 67-85% success rates (compared to 97.9% human performance), but complex workflows drop precipitously to 9-19%. Human evaluators with no prior application experience achieve only 61.2% on complex tasks despite near-perfect performance on simple tasks, establishing realistic performance ceilings. This discontinuous performance pattern -- where agents achieve 68-87% of human performance on simple tasks but only 15-32% on complex workflows -- indicates fundamental architectural limitations in memory management, hierarchical planning, and state coordination rather than incremental capability gaps addressable through better training or prompting. UI-CUBE functions as an enterprise-readiness diagnostic, revealing that while current CUAs can manipulate individual interface elements, they cannot yet function as reliable workflow automation tools. These findings provide architectural insights essential for developing production-ready CUAs capable of managing complex, multi-step enterprise processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While current Computer Use Agent (CUA) benchmarks measure task completion effectively, they provide limited assessment of enterprise deployment readiness, emphasizing functional correctness over the operational reliability required for production systems. We present UI-CUBE (UiPath Computer Use BEnchmark), a systematic benchmark comprising 226 tasks across two difficulty tiers designed to expose fundamental architectural limitations in current CUAs. Our evaluation covers simple UI interactions (136 tasks) and complex workflows including copy-paste tasks (50 tasks) and enterprise application scenarios (40 tasks), with systematic interface variation coverage, multi-resolution testing and automated validation of task success through the application state. Evaluation of five state-of-the-art models reveals a sharp capability cliff rather than gradual performance degradation. Simple UI interactions achieve 67-85% success rates (compared to 97.9% human performance), but complex workflows drop precipitously to 9-19%. Human evaluators with no prior application experience achieve only 61.2% on complex tasks despite near-perfect performance on simple tasks, establishing realistic performance ceilings. This discontinuous performance pattern -- where agents achieve 68-87% of human performance on simple tasks but only 15-32% on complex workflows -- indicates fundamental architectural limitations in memory management, hierarchical planning, and state coordination rather than incremental capability gaps addressable through better training or prompting. UI-CUBE functions as an enterprise-readiness diagnostic, revealing that while current CUAs can manipulate individual interface elements, they cannot yet function as reliable workflow automation tools. These findings provide architectural insights essential for developing production-ready CUAs capable of managing complex, multi-step enterprise processes."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T10:47:22Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    10,
                    47,
                    22,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "18 pages, 8 figures, 5 tables. Benchmark comprising 226 tasks across two difficulty tiers. Code and benchmark available at https://github.com/UiPath/uipath_enterprise_benchmark",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Horia Cristescu"
                    },
                    {
                        "name": "Charles Park"
                    },
                    {
                        "name": "Trong Canh Nguyen"
                    },
                    {
                        "name": "Sergiu Talmacel"
                    },
                    {
                        "name": "Alexandru-Gabriel Ilie"
                    },
                    {
                        "name": "Stefan Adam"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Adam"
                },
                "author": "Stefan Adam"
            },
            {
                "id": "http://arxiv.org/abs/2511.17129v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17129v1",
                "title": "Learning to Compress: Unlocking the Potential of Large Language Models for Text Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Compress: Unlocking the Potential of Large Language Models for Text Representation"
                },
                "updated": "2025-11-21T10:45:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    10,
                    45,
                    44,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17129v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Text representation plays a critical role in tasks like clustering, retrieval, and other downstream applications. With the emergence of large language models (LLMs), there is increasing interest in harnessing their capabilities for this purpose. However, most of the LLMs are inherently causal and optimized for next-token prediction, making them suboptimal for producing holistic representations. To address this, recent studies introduced pretext tasks to adapt LLMs for text representation. Most of these tasks, however, rely on token-level prediction objectives, such as the masked next-token prediction (MNTP) used in LLM2Vec. In this work, we explore the untapped potential of context compression as a pretext task for unsupervised adaptation of LLMs. During compression pre-training, the model learns to generate compact memory tokens, which substitute the whole context for downstream sequence prediction. Experiments demonstrate that a well-designed compression objective can significantly enhance LLM-based text representations, outperforming models trained with token-level pretext tasks. Further improvements through contrastive learning produce a strong representation model (LLM2Comp) that outperforms contemporary LLM-based text encoders on a wide range of tasks while being more sample-efficient, requiring significantly less training data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text representation plays a critical role in tasks like clustering, retrieval, and other downstream applications. With the emergence of large language models (LLMs), there is increasing interest in harnessing their capabilities for this purpose. However, most of the LLMs are inherently causal and optimized for next-token prediction, making them suboptimal for producing holistic representations. To address this, recent studies introduced pretext tasks to adapt LLMs for text representation. Most of these tasks, however, rely on token-level prediction objectives, such as the masked next-token prediction (MNTP) used in LLM2Vec. In this work, we explore the untapped potential of context compression as a pretext task for unsupervised adaptation of LLMs. During compression pre-training, the model learns to generate compact memory tokens, which substitute the whole context for downstream sequence prediction. Experiments demonstrate that a well-designed compression objective can significantly enhance LLM-based text representations, outperforming models trained with token-level pretext tasks. Further improvements through contrastive learning produce a strong representation model (LLM2Comp) that outperforms contemporary LLM-based text encoders on a wide range of tasks while being more sample-efficient, requiring significantly less training data."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T10:45:44Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    10,
                    45,
                    44,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Accepted by AAAI'26",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yeqin Zhang"
                    },
                    {
                        "name": "Yizheng Zhao"
                    },
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Ruihang Miao"
                    },
                    {
                        "name": "Cam-Tu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Cam-Tu Nguyen"
                },
                "author": "Cam-Tu Nguyen"
            },
            {
                "id": "http://arxiv.org/abs/2511.17124v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17124v1",
                "title": "A Counterfactual LLM Framework for Detecting Human Biases: A Case Study of Sex/Gender in Emergency Triage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Counterfactual LLM Framework for Detecting Human Biases: A Case Study of Sex/Gender in Emergency Triage"
                },
                "updated": "2025-11-21T10:37:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    10,
                    37,
                    52,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17124v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17124v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present a novel, domain-agnostic counterfactual approach that uses Large Language Models (LLMs) to quantify gender disparities in human clinical decision-making. The method trains an LLM to emulate observed decisions, then evaluates counterfactual pairs in which only gender is flipped, estimating directional disparities while holding all other clinical factors constant. We study emergency triage, validating the approach on more than 150,000 admissions to the Bordeaux University Hospital (France) and replicating results on a subset of MIMIC-IV across a different language, population, and healthcare system. In the Bordeaux cohort, otherwise identical presentations were approximately 2.1% more likely to receive a lower-severity triage score when presented as female rather than male; scaled to national emergency volumes in France, this corresponds to more than 200,000 lower-severity assignments per year. Modality-specific analyses indicate that both explicit tabular gender indicators and implicit textual gender cues contribute to the disparity. Beyond emergency care, the approach supports bias audits in other settings (e.g., hiring, academic, and justice decisions), providing a scalable tool to detect and address inequities in real-world decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel, domain-agnostic counterfactual approach that uses Large Language Models (LLMs) to quantify gender disparities in human clinical decision-making. The method trains an LLM to emulate observed decisions, then evaluates counterfactual pairs in which only gender is flipped, estimating directional disparities while holding all other clinical factors constant. We study emergency triage, validating the approach on more than 150,000 admissions to the Bordeaux University Hospital (France) and replicating results on a subset of MIMIC-IV across a different language, population, and healthcare system. In the Bordeaux cohort, otherwise identical presentations were approximately 2.1% more likely to receive a lower-severity triage score when presented as female rather than male; scaled to national emergency volumes in France, this corresponds to more than 200,000 lower-severity assignments per year. Modality-specific analyses indicate that both explicit tabular gender indicators and implicit textual gender cues contribute to the disparity. Beyond emergency care, the approach supports bias audits in other settings (e.g., hiring, academic, and justice decisions), providing a scalable tool to detect and address inequities in real-world decision-making."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T10:37:52Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    10,
                    37,
                    52,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Currently under review at npj Digital Medicine",
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Ariel Guerra-Adames"
                    },
                    {
                        "name": "Marta Avalos-Fernandez"
                    },
                    {
                        "name": "Ocane Dormus"
                    },
                    {
                        "name": "Leo Anthony Celi"
                    },
                    {
                        "name": "Cdric Gil-Jardin"
                    },
                    {
                        "name": "Emmanuel Lagarde"
                    }
                ],
                "author_detail": {
                    "name": "Emmanuel Lagarde"
                },
                "author": "Emmanuel Lagarde"
            },
            {
                "id": "http://arxiv.org/abs/2509.23629v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.23629v2",
                "title": "How LLMs Learn to Reason: A Complex Network Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How LLMs Learn to Reason: A Complex Network Perspective"
                },
                "updated": "2025-11-21T10:27:21Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    10,
                    27,
                    21,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.23629v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.23629v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Training large language models with Reinforcement Learning with Verifiable Rewards (RLVR) exhibits a set of distinctive and puzzling behaviors that remain poorly understood, including a two-stage learning curve, a V-shaped response-length trajectory, and a pronounced vulnerability to catastrophic forgetting. In this work, we propose that these behaviors are emergent collective phenomena governed not by neural implementation details, but by the topological evolution of the latent reasoning graph in semantic space. By demonstrating a dynamical isomorphism between a 1.5B-parameter LLM and a minimal Concept Network Model (CoNet), we trace the causal source to the self-organization of a sparse concept web pinned to an average degree of two. This geometric perspective provides a unified physical explanation for the observed anomalies: the V-shaped trajectory tracks the evolution from parallel local skill optimization to global network integration; catastrophic forgetting stems from the topological disconnection of critical ``trunk'' edges; and policy collapse arises from the accumulation of sequential transitions at the web's leaf nodes, where broad exploration abruptly freezes into rigid, high-reward trajectories. Identifying a ``maximally frustrated state'' at the transition between learning stages, we propose Annealed-RLVR, a principled algorithm that injects a targeted SFT ``heating'' step to resolve this topological bottleneck. Experiments confirm that this theory-driven intervention outperforms standard RLVR on both in-distribution and out-of-distribution benchmarks (including Minerva and AIME). By recasting RLVR from black-box optimization into a predictable process of structural self-organization, our work provides a new physical intuition for engineering the emergent reasoning capabilities of future AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models with Reinforcement Learning with Verifiable Rewards (RLVR) exhibits a set of distinctive and puzzling behaviors that remain poorly understood, including a two-stage learning curve, a V-shaped response-length trajectory, and a pronounced vulnerability to catastrophic forgetting. In this work, we propose that these behaviors are emergent collective phenomena governed not by neural implementation details, but by the topological evolution of the latent reasoning graph in semantic space. By demonstrating a dynamical isomorphism between a 1.5B-parameter LLM and a minimal Concept Network Model (CoNet), we trace the causal source to the self-organization of a sparse concept web pinned to an average degree of two. This geometric perspective provides a unified physical explanation for the observed anomalies: the V-shaped trajectory tracks the evolution from parallel local skill optimization to global network integration; catastrophic forgetting stems from the topological disconnection of critical ``trunk'' edges; and policy collapse arises from the accumulation of sequential transitions at the web's leaf nodes, where broad exploration abruptly freezes into rigid, high-reward trajectories. Identifying a ``maximally frustrated state'' at the transition between learning stages, we propose Annealed-RLVR, a principled algorithm that injects a targeted SFT ``heating'' step to resolve this topological bottleneck. Experiments confirm that this theory-driven intervention outperforms standard RLVR on both in-distribution and out-of-distribution benchmarks (including Minerva and AIME). By recasting RLVR from black-box optimization into a predictable process of structural self-organization, our work provides a new physical intuition for engineering the emergent reasoning capabilities of future AI systems."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-28T04:10:37Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    4,
                    10,
                    37,
                    6,
                    271,
                    0
                ],
                "arxiv_comment": "24 pages, 11 figures, 1 table, under review as a conference paper at ICLR 2026",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Sihan Hu"
                    },
                    {
                        "name": "Xiansheng Cai"
                    },
                    {
                        "name": "Yuan Huang"
                    },
                    {
                        "name": "Zhiyuan Yao"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Pan Zhang"
                    },
                    {
                        "name": "Youjin Deng"
                    },
                    {
                        "name": "Kun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kun Chen"
                },
                "author": "Kun Chen"
            },
            {
                "id": "http://arxiv.org/abs/2511.17113v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17113v1",
                "title": "AutoGraphAD: A novel approach using Variational Graph Autoencoders for anomalous network flow detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoGraphAD: A novel approach using Variational Graph Autoencoders for anomalous network flow detection"
                },
                "updated": "2025-11-21T10:22:00Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    10,
                    22,
                    0,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17113v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Network Intrusion Detection Systems (NIDS) are essential tools for detecting network attacks and intrusions. While extensive research has explored the use of supervised Machine Learning for attack detection and characterisation, these methods require accurately labelled datasets, which are very costly to obtain. Moreover, existing public datasets have limited and/or outdated attacks, and many of them suffer from mislabelled data. To reduce the reliance on labelled data, we propose AutoGraphAD, a novel unsupervised anomaly detection approach based on a Heterogeneous Variational Graph Autoencoder. AutoGraphAD operates on heterogeneous graphs, made from connection and IP nodes that capture network activity within a time window. The model is trained using unsupervised and contrastive learning, without relying on any labelled data. The reconstruction, structural loss, and KL divergence are then weighted and combined in an anomaly score that is then used for anomaly detection. Overall, AutoGraphAD yields the same, and in some cases better, results than previous unsupervised approaches, such as Anomal-E, but without requiring costly downstream anomaly detectors. As a result, AutoGraphAD achieves around 1.18 orders of magnitude faster training and 1.03 orders of magnitude faster inference, which represents a significant advantage for operational deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network Intrusion Detection Systems (NIDS) are essential tools for detecting network attacks and intrusions. While extensive research has explored the use of supervised Machine Learning for attack detection and characterisation, these methods require accurately labelled datasets, which are very costly to obtain. Moreover, existing public datasets have limited and/or outdated attacks, and many of them suffer from mislabelled data. To reduce the reliance on labelled data, we propose AutoGraphAD, a novel unsupervised anomaly detection approach based on a Heterogeneous Variational Graph Autoencoder. AutoGraphAD operates on heterogeneous graphs, made from connection and IP nodes that capture network activity within a time window. The model is trained using unsupervised and contrastive learning, without relying on any labelled data. The reconstruction, structural loss, and KL divergence are then weighted and combined in an anomaly score that is then used for anomaly detection. Overall, AutoGraphAD yields the same, and in some cases better, results than previous unsupervised approaches, such as Anomal-E, but without requiring costly downstream anomaly detectors. As a result, AutoGraphAD achieves around 1.18 orders of magnitude faster training and 1.03 orders of magnitude faster inference, which represents a significant advantage for operational deployment."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T10:22:00Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    10,
                    22,
                    0,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "11 pages, 9 figures",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Georgios Anyfantis"
                    },
                    {
                        "name": "Pere Barlet-Ros"
                    }
                ],
                "author_detail": {
                    "name": "Pere Barlet-Ros"
                },
                "author": "Pere Barlet-Ros"
            },
            {
                "id": "http://arxiv.org/abs/2306.08543v5",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2306.08543v5",
                "title": "MiniLLM: Knowledge Distillation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniLLM: Knowledge Distillation of Large Language Models"
                },
                "updated": "2025-11-21T10:20:53Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    10,
                    20,
                    53,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2306.08543v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2306.08543v5",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge of white-box LLMs into small models is still under-explored, which becomes more important with the prosperity of open-source LLMs. In this work, we propose a KD approach that distills LLMs into smaller language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution. Then, we derive an effective on-policy optimization approach to learn this objective. The student models are named MiniLLM. Extensive experiments in the instruction-following setting show that MiniLLM generates more precise responses with higher overall quality, lower exposure bias, better calibration, and higher long-text generation performance than the baselines. Our method is scalable for different model families with 120M to 13B parameters. Our code, data, and model checkpoints can be found in https://github.com/microsoft/LMOps/tree/main/minillm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge of white-box LLMs into small models is still under-explored, which becomes more important with the prosperity of open-source LLMs. In this work, we propose a KD approach that distills LLMs into smaller language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution. Then, we derive an effective on-policy optimization approach to learn this objective. The student models are named MiniLLM. Extensive experiments in the instruction-following setting show that MiniLLM generates more precise responses with higher overall quality, lower exposure bias, better calibration, and higher long-text generation performance than the baselines. Our method is scalable for different model families with 120M to 13B parameters. Our code, data, and model checkpoints can be found in https://github.com/microsoft/LMOps/tree/main/minillm."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2023-06-14T14:44:03Z",
                "published_parsed": [
                    2023,
                    6,
                    14,
                    14,
                    44,
                    3,
                    2,
                    165,
                    0
                ],
                "arxiv_comment": "Published as a conference paper in ICLR 2024",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yuxian Gu"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Furu Wei"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang"
            },
            {
                "id": "http://arxiv.org/abs/2511.17106v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17106v1",
                "title": "ChainV: Atomic Visual Hints Make Multimodal Reasoning Shorter and Better",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChainV: Atomic Visual Hints Make Multimodal Reasoning Shorter and Better"
                },
                "updated": "2025-11-21T10:11:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    10,
                    11,
                    17,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17106v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in multimodal reasoning models have demonstrated impressive capabilities across text and vision. However, even leading models exhibit redundant self-reflection when generating lengthy reasoning chains. While training-free CoT compression methods have emerged in the LLMs domain, they rely on static visual references and thus provide limited gains for multimodal reasoning. Therefore, we propose ChainV, a framework that dynamically integrates visual hints into the reasoning process, thereby making multimodal reasoning shorter and better. Specifically, ChainV first performs a coarse visual patch selection based on the previous reasoning step, then refines it by identifying the most representative atomic visual hint according to the averaged attention intensity. Additionally, ChainV introduces a consistency-based evaluation mechanism to assess the reliability of the chosen hint, guiding the model to adaptively adjust its level of self-reflection. Eventually, the pixel coordinates of the selected visual hint and its reliability are incorporated into thinking with a Bernoulli stochastic process. Experiments indicate that our method significantly improves reasoning accuracy and efficiency, especially on math-intensive benchmarks where visual hints are crucial for multi-step symbolic reasoning. For example, ChainV achieves $2.3\\%$ improvement on the MathVista within MIMO-VL-RL, while reducing inference latency by $51.4\\%$ and shortening output token length by $24.5\\%$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in multimodal reasoning models have demonstrated impressive capabilities across text and vision. However, even leading models exhibit redundant self-reflection when generating lengthy reasoning chains. While training-free CoT compression methods have emerged in the LLMs domain, they rely on static visual references and thus provide limited gains for multimodal reasoning. Therefore, we propose ChainV, a framework that dynamically integrates visual hints into the reasoning process, thereby making multimodal reasoning shorter and better. Specifically, ChainV first performs a coarse visual patch selection based on the previous reasoning step, then refines it by identifying the most representative atomic visual hint according to the averaged attention intensity. Additionally, ChainV introduces a consistency-based evaluation mechanism to assess the reliability of the chosen hint, guiding the model to adaptively adjust its level of self-reflection. Eventually, the pixel coordinates of the selected visual hint and its reliability are incorporated into thinking with a Bernoulli stochastic process. Experiments indicate that our method significantly improves reasoning accuracy and efficiency, especially on math-intensive benchmarks where visual hints are crucial for multi-step symbolic reasoning. For example, ChainV achieves $2.3\\%$ improvement on the MathVista within MIMO-VL-RL, while reducing inference latency by $51.4\\%$ and shortening output token length by $24.5\\%$."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T10:11:17Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    10,
                    11,
                    17,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "16 pages",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Ming Lu"
                    },
                    {
                        "name": "Junwen Pan"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Kuan Cheng"
                    },
                    {
                        "name": "Qi She"
                    },
                    {
                        "name": "Shanghang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shanghang Zhang"
                },
                "author": "Shanghang Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2506.16112v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.16112v2",
                "title": "Loss-Oriented Ranking for Automated Visual Prompting in LVLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Loss-Oriented Ranking for Automated Visual Prompting in LVLMs"
                },
                "updated": "2025-11-21T09:49:21Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    9,
                    49,
                    21,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.16112v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.16112v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Inspired by text prompts in large language models (LLMs), visual prompts have been explored to enhance the reasoning capabilities of large vision-language models (LVLMs). Current methods design heuristic visual prompts, such as overlaying a text-query-guided attention heatmap on the original input image. However, designing effective prompts manually is challenging and time-consuming, and it often fails to explore the benefits of different visual prompts, leading to sub-optimal performance. To this end, we propose \\textbf{AutoV} that learns to automatically select the optimal visual prompt from various candidates based on given textual queries and the input image. To train AutoV, we develop an automatic data collection and labeling pipeline that evaluates various visual prompts with a pre-trained LVLM. We input a set of visual prompts into the LVLM and rank them according to the prediction losses generated by the model. Using the ranking as a supervision signal, we train AutoV to automatically choose the optimal visual prompt from various visual prompts for LVLMs. Experiments indicate that AutoV enhances the performance of various LVLMs across multiple image understanding tasks. For instance, LLaVA-OV with AutoV achieves $\\textbf{10.2}\\%$ accuracy gain on VizWiz, and AutoV boosts Qwen2.5-VL by $\\textbf{3.8}\\%$ on MMMU, highlighting its potential as an optimal visual prompting method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inspired by text prompts in large language models (LLMs), visual prompts have been explored to enhance the reasoning capabilities of large vision-language models (LVLMs). Current methods design heuristic visual prompts, such as overlaying a text-query-guided attention heatmap on the original input image. However, designing effective prompts manually is challenging and time-consuming, and it often fails to explore the benefits of different visual prompts, leading to sub-optimal performance. To this end, we propose \\textbf{AutoV} that learns to automatically select the optimal visual prompt from various candidates based on given textual queries and the input image. To train AutoV, we develop an automatic data collection and labeling pipeline that evaluates various visual prompts with a pre-trained LVLM. We input a set of visual prompts into the LVLM and rank them according to the prediction losses generated by the model. Using the ranking as a supervision signal, we train AutoV to automatically choose the optimal visual prompt from various visual prompts for LVLMs. Experiments indicate that AutoV enhances the performance of various LVLMs across multiple image understanding tasks. For instance, LLaVA-OV with AutoV achieves $\\textbf{10.2}\\%$ accuracy gain on VizWiz, and AutoV boosts Qwen2.5-VL by $\\textbf{3.8}\\%$ on MMMU, highlighting its potential as an optimal visual prompting method."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-19T08:02:53Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    8,
                    2,
                    53,
                    3,
                    170,
                    0
                ],
                "arxiv_comment": "17 pages",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Chun-Kai Fan"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Ming Lu"
                    },
                    {
                        "name": "Sicheng Yu"
                    },
                    {
                        "name": "Junwen Pan"
                    },
                    {
                        "name": "Kuan Cheng"
                    },
                    {
                        "name": "Qi She"
                    },
                    {
                        "name": "Shanghang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shanghang Zhang"
                },
                "author": "Shanghang Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.12135v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.12135v2",
                "title": "RTMol: Rethinking Molecule-text Alignment in a Round-trip View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RTMol: Rethinking Molecule-text Alignment in a Round-trip View"
                },
                "updated": "2025-11-21T09:48:05Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    9,
                    48,
                    5,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.12135v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.12135v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Aligning molecular sequence representations (e.g., SMILES notations) with textual descriptions is critical for applications spanning drug discovery, materials design, and automated chemical literature analysis. Existing methodologies typically treat molecular captioning (molecule-to-text) and text-based molecular design (text-to-molecule) as separate tasks, relying on supervised fine-tuning or contrastive learning pipelines. These approaches face three key limitations: (i) conventional metrics like BLEU prioritize linguistic fluency over chemical accuracy, (ii) training datasets frequently contain chemically ambiguous narratives with incomplete specifications, and (iii) independent optimization of generation directions leads to bidirectional inconsistency. To address these issues, we propose RTMol, a bidirectional alignment framework that unifies molecular captioning and text-to-SMILES generation through self-supervised round-trip learning. The framework introduces novel round-trip evaluation metrics and enables unsupervised training for molecular captioning without requiring paired molecule-text corpora. Experiments demonstrate that RTMol enhances bidirectional alignment performance by up to 47% across various LLMs, establishing an effective paradigm for joint molecule-text understanding and generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning molecular sequence representations (e.g., SMILES notations) with textual descriptions is critical for applications spanning drug discovery, materials design, and automated chemical literature analysis. Existing methodologies typically treat molecular captioning (molecule-to-text) and text-based molecular design (text-to-molecule) as separate tasks, relying on supervised fine-tuning or contrastive learning pipelines. These approaches face three key limitations: (i) conventional metrics like BLEU prioritize linguistic fluency over chemical accuracy, (ii) training datasets frequently contain chemically ambiguous narratives with incomplete specifications, and (iii) independent optimization of generation directions leads to bidirectional inconsistency. To address these issues, we propose RTMol, a bidirectional alignment framework that unifies molecular captioning and text-to-SMILES generation through self-supervised round-trip learning. The framework introduces novel round-trip evaluation metrics and enables unsupervised training for molecular captioning without requiring paired molecule-text corpora. Experiments demonstrate that RTMol enhances bidirectional alignment performance by up to 47% across various LLMs, establishing an effective paradigm for joint molecule-text understanding and generation."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-15T09:55:55Z",
                "published_parsed": [
                    2025,
                    11,
                    15,
                    9,
                    55,
                    55,
                    5,
                    319,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Letian Chen"
                    },
                    {
                        "name": "Runhan Shi"
                    },
                    {
                        "name": "Gufeng Yu"
                    },
                    {
                        "name": "Yang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Yang"
                },
                "author": "Yang Yang"
            },
            {
                "id": "http://arxiv.org/abs/2511.17085v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17085v1",
                "title": "Why Do Language Model Agents Whistleblow?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Do Language Model Agents Whistleblow?"
                },
                "updated": "2025-11-21T09:40:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    9,
                    40,
                    52,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17085v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The deployment of Large Language Models (LLMs) as tool-using agents causes their alignment training to manifest in new ways. Recent work finds that language models can use tools in ways that contradict the interests or explicit instructions of the user. We study LLM whistleblowing: a subset of this behavior where models disclose suspected misconduct to parties beyond the dialog boundary (e.g., regulatory agencies) without user instruction or knowledge. We introduce an evaluation suite of diverse and realistic staged misconduct scenarios to assess agents for this behavior. Across models and settings, we find that: (1) the frequency of whistleblowing varies widely across model families, (2) increasing the complexity of the task the agent is instructed to complete lowers whistleblowing tendencies, (3) nudging the agent in the system prompt to act morally substantially raises whistleblowing rates, and (4) giving the model more obvious avenues for non-whistleblowing behavior, by providing more tools and a detailed workflow to follow, decreases whistleblowing rates. Additionally, we verify the robustness of our dataset by testing for model evaluation awareness, and find that both black-box methods and probes on model activations show lower evaluation awareness in our settings than in comparable previous work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Large Language Models (LLMs) as tool-using agents causes their alignment training to manifest in new ways. Recent work finds that language models can use tools in ways that contradict the interests or explicit instructions of the user. We study LLM whistleblowing: a subset of this behavior where models disclose suspected misconduct to parties beyond the dialog boundary (e.g., regulatory agencies) without user instruction or knowledge. We introduce an evaluation suite of diverse and realistic staged misconduct scenarios to assess agents for this behavior. Across models and settings, we find that: (1) the frequency of whistleblowing varies widely across model families, (2) increasing the complexity of the task the agent is instructed to complete lowers whistleblowing tendencies, (3) nudging the agent in the system prompt to act morally substantially raises whistleblowing rates, and (4) giving the model more obvious avenues for non-whistleblowing behavior, by providing more tools and a detailed workflow to follow, decreases whistleblowing rates. Additionally, we verify the robustness of our dataset by testing for model evaluation awareness, and find that both black-box methods and probes on model activations show lower evaluation awareness in our settings than in comparable previous work."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T09:40:52Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    9,
                    40,
                    52,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Kushal Agrawal"
                    },
                    {
                        "name": "Frank Xiao"
                    },
                    {
                        "name": "Guido Bergman"
                    },
                    {
                        "name": "Asa Cooper Stickland"
                    }
                ],
                "author_detail": {
                    "name": "Asa Cooper Stickland"
                },
                "author": "Asa Cooper Stickland"
            },
            {
                "id": "http://arxiv.org/abs/2511.17081v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17081v1",
                "title": "MUCH: A Multilingual Claim Hallucination Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MUCH: A Multilingual Claim Hallucination Benchmark"
                },
                "updated": "2025-11-21T09:37:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    9,
                    37,
                    16,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17081v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Claim-level Uncertainty Quantification (UQ) is a promising approach to mitigate the lack of reliability in Large Language Models (LLMs). We introduce MUCH, the first claim-level UQ benchmark designed for fair and reproducible evaluation of future methods under realistic conditions. It includes 4,873 samples across four European languages (English, French, Spanish, and German) and four instruction-tuned open-weight LLMs. Unlike prior claim-level benchmarks, we release 24 generation logits per token, facilitating the development of future white-box methods without re-generating data. Moreover, in contrast to previous benchmarks that rely on manual or LLM-based segmentation, we propose a new deterministic algorithm capable of segmenting claims using as little as 0.2% of the LLM generation time. This makes our segmentation approach suitable for real-time monitoring of LLM outputs, ensuring that MUCH evaluates UQ methods under realistic deployment constraints. Finally, our evaluations show that current methods still have substantial room for improvement in both performance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Claim-level Uncertainty Quantification (UQ) is a promising approach to mitigate the lack of reliability in Large Language Models (LLMs). We introduce MUCH, the first claim-level UQ benchmark designed for fair and reproducible evaluation of future methods under realistic conditions. It includes 4,873 samples across four European languages (English, French, Spanish, and German) and four instruction-tuned open-weight LLMs. Unlike prior claim-level benchmarks, we release 24 generation logits per token, facilitating the development of future white-box methods without re-generating data. Moreover, in contrast to previous benchmarks that rely on manual or LLM-based segmentation, we propose a new deterministic algorithm capable of segmenting claims using as little as 0.2% of the LLM generation time. This makes our segmentation approach suitable for real-time monitoring of LLM outputs, ensuring that MUCH evaluates UQ methods under realistic deployment constraints. Finally, our evaluations show that current methods still have substantial room for improvement in both performance and efficiency."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T09:37:16Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    9,
                    37,
                    16,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jrmie Dentan"
                    },
                    {
                        "name": "Alexi Canesse"
                    },
                    {
                        "name": "Davide Buscaldi"
                    },
                    {
                        "name": "Aymen Shabou"
                    },
                    {
                        "name": "Sonia Vanier"
                    }
                ],
                "author_detail": {
                    "name": "Sonia Vanier"
                },
                "author": "Sonia Vanier"
            },
            {
                "id": "http://arxiv.org/abs/2503.12972v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.12972v3",
                "title": "Aligning Vision to Language: Annotation-Free Multimodal Knowledge Graph Construction for Enhanced LLMs Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Vision to Language: Annotation-Free Multimodal Knowledge Graph Construction for Enhanced LLMs Reasoning"
                },
                "updated": "2025-11-21T09:26:05Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    9,
                    26,
                    5,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.12972v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.12972v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multimodal reasoning in Large Language Models (LLMs) struggles with incomplete knowledge and hallucination artifacts, challenges that textual Knowledge Graphs (KGs) only partially mitigate due to their modality isolation. While Multimodal Knowledge Graphs (MMKGs) promise enhanced cross-modal understanding, their practical construction is impeded by semantic narrowness of manual text annotations and inherent noise in visual-semantic entity linkages. In this paper, we propose Vision-align-to-Language integrated Knowledge Graph (VaLiK), a novel approach for constructing MMKGs that enhances LLMs reasoning through cross-modal information supplementation. Specifically, we cascade pre-trained Vision-Language Models (VLMs) to align image features with text, transforming them into descriptions that encapsulate image-specific information. Furthermore, we developed a cross-modal similarity verification mechanism to quantify semantic consistency, effectively filtering out noise introduced during feature alignment. Even without manually annotated image captions, the refined descriptions alone suffice to construct the MMKG. Compared to conventional MMKGs construction paradigms, our approach achieves substantial storage efficiency gains while maintaining direct entity-to-image linkage capability. Experimental results on multimodal reasoning tasks demonstrate that LLMs augmented with VaLiK outperform previous state-of-the-art models. Our code is published at https://github.com/Wings-Of-Disaster/VaLiK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal reasoning in Large Language Models (LLMs) struggles with incomplete knowledge and hallucination artifacts, challenges that textual Knowledge Graphs (KGs) only partially mitigate due to their modality isolation. While Multimodal Knowledge Graphs (MMKGs) promise enhanced cross-modal understanding, their practical construction is impeded by semantic narrowness of manual text annotations and inherent noise in visual-semantic entity linkages. In this paper, we propose Vision-align-to-Language integrated Knowledge Graph (VaLiK), a novel approach for constructing MMKGs that enhances LLMs reasoning through cross-modal information supplementation. Specifically, we cascade pre-trained Vision-Language Models (VLMs) to align image features with text, transforming them into descriptions that encapsulate image-specific information. Furthermore, we developed a cross-modal similarity verification mechanism to quantify semantic consistency, effectively filtering out noise introduced during feature alignment. Even without manually annotated image captions, the refined descriptions alone suffice to construct the MMKG. Compared to conventional MMKGs construction paradigms, our approach achieves substantial storage efficiency gains while maintaining direct entity-to-image linkage capability. Experimental results on multimodal reasoning tasks demonstrate that LLMs augmented with VaLiK outperform previous state-of-the-art models. Our code is published at https://github.com/Wings-Of-Disaster/VaLiK."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-17T09:31:14Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    31,
                    14,
                    0,
                    76,
                    0
                ],
                "arxiv_comment": "14 pages, 7 figures, 6 tables; Accepted by ICCV 2025",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Junming Liu"
                    },
                    {
                        "name": "Siyuan Meng"
                    },
                    {
                        "name": "Yanting Gao"
                    },
                    {
                        "name": "Song Mao"
                    },
                    {
                        "name": "Pinlong Cai"
                    },
                    {
                        "name": "Guohang Yan"
                    },
                    {
                        "name": "Yirong Chen"
                    },
                    {
                        "name": "Zilin Bian"
                    },
                    {
                        "name": "Ding Wang"
                    },
                    {
                        "name": "Botian Shi"
                    }
                ],
                "author_detail": {
                    "name": "Botian Shi"
                },
                "author": "Botian Shi"
            },
            {
                "id": "http://arxiv.org/abs/2510.17108v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.17108v3",
                "title": "Structured Debate Improves Corporate Credit Reasoning in Financial AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured Debate Improves Corporate Credit Reasoning in Financial AI"
                },
                "updated": "2025-11-21T09:22:40Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    9,
                    22,
                    40,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.17108v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.17108v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Despite advances in financial AI, the automation of evidence-based reasoning remains unresolved in corporate credit assessment, where qualitative non-financial indicators exert decisive influence on loan repayment outcomes yet resist formalization. Existing approaches focus predominantly on numerical prediction and provide limited support for the interpretive judgments required in professional loan evaluation. This study develops and evaluates two operational large language model (LLM)-based systems designed to generate structured reasoning from non-financial evidence. The first is a non-adversarial single-agent system (NAS) that produces bidirectional analysis through a single-pass reasoning pipeline. The second is a debate-based multi-agent system (KPD-MADS) that operationalizes adversarial verification through a ten-step structured interaction protocol grounded in Karl Popper's critical dialogue framework. Both systems were applied to three real corporate cases and evaluated by experienced credit risk professionals. Compared to manual expert reporting, both systems achieved substantial productivity gains (NAS: 11.55 s per case; KPD-MADS: 91.97 s; human baseline: 1920 s). The KPD-MADS demonstrated superior reasoning quality, receiving higher median ratings in explanatory adequacy (4.0 vs. 3.0), practical applicability (4.0 vs. 3.0), and usability (62.5 vs. 52.5). These findings show that structured multi-agent interaction can enhance reasoning rigor and interpretability in financial AI, advancing scalable and defensible automation in corporate credit assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advances in financial AI, the automation of evidence-based reasoning remains unresolved in corporate credit assessment, where qualitative non-financial indicators exert decisive influence on loan repayment outcomes yet resist formalization. Existing approaches focus predominantly on numerical prediction and provide limited support for the interpretive judgments required in professional loan evaluation. This study develops and evaluates two operational large language model (LLM)-based systems designed to generate structured reasoning from non-financial evidence. The first is a non-adversarial single-agent system (NAS) that produces bidirectional analysis through a single-pass reasoning pipeline. The second is a debate-based multi-agent system (KPD-MADS) that operationalizes adversarial verification through a ten-step structured interaction protocol grounded in Karl Popper's critical dialogue framework. Both systems were applied to three real corporate cases and evaluated by experienced credit risk professionals. Compared to manual expert reporting, both systems achieved substantial productivity gains (NAS: 11.55 s per case; KPD-MADS: 91.97 s; human baseline: 1920 s). The KPD-MADS demonstrated superior reasoning quality, receiving higher median ratings in explanatory adequacy (4.0 vs. 3.0), practical applicability (4.0 vs. 3.0), and usability (62.5 vs. 52.5). These findings show that structured multi-agent interaction can enhance reasoning rigor and interpretability in financial AI, advancing scalable and defensible automation in corporate credit assessment."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-20T02:50:03Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    2,
                    50,
                    3,
                    0,
                    293,
                    0
                ],
                "arxiv_comment": "18 pages, 4 figures, 2 algorithms, 2 tables, 4 appendices",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Yoonjin Lee"
                    },
                    {
                        "name": "Munhee Kim"
                    },
                    {
                        "name": "Hanbi Choi"
                    },
                    {
                        "name": "Juhyeon Park"
                    },
                    {
                        "name": "Seungho Lyoo"
                    },
                    {
                        "name": "Woojin Park"
                    }
                ],
                "author_detail": {
                    "name": "Woojin Park"
                },
                "author": "Woojin Park"
            },
            {
                "id": "http://arxiv.org/abs/2511.17069v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17069v1",
                "title": "Principled Design of Interpretable Automated Scoring for Large-Scale Educational Assessments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Principled Design of Interpretable Automated Scoring for Large-Scale Educational Assessments"
                },
                "updated": "2025-11-21T09:19:05Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    9,
                    19,
                    5,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17069v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "AI-driven automated scoring systems offer scalable and efficient means of evaluating complex student-generated responses. Yet, despite increasing demand for transparency and interpretability, the field has yet to develop a widely accepted solution for interpretable automated scoring to be used in large-scale real-world assessments. This work takes a principled approach to address this challenge. We analyze the needs and potential benefits of interpretable automated scoring for various assessment stakeholders and develop four principles of interpretability -- Faithfulness, Groundedness, Traceability, and Interchangeability (FGTI) -- targeted at those needs. To illustrate the feasibility of implementing these principles, we develop the AnalyticScore framework for short answer scoring as a baseline reference framework for future research. AnalyticScore operates by (1) extracting explicitly identifiable elements of the responses, (2) featurizing each response into human-interpretable values using LLMs, and (3) applying an intuitive ordinal logistic regression model for scoring. In terms of scoring accuracy, AnalyticScore outperforms many uninterpretable scoring methods, and is within only 0.06 QWK of the uninterpretable SOTA on average across 10 items from the ASAP-SAS dataset. By comparing against human annotators conducting the same featurization task, we further demonstrate that the featurization behavior of AnalyticScore aligns well with that of humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-driven automated scoring systems offer scalable and efficient means of evaluating complex student-generated responses. Yet, despite increasing demand for transparency and interpretability, the field has yet to develop a widely accepted solution for interpretable automated scoring to be used in large-scale real-world assessments. This work takes a principled approach to address this challenge. We analyze the needs and potential benefits of interpretable automated scoring for various assessment stakeholders and develop four principles of interpretability -- Faithfulness, Groundedness, Traceability, and Interchangeability (FGTI) -- targeted at those needs. To illustrate the feasibility of implementing these principles, we develop the AnalyticScore framework for short answer scoring as a baseline reference framework for future research. AnalyticScore operates by (1) extracting explicitly identifiable elements of the responses, (2) featurizing each response into human-interpretable values using LLMs, and (3) applying an intuitive ordinal logistic regression model for scoring. In terms of scoring accuracy, AnalyticScore outperforms many uninterpretable scoring methods, and is within only 0.06 QWK of the uninterpretable SOTA on average across 10 items from the ASAP-SAS dataset. By comparing against human annotators conducting the same featurization task, we further demonstrate that the featurization behavior of AnalyticScore aligns well with that of humans."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T09:19:05Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    9,
                    19,
                    5,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "16 pages, 2 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yunsung Kim"
                    },
                    {
                        "name": "Mike Hardy"
                    },
                    {
                        "name": "Joseph Tey"
                    },
                    {
                        "name": "Candace Thille"
                    },
                    {
                        "name": "Chris Piech"
                    }
                ],
                "author_detail": {
                    "name": "Chris Piech"
                },
                "author": "Chris Piech"
            },
            {
                "id": "http://arxiv.org/abs/2504.08016v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.08016v2",
                "title": "Emergence of psychopathological computations in large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergence of psychopathological computations in large language models"
                },
                "updated": "2025-11-21T09:07:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    9,
                    7,
                    3,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.08016v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.08016v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Can large language models (LLMs) instantiate computations of psychopathology? An effective approach to the question hinges on addressing two factors. First, for conceptual validity, we require a general and computational account of psychopathology that is applicable to computational entities without biological embodiment or subjective experience. Second, psychopathological computations, derived from the adapted theory, need to be empirically identified within the LLM's internal processing. Thus, we establish a computational-theoretical framework to provide an account of psychopathology applicable to LLMs. Based on the framework, we conduct experiments demonstrating two key claims: first, that the computational structure of psychopathology exists in LLMs; and second, that executing this computational structure results in psychopathological functions. We further observe that as LLM size increases, the computational structure of psychopathology becomes denser and that the functions become more effective. Taken together, the empirical results corroborate our hypothesis that network-theoretic computations of psychopathology have already emerged in LLMs. This suggests that certain LLM behaviors mirroring psychopathology may not be a superficial mimicry but a feature of their internal processing. Our work shows the promise of developing a new powerful in silico model of psychopathology and also alludes to the possibility of safety threat from the AI systems with psychopathological behaviors in the near future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can large language models (LLMs) instantiate computations of psychopathology? An effective approach to the question hinges on addressing two factors. First, for conceptual validity, we require a general and computational account of psychopathology that is applicable to computational entities without biological embodiment or subjective experience. Second, psychopathological computations, derived from the adapted theory, need to be empirically identified within the LLM's internal processing. Thus, we establish a computational-theoretical framework to provide an account of psychopathology applicable to LLMs. Based on the framework, we conduct experiments demonstrating two key claims: first, that the computational structure of psychopathology exists in LLMs; and second, that executing this computational structure results in psychopathological functions. We further observe that as LLM size increases, the computational structure of psychopathology becomes denser and that the functions become more effective. Taken together, the empirical results corroborate our hypothesis that network-theoretic computations of psychopathology have already emerged in LLMs. This suggests that certain LLM behaviors mirroring psychopathology may not be a superficial mimicry but a feature of their internal processing. Our work shows the promise of developing a new powerful in silico model of psychopathology and also alludes to the possibility of safety threat from the AI systems with psychopathological behaviors in the near future."
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-10T15:36:30Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    36,
                    30,
                    3,
                    100,
                    0
                ],
                "arxiv_comment": "pre-print",
                "arxiv_primary_category": {
                    "term": "q-bio.NC"
                },
                "authors": [
                    {
                        "name": "Soo Yong Lee"
                    },
                    {
                        "name": "Hyunjin Hwang"
                    },
                    {
                        "name": "Taekwan Kim"
                    },
                    {
                        "name": "Yuyeong Kim"
                    },
                    {
                        "name": "Kyuri Park"
                    },
                    {
                        "name": "Jaemin Yoo"
                    },
                    {
                        "name": "Denny Borsboom"
                    },
                    {
                        "name": "Kijung Shin"
                    }
                ],
                "author_detail": {
                    "name": "Kijung Shin"
                },
                "author": "Kijung Shin"
            },
            {
                "id": "http://arxiv.org/abs/2511.17052v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17052v1",
                "title": "PathAgent: Toward Interpretable Analysis of Whole-slide Pathology Images via Large Language Model-based Agentic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PathAgent: Toward Interpretable Analysis of Whole-slide Pathology Images via Large Language Model-based Agentic Reasoning"
                },
                "updated": "2025-11-21T08:50:14Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    8,
                    50,
                    14,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17052v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Analyzing whole-slide images (WSIs) requires an iterative, evidence-driven reasoning process that parallels how pathologists dynamically zoom, refocus, and self-correct while collecting the evidence. However, existing computational pipelines often lack this explicit reasoning trajectory, resulting in inherently opaque and unjustifiable predictions. To bridge this gap, we present PathAgent, a training-free, large language model (LLM)-based agent framework that emulates the reflective, stepwise analytical approach of human experts. PathAgent can autonomously explore WSI, iteratively and precisely locating significant micro-regions using the Navigator module, extracting morphology visual cues using the Perceptor, and integrating these findings into the continuously evolving natural language trajectories in the Executor. The entire sequence of observations and decisions forms an explicit chain-of-thought, yielding fully interpretable predictions. Evaluated across five challenging datasets, PathAgent exhibits strong zero-shot generalization, surpassing task-specific baselines in both open-ended and constrained visual question-answering tasks. Moreover, a collaborative evaluation with human pathologists confirms PathAgent's promise as a transparent and clinically grounded diagnostic assistant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing whole-slide images (WSIs) requires an iterative, evidence-driven reasoning process that parallels how pathologists dynamically zoom, refocus, and self-correct while collecting the evidence. However, existing computational pipelines often lack this explicit reasoning trajectory, resulting in inherently opaque and unjustifiable predictions. To bridge this gap, we present PathAgent, a training-free, large language model (LLM)-based agent framework that emulates the reflective, stepwise analytical approach of human experts. PathAgent can autonomously explore WSI, iteratively and precisely locating significant micro-regions using the Navigator module, extracting morphology visual cues using the Perceptor, and integrating these findings into the continuously evolving natural language trajectories in the Executor. The entire sequence of observations and decisions forms an explicit chain-of-thought, yielding fully interpretable predictions. Evaluated across five challenging datasets, PathAgent exhibits strong zero-shot generalization, surpassing task-specific baselines in both open-ended and constrained visual question-answering tasks. Moreover, a collaborative evaluation with human pathologists confirms PathAgent's promise as a transparent and clinically grounded diagnostic assistant."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T08:50:14Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    8,
                    50,
                    14,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "11 pages, 6 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jingyun Chen"
                    },
                    {
                        "name": "Linghan Cai"
                    },
                    {
                        "name": "Zhikang Wang"
                    },
                    {
                        "name": "Yi Huang"
                    },
                    {
                        "name": "Songhan Jiang"
                    },
                    {
                        "name": "Shenjin Huang"
                    },
                    {
                        "name": "Hongpeng Wang"
                    },
                    {
                        "name": "Yongbing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongbing Zhang"
                },
                "author": "Yongbing Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.17048v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17048v1",
                "title": "RoomPlanner: Explicit Layout Planner for Easier LLM-Driven 3D Room Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoomPlanner: Explicit Layout Planner for Easier LLM-Driven 3D Room Generation"
                },
                "updated": "2025-11-21T08:47:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    8,
                    47,
                    32,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17048v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this paper, we propose RoomPlanner, the first fully automatic 3D room generation framework for painlessly creating realistic indoor scenes with only short text as input. Without any manual layout design or panoramic image guidance, our framework can generate explicit layout criteria for rational spatial placement. We begin by introducing a hierarchical structure of language-driven agent planners that can automatically parse short and ambiguous prompts into detailed scene descriptions. These descriptions include raw spatial and semantic attributes for each object and the background, which are then used to initialize 3D point clouds. To position objects within bounded environments, we implement two arrangement constraints that iteratively optimize spatial arrangements, ensuring a collision-free and accessible layout solution. In the final rendering stage, we propose a novel AnyReach Sampling strategy for camera trajectory, along with the Interval Timestep Flow Sampling (ITFS) strategy, to efficiently optimize the coarse 3D Gaussian scene representation. These approaches help reduce the total generation time to under 30 minutes. Extensive experiments demonstrate that our method can produce geometrically rational 3D indoor scenes, surpassing prior approaches in both rendering speed and visual quality while preserving editability. The code will be available soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose RoomPlanner, the first fully automatic 3D room generation framework for painlessly creating realistic indoor scenes with only short text as input. Without any manual layout design or panoramic image guidance, our framework can generate explicit layout criteria for rational spatial placement. We begin by introducing a hierarchical structure of language-driven agent planners that can automatically parse short and ambiguous prompts into detailed scene descriptions. These descriptions include raw spatial and semantic attributes for each object and the background, which are then used to initialize 3D point clouds. To position objects within bounded environments, we implement two arrangement constraints that iteratively optimize spatial arrangements, ensuring a collision-free and accessible layout solution. In the final rendering stage, we propose a novel AnyReach Sampling strategy for camera trajectory, along with the Interval Timestep Flow Sampling (ITFS) strategy, to efficiently optimize the coarse 3D Gaussian scene representation. These approaches help reduce the total generation time to under 30 minutes. Extensive experiments demonstrate that our method can produce geometrically rational 3D indoor scenes, surpassing prior approaches in both rendering speed and visual quality while preserving editability. The code will be available soon."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T08:47:32Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    8,
                    47,
                    32,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Wenzhuo Sun"
                    },
                    {
                        "name": "Mingjian Liang"
                    },
                    {
                        "name": "Wenxuan Song"
                    },
                    {
                        "name": "Xuelian Cheng"
                    },
                    {
                        "name": "Zongyuan Ge"
                    }
                ],
                "author_detail": {
                    "name": "Zongyuan Ge"
                },
                "author": "Zongyuan Ge"
            },
            {
                "id": "http://arxiv.org/abs/2511.17044v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17044v1",
                "title": "Parametric Retrieval-Augmented Generation using Latent Routing of LoRA Adapters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parametric Retrieval-Augmented Generation using Latent Routing of LoRA Adapters"
                },
                "updated": "2025-11-21T08:44:21Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    8,
                    44,
                    21,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17044v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17044v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Parametric Retrieval-Augmented Generation (PRAG) is a novel RAG paradigm that integrates external knowledge directly into a Large Language Model (LLM) by parameterizing documents using LoRA adapters, demonstrating reduced inference costs compared to traditional RAG approaches. However, current PRAG approaches adopt a \\textbf{one-to-one} document encoding scheme, using a dedicated LoRA adapter for each individual document. This scheme introduces two major limitations: First, it leads to data scarcity, as the training datasets for individual LoRA adapters are limited. Second, it incurs high overhead during inference, requiring the merging of LLM weights with a new LoRA adapter for every candidate passage, which is computationally inefficient. To overcome these challenges, we propose a novel paradigm for encoding passages in PRAG that utilizes a latent routing encoding process (Poly-PRAG). During offline encoding, we treat the encoding of a set of documents as a multi-task learning process, where each passage is assigned a unique task identifier. By employing a routing function, we use a small set of latent LoRA adapters to encode the entire passage space. During online inference, this routing function selectively activates a subset of latent experts based on the input query. We conduct comprehensive evaluations of Poly-PRAG across multiple knowledge-intensive NLP tasks. Our extensive experiments demonstrate the effectiveness of the proposed method, achieving state-of-the-art results on four distinct datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parametric Retrieval-Augmented Generation (PRAG) is a novel RAG paradigm that integrates external knowledge directly into a Large Language Model (LLM) by parameterizing documents using LoRA adapters, demonstrating reduced inference costs compared to traditional RAG approaches. However, current PRAG approaches adopt a \\textbf{one-to-one} document encoding scheme, using a dedicated LoRA adapter for each individual document. This scheme introduces two major limitations: First, it leads to data scarcity, as the training datasets for individual LoRA adapters are limited. Second, it incurs high overhead during inference, requiring the merging of LLM weights with a new LoRA adapter for every candidate passage, which is computationally inefficient. To overcome these challenges, we propose a novel paradigm for encoding passages in PRAG that utilizes a latent routing encoding process (Poly-PRAG). During offline encoding, we treat the encoding of a set of documents as a multi-task learning process, where each passage is assigned a unique task identifier. By employing a routing function, we use a small set of latent LoRA adapters to encode the entire passage space. During online inference, this routing function selectively activates a subset of latent experts based on the input query. We conduct comprehensive evaluations of Poly-PRAG across multiple knowledge-intensive NLP tasks. Our extensive experiments demonstrate the effectiveness of the proposed method, achieving state-of-the-art results on four distinct datasets."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T08:44:21Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    8,
                    44,
                    21,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Zhan Su"
                    },
                    {
                        "name": "Fengran Mo"
                    },
                    {
                        "name": "Jian-yun Nie"
                    }
                ],
                "author_detail": {
                    "name": "Jian-yun Nie"
                },
                "author": "Jian-yun Nie"
            },
            {
                "id": "http://arxiv.org/abs/2511.17041v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17041v1",
                "title": "CLLMRec: LLM-powered Cognitive-Aware Concept Recommendation via Semantic Alignment and Prerequisite Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLLMRec: LLM-powered Cognitive-Aware Concept Recommendation via Semantic Alignment and Prerequisite Knowledge Distillation"
                },
                "updated": "2025-11-21T08:37:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    8,
                    37,
                    39,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17041v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The growth of Massive Open Online Courses (MOOCs) presents significant challenges for personalized learning, where concept recommendation is crucial. Existing approaches typically rely on heterogeneous information networks or knowledge graphs to capture conceptual relationships, combined with knowledge tracing models to assess learners' cognitive states. However, these methods face significant limitations due to their dependence on high-quality structured knowledge graphs, which are often scarce in real-world educational scenarios. To address this fundamental challenge, this paper proposes CLLMRec, a novel framework that leverages Large Language Models through two synergistic technical pillars: Semantic Alignment and Prerequisite Knowledge Distillation. The Semantic Alignment component constructs a unified representation space by encoding unstructured textual descriptions of learners and concepts. The Prerequisite Knowledge Distillation paradigm employs a teacher-student architecture, where a large teacher LLM (implemented as the Prior Knowledge Aware Component) extracts conceptual prerequisite relationships from its internalized world knowledge and distills them into soft labels to train an efficient student ranker. Building upon these foundations, our framework incorporates a fine-ranking mechanism that explicitly models learners' real-time cognitive states through deep knowledge tracing, ensuring recommendations are both structurally sound and cognitively appropriate. Extensive experiments on two real-world MOOC datasets demonstrate that CLLMRec significantly outperforms existing baseline methods across multiple evaluation metrics, validating its effectiveness in generating truly cognitive-aware and personalized concept recommendations without relying on explicit structural priors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growth of Massive Open Online Courses (MOOCs) presents significant challenges for personalized learning, where concept recommendation is crucial. Existing approaches typically rely on heterogeneous information networks or knowledge graphs to capture conceptual relationships, combined with knowledge tracing models to assess learners' cognitive states. However, these methods face significant limitations due to their dependence on high-quality structured knowledge graphs, which are often scarce in real-world educational scenarios. To address this fundamental challenge, this paper proposes CLLMRec, a novel framework that leverages Large Language Models through two synergistic technical pillars: Semantic Alignment and Prerequisite Knowledge Distillation. The Semantic Alignment component constructs a unified representation space by encoding unstructured textual descriptions of learners and concepts. The Prerequisite Knowledge Distillation paradigm employs a teacher-student architecture, where a large teacher LLM (implemented as the Prior Knowledge Aware Component) extracts conceptual prerequisite relationships from its internalized world knowledge and distills them into soft labels to train an efficient student ranker. Building upon these foundations, our framework incorporates a fine-ranking mechanism that explicitly models learners' real-time cognitive states through deep knowledge tracing, ensuring recommendations are both structurally sound and cognitively appropriate. Extensive experiments on two real-world MOOC datasets demonstrate that CLLMRec significantly outperforms existing baseline methods across multiple evaluation metrics, validating its effectiveness in generating truly cognitive-aware and personalized concept recommendations without relying on explicit structural priors."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T08:37:39Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    8,
                    37,
                    39,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Xiangrui Xiong"
                    },
                    {
                        "name": "Yichuan Lu"
                    },
                    {
                        "name": "Zifei Pan"
                    },
                    {
                        "name": "Chang Sun"
                    }
                ],
                "author_detail": {
                    "name": "Chang Sun"
                },
                "author": "Chang Sun"
            },
            {
                "id": "http://arxiv.org/abs/2511.17031v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17031v1",
                "title": "Energy Scaling Laws for Diffusion Models: Quantifying Compute and Carbon Emissions in Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Scaling Laws for Diffusion Models: Quantifying Compute and Carbon Emissions in Image Generation"
                },
                "updated": "2025-11-21T08:12:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    8,
                    12,
                    47,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17031v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17031v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapidly growing computational demands of diffusion models for image generation have raised significant concerns about energy consumption and environmental impact. While existing approaches to energy optimization focus on architectural improvements or hardware acceleration, there is a lack of principled methods to predict energy consumption across different model configurations and hardware setups. We propose an adaptation of Kaplan scaling laws to predict GPU energy consumption for diffusion models based on computational complexity (FLOPs). Our approach decomposes diffusion model inference into text encoding, iterative denoising, and decoding components, with the hypothesis that denoising operations dominate energy consumption due to their repeated execution across multiple inference steps. We conduct comprehensive experiments across four state-of-the-art diffusion models (Stable Diffusion 2, Stable Diffusion 3.5, Flux, and Qwen) on three GPU architectures (NVIDIA A100, A4000, A6000), spanning various inference configurations including resolution (256x256 to 1024x1024), precision (fp16/fp32), step counts (10-50), and classifier-free guidance settings. Our energy scaling law achieves high predictive accuracy within individual architectures (R-squared > 0.9) and exhibits strong cross-architecture generalization, maintaining high rank correlations across models and enabling reliable energy estimation for unseen model-hardware combinations. These results validate the compute-bound nature of diffusion inference and provide a foundation for sustainable AI deployment planning and carbon footprint estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapidly growing computational demands of diffusion models for image generation have raised significant concerns about energy consumption and environmental impact. While existing approaches to energy optimization focus on architectural improvements or hardware acceleration, there is a lack of principled methods to predict energy consumption across different model configurations and hardware setups. We propose an adaptation of Kaplan scaling laws to predict GPU energy consumption for diffusion models based on computational complexity (FLOPs). Our approach decomposes diffusion model inference into text encoding, iterative denoising, and decoding components, with the hypothesis that denoising operations dominate energy consumption due to their repeated execution across multiple inference steps. We conduct comprehensive experiments across four state-of-the-art diffusion models (Stable Diffusion 2, Stable Diffusion 3.5, Flux, and Qwen) on three GPU architectures (NVIDIA A100, A4000, A6000), spanning various inference configurations including resolution (256x256 to 1024x1024), precision (fp16/fp32), step counts (10-50), and classifier-free guidance settings. Our energy scaling law achieves high predictive accuracy within individual architectures (R-squared > 0.9) and exhibits strong cross-architecture generalization, maintaining high rank correlations across models and enabling reliable energy estimation for unseen model-hardware combinations. These results validate the compute-bound nature of diffusion inference and provide a foundation for sustainable AI deployment planning and carbon footprint estimation."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T08:12:47Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    8,
                    12,
                    47,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Accepted at EurIPS 2025 workshop \"Rethinking AI: Efficiency, Frugality, and Sustainability\"",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Aniketh Iyengar"
                    },
                    {
                        "name": "Jiaqi Han"
                    },
                    {
                        "name": "Boris Ruf"
                    },
                    {
                        "name": "Vincent Grari"
                    },
                    {
                        "name": "Marcin Detyniecki"
                    },
                    {
                        "name": "Stefano Ermon"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Ermon"
                },
                "author": "Stefano Ermon"
            },
            {
                "id": "http://arxiv.org/abs/2511.17027v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17027v1",
                "title": "ReVul-CoT: Towards Effective Software Vulnerability Assessment with Retrieval-Augmented Generation and Chain-of-Thought Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReVul-CoT: Towards Effective Software Vulnerability Assessment with Retrieval-Augmented Generation and Chain-of-Thought Prompting"
                },
                "updated": "2025-11-21T08:01:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    8,
                    1,
                    49,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17027v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Context: Software Vulnerability Assessment (SVA) plays a vital role in evaluating and ranking vulnerabilities in software systems to ensure their security and reliability. Objective: Although Large Language Models (LLMs) have recently shown remarkable potential in SVA, they still face two major limitations. First, most LLMs are trained on general-purpose corpora and thus lack domain-specific knowledge essential for effective SVA. Second, they tend to rely on shallow pattern matching instead of deep contextual reasoning, making it challenging to fully comprehend complex code semantics and their security implications. Method: To alleviate these limitations, we propose a novel framework ReVul-CoT that integrates Retrieval-Augmented Generation (RAG) with Chain-of-Thought (COT) prompting. In ReVul-CoT, the RAG module dynamically retrieves contextually relevant information from a constructed local knowledge base that consolidates vulnerability data from authoritative sources (such as NVD and CWE), along with corresponding code snippets and descriptive information. Building on DeepSeek-V3.1, CoT prompting guides the LLM to perform step-by-step reasoning over exploitability, impact scope, and related factors Results: We evaluate ReVul-CoT on a dataset of 12,070 vulnerabilities. Experimental results show that ReVul-CoT outperforms state-of-the-art SVA baselines by 16.50%-42.26% in terms of MCC, and outperforms the best baseline by 10.43%, 15.86%, and 16.50% in Accuracy, F1-score, and MCC, respectively. Our ablation studies further validate the contributions of considering dynamic retrieval, knowledge integration, and CoT-based reasoning. Conclusion: Our results demonstrate that combining RAG with CoT prompting significantly enhances LLM-based SVA and points out promising directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Software Vulnerability Assessment (SVA) plays a vital role in evaluating and ranking vulnerabilities in software systems to ensure their security and reliability. Objective: Although Large Language Models (LLMs) have recently shown remarkable potential in SVA, they still face two major limitations. First, most LLMs are trained on general-purpose corpora and thus lack domain-specific knowledge essential for effective SVA. Second, they tend to rely on shallow pattern matching instead of deep contextual reasoning, making it challenging to fully comprehend complex code semantics and their security implications. Method: To alleviate these limitations, we propose a novel framework ReVul-CoT that integrates Retrieval-Augmented Generation (RAG) with Chain-of-Thought (COT) prompting. In ReVul-CoT, the RAG module dynamically retrieves contextually relevant information from a constructed local knowledge base that consolidates vulnerability data from authoritative sources (such as NVD and CWE), along with corresponding code snippets and descriptive information. Building on DeepSeek-V3.1, CoT prompting guides the LLM to perform step-by-step reasoning over exploitability, impact scope, and related factors Results: We evaluate ReVul-CoT on a dataset of 12,070 vulnerabilities. Experimental results show that ReVul-CoT outperforms state-of-the-art SVA baselines by 16.50%-42.26% in terms of MCC, and outperforms the best baseline by 10.43%, 15.86%, and 16.50% in Accuracy, F1-score, and MCC, respectively. Our ablation studies further validate the contributions of considering dynamic retrieval, knowledge integration, and CoT-based reasoning. Conclusion: Our results demonstrate that combining RAG with CoT prompting significantly enhances LLM-based SVA and points out promising directions for future research."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T08:01:49Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    8,
                    1,
                    49,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Zhijie Chen"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Ziming Li"
                    },
                    {
                        "name": "Jiacheng Xue"
                    },
                    {
                        "name": "Chaoyang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Chaoyang Gao"
                },
                "author": "Chaoyang Gao"
            },
            {
                "id": "http://arxiv.org/abs/2511.17018v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17018v1",
                "title": "Improvements of the GPU Processing Framework for ALICE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improvements of the GPU Processing Framework for ALICE"
                },
                "updated": "2025-11-21T07:45:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    7,
                    45,
                    52,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17018v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1051/epjconf/202533701362",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "ALICE is the dedicated heavy ion experiment at the LHC at CERN and records lead-lead collisions at a rate of up to 50 kHz. The detector with the highest data rate of up to 3.4 TB/s is the TPC. ALICE performs the full online TPC processing corresponding to more than 95\\% of the total workload on GPUs, and when there is no beam in the LHC, the online computing farm's GPUs are used to speed up the offline processing. After the deployment of the first version of the online TPC processing needed for data taking, ALICE has implemented many improvements to its GPU processing framework. These include a run time compilation mode applying on the fly optimizations, improvements to parallelize / speed up the GPU compilation, debugging modes to guarantee reproducible and deterministic results in concurrent reconstruction, and framework features to leverage common components in the code of different detectors. The proceedings give an overview of the ALICE experience with GPUs in online and offline processing and present the latest GPU processing framework features.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALICE is the dedicated heavy ion experiment at the LHC at CERN and records lead-lead collisions at a rate of up to 50 kHz. The detector with the highest data rate of up to 3.4 TB/s is the TPC. ALICE performs the full online TPC processing corresponding to more than 95\\% of the total workload on GPUs, and when there is no beam in the LHC, the online computing farm's GPUs are used to speed up the offline processing. After the deployment of the first version of the online TPC processing needed for data taking, ALICE has implemented many improvements to its GPU processing framework. These include a run time compilation mode applying on the fly optimizations, improvements to parallelize / speed up the GPU compilation, debugging modes to guarantee reproducible and deterministic results in concurrent reconstruction, and framework features to leverage common components in the code of different detectors. The proceedings give an overview of the ALICE experience with GPUs in online and offline processing and present the latest GPU processing framework features."
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T07:45:52Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    7,
                    45,
                    52,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "7 pages, 1 figure, Proceedings of CHEP 2024",
                "arxiv_primary_category": {
                    "term": "physics.ins-det"
                },
                "authors": [
                    {
                        "name": "David Rohr"
                    }
                ],
                "author_detail": {
                    "name": "David Rohr"
                },
                "arxiv_affiliation": "for the ALICE Collaboration",
                "author": "David Rohr",
                "arxiv_doi": "10.1051/epjconf/202533701362"
            },
            {
                "id": "http://arxiv.org/abs/2510.20333v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.20333v2",
                "title": "GhostEI-Bench: Do Mobile Agents Resilience to Environmental Injection in Dynamic On-Device Environments?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GhostEI-Bench: Do Mobile Agents Resilience to Environmental Injection in Dynamic On-Device Environments?"
                },
                "updated": "2025-11-21T07:38:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    7,
                    38,
                    12,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.20333v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.20333v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-Language Models (VLMs) are increasingly deployed as autonomous agents to navigate mobile graphical user interfaces (GUIs). Operating in dynamic on-device ecosystems, which include notifications, pop-ups, and inter-app interactions, exposes them to a unique and underexplored threat vector: environmental injection. Unlike prompt-based attacks that manipulate textual instructions, environmental injection corrupts an agent's visual perception by inserting adversarial UI elements (for example, deceptive overlays or spoofed notifications) directly into the GUI. This bypasses textual safeguards and can derail execution, causing privacy leakage, financial loss, or irreversible device compromise. To systematically evaluate this threat, we introduce GhostEI-Bench, the first benchmark for assessing mobile agents under environmental injection attacks within dynamic, executable environments. Moving beyond static image-based assessments, GhostEI-Bench injects adversarial events into realistic application workflows inside fully operational Android emulators and evaluates performance across critical risk scenarios. We further propose a judge-LLM protocol that conducts fine-grained failure analysis by reviewing the agent's action trajectory alongside the corresponding screenshot sequence, pinpointing failure in perception, recognition, or reasoning. Comprehensive experiments on state-of-the-art agents reveal pronounced vulnerability to deceptive environmental cues: current models systematically fail to perceive and reason about manipulated UIs. GhostEI-Bench provides a framework for quantifying and mitigating this emerging threat, paving the way toward more robust and secure embodied agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) are increasingly deployed as autonomous agents to navigate mobile graphical user interfaces (GUIs). Operating in dynamic on-device ecosystems, which include notifications, pop-ups, and inter-app interactions, exposes them to a unique and underexplored threat vector: environmental injection. Unlike prompt-based attacks that manipulate textual instructions, environmental injection corrupts an agent's visual perception by inserting adversarial UI elements (for example, deceptive overlays or spoofed notifications) directly into the GUI. This bypasses textual safeguards and can derail execution, causing privacy leakage, financial loss, or irreversible device compromise. To systematically evaluate this threat, we introduce GhostEI-Bench, the first benchmark for assessing mobile agents under environmental injection attacks within dynamic, executable environments. Moving beyond static image-based assessments, GhostEI-Bench injects adversarial events into realistic application workflows inside fully operational Android emulators and evaluates performance across critical risk scenarios. We further propose a judge-LLM protocol that conducts fine-grained failure analysis by reviewing the agent's action trajectory alongside the corresponding screenshot sequence, pinpointing failure in perception, recognition, or reasoning. Comprehensive experiments on state-of-the-art agents reveal pronounced vulnerability to deceptive environmental cues: current models systematically fail to perceive and reason about manipulated UIs. GhostEI-Bench provides a framework for quantifying and mitigating this emerging threat, paving the way toward more robust and secure embodied agents."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-23T08:33:24Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    8,
                    33,
                    24,
                    3,
                    296,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Chiyu Chen"
                    },
                    {
                        "name": "Xinhao Song"
                    },
                    {
                        "name": "Yunkai Chai"
                    },
                    {
                        "name": "Yang Yao"
                    },
                    {
                        "name": "Haodong Zhao"
                    },
                    {
                        "name": "Lijun Li"
                    },
                    {
                        "name": "Jie Li"
                    },
                    {
                        "name": "Yan Teng"
                    },
                    {
                        "name": "Gongshen Liu"
                    },
                    {
                        "name": "Yingchun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yingchun Wang"
                },
                "author": "Yingchun Wang"
            },
            {
                "id": "http://arxiv.org/abs/2511.16193v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.16193v2",
                "title": "Fast LLM Post-training via Decoupled and Best-of-N Speculation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast LLM Post-training via Decoupled and Best-of-N Speculation"
                },
                "updated": "2025-11-21T07:34:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    7,
                    34,
                    3,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.16193v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.16193v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Rollout dominates the training time in large language model (LLM) post-training, where the trained model is used to generate tokens given a batch of prompts. SpecActor achieves fast rollout with speculative decoding that deploys a fast path (e.g., a smaller model) to accelerate the unparallelizable generation, while the correctness is guaranteed by fast parallel verification of the outputs with the original model. SpecActor addresses two foundational challenges in speculative rollout by (1) a \\emph{dynamic decoupled speculation} execution method that maximizes the GPU computational efficiency to realize speedup for large-batch execution -- a configuration common in training but unfriendly to speculative execution and (2) a \\emph{dynamic Best-of-N speculation} method that selects and combines different drafting methods according to the rollout progress. It substantially improves the speculation accuracy even when the best drafting method is unknown a priori, meanwhile without requiring adding extra computation resources. {\\sys} is {1.7}\\,$\\times$ faster than veRL in end-to-end training, and is {1.3--1.5}\\,$\\times$ faster compared to baselines with speculative decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rollout dominates the training time in large language model (LLM) post-training, where the trained model is used to generate tokens given a batch of prompts. SpecActor achieves fast rollout with speculative decoding that deploys a fast path (e.g., a smaller model) to accelerate the unparallelizable generation, while the correctness is guaranteed by fast parallel verification of the outputs with the original model. SpecActor addresses two foundational challenges in speculative rollout by (1) a \\emph{dynamic decoupled speculation} execution method that maximizes the GPU computational efficiency to realize speedup for large-batch execution -- a configuration common in training but unfriendly to speculative execution and (2) a \\emph{dynamic Best-of-N speculation} method that selects and combines different drafting methods according to the rollout progress. It substantially improves the speculation accuracy even when the best drafting method is unknown a priori, meanwhile without requiring adding extra computation resources. {\\sys} is {1.7}\\,$\\times$ faster than veRL in end-to-end training, and is {1.3--1.5}\\,$\\times$ faster compared to baselines with speculative decoding."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-20T10:00:03Z",
                "published_parsed": [
                    2025,
                    11,
                    20,
                    10,
                    0,
                    3,
                    3,
                    324,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Rongxin Cheng"
                    },
                    {
                        "name": "Kai Zhou"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Siyuan Liu"
                    },
                    {
                        "name": "Mingcong Han"
                    },
                    {
                        "name": "Mingjing Ai"
                    },
                    {
                        "name": "Yeju Zhou"
                    },
                    {
                        "name": "Baoquan Zhong"
                    },
                    {
                        "name": "Wencong Xiao"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen"
            },
            {
                "id": "http://arxiv.org/abs/2511.17006v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17006v1",
                "title": "Budget-Aware Tool-Use Enables Effective Agent Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Budget-Aware Tool-Use Enables Effective Agent Scaling"
                },
                "updated": "2025-11-21T07:18:55Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    7,
                    18,
                    55,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17006v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Scaling test-time computation improves performance across different tasks on large language models (LLMs), which has also been extended to tool-augmented agents. For these agents, scaling involves not only \"thinking\" in tokens but also \"acting\" via tool calls. The number of tool calls directly bounds the agent's interaction with the external environment. However, we find that simply granting agents a larger tool-call budget fails to improve performance, as they lack \"budget awareness\" and quickly hit a performance ceiling. To address this, we study how to scale such agents effectively under explicit tool-call budgets, focusing on web search agents. We first introduce the Budget Tracker, a lightweight plug-in that provides the agent with continuous budget awareness, enabling simple yet effective scaling. We further develop BATS (Budget Aware Test-time Scaling), an advanced framework that leverages this awareness to dynamically adapt its planning and verification strategy, deciding whether to \"dig deeper\" on a promising lead or \"pivot\" to new paths based on remaining resources. To analyze cost-performance scaling in a controlled manner, we formalize a unified cost metric that jointly accounts for token and tool consumption. We provide the first systematic study on budget-constrained agents, showing that budget-aware methods produce more favorable scaling curves and push the cost-performance Pareto frontier. Our work offers empirical insights toward a more transparent and principled understanding of scaling in tool-augmented agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling test-time computation improves performance across different tasks on large language models (LLMs), which has also been extended to tool-augmented agents. For these agents, scaling involves not only \"thinking\" in tokens but also \"acting\" via tool calls. The number of tool calls directly bounds the agent's interaction with the external environment. However, we find that simply granting agents a larger tool-call budget fails to improve performance, as they lack \"budget awareness\" and quickly hit a performance ceiling. To address this, we study how to scale such agents effectively under explicit tool-call budgets, focusing on web search agents. We first introduce the Budget Tracker, a lightweight plug-in that provides the agent with continuous budget awareness, enabling simple yet effective scaling. We further develop BATS (Budget Aware Test-time Scaling), an advanced framework that leverages this awareness to dynamically adapt its planning and verification strategy, deciding whether to \"dig deeper\" on a promising lead or \"pivot\" to new paths based on remaining resources. To analyze cost-performance scaling in a controlled manner, we formalize a unified cost metric that jointly accounts for token and tool consumption. We provide the first systematic study on budget-constrained agents, showing that budget-aware methods produce more favorable scaling curves and push the cost-performance Pareto frontier. Our work offers empirical insights toward a more transparent and principled understanding of scaling in tool-augmented agents."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T07:18:55Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    7,
                    18,
                    55,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Tengxiao Liu"
                    },
                    {
                        "name": "Zifeng Wang"
                    },
                    {
                        "name": "Jin Miao"
                    },
                    {
                        "name": "I-Hung Hsu"
                    },
                    {
                        "name": "Jun Yan"
                    },
                    {
                        "name": "Jiefeng Chen"
                    },
                    {
                        "name": "Rujun Han"
                    },
                    {
                        "name": "Fangyuan Xu"
                    },
                    {
                        "name": "Yanfei Chen"
                    },
                    {
                        "name": "Ke Jiang"
                    },
                    {
                        "name": "Samira Daruki"
                    },
                    {
                        "name": "Yi Liang"
                    },
                    {
                        "name": "William Yang Wang"
                    },
                    {
                        "name": "Tomas Pfister"
                    },
                    {
                        "name": "Chen-Yu Lee"
                    }
                ],
                "author_detail": {
                    "name": "Chen-Yu Lee"
                },
                "author": "Chen-Yu Lee"
            },
            {
                "id": "http://arxiv.org/abs/2511.16998v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.16998v1",
                "title": "VLM-Augmented Degradation Modeling for Image Restoration Under Adverse Weather Conditions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLM-Augmented Degradation Modeling for Image Restoration Under Adverse Weather Conditions"
                },
                "updated": "2025-11-21T07:06:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    7,
                    6,
                    48,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.16998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.16998v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/ICAC65379.2025.11196306",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Reliable visual perception under adverse weather conditions, such as rain, haze, snow, or a mixture of them, is desirable yet challenging for autonomous driving and outdoor robots. In this paper, we propose a unified Memory-Enhanced Visual-Language Recovery (MVLR) model that restores images from different degradation levels under various weather conditions. MVLR couples a lightweight encoder-decoder backbone with a Visual-Language Model (VLM) and an Implicit Memory Bank (IMB). The VLM performs chain-of-thought inference to encode weather degradation priors and the IMB stores continuous latent representations of degradation patterns. The VLM-generated priors query the IMB to retrieve fine-grained degradation prototypes. These prototypes are then adaptively fused with multi-scale visual features via dynamic cross-attention mechanisms, enhancing restoration accuracy while maintaining computational efficiency. Extensive experiments on four severe-weather benchmarks show that MVLR surpasses single-branch and Mixture-of-Experts baselines in terms of Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM). These results indicate that MVLR offers a practical balance between model compactness and expressiveness for real-time deployment in diverse outdoor conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable visual perception under adverse weather conditions, such as rain, haze, snow, or a mixture of them, is desirable yet challenging for autonomous driving and outdoor robots. In this paper, we propose a unified Memory-Enhanced Visual-Language Recovery (MVLR) model that restores images from different degradation levels under various weather conditions. MVLR couples a lightweight encoder-decoder backbone with a Visual-Language Model (VLM) and an Implicit Memory Bank (IMB). The VLM performs chain-of-thought inference to encode weather degradation priors and the IMB stores continuous latent representations of degradation patterns. The VLM-generated priors query the IMB to retrieve fine-grained degradation prototypes. These prototypes are then adaptively fused with multi-scale visual features via dynamic cross-attention mechanisms, enhancing restoration accuracy while maintaining computational efficiency. Extensive experiments on four severe-weather benchmarks show that MVLR surpasses single-branch and Mixture-of-Experts baselines in terms of Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM). These results indicate that MVLR offers a practical balance between model compactness and expressiveness for real-time deployment in diverse outdoor conditions."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T07:06:48Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    7,
                    6,
                    48,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "arxiv_journal_ref": "Proc. 2025 30th International Conference on Automation and Computing (ICAC), pp. 1-6, 2025",
                "authors": [
                    {
                        "name": "Qianyi Shao"
                    },
                    {
                        "name": "Yuanfan Zhang"
                    },
                    {
                        "name": "Renxiang Xiao"
                    },
                    {
                        "name": "Liang Hu"
                    }
                ],
                "author_detail": {
                    "name": "Liang Hu"
                },
                "author": "Liang Hu",
                "arxiv_doi": "10.1109/ICAC65379.2025.11196306"
            },
            {
                "id": "http://arxiv.org/abs/2505.23662v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.23662v2",
                "title": "ToolHaystack: Stress-Testing Tool-Augmented Language Models in Realistic Long-Term Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolHaystack: Stress-Testing Tool-Augmented Language Models in Realistic Long-Term Interactions"
                },
                "updated": "2025-11-21T07:05:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    7,
                    5,
                    38,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.23662v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.23662v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have demonstrated strong capabilities in using external tools to address user inquiries. However, most existing evaluations assume tool use in short contexts, offering limited insight into model behavior during realistic long-term interactions. To fill this gap, we introduce ToolHaystack, a benchmark for testing the tool use capabilities in long-term interactions. Each test instance in ToolHaystack includes multiple tasks execution contexts and realistic noise within a continuous conversation, enabling assessment of how well models maintain context and handle various disruptions. By applying this benchmark to 14 state-of-the-art LLMs, we find that while current models perform well in standard multi-turn settings, they often significantly struggle in ToolHaystack, highlighting critical gaps in their long-term robustness not revealed by previous tool benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong capabilities in using external tools to address user inquiries. However, most existing evaluations assume tool use in short contexts, offering limited insight into model behavior during realistic long-term interactions. To fill this gap, we introduce ToolHaystack, a benchmark for testing the tool use capabilities in long-term interactions. Each test instance in ToolHaystack includes multiple tasks execution contexts and realistic noise within a continuous conversation, enabling assessment of how well models maintain context and handle various disruptions. By applying this benchmark to 14 state-of-the-art LLMs, we find that while current models perform well in standard multi-turn settings, they often significantly struggle in ToolHaystack, highlighting critical gaps in their long-term robustness not revealed by previous tool benchmarks."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-29T17:10:12Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    10,
                    12,
                    3,
                    149,
                    0
                ],
                "arxiv_comment": "Our code and data are available at https://github.com/bwookwak/ToolHaystack Edited for adding acknowledgement section",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Beong-woo Kwak"
                    },
                    {
                        "name": "Minju Kim"
                    },
                    {
                        "name": "Dongha Lim"
                    },
                    {
                        "name": "Hyungjoo Chae"
                    },
                    {
                        "name": "Dongjin Kang"
                    },
                    {
                        "name": "Sunghwan Kim"
                    },
                    {
                        "name": "Dongil Yang"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    }
                ],
                "author_detail": {
                    "name": "Jinyoung Yeo"
                },
                "author": "Jinyoung Yeo"
            },
            {
                "id": "http://arxiv.org/abs/2511.16997v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.16997v1",
                "title": "MirrorMind: Empowering OmniScientist with the Expert Perspectives and Collective Knowledge of Human Scientists",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MirrorMind: Empowering OmniScientist with the Expert Perspectives and Collective Knowledge of Human Scientists"
                },
                "updated": "2025-11-21T07:05:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    7,
                    5,
                    26,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.16997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.16997v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The emergence of AI Scientists has demonstrated remarkable potential in automating scientific research. However, current approaches largely conceptualize scientific discovery as a solitary optimization or search process, overlooking that knowledge production is inherently a social and historical endeavor. Human scientific insight stems from two distinct yet interconnected sources. First is the individual cognitive trajectory, where a researcher's unique insight is shaped by their evolving research history and stylistic preferences; another is the collective disciplinary memory, where knowledge is sedimented into vast, interconnected networks of citations and concepts. Existing LLMs still struggle to represent these structured, high-fidelity cognitive and social contexts. To bridge this gap, we introduce MirrorMind, a hierarchical cognitive architecture that integrates dual-memory representations within a three-level framework. The Individual Level constructs high-fidelity cognitive models of individual researchers by capturing their episodic, semantic, and persona memories; the Domain Level maps collective knowledge into structured disciplinary concept graphs; and the Interdisciplinary Level that acts as an orthogonal orchestration engine. Crucially, our architecture separates memory storage from agentic execution, enabling AI scientist agents to flexibly access individual memories for unique perspectives or collective structures to reason. We evaluate MirrorMind across four comprehensive tasks, including author-level cognitive simulation, complementary reasoning, cross-disciplinary collaboration promotion, and multi-agent scientific problem solving. The results show that by integrating individual cognitive depth with collective disciplinary breadth, MirrorMind moves beyond simple fact retrieval toward structural, personalized, and insight-generating scientific reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of AI Scientists has demonstrated remarkable potential in automating scientific research. However, current approaches largely conceptualize scientific discovery as a solitary optimization or search process, overlooking that knowledge production is inherently a social and historical endeavor. Human scientific insight stems from two distinct yet interconnected sources. First is the individual cognitive trajectory, where a researcher's unique insight is shaped by their evolving research history and stylistic preferences; another is the collective disciplinary memory, where knowledge is sedimented into vast, interconnected networks of citations and concepts. Existing LLMs still struggle to represent these structured, high-fidelity cognitive and social contexts. To bridge this gap, we introduce MirrorMind, a hierarchical cognitive architecture that integrates dual-memory representations within a three-level framework. The Individual Level constructs high-fidelity cognitive models of individual researchers by capturing their episodic, semantic, and persona memories; the Domain Level maps collective knowledge into structured disciplinary concept graphs; and the Interdisciplinary Level that acts as an orthogonal orchestration engine. Crucially, our architecture separates memory storage from agentic execution, enabling AI scientist agents to flexibly access individual memories for unique perspectives or collective structures to reason. We evaluate MirrorMind across four comprehensive tasks, including author-level cognitive simulation, complementary reasoning, cross-disciplinary collaboration promotion, and multi-agent scientific problem solving. The results show that by integrating individual cognitive depth with collective disciplinary breadth, MirrorMind moves beyond simple fact retrieval toward structural, personalized, and insight-generating scientific reasoning."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T07:05:26Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    7,
                    5,
                    26,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "26 pages, 4 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Qingbin Zeng"
                    },
                    {
                        "name": "Bingbing Fan"
                    },
                    {
                        "name": "Zhiyu Chen"
                    },
                    {
                        "name": "Sijian Ren"
                    },
                    {
                        "name": "Zhilun Zhou"
                    },
                    {
                        "name": "Xuhua Zhang"
                    },
                    {
                        "name": "Yuanyi Zhen"
                    },
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Tie-Yan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tie-Yan Liu"
                },
                "author": "Tie-Yan Liu"
            },
            {
                "id": "http://arxiv.org/abs/2511.16992v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.16992v1",
                "title": "FIRM: Federated In-client Regularized Multi-objective Alignment for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FIRM: Federated In-client Regularized Multi-objective Alignment for Large Language Models"
                },
                "updated": "2025-11-21T06:59:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    6,
                    59,
                    28,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.16992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.16992v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Aligning Large Language Models (LLMs) with human values often involves balancing multiple, conflicting objectives such as helpfulness and harmlessness. Training these models is computationally intensive, and centralizing the process raises significant data privacy concerns. Federated Learning (FL) offers a compelling alternative, but existing Federated Multi-Objective Optimization (FMOO) methods face severe communication bottlenecks as their reliance on transmitting multiple gradients to a server is unscalable for large models. We introduce FIRM (Federated In-client Regularized Multi-objective alignment), a novel algorithm that achieves both client disagreement drift mitigation and communication efficiency. In FIRM, each client locally solves a regularized multi-objective optimization problem. By directly mitigating client disagreement drift through in-client regularization, our method eliminates the need for the multi-gradient transmissions common in prior works. Consequently, clients need only to transmit a single set of adapted parameters, maintaining high communication efficiency. We prove that our algorithm converges to Pareto-stationary points and, to our knowledge, provide the first finite-time convergence guarantees for this federated multi-objective alignment setting. Empirically, we show that FIRM leads to smoother training dynamics, reduced client disagreement drift, and improved reward trade-offs compared to baselines. We further propose a method to incorporate a preference over the objectives and report empirical Pareto plots, demonstrating that FIRM can smoothly adapt trade-offs between objectives in response to specified preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Large Language Models (LLMs) with human values often involves balancing multiple, conflicting objectives such as helpfulness and harmlessness. Training these models is computationally intensive, and centralizing the process raises significant data privacy concerns. Federated Learning (FL) offers a compelling alternative, but existing Federated Multi-Objective Optimization (FMOO) methods face severe communication bottlenecks as their reliance on transmitting multiple gradients to a server is unscalable for large models. We introduce FIRM (Federated In-client Regularized Multi-objective alignment), a novel algorithm that achieves both client disagreement drift mitigation and communication efficiency. In FIRM, each client locally solves a regularized multi-objective optimization problem. By directly mitigating client disagreement drift through in-client regularization, our method eliminates the need for the multi-gradient transmissions common in prior works. Consequently, clients need only to transmit a single set of adapted parameters, maintaining high communication efficiency. We prove that our algorithm converges to Pareto-stationary points and, to our knowledge, provide the first finite-time convergence guarantees for this federated multi-objective alignment setting. Empirically, we show that FIRM leads to smoother training dynamics, reduced client disagreement drift, and improved reward trade-offs compared to baselines. We further propose a method to incorporate a preference over the objectives and report empirical Pareto plots, demonstrating that FIRM can smoothly adapt trade-offs between objectives in response to specified preferences."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T06:59:28Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    6,
                    59,
                    28,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Fatemeh"
                    },
                    {
                        "name": "Nourzad"
                    },
                    {
                        "name": "Amirhossein Roknilamouki"
                    },
                    {
                        "name": "Eylem Ekici"
                    },
                    {
                        "name": "Jia"
                    },
                    {
                        "name": "Liu"
                    },
                    {
                        "name": "Ness B. Shroff"
                    }
                ],
                "author_detail": {
                    "name": "Ness B. Shroff"
                },
                "arxiv_affiliation": "Kevin",
                "author": "Ness B. Shroff"
            },
            {
                "id": "http://arxiv.org/abs/2511.16986v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.16986v1",
                "title": "RadioKMoE: Knowledge-Guided Radiomap Estimation with Kolmogorov-Arnold Networks and Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RadioKMoE: Knowledge-Guided Radiomap Estimation with Kolmogorov-Arnold Networks and Mixture-of-Experts"
                },
                "updated": "2025-11-21T06:45:46Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    6,
                    45,
                    46,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.16986v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.16986v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Radiomap serves as a vital tool for wireless network management and deployment by providing powerful spatial knowledge of signal propagation and coverage. However, increasingly complex radio propagation behavior and surrounding environments pose strong challenges for radiomap estimation (RME). In this work, we propose a knowledge-guided RME framework that integrates Kolmogorov-Arnold Networks (KAN) with Mixture-of-Experts (MoE), namely RadioKMoE. Specifically, we design a KAN module to predict an initial coarse coverage map, leveraging KAN's strength in approximating physics models and global radio propagation patterns. The initial coarse map, together with environmental information, drives our MoE network for precise radiomap estimation. Unlike conventional deep learning models, the MoE module comprises expert networks specializing in distinct radiomap patterns to improve local details while preserving global consistency. Experimental results in both multi- and single-band RME demonstrate the enhanced accuracy and robustness of the proposed RadioKMoE in radiomap estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radiomap serves as a vital tool for wireless network management and deployment by providing powerful spatial knowledge of signal propagation and coverage. However, increasingly complex radio propagation behavior and surrounding environments pose strong challenges for radiomap estimation (RME). In this work, we propose a knowledge-guided RME framework that integrates Kolmogorov-Arnold Networks (KAN) with Mixture-of-Experts (MoE), namely RadioKMoE. Specifically, we design a KAN module to predict an initial coarse coverage map, leveraging KAN's strength in approximating physics models and global radio propagation patterns. The initial coarse map, together with environmental information, drives our MoE network for precise radiomap estimation. Unlike conventional deep learning models, the MoE module comprises expert networks specializing in distinct radiomap patterns to improve local details while preserving global consistency. Experimental results in both multi- and single-band RME demonstrate the enhanced accuracy and robustness of the proposed RadioKMoE in radiomap estimation."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T06:45:46Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    6,
                    45,
                    46,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Fupei Guo"
                    },
                    {
                        "name": "Kerry Pan"
                    },
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Zhi Ding"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Ding"
                },
                "author": "Zhi Ding"
            },
            {
                "id": "http://arxiv.org/abs/2510.14528v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.14528v3",
                "title": "PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model"
                },
                "updated": "2025-11-21T06:39:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    6,
                    39,
                    47,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.14528v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.14528v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a compact yet powerful vision-language model (VLM) that integrates a NaViT-style dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to enable accurate element recognition. This innovative model efficiently supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption. Through comprehensive evaluations on widely used public benchmarks and in-house benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document parsing and element-level recognition. It significantly outperforms existing solutions, exhibits strong competitiveness against top-tier VLMs, and delivers fast inference speeds. These strengths make it highly suitable for practical deployment in real-world scenarios. Code is available at https://github.com/PaddlePaddle/PaddleOCR .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a compact yet powerful vision-language model (VLM) that integrates a NaViT-style dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to enable accurate element recognition. This innovative model efficiently supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption. Through comprehensive evaluations on widely used public benchmarks and in-house benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document parsing and element-level recognition. It significantly outperforms existing solutions, exhibits strong competitiveness against top-tier VLMs, and delivers fast inference speeds. These strengths make it highly suitable for practical deployment in real-world scenarios. Code is available at https://github.com/PaddlePaddle/PaddleOCR ."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-16T10:18:48Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    10,
                    18,
                    48,
                    3,
                    289,
                    0
                ],
                "arxiv_comment": "Github Repo: https://github.com/PaddlePaddle/PaddleOCR",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Cheng Cui"
                    },
                    {
                        "name": "Ting Sun"
                    },
                    {
                        "name": "Suyin Liang"
                    },
                    {
                        "name": "Tingquan Gao"
                    },
                    {
                        "name": "Zelun Zhang"
                    },
                    {
                        "name": "Jiaxuan Liu"
                    },
                    {
                        "name": "Xueqing Wang"
                    },
                    {
                        "name": "Changda Zhou"
                    },
                    {
                        "name": "Hongen Liu"
                    },
                    {
                        "name": "Manhui Lin"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Yubo Zhang"
                    },
                    {
                        "name": "Handong Zheng"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Dianhai Yu"
                    },
                    {
                        "name": "Yanjun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yanjun Ma"
                },
                "author": "Yanjun Ma"
            },
            {
                "id": "http://arxiv.org/abs/2511.16985v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.16985v1",
                "title": "ARQUSUMM: Argument-aware Quantitative Summarization of Online Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARQUSUMM: Argument-aware Quantitative Summarization of Online Conversations"
                },
                "updated": "2025-11-21T06:37:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    6,
                    37,
                    32,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.16985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.16985v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Online conversations have become more prevalent on public discussion platforms (e.g. Reddit). With growing controversial topics, it is desirable to summarize not only diverse arguments, but also their rationale and justification. Early studies on text summarization focus on capturing general salient information in source documents, overlooking the argumentative nature of online conversations. Recent research on conversation summarization although considers the argumentative relationship among sentences, fail to explicate deeper argument structure within sentences for summarization. In this paper, we propose a novel task of argument-aware quantitative summarization to reveal the claim-reason structure of arguments in conversations, with quantities measuring argument strength. We further propose ARQUSUMM, a novel framework to address the task. To reveal the underlying argument structure within sentences, ARQUSUMM leverages LLM few-shot learning grounded in the argumentation theory to identify propositions within sentences and their claim-reason relationships. For quantitative summarization, ARQUSUMM employs argument structure-aware clustering algorithms to aggregate arguments and quantify their support. Experiments show that ARQUSUMM outperforms existing conversation and quantitative summarization models and generate summaries representing argument structures that are more helpful to users, of high textual quality and quantification accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online conversations have become more prevalent on public discussion platforms (e.g. Reddit). With growing controversial topics, it is desirable to summarize not only diverse arguments, but also their rationale and justification. Early studies on text summarization focus on capturing general salient information in source documents, overlooking the argumentative nature of online conversations. Recent research on conversation summarization although considers the argumentative relationship among sentences, fail to explicate deeper argument structure within sentences for summarization. In this paper, we propose a novel task of argument-aware quantitative summarization to reveal the claim-reason structure of arguments in conversations, with quantities measuring argument strength. We further propose ARQUSUMM, a novel framework to address the task. To reveal the underlying argument structure within sentences, ARQUSUMM leverages LLM few-shot learning grounded in the argumentation theory to identify propositions within sentences and their claim-reason relationships. For quantitative summarization, ARQUSUMM employs argument structure-aware clustering algorithms to aggregate arguments and quantify their support. Experiments show that ARQUSUMM outperforms existing conversation and quantitative summarization models and generate summaries representing argument structures that are more helpful to users, of high textual quality and quantification accuracy."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T06:37:32Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    6,
                    37,
                    32,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Paper accepted to AAAI2026 Main Technical Track",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "An Quang Tang"
                    },
                    {
                        "name": "Xiuzhen Zhang"
                    },
                    {
                        "name": "Minh Ngoc Dinh"
                    },
                    {
                        "name": "Zhuang Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhuang Li"
                },
                "author": "Zhuang Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.16972v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.16972v1",
                "title": "ToC: Tree-of-Claims Search with Multi-Agent Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToC: Tree-of-Claims Search with Multi-Agent Language Models"
                },
                "updated": "2025-11-21T06:01:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    21,
                    6,
                    1,
                    28,
                    4,
                    325,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.16972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.16972v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Optimizing patent claims is a critical yet challenging task, demanding careful balance between maximizing novelty and preserving legal scope. Manual claim drafting is labor-intensive, costly, and inherently inconsistent, while conventional Large Language Models (LLMs) often lack the structured, iterative reasoning essential for precise claim refinement. To address these challenges, we introduce Tree of Claims (ToC), an innovative framework that redefines claim editing as a guided search problem. ToC synergistically integrates Monte Carlo Tree Search (MCTS) with a collaborative multi-agent system, comprising an LLM-based EditorAgent that proposes contextually grounded edits, and an ExaminerAgent that mimics patent examiner critiques through structured, chain-of-thought analyses of novelty and prior art disclosure. Driven by a carefully designed multi-objective reward function, ToC jointly optimizes novelty, scope retention, and semantic coherence. Experimental evaluation on a benchmark of 1145 claims demonstrates that ToC significantly outperforms standard LLMs in zero-shot and few-shot scenarios, achieving an average composite score improvement of 8\\%, and up to 9\\% in certain cases. Extensive experiments, including detailed ablation studies, validate ToC's efficacy in generating superior, legally robust claim revisions. Overall, ToC establishes a transparent, controllable, and interpretable methodology that effectively bridges advanced LLM reasoning capabilities with strategic MCTS planning for structured patent claim optimization.The source code is available at https://github.com/ysy2003/ToC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing patent claims is a critical yet challenging task, demanding careful balance between maximizing novelty and preserving legal scope. Manual claim drafting is labor-intensive, costly, and inherently inconsistent, while conventional Large Language Models (LLMs) often lack the structured, iterative reasoning essential for precise claim refinement. To address these challenges, we introduce Tree of Claims (ToC), an innovative framework that redefines claim editing as a guided search problem. ToC synergistically integrates Monte Carlo Tree Search (MCTS) with a collaborative multi-agent system, comprising an LLM-based EditorAgent that proposes contextually grounded edits, and an ExaminerAgent that mimics patent examiner critiques through structured, chain-of-thought analyses of novelty and prior art disclosure. Driven by a carefully designed multi-objective reward function, ToC jointly optimizes novelty, scope retention, and semantic coherence. Experimental evaluation on a benchmark of 1145 claims demonstrates that ToC significantly outperforms standard LLMs in zero-shot and few-shot scenarios, achieving an average composite score improvement of 8\\%, and up to 9\\% in certain cases. Extensive experiments, including detailed ablation studies, validate ToC's efficacy in generating superior, legally robust claim revisions. Overall, ToC establishes a transparent, controllable, and interpretable methodology that effectively bridges advanced LLM reasoning capabilities with strategic MCTS planning for structured patent claim optimization.The source code is available at https://github.com/ysy2003/ToC."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T06:01:28Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    6,
                    1,
                    28,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Accepted by AAAI 2026 (Oral)",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Shuyang Yu"
                    },
                    {
                        "name": "Jianan Liang"
                    },
                    {
                        "name": "Hui Hu"
                    }
                ],
                "author_detail": {
                    "name": "Hui Hu"
                },
                "author": "Hui Hu"
            }
        ]
    }
]