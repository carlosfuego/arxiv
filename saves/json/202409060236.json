[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2312.04985v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04985v6",
                "updated": "2024-09-04T10:04:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    4,
                    52,
                    2,
                    248,
                    0
                ],
                "published": "2023-12-08T11:47:35Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    11,
                    47,
                    35,
                    4,
                    342,
                    0
                ],
                "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQ Attention: Bandwidth-Efficient LLM Inference"
                },
                "summary": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Luka Ribar"
                    },
                    {
                        "name": "Ivan Chelombiev"
                    },
                    {
                        "name": "Luke Hudlass-Galley"
                    },
                    {
                        "name": "Charlie Blake"
                    },
                    {
                        "name": "Carlo Luschi"
                    },
                    {
                        "name": "Douglas Orr"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Orr"
                },
                "author": "Douglas Orr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04985v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04985v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02480v1",
                "updated": "2024-09-04T07:13:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T07:13:01Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "title": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel"
                },
                "summary": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage."
                },
                "authors": [
                    {
                        "name": "S. -B. Qian"
                    },
                    {
                        "name": "L. -Y. Zhu"
                    },
                    {
                        "name": "F. -X. Li"
                    },
                    {
                        "name": "L. -J. Li"
                    },
                    {
                        "name": "Z. -T. Han"
                    },
                    {
                        "name": "J. -J. He"
                    },
                    {
                        "name": "L. Zang"
                    },
                    {
                        "name": "L. -F. Chang"
                    },
                    {
                        "name": "Q. -B. Sun"
                    },
                    {
                        "name": "M. -Y. Li"
                    },
                    {
                        "name": "H. -T. Zhang"
                    },
                    {
                        "name": "F. -Z. Yan"
                    }
                ],
                "author_detail": {
                    "name": "F. -Z. Yan"
                },
                "author": "F. -Z. Yan",
                "arxiv_doi": "10.3847/1538-4357/ad631a",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ad631a",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v2",
                "updated": "2024-09-05T01:12:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    12,
                    4,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01990v1",
                "updated": "2024-09-03T15:35:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T15:35:01Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "title": "Contemporary Model Compression on Large Language Models Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary Model Compression on Large Language Models Inference"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Liu"
                },
                "author": "Dong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01890v1",
                "updated": "2024-09-03T13:29:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T13:29:13Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks"
                },
                "summary": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost."
                },
                "authors": [
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Will Grathwohl"
                    },
                    {
                        "name": "Michael Boratko"
                    },
                    {
                        "name": "Rob Fergus"
                    },
                    {
                        "name": "Andrew McCallum"
                    },
                    {
                        "name": "Manzil Zaheer"
                    }
                ],
                "author_detail": {
                    "name": "Manzil Zaheer"
                },
                "author": "Manzil Zaheer",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02137v1",
                "updated": "2024-09-02T15:07:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T15:07:05Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "title": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems"
                },
                "summary": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding."
                },
                "authors": [
                    {
                        "name": "Andrea Borgarelli"
                    },
                    {
                        "name": "Constantin Enea"
                    },
                    {
                        "name": "Rupak Majumdar"
                    },
                    {
                        "name": "Srinidhi Nagendra"
                    }
                ],
                "author_detail": {
                    "name": "Srinidhi Nagendra"
                },
                "author": "Srinidhi Nagendra",
                "arxiv_doi": "10.1145/3689779",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689779",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01066v1",
                "updated": "2024-09-02T08:41:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T08:41:45Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "title": "Learning in Hybrid Active Inference Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning in Hybrid Active Inference Models"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "11 pages (+ appendix). Accepted to the International Workshop on\n  Active Inference 2024. arXiv admin note: substantial text overlap with\n  arXiv:2408.10970",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00905v1",
                "updated": "2024-09-02T02:36:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T02:36:22Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "title": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach"
                },
                "summary": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy."
                },
                "authors": [
                    {
                        "name": "Zhou Zhang"
                    },
                    {
                        "name": "Saman Atapattu"
                    },
                    {
                        "name": "Yizhu Wang"
                    },
                    {
                        "name": "Marco Di Renzo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Di Renzo"
                },
                "author": "Marco Di Renzo",
                "arxiv_comment": "2024 IEEE GLOBECOM, Cape Town, South Africa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00876v1",
                "updated": "2024-09-02T00:05:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T00:05:20Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "title": "Rapid GPU-Based Pangenome Graph Layout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid GPU-Based Pangenome Graph Layout"
                },
                "summary": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes."
                },
                "authors": [
                    {
                        "name": "Jiajie Li"
                    },
                    {
                        "name": "Jan-Niklas Schmelzle"
                    },
                    {
                        "name": "Yixiao Du"
                    },
                    {
                        "name": "Simon Heumos"
                    },
                    {
                        "name": "Andrea Guarracino"
                    },
                    {
                        "name": "Giulia Guidi"
                    },
                    {
                        "name": "Pjotr Prins"
                    },
                    {
                        "name": "Erik Garrison"
                    },
                    {
                        "name": "Zhiru Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiru Zhang"
                },
                "author": "Zhiru Zhang",
                "arxiv_comment": "SC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00364v1",
                "updated": "2024-08-31T06:33:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T06:33:50Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "title": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems"
                },
                "summary": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme."
                },
                "authors": [
                    {
                        "name": "Wanming Hao"
                    },
                    {
                        "name": "Xue Wu"
                    },
                    {
                        "name": "Xingwang Li"
                    },
                    {
                        "name": "Gangcan Sun"
                    },
                    {
                        "name": "Qingqing Wu"
                    },
                    {
                        "name": "Liang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Yang"
                },
                "author": "Liang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00344v1",
                "updated": "2024-08-31T04:20:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T04:20:58Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "title": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on"
                },
                "summary": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing."
                },
                "authors": [
                    {
                        "name": "Advait Gilankar"
                    },
                    {
                        "name": "Abishek Katta"
                    },
                    {
                        "name": "Nabasindhu Das"
                    },
                    {
                        "name": "Nidhin Kurian Kalarickal"
                    }
                ],
                "author_detail": {
                    "name": "Nidhin Kurian Kalarickal"
                },
                "author": "Nidhin Kurian Kalarickal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00184v1",
                "updated": "2024-08-30T18:04:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T18:04:53Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "title": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation"
                },
                "summary": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality."
                },
                "authors": [
                    {
                        "name": "Jianxin Sun"
                    },
                    {
                        "name": "David Lenz"
                    },
                    {
                        "name": "Hongfeng Yu"
                    },
                    {
                        "name": "Tom Peterka"
                    }
                ],
                "author_detail": {
                    "name": "Tom Peterka"
                },
                "author": "Tom Peterka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17178v1",
                "updated": "2024-08-30T10:26:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T10:26:50Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "title": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond"
                },
                "summary": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations."
                },
                "authors": [
                    {
                        "name": "Bobby Xiong"
                    },
                    {
                        "name": "Davide Fioriti"
                    },
                    {
                        "name": "Fabian Neumann"
                    },
                    {
                        "name": "Iegor Riepin"
                    },
                    {
                        "name": "Tom Brown"
                    }
                ],
                "author_detail": {
                    "name": "Tom Brown"
                },
                "author": "Tom Brown",
                "arxiv_comment": "20 pages, 15 figures, 8 tables. For associated prebuilt electricity\n  network, see https://doi.org/10.5281/zenodo.13358976",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16967v1",
                "updated": "2024-08-30T02:01:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T02:01:56Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemLong: Memory-Augmented Retrieval for Long Text Modeling"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong"
                },
                "authors": [
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Zecheng Tang"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.07975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.07975v2",
                "updated": "2024-08-29T17:43:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    43,
                    26,
                    3,
                    242,
                    0
                ],
                "published": "2023-09-14T18:18:10Z",
                "published_parsed": [
                    2023,
                    9,
                    14,
                    18,
                    18,
                    10,
                    3,
                    257,
                    0
                ],
                "title": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load"
                },
                "summary": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs."
                },
                "authors": [
                    {
                        "name": "Hesameddin Mokhtarzadeh"
                    },
                    {
                        "name": "Mohammed S. Al-Abiad"
                    },
                    {
                        "name": "Md Jahangir Hossain"
                    },
                    {
                        "name": "Julian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Julian Cheng"
                },
                "author": "Julian Cheng",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.07975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.07975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16730v1",
                "updated": "2024-08-29T17:21:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:21:58Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "title": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation"
                },
                "summary": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets."
                },
                "authors": [
                    {
                        "name": "Shiwei Wu"
                    },
                    {
                        "name": "Joya Chen"
                    },
                    {
                        "name": "Kevin Qinghong Lin"
                    },
                    {
                        "name": "Qimeng Wang"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v3",
                "updated": "2024-08-29T16:48:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    48,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16220v1",
                "updated": "2024-08-29T02:31:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T02:31:28Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "title": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening"
                },
                "summary": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses."
                },
                "authors": [
                    {
                        "name": "Yiming Zhu"
                    },
                    {
                        "name": "Wenchao Huang"
                    },
                    {
                        "name": "Yan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Yan Xiong"
                },
                "author": "Yan Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.06942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.06942v3",
                "updated": "2024-08-28T08:41:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    41,
                    45,
                    2,
                    241,
                    0
                ],
                "published": "2023-06-12T08:24:14Z",
                "published_parsed": [
                    2023,
                    6,
                    12,
                    8,
                    24,
                    14,
                    0,
                    163,
                    0
                ],
                "title": "RIP Linked List",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIP Linked List"
                },
                "summary": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations."
                },
                "authors": [
                    {
                        "name": "Benot Sonntag"
                    },
                    {
                        "name": "Dominique Colnet"
                    }
                ],
                "author_detail": {
                    "name": "Dominique Colnet"
                },
                "arxiv_affiliation": "LORIA",
                "author": "Dominique Colnet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.06942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.06942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v2",
                "updated": "2024-08-27T22:06:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    22,
                    6,
                    20,
                    1,
                    240,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06893v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06893v3",
                "updated": "2024-08-27T17:30:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    30,
                    41,
                    1,
                    240,
                    0
                ],
                "published": "2023-12-11T23:34:23Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    23,
                    34,
                    23,
                    0,
                    345,
                    0
                ],
                "title": "Styx: Transactional Stateful Functions on Streaming Dataflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Styx: Transactional Stateful Functions on Streaming Dataflows"
                },
                "summary": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency."
                },
                "authors": [
                    {
                        "name": "Kyriakos Psarakis"
                    },
                    {
                        "name": "George Siachamis"
                    },
                    {
                        "name": "George Christodoulou"
                    },
                    {
                        "name": "Marios Fragkoulis"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    }
                ],
                "author_detail": {
                    "name": "Asterios Katsifodimos"
                },
                "author": "Asterios Katsifodimos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06893v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06893v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14906v1",
                "updated": "2024-08-27T09:34:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T09:34:38Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "title": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval"
                },
                "summary": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins."
                },
                "authors": [
                    {
                        "name": "Melisa Russak"
                    },
                    {
                        "name": "Umar Jamil"
                    },
                    {
                        "name": "Christopher Bryant"
                    },
                    {
                        "name": "Kiran Kamble"
                    },
                    {
                        "name": "Axel Magnuson"
                    },
                    {
                        "name": "Mateusz Russak"
                    },
                    {
                        "name": "Waseem AlShikh"
                    }
                ],
                "author_detail": {
                    "name": "Waseem AlShikh"
                },
                "author": "Waseem AlShikh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14735v1",
                "updated": "2024-08-27T02:03:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T02:03:36Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "title": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy"
                },
                "summary": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10774v2",
                "updated": "2024-08-26T21:01:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    21,
                    1,
                    2,
                    0,
                    239,
                    0
                ],
                "published": "2024-06-16T01:33:02Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    1,
                    33,
                    2,
                    6,
                    168,
                    0
                ],
                "title": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"
                },
                "summary": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest ."
                },
                "authors": [
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yilong Zhao"
                    },
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14434v1",
                "updated": "2024-08-26T17:21:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:21:19Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "title": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena"
                },
                "summary": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing."
                },
                "authors": [
                    {
                        "name": "Logan Ward"
                    },
                    {
                        "name": "J. Gregory Pauloski"
                    },
                    {
                        "name": "Valerie Hayot-Sasson"
                    },
                    {
                        "name": "Yadu Babuji"
                    },
                    {
                        "name": "Alexander Brace"
                    },
                    {
                        "name": "Ryan Chard"
                    },
                    {
                        "name": "Kyle Chard"
                    },
                    {
                        "name": "Rajeev Thakur"
                    },
                    {
                        "name": "Ian Foster"
                    }
                ],
                "author_detail": {
                    "name": "Ian Foster"
                },
                "author": "Ian Foster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06876v2",
                "updated": "2024-08-26T11:29:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    11,
                    29,
                    7,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-13T13:14:54Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "title": "Decision-Focused Learning to Predict Action Costs for Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Focused Learning to Predict Action Costs for Planning"
                },
                "summary": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements."
                },
                "authors": [
                    {
                        "name": "Jayanta Mandi"
                    },
                    {
                        "name": "Marco Foschini"
                    },
                    {
                        "name": "Daniel Holler"
                    },
                    {
                        "name": "Sylvie Thiebaux"
                    },
                    {
                        "name": "Jorg Hoffmann"
                    },
                    {
                        "name": "Tias Guns"
                    }
                ],
                "author_detail": {
                    "name": "Tias Guns"
                },
                "author": "Tias Guns",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16343v2",
                "updated": "2024-08-26T07:26:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    7,
                    26,
                    27,
                    0,
                    239,
                    0
                ],
                "published": "2024-02-26T06:55:36Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    6,
                    55,
                    36,
                    0,
                    57,
                    0
                ],
                "title": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems"
                },
                "summary": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures."
                },
                "authors": [
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "arxiv_comment": "Accepted by PACT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08795v2",
                "updated": "2024-08-26T04:32:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    4,
                    32,
                    56,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-16T15:11:12Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    11,
                    12,
                    4,
                    229,
                    0
                ],
                "title": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks"
                },
                "summary": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding"
                },
                "authors": [
                    {
                        "name": "Divya Ojha"
                    },
                    {
                        "name": "Sandhya Dwarkadas"
                    }
                ],
                "author_detail": {
                    "name": "Sandhya Dwarkadas"
                },
                "author": "Sandhya Dwarkadas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14001v1",
                "updated": "2024-08-26T03:58:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T03:58:20Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "title": "Decentralized Federated Learning with Model Caching on Mobile Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Learning with Model Caching on Mobile Agents"
                },
                "summary": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Houwei Cao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "27 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13605v1",
                "updated": "2024-08-24T15:23:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "published": "2024-08-24T15:23:32Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "title": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning"
                },
                "summary": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods."
                },
                "authors": [
                    {
                        "name": "Yuhan Yi"
                    },
                    {
                        "name": "Guanglin Zhang"
                    },
                    {
                        "name": "Hai Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hai Jiang"
                },
                "author": "Hai Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v3",
                "updated": "2024-08-23T17:54:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    54,
                    34,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/."
                },
                "authors": [
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13165v1",
                "updated": "2024-08-23T15:39:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T15:39:20Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "title": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches"
                },
                "summary": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme."
                },
                "authors": [
                    {
                        "name": "Dhruv Pratap Singh"
                    },
                    {
                        "name": "Anjana A. Mahesh"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "15 pages, 5 figures and one table. Some overlap of introductory and\n  background materials with our earlier submission arXiv:2407.00677v1 dated 30\n  June 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.05332v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.05332v5",
                "updated": "2024-08-23T13:25:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    13,
                    25,
                    7,
                    4,
                    236,
                    0
                ],
                "published": "2023-05-09T10:41:36Z",
                "published_parsed": [
                    2023,
                    5,
                    9,
                    10,
                    41,
                    36,
                    1,
                    129,
                    0
                ],
                "title": "Fundamental Limits of Multi-Message Private Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limits of Multi-Message Private Computation"
                },
                "summary": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$."
                },
                "authors": [
                    {
                        "name": "Ali Gholami"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Tayyebeh Jahani-Nezhad"
                    },
                    {
                        "name": "Hua Sun"
                    },
                    {
                        "name": "Mingyue Ji"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "A version of this paper is submitted to IEEE Transactions on\n  Communications. A short version was accepted and presented at ISIT 2024 in\n  Athens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.05332v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.05332v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12947v1",
                "updated": "2024-08-23T09:54:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T09:54:22Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "title": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis"
                },
                "summary": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness."
                },
                "authors": [
                    {
                        "name": "Vini Kanvar"
                    },
                    {
                        "name": "Uday P. Khedker"
                    }
                ],
                "author_detail": {
                    "name": "Uday P. Khedker"
                },
                "author": "Uday P. Khedker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v1",
                "updated": "2024-08-22T17:56:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jimnez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "13 pages, 16 figures, Submitted to ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14533v2",
                "updated": "2024-08-22T17:47:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    47,
                    49,
                    3,
                    235,
                    0
                ],
                "published": "2023-09-25T21:17:17Z",
                "published_parsed": [
                    2023,
                    9,
                    25,
                    21,
                    17,
                    17,
                    0,
                    268,
                    0
                ],
                "title": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties"
                },
                "summary": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry."
                },
                "authors": [
                    {
                        "name": "Simon Hettler"
                    },
                    {
                        "name": "Kankona Singha Roy"
                    },
                    {
                        "name": "Raul Arenal"
                    },
                    {
                        "name": "Leela S. Panchakarla"
                    }
                ],
                "author_detail": {
                    "name": "Leela S. Panchakarla"
                },
                "author": "Leela S. Panchakarla",
                "arxiv_doi": "10.1002/admi.202400317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1002/admi.202400317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.14533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Adv. Mater. Interfaces 2024, 2400317",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11506v1",
                "updated": "2024-08-21T10:26:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T10:26:26Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "title": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage"
                },
                "summary": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration."
                },
                "authors": [
                    {
                        "name": "Pedro C Rijo"
                    },
                    {
                        "name": "Francisco J. Galindo-Rosales"
                    }
                ],
                "author_detail": {
                    "name": "Francisco J. Galindo-Rosales"
                },
                "author": "Francisco J. Galindo-Rosales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.10685v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.10685v2",
                "updated": "2024-08-21T06:10:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    10,
                    2,
                    2,
                    234,
                    0
                ],
                "published": "2024-01-19T13:32:55Z",
                "published_parsed": [
                    2024,
                    1,
                    19,
                    13,
                    32,
                    55,
                    4,
                    19,
                    0
                ],
                "title": "Towards End-to-End GPS Localization with Neural Pseudorange Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards End-to-End GPS Localization with Neural Pseudorange Correction"
                },
                "summary": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet."
                },
                "authors": [
                    {
                        "name": "Xu Weng"
                    },
                    {
                        "name": "KV Ling"
                    },
                    {
                        "name": "Haochen Liu"
                    },
                    {
                        "name": "Kun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Kun Cao"
                },
                "author": "Kun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.10685v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.10685v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11325v1",
                "updated": "2024-08-21T04:16:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T04:16:49Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "title": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory"
                },
                "summary": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads."
                },
                "authors": [
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Ehsan Hajyjasini"
                    },
                    {
                        "name": "Seungjin Lee"
                    },
                    {
                        "name": "Zifeng Zhang"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Steven Swanson"
                    }
                ],
                "author_detail": {
                    "name": "Steven Swanson"
                },
                "author": "Steven Swanson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03637v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v3",
                "updated": "2024-08-21T02:32:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    2,
                    32,
                    43,
                    2,
                    234,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering"
                },
                "summary": "Matrix quantization compresses matrix elements into a more compact form to\nreduce storage requirements, with dequantization enabling reconstruction for\nuse. We define the Quantization Error Minimization (QEM) problem as minimizing\nthe difference between the original and quantized matrices while ensuring the\nquantized matrix remains within fixed memory constraints. This technique is\ncrucial in applications like Large Language Model (LLM) weight compression and\nKV cache compression, where large matrix sizes demand efficient storage\nsolutions.\n  As modern LLMs like GPT-4 and BERT continue to grow, effective matrix\ncompression is increasingly important. These models contain billions of\nparameters in matrix form, making efficient weight quantization essential for\nboth storage and computational efficiency. Similarly, KV caches, storing\nintermediate inference results, are matrix-based and benefit significantly from\noptimized compression techniques.\n  To address the QEM problem in the context of LLM weight and KV cache\ncompression, we propose Quantum Entanglement Trees (QET). QET leverages the\nlocal structure of matrix elements by iteratively swapping elements to create a\nlocally ordered matrix, which is then grouped and quantized column by column.\nTo enhance QET, we introduce two optimizations: residual quantization to\nfurther reduce Mean Squared Error (MSE) and masking with batch processing to\naccelerate the algorithm.\n  Our experiments demonstrate that QET can reduce MSE to 12.3% of its original\nvalue at the same compression ratio, outperforming leading baseline methods.\nOur contributions include framing the QEM problem specifically for LLM and KV\ncache compression, developing the QET algorithm, and implementing optimizations\nthat improve accuracy and processing speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix quantization compresses matrix elements into a more compact form to\nreduce storage requirements, with dequantization enabling reconstruction for\nuse. We define the Quantization Error Minimization (QEM) problem as minimizing\nthe difference between the original and quantized matrices while ensuring the\nquantized matrix remains within fixed memory constraints. This technique is\ncrucial in applications like Large Language Model (LLM) weight compression and\nKV cache compression, where large matrix sizes demand efficient storage\nsolutions.\n  As modern LLMs like GPT-4 and BERT continue to grow, effective matrix\ncompression is increasingly important. These models contain billions of\nparameters in matrix form, making efficient weight quantization essential for\nboth storage and computational efficiency. Similarly, KV caches, storing\nintermediate inference results, are matrix-based and benefit significantly from\noptimized compression techniques.\n  To address the QEM problem in the context of LLM weight and KV cache\ncompression, we propose Quantum Entanglement Trees (QET). QET leverages the\nlocal structure of matrix elements by iteratively swapping elements to create a\nlocally ordered matrix, which is then grouped and quantized column by column.\nTo enhance QET, we introduce two optimizations: residual quantization to\nfurther reduce Mean Squared Error (MSE) and masking with batch processing to\naccelerate the algorithm.\n  Our experiments demonstrate that QET can reduce MSE to 12.3% of its original\nvalue at the same compression ratio, outperforming leading baseline methods.\nOur contributions include framing the QEM problem specifically for LLM and KV\ncache compression, developing the QET algorithm, and implementing optimizations\nthat improve accuracy and processing speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10970v1",
                "updated": "2024-08-20T16:02:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T16:02:54Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "title": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "4 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10746v1",
                "updated": "2024-08-20T11:30:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T11:30:12Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "title": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint."
                },
                "authors": [
                    {
                        "name": "Bei Ouyang"
                    },
                    {
                        "name": "Shengyuan Ye"
                    },
                    {
                        "name": "Liekang Zeng"
                    },
                    {
                        "name": "Tianyi Qian"
                    },
                    {
                        "name": "Jingyi Li"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen",
                "arxiv_comment": "Accepted by The 53rd International Conference on Parallel Processing\n  (ICPP'24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09697v2",
                "updated": "2024-08-20T04:46:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    4,
                    46,
                    18,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T04:43:56Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    43,
                    56,
                    0,
                    232,
                    0
                ],
                "title": "Heta: Distributed Training of Heterogeneous Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heta: Distributed Training of Heterogeneous Graph Neural Networks"
                },
                "summary": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively."
                },
                "authors": [
                    {
                        "name": "Yuchen Zhong"
                    },
                    {
                        "name": "Junwei Su"
                    },
                    {
                        "name": "Chuan Wu"
                    },
                    {
                        "name": "Minjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Minjie Wang"
                },
                "author": "Minjie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10104v1",
                "updated": "2024-08-19T15:47:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T15:47:17Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "title": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory"
                },
                "summary": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging."
                },
                "authors": [
                    {
                        "name": "Olena Tkach"
                    },
                    {
                        "name": "Gerd Schoenhense"
                    }
                ],
                "author_detail": {
                    "name": "Gerd Schoenhense"
                },
                "author": "Gerd Schoenhense",
                "arxiv_comment": "17 pages, 4 figures, 44 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09848v1",
                "updated": "2024-08-19T09:50:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:50:35Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "title": "Abstract Environment Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstract Environment Trimming"
                },
                "summary": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times."
                },
                "authors": [
                    {
                        "name": "Daniel Jurjo-Rivas"
                    },
                    {
                        "name": "Jose F. Morales"
                    },
                    {
                        "name": "Pedro Lpez-Garca"
                    },
                    {
                        "name": "Manuel V. Hermenegildo"
                    }
                ],
                "author_detail": {
                    "name": "Manuel V. Hermenegildo"
                },
                "author": "Manuel V. Hermenegildo",
                "arxiv_comment": "61 pages, 10 figures, 7 tables, submitted to ICLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10284v1",
                "updated": "2024-08-19T03:27:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T03:27:15Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "title": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_doi": "10.1145/3676536.3676741",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676741",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.10284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07092v2",
                "updated": "2024-08-18T17:27:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    17,
                    27,
                    17,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-11T18:40:36Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    40,
                    36,
                    6,
                    224,
                    0
                ],
                "title": "Post-Training Sparse Attention with Double Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Sparse Attention with Double Sparsity"
                },
                "summary": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Lianmin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Lianmin Zheng"
                },
                "author": "Lianmin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09483v1",
                "updated": "2024-08-18T13:54:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-18T13:54:46Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "title": "CMD: A Cache-assisted GPU Memory Deduplication Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMD: A Cache-assisted GPU Memory Deduplication Architecture"
                },
                "summary": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Dan Feng"
                    },
                    {
                        "name": "Wei Tong"
                    },
                    {
                        "name": "Xueliang Wei"
                    },
                    {
                        "name": "Bing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Bing Wu"
                },
                "author": "Bing Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v3",
                "updated": "2024-08-16T08:46:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    46,
                    33,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v1",
                "updated": "2024-08-16T06:11:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v2",
                "updated": "2024-08-16T04:12:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    4,
                    12,
                    25,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v3",
                "updated": "2024-08-15T05:24:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    5,
                    24,
                    19,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07853v1",
                "updated": "2024-08-14T23:42:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T23:42:46Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "title": "A Case for Enabling Delegation of 5G Core Decisions to the RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Case for Enabling Delegation of 5G Core Decisions to the RAN"
                },
                "summary": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation."
                },
                "authors": [
                    {
                        "name": "Lucas Vancina"
                    },
                    {
                        "name": "Geoffrey Xie"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Xie"
                },
                "author": "Geoffrey Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v2",
                "updated": "2024-08-14T09:18:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    18,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07304v1",
                "updated": "2024-08-14T05:42:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T05:42:35Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "title": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption"
                },
                "summary": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS."
                },
                "authors": [
                    {
                        "name": "Jonathan Ly"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Ly"
                },
                "author": "Jonathan Ly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15743v2",
                "updated": "2024-08-13T13:56:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    56,
                    14,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-22T15:42:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    42,
                    59,
                    0,
                    204,
                    0
                ],
                "title": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization"
                },
                "summary": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tlli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tlli"
                },
                "author": "Antti Tlli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04043v3",
                "updated": "2024-08-13T13:31:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    31,
                    34,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-07T18:51:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    18,
                    51,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "Ownership in low-level intermediate representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ownership in low-level intermediate representation"
                },
                "summary": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving."
                },
                "authors": [
                    {
                        "name": "Siddharth Priya"
                    },
                    {
                        "name": "Arie Gurfinkel"
                    }
                ],
                "author_detail": {
                    "name": "Arie Gurfinkel"
                },
                "author": "Arie Gurfinkel",
                "arxiv_comment": "FMCAD 2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v3",
                "updated": "2024-08-13T09:55:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    55,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "to be published in CoLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00167v2",
                "updated": "2024-08-13T09:08:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    8,
                    55,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-31T21:33:56Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    21,
                    33,
                    56,
                    2,
                    213,
                    0
                ],
                "title": "Finch: Prompt-guided Key-Value Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finch: Prompt-guided Key-Value Cache Compression"
                },
                "summary": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "arxiv_comment": "Accepted for publication at TACL - pre-MIT Press publication version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v1",
                "updated": "2024-08-12T08:46:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles"
                },
                "summary": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19895v2",
                "updated": "2024-08-12T07:47:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    47,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-29T11:17:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    11,
                    17,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor"
                },
                "summary": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Luca Valente"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Massimiliano Giacometti"
                    },
                    {
                        "name": "Abdul Basit Sajjad"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "4 pages, 4 figures, DSD2024 and SEAA2024 Works in Progress Session\n  AUG 2024; Updated the acknowledgments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05912v1",
                "updated": "2024-08-12T03:53:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T03:53:51Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "title": "Correct Wrong Path",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correct Wrong Path"
                },
                "summary": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP."
                },
                "authors": [
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Sankara Prasad Ramesh"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Svilen Kanev"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "Daniel A. Jimnez"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "5 pages, 7 Figures, Submited to Computer Architecture Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v2",
                "updated": "2024-08-11T16:35:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    16,
                    35,
                    10,
                    6,
                    224,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "Added Section IV - (performance analysis of proposed HPDA\n  construction). The term 'coding delay' is formally defined (page no. 5). 14\n  pages, 10 figures and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19410v2",
                "updated": "2024-08-11T08:07:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    8,
                    7,
                    28,
                    6,
                    224,
                    0
                ],
                "published": "2024-02-29T18:07:58Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    18,
                    7,
                    58,
                    3,
                    60,
                    0
                ],
                "title": "Genie: Smart ROS-based Caching for Connected Autonomous Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genie: Smart ROS-based Caching for Connected Autonomous Robots"
                },
                "summary": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time."
                },
                "authors": [
                    {
                        "name": "Zexin Li"
                    },
                    {
                        "name": "Soroush Bateni"
                    },
                    {
                        "name": "Cong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Cong Liu"
                },
                "author": "Cong Liu",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v1",
                "updated": "2024-08-10T22:47:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05614v1",
                "updated": "2024-08-10T19:17:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T19:17:46Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "title": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model"
                },
                "summary": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources."
                },
                "authors": [
                    {
                        "name": "Hanqiu Chen"
                    },
                    {
                        "name": "Yitu Wang"
                    },
                    {
                        "name": "Luis Vitorio Cargnini"
                    },
                    {
                        "name": "Mohammadreza Soltaniyeh"
                    },
                    {
                        "name": "Dongyang Li"
                    },
                    {
                        "name": "Gongjin Sun"
                    },
                    {
                        "name": "Pradeep Subedi"
                    },
                    {
                        "name": "Andrew Chang"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Cong Hao"
                    }
                ],
                "author_detail": {
                    "name": "Cong Hao"
                },
                "author": "Cong Hao",
                "arxiv_comment": "This paper is accepted by DAC2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05171v1",
                "updated": "2024-08-09T16:48:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T16:48:01Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "title": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch"
                },
                "summary": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin."
                },
                "authors": [
                    {
                        "name": "R. A. Ryan"
                    },
                    {
                        "name": "P. E. Tsai"
                    },
                    {
                        "name": "A. R. Johansen"
                    },
                    {
                        "name": "A. Youmans"
                    },
                    {
                        "name": "D. P. Higginson"
                    },
                    {
                        "name": "J. M. Mitrani"
                    },
                    {
                        "name": "C. S. Adams"
                    },
                    {
                        "name": "D. A. Sutherland"
                    },
                    {
                        "name": "B. Levitt"
                    },
                    {
                        "name": "U. Shumlak"
                    }
                ],
                "author_detail": {
                    "name": "U. Shumlak"
                },
                "author": "U. Shumlak",
                "arxiv_comment": "16 pages, 11 figures, submitted to Journal of Nuclear Fusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03675v2",
                "updated": "2024-08-08T01:20:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    8,
                    1,
                    20,
                    13,
                    3,
                    221,
                    0
                ],
                "published": "2024-08-07T10:31:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    10,
                    31,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time"
                },
                "summary": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Guoxia Wang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Shiyao Cui"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Dianhai Yu"
                    },
                    {
                        "name": "Hua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wu"
                },
                "author": "Hua Wu",
                "arxiv_comment": "Accepted by ACL 2024 (main conference, long paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.10978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.10978v2",
                "updated": "2024-08-07T23:48:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    23,
                    48,
                    59,
                    2,
                    220,
                    0
                ],
                "published": "2022-10-20T02:58:36Z",
                "published_parsed": [
                    2022,
                    10,
                    20,
                    2,
                    58,
                    36,
                    3,
                    293,
                    0
                ],
                "title": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends"
                },
                "summary": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem."
                },
                "authors": [
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Youyang Qu"
                    },
                    {
                        "name": "Yong Xiang"
                    },
                    {
                        "name": "Md Palash Uddin"
                    },
                    {
                        "name": "Dezhong Peng"
                    },
                    {
                        "name": "Longxiang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Longxiang Gao"
                },
                "author": "Longxiang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.10978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.10978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v1",
                "updated": "2024-08-07T22:10:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference"
                },
                "summary": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v2",
                "updated": "2024-08-07T20:43:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    20,
                    43,
                    10,
                    2,
                    220,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration..",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03652v1",
                "updated": "2024-08-07T09:34:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T09:34:55Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "title": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search"
                },
                "summary": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdou"
                    },
                    {
                        "name": "Tasneem Mohsen"
                    }
                ],
                "author_detail": {
                    "name": "Tasneem Mohsen"
                },
                "author": "Tasneem Mohsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v1",
                "updated": "2024-08-06T17:16:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02999v1",
                "updated": "2024-08-06T07:12:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T07:12:09Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "title": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning"
                },
                "summary": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop."
                },
                "authors": [
                    {
                        "name": "Lekai Chen"
                    },
                    {
                        "name": "Ashutosh Trivedi"
                    },
                    {
                        "name": "Alvaro Velasquez"
                    }
                ],
                "author_detail": {
                    "name": "Alvaro Velasquez"
                },
                "author": "Alvaro Velasquez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02911v1",
                "updated": "2024-08-06T02:51:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T02:51:22Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "title": "NVPC: A Transparent NVM Page Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVPC: A Transparent NVM Page Cache"
                },
                "summary": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases."
                },
                "authors": [
                    {
                        "name": "Guoyu Wang"
                    },
                    {
                        "name": "Xilong Che"
                    },
                    {
                        "name": "Haoyang Wei"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Puyi He"
                    },
                    {
                        "name": "Juncheng Hu"
                    }
                ],
                "author_detail": {
                    "name": "Juncheng Hu"
                },
                "author": "Juncheng Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02409v1",
                "updated": "2024-08-05T12:09:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T12:09:50Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "title": "Electron-beam-induced modification of gold microparticles in an SEM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced modification of gold microparticles in an SEM"
                },
                "summary": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings."
                },
                "authors": [
                    {
                        "name": "Kristina Weinel"
                    },
                    {
                        "name": "Marc Benjamin Hahn"
                    },
                    {
                        "name": "Axel Lubk"
                    },
                    {
                        "name": "Wen Feng"
                    },
                    {
                        "name": "Ignacio Gonzalez Martinez"
                    },
                    {
                        "name": "Bernd Bchner"
                    },
                    {
                        "name": "Leonardo Agudo Jcome"
                    }
                ],
                "author_detail": {
                    "name": "Leonardo Agudo Jcome"
                },
                "author": "Leonardo Agudo Jcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05235v1",
                "updated": "2024-08-05T09:07:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T09:07:06Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "title": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving"
                },
                "summary": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server."
                },
                "authors": [
                    {
                        "name": "Andreas Kosmas Kakolyris"
                    },
                    {
                        "name": "Dimosthenis Masouros"
                    },
                    {
                        "name": "Petros Vavaroutsos"
                    },
                    {
                        "name": "Sotirios Xydis"
                    },
                    {
                        "name": "Dimitrios Soudris"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Soudris"
                },
                "author": "Dimitrios Soudris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11912v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11912v3",
                "updated": "2024-08-04T00:58:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    58,
                    4,
                    6,
                    217,
                    0
                ],
                "published": "2024-04-18T05:25:54Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    5,
                    25,
                    54,
                    3,
                    109,
                    0
                ],
                "title": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding"
                },
                "summary": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11912v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11912v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01890v1",
                "updated": "2024-08-04T00:38:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "published": "2024-08-04T00:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "title": "Cross-layer Attention Sharing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-layer Attention Sharing for Large Language Models"
                },
                "summary": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B."
                },
                "authors": [
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yuzhang Wu"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hengyu Li"
                    },
                    {
                        "name": "Qiaozhi He"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Working in process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01519v1",
                "updated": "2024-08-02T18:25:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-02T18:25:57Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "title": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling"
                },
                "summary": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition."
                },
                "authors": [
                    {
                        "name": "Xiao Jiang"
                    },
                    {
                        "name": "Grace J. Gang"
                    },
                    {
                        "name": "J. Webster Stayman"
                    }
                ],
                "author_detail": {
                    "name": "J. Webster Stayman"
                },
                "author": "J. Webster Stayman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00327v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00327v2",
                "updated": "2024-08-02T07:37:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    7,
                    37,
                    51,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-01T07:00:18Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    0,
                    18,
                    3,
                    214,
                    0
                ],
                "title": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration"
                },
                "summary": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Yuan-Hao Chang"
                    },
                    {
                        "name": "Tei-Wei Kuo"
                    }
                ],
                "author_detail": {
                    "name": "Tei-Wei Kuo"
                },
                "author": "Tei-Wei Kuo",
                "arxiv_comment": "This paper has been accepted for presentation at the The\n  International Conference on Hardware/Software Codesign and System Synthesis\n  (CODES+ISSS) in September, 2024. An extended abstract of this paper was\n  presented in Design, Automation & Test in Europe Conference & Exhibition\n  (DATE), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00327v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00327v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00957v1",
                "updated": "2024-08-01T23:52:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T23:52:43Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "title": "Caching Aided Multi-Tenant Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Aided Multi-Tenant Serverless Computing"
                },
                "summary": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead."
                },
                "authors": [
                    {
                        "name": "Chu Qiao"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Zhenkai Zhang"
                    },
                    {
                        "name": "Yuede Ji"
                    },
                    {
                        "name": "Xing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xing Gao"
                },
                "author": "Xing Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00859v2",
                "updated": "2024-08-01T21:21:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    21,
                    21,
                    28,
                    3,
                    214,
                    0
                ],
                "published": "2024-04-01T02:01:28Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    2,
                    1,
                    28,
                    0,
                    92,
                    0
                ],
                "title": "Do language models plan ahead for future tokens?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do language models plan ahead for future tokens?"
                },
                "summary": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale."
                },
                "authors": [
                    {
                        "name": "Wilson Wu"
                    },
                    {
                        "name": "John X. Morris"
                    },
                    {
                        "name": "Lionel Levine"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Levine"
                },
                "author": "Lionel Levine",
                "arxiv_comment": "24 pages, 11 figures. Camera-ready for COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00539v1",
                "updated": "2024-08-01T13:22:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T13:22:01Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs"
                },
                "summary": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance."
                },
                "authors": [
                    {
                        "name": "Mingcong Lu"
                    },
                    {
                        "name": "Jiangcai Zhu"
                    },
                    {
                        "name": "Wang Hao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Shusheng Zhang"
                    },
                    {
                        "name": "Kailai Shao"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Nan Li"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Xin Lu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Lu"
                },
                "author": "Xin Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14361v2",
                "updated": "2024-08-01T13:21:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    21,
                    24,
                    3,
                    214,
                    0
                ],
                "published": "2024-01-25T18:07:50Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    7,
                    50,
                    3,
                    25,
                    0
                ],
                "title": "MoE-Infinity: Offloading-Efficient MoE Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Infinity: Offloading-Efficient MoE Model Serving"
                },
                "summary": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity"
                },
                "authors": [
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Mahesh Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Marina"
                },
                "author": "Mahesh Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15220v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15220v4",
                "updated": "2024-08-01T07:51:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    51,
                    25,
                    3,
                    214,
                    0
                ],
                "published": "2024-02-23T09:29:19Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    9,
                    29,
                    19,
                    4,
                    54,
                    0
                ],
                "title": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition"
                },
                "summary": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096."
                },
                "authors": [
                    {
                        "name": "Lu Ye"
                    },
                    {
                        "name": "Ze Tao"
                    },
                    {
                        "name": "Yong Huang"
                    },
                    {
                        "name": "Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yang Li"
                },
                "author": "Yang Li",
                "arxiv_comment": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15220v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15220v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00232v1",
                "updated": "2024-08-01T01:57:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T01:57:09Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "title": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction"
                },
                "summary": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks."
                },
                "authors": [
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Zite Jiang"
                    },
                    {
                        "name": "Haihang You"
                    }
                ],
                "author_detail": {
                    "name": "Haihang You"
                },
                "author": "Haihang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v2",
                "updated": "2024-08-01T00:41:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    0,
                    41,
                    52,
                    3,
                    214,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Towards Variable-Length In-Network Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Variable-Length In-Network Caching"
                },
                "summary": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20485v2",
                "updated": "2024-07-31T02:02:40Z",
                "updated_parsed": [
                    2024,
                    7,
                    31,
                    2,
                    2,
                    40,
                    2,
                    213,
                    0
                ],
                "published": "2024-07-30T01:13:42Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    1,
                    13,
                    42,
                    1,
                    212,
                    0
                ],
                "title": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder"
                },
                "summary": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot."
                },
                "authors": [
                    {
                        "name": "Hyun-rae Jo"
                    },
                    {
                        "name": "Dongkun Shin"
                    }
                ],
                "author_detail": {
                    "name": "Dongkun Shin"
                },
                "author": "Dongkun Shin",
                "arxiv_comment": "11 pages(9 pages + reference 2 pages), 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21201v1",
                "updated": "2024-07-30T21:27:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T21:27:00Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "title": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite"
                },
                "summary": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE"
                },
                "authors": [
                    {
                        "name": "Abdulkarim A. Amirov"
                    },
                    {
                        "name": "Maksim A. Koliushenkov"
                    },
                    {
                        "name": "Abdula A. Mukhuchev"
                    },
                    {
                        "name": "Dibir M. Yusupov"
                    },
                    {
                        "name": "Valeriya V. Govorina"
                    },
                    {
                        "name": "Dmitriy S. Neznakhin"
                    },
                    {
                        "name": "Gennady A. Govor"
                    },
                    {
                        "name": "Akhmed M. Aliev"
                    }
                ],
                "author_detail": {
                    "name": "Akhmed M. Aliev"
                },
                "author": "Akhmed M. Aliev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v1",
                "updated": "2024-07-30T18:19:38Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v1",
                "updated": "2024-07-30T17:59:08Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.06944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.06944v2",
                "updated": "2024-07-30T13:06:36Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    13,
                    6,
                    36,
                    1,
                    212,
                    0
                ],
                "published": "2023-04-14T06:21:57Z",
                "published_parsed": [
                    2023,
                    4,
                    14,
                    6,
                    21,
                    57,
                    4,
                    104,
                    0
                ],
                "title": "SpChar: Characterizing the Sparse Puzzle via Decision Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpChar: Characterizing the Sparse Puzzle via Decision Trees"
                },
                "summary": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied."
                },
                "authors": [
                    {
                        "name": "Francesco Sgherzi"
                    },
                    {
                        "name": "Marco Siracusa"
                    },
                    {
                        "name": "Ivan Fernandez"
                    },
                    {
                        "name": "Adri Armejach"
                    },
                    {
                        "name": "Miquel Moret"
                    }
                ],
                "author_detail": {
                    "name": "Miquel Moret"
                },
                "author": "Miquel Moret",
                "arxiv_comment": "27 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.06944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.06944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20773v1",
                "updated": "2024-07-30T12:16:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T12:16:39Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "title": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications"
                },
                "summary": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability."
                },
                "authors": [
                    {
                        "name": "Andronicus Rajasukumar"
                    },
                    {
                        "name": "Jiya Su"
                    },
                    {
                        "name": "Yuqing"
                    },
                    {
                        "name": "Wang"
                    },
                    {
                        "name": "Tianshuo Su"
                    },
                    {
                        "name": "Marziyeh Nourian"
                    },
                    {
                        "name": "Jose M Monsalve Diaz"
                    },
                    {
                        "name": "Tianchi Zhang"
                    },
                    {
                        "name": "Jianru Ding"
                    },
                    {
                        "name": "Wenyi Wang"
                    },
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Moubarak Jeje"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Yanjing Li"
                    },
                    {
                        "name": "Andrew A. Chien"
                    }
                ],
                "author_detail": {
                    "name": "Andrew A. Chien"
                },
                "arxiv_affiliation": "Ivy",
                "author": "Andrew A. Chien",
                "arxiv_comment": "14 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14928v3",
                "updated": "2024-07-30T08:39:52Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    39,
                    52,
                    1,
                    212,
                    0
                ],
                "published": "2023-09-26T13:35:31Z",
                "published_parsed": [
                    2023,
                    9,
                    26,
                    13,
                    35,
                    31,
                    1,
                    269,
                    0
                ],
                "title": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models"
                },
                "summary": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Eman Ali"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Haris Khan"
                },
                "author": "Muhammad Haris Khan",
                "arxiv_comment": "Accepted at BMVC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.14928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03088v2",
                "updated": "2024-07-30T08:19:53Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    19,
                    53,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-03T22:03:28Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    22,
                    3,
                    28,
                    2,
                    94,
                    0
                ],
                "title": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation"
                },
                "summary": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation."
                },
                "authors": [
                    {
                        "name": "Zexin Fang"
                    },
                    {
                        "name": "Bin Han"
                    },
                    {
                        "name": "Hans D. Schotten"
                    }
                ],
                "author_detail": {
                    "name": "Hans D. Schotten"
                },
                "author": "Hans D. Schotten",
                "arxiv_comment": "Submitted to IEEE GLOBECOM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16219v3",
                "updated": "2024-07-30T04:01:25Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    4,
                    1,
                    25,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-24T21:35:12Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    21,
                    35,
                    12,
                    2,
                    115,
                    0
                ],
                "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)"
                },
                "summary": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU."
                },
                "authors": [
                    {
                        "name": "Ziyue Qiu"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Mor Harchol-Balter"
                    }
                ],
                "author_detail": {
                    "name": "Mor Harchol-Balter"
                },
                "author": "Mor Harchol-Balter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19637v1",
                "updated": "2024-07-29T01:43:26Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:43:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "STT-RAM-based Hierarchical In-Memory Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STT-RAM-based Hierarchical In-Memory Computing"
                },
                "summary": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kevin Antony Gomez"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/TPDS.2024.3430853",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TPDS.2024.3430853",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: IEEE Transactions on Parallel and Distributed Systems (\n  Volume: 35, Issue: 9, September 2024)",
                "arxiv_journal_ref": "IEEE Transactions on Parallel and Distributed Systems, vol. 35,\n  no. 9, pp. 1615-1629, Sept. 2024",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19627v1",
                "updated": "2024-07-29T01:17:54Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:17:54Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "title": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing"
                },
                "summary": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    },
                    {
                        "name": "Kevin Gomez"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Gomez"
                },
                "author": "Kevin Gomez",
                "arxiv_comment": "Accepted in 35th IEEE International Conference on\n  Application-specific Systems, Architectures and Processors (ASAP 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19612v1",
                "updated": "2024-07-28T23:43:59Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T23:43:59Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "title": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors"
                },
                "summary": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1145/3357526.3357553",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3357526.3357553",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the international symposium on memory systems, pp.\n  439-450. 2019",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2406.19437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19437v2",
                "updated": "2024-09-04T17:59:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    59,
                    53,
                    2,
                    248,
                    0
                ],
                "published": "2024-06-27T18:00:00Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    18,
                    0,
                    0,
                    3,
                    179,
                    0
                ],
                "title": "pop-cosmos: Scaleable inference of galaxy properties and redshifts with\n  a data-driven population model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "pop-cosmos: Scaleable inference of galaxy properties and redshifts with\n  a data-driven population model"
                },
                "summary": "We present an efficient Bayesian method for estimating individual photometric\nredshifts and galaxy properties under a pre-trained population model\n(pop-cosmos) that was calibrated using purely photometric data. This model\nspecifies a prior distribution over 16 stellar population synthesis (SPS)\nparameters using a score-based diffusion model, and includes a data model with\ndetailed treatment of nebular emission. We use a GPU-accelerated affine\ninvariant ensemble sampler to achieve fast posterior sampling under this model\nfor 292,300 individual galaxies in the COSMOS2020 catalog, leveraging a neural\nnetwork emulator (Speculator) to speed up the SPS calculations. We apply both\nthe pop-cosmos population model and a baseline prior inspired by\nProspector-$\\alpha$, and compare these results to published COSMOS2020 redshift\nestimates from the widely-used EAZY and LePhare codes. For the $\\sim 12,000$\ngalaxies with spectroscopic redshifts, we find that pop-cosmos yields redshift\nestimates that have minimal bias ($\\sim10^{-4}$), high accuracy\n($\\sigma_\\text{MAD}=7\\times10^{-3}$), and a low outlier rate ($1.6\\%$). We show\nthat the pop-cosmos population model generalizes well to galaxies fainter than\nits $r<25$ mag training set. The sample we have analyzed is $\\gtrsim3\\times$\nlarger than has previously been possible via posterior sampling with a full SPS\nmodel, with average throughput of 15 GPU-sec per galaxy under the pop-cosmos\nprior, and 0.6 GPU-sec per galaxy under the Prospector prior. This paves the\nway for principled modeling of the huge catalogs expected from upcoming Stage\nIV galaxy surveys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an efficient Bayesian method for estimating individual photometric\nredshifts and galaxy properties under a pre-trained population model\n(pop-cosmos) that was calibrated using purely photometric data. This model\nspecifies a prior distribution over 16 stellar population synthesis (SPS)\nparameters using a score-based diffusion model, and includes a data model with\ndetailed treatment of nebular emission. We use a GPU-accelerated affine\ninvariant ensemble sampler to achieve fast posterior sampling under this model\nfor 292,300 individual galaxies in the COSMOS2020 catalog, leveraging a neural\nnetwork emulator (Speculator) to speed up the SPS calculations. We apply both\nthe pop-cosmos population model and a baseline prior inspired by\nProspector-$\\alpha$, and compare these results to published COSMOS2020 redshift\nestimates from the widely-used EAZY and LePhare codes. For the $\\sim 12,000$\ngalaxies with spectroscopic redshifts, we find that pop-cosmos yields redshift\nestimates that have minimal bias ($\\sim10^{-4}$), high accuracy\n($\\sigma_\\text{MAD}=7\\times10^{-3}$), and a low outlier rate ($1.6\\%$). We show\nthat the pop-cosmos population model generalizes well to galaxies fainter than\nits $r<25$ mag training set. The sample we have analyzed is $\\gtrsim3\\times$\nlarger than has previously been possible via posterior sampling with a full SPS\nmodel, with average throughput of 15 GPU-sec per galaxy under the pop-cosmos\nprior, and 0.6 GPU-sec per galaxy under the Prospector prior. This paves the\nway for principled modeling of the huge catalogs expected from upcoming Stage\nIV galaxy surveys."
                },
                "authors": [
                    {
                        "name": "Stephen Thorp"
                    },
                    {
                        "name": "Justin Alsing"
                    },
                    {
                        "name": "Hiranya V. Peiris"
                    },
                    {
                        "name": "Sinan Deger"
                    },
                    {
                        "name": "Daniel J. Mortlock"
                    },
                    {
                        "name": "Boris Leistedt"
                    },
                    {
                        "name": "Joel Leja"
                    },
                    {
                        "name": "Arthur Loureiro"
                    }
                ],
                "author_detail": {
                    "name": "Arthur Loureiro"
                },
                "author": "Arthur Loureiro",
                "arxiv_comment": "24 pages, 15 figures. Accepted for publication in ApJ. Catalog of\n  redshifts and galaxy properties available on Zenodo at\n  https://zenodo.org/doi/10.5281/zenodo.13627488",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13989v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13989v3",
                "updated": "2024-09-04T17:52:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    52,
                    37,
                    2,
                    248,
                    0
                ],
                "published": "2024-07-19T02:34:10Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    2,
                    34,
                    10,
                    4,
                    201,
                    0
                ],
                "title": "Enhancing Graph Neural Networks with Limited Labeled Data by Actively\n  Distilling Knowledge from Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Graph Neural Networks with Limited Labeled Data by Actively\n  Distilling Knowledge from Large Language Models"
                },
                "summary": "Graphs are pervasive in the real-world, such as social network analysis,\nbioinformatics, and knowledge graphs. Graph neural networks (GNNs) have great\nability in node classification, a fundamental task on graphs. Unfortunately,\nconventional GNNs still face challenges in scenarios with few labeled nodes,\ndespite the prevalence of few-shot node classification tasks in real-world\napplications. To address this challenge, various approaches have been proposed,\nincluding graph meta-learning, transfer learning, and methods based on Large\nLanguage Models (LLMs). However, traditional meta-learning and transfer\nlearning methods often require prior knowledge from base classes or fail to\nexploit the potential advantages of unlabeled nodes. Meanwhile, LLM-based\nmethods may overlook the zero-shot capabilities of LLMs and rely heavily on the\nquality of generated contexts. In this paper, we propose a novel approach that\nintegrates LLMs and GNNs, leveraging the zero-shot inference and reasoning\ncapabilities of LLMs and employing a Graph-LLM-based active learning paradigm\nto enhance GNNs' performance. Extensive experiments demonstrate the\neffectiveness of our model in improving node classification accuracy with\nconsiderably limited labeled data, surpassing state-of-the-art baselines by\nsignificant margins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphs are pervasive in the real-world, such as social network analysis,\nbioinformatics, and knowledge graphs. Graph neural networks (GNNs) have great\nability in node classification, a fundamental task on graphs. Unfortunately,\nconventional GNNs still face challenges in scenarios with few labeled nodes,\ndespite the prevalence of few-shot node classification tasks in real-world\napplications. To address this challenge, various approaches have been proposed,\nincluding graph meta-learning, transfer learning, and methods based on Large\nLanguage Models (LLMs). However, traditional meta-learning and transfer\nlearning methods often require prior knowledge from base classes or fail to\nexploit the potential advantages of unlabeled nodes. Meanwhile, LLM-based\nmethods may overlook the zero-shot capabilities of LLMs and rely heavily on the\nquality of generated contexts. In this paper, we propose a novel approach that\nintegrates LLMs and GNNs, leveraging the zero-shot inference and reasoning\ncapabilities of LLMs and employing a Graph-LLM-based active learning paradigm\nto enhance GNNs' performance. Extensive experiments demonstrate the\neffectiveness of our model in improving node classification accuracy with\nconsiderably limited labeled data, surpassing state-of-the-art baselines by\nsignificant margins."
                },
                "authors": [
                    {
                        "name": "Quan Li"
                    },
                    {
                        "name": "Tianxiang Zhao"
                    },
                    {
                        "name": "Lingwei Chen"
                    },
                    {
                        "name": "Junjie Xu"
                    },
                    {
                        "name": "Suhang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Suhang Wang"
                },
                "author": "Suhang Wang",
                "arxiv_comment": "10 pages, 3 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13989v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13989v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02912v1",
                "updated": "2024-09-04T17:51:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    51,
                    18,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T17:51:18Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    51,
                    18,
                    2,
                    248,
                    0
                ],
                "title": "Design of a Standard-Compliant Real-Time Neural Receiver for 5G NR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design of a Standard-Compliant Real-Time Neural Receiver for 5G NR"
                },
                "summary": "We detail the steps required to deploy a multi-user multiple-input\nmultiple-output (MU-MIMO) neural receiver (NRX) in an actual cellular\ncommunication system. This raises several exciting research challenges,\nincluding the need for real-time inference and compatibility with the 5G NR\nstandard. As the network configuration in a practical setup can change\ndynamically within milliseconds, we propose an adaptive NRX architecture\ncapable of supporting dynamic modulation and coding scheme (MCS) configurations\nwithout the need for any re-training and without additional inference cost. We\noptimize the latency of the neural network (NN) architecture to achieve\ninference times of less than 1ms on an NVIDIA A100 GPU using the TensorRT\ninference library. These latency constraints effectively limit the size of the\nNN and we quantify the resulting signal-to-noise ratio (SNR) degradation as\nless than 0.7 dB when compared to a preliminary non-real-time NRX architecture.\nFinally, we explore the potential for site-specific adaptation of the receiver\nby investigating the required size of the training dataset and the number of\nfine-tuning iterations to optimize the NRX for specific radio environments\nusing a ray tracing-based channel model. The resulting NRX is ready for\ndeployment in a real-time 5G NR system and the source code including the\nTensorRT experiments is available online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We detail the steps required to deploy a multi-user multiple-input\nmultiple-output (MU-MIMO) neural receiver (NRX) in an actual cellular\ncommunication system. This raises several exciting research challenges,\nincluding the need for real-time inference and compatibility with the 5G NR\nstandard. As the network configuration in a practical setup can change\ndynamically within milliseconds, we propose an adaptive NRX architecture\ncapable of supporting dynamic modulation and coding scheme (MCS) configurations\nwithout the need for any re-training and without additional inference cost. We\noptimize the latency of the neural network (NN) architecture to achieve\ninference times of less than 1ms on an NVIDIA A100 GPU using the TensorRT\ninference library. These latency constraints effectively limit the size of the\nNN and we quantify the resulting signal-to-noise ratio (SNR) degradation as\nless than 0.7 dB when compared to a preliminary non-real-time NRX architecture.\nFinally, we explore the potential for site-specific adaptation of the receiver\nby investigating the required size of the training dataset and the number of\nfine-tuning iterations to optimize the NRX for specific radio environments\nusing a ray tracing-based channel model. The resulting NRX is ready for\ndeployment in a real-time 5G NR system and the source code including the\nTensorRT experiments is available online."
                },
                "authors": [
                    {
                        "name": "Reinhard Wiesmayr"
                    },
                    {
                        "name": "Sebastian Cammerer"
                    },
                    {
                        "name": "Fayal At Aoudia"
                    },
                    {
                        "name": "Jakob Hoydis"
                    },
                    {
                        "name": "Jakub Zakrzewski"
                    },
                    {
                        "name": "Alexander Keller"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Keller"
                },
                "author": "Alexander Keller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02897v2",
                "updated": "2024-09-05T03:53:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    3,
                    53,
                    13,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-04T17:41:19Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    41,
                    19,
                    2,
                    248,
                    0
                ],
                "title": "LongCite: Enabling LLMs to Generate Fine-grained Citations in\n  Long-context QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongCite: Enabling LLMs to Generate Fine-grained Citations in\n  Long-context QA"
                },
                "summary": "Though current long-context large language models (LLMs) have demonstrated\nimpressive capacities in answering user questions based on extensive text, the\nlack of citations in their responses makes user verification difficult, leading\nto concerns about their trustworthiness due to their potential hallucinations.\nIn this work, we aim to enable long-context LLMs to generate responses with\nfine-grained sentence-level citations, improving their faithfulness and\nverifiability. We first introduce LongBench-Cite, an automated benchmark for\nassessing current LLMs' performance in Long-Context Question Answering with\nCitations (LQAC), revealing considerable room for improvement. To this end, we\npropose CoF (Coarse to Fine), a novel pipeline that utilizes off-the-shelf LLMs\nto automatically generate long-context QA instances with precise sentence-level\ncitations, and leverage this pipeline to construct LongCite-45k, a large-scale\nSFT dataset for LQAC. Finally, we train LongCite-8B and LongCite-9B using the\nLongCite-45k dataset, successfully enabling their generation of accurate\nresponses and fine-grained sentence-level citations in a single output. The\nevaluation results on LongBench-Cite show that our trained models achieve\nstate-of-the-art citation quality, surpassing advanced proprietary models\nincluding GPT-4o.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Though current long-context large language models (LLMs) have demonstrated\nimpressive capacities in answering user questions based on extensive text, the\nlack of citations in their responses makes user verification difficult, leading\nto concerns about their trustworthiness due to their potential hallucinations.\nIn this work, we aim to enable long-context LLMs to generate responses with\nfine-grained sentence-level citations, improving their faithfulness and\nverifiability. We first introduce LongBench-Cite, an automated benchmark for\nassessing current LLMs' performance in Long-Context Question Answering with\nCitations (LQAC), revealing considerable room for improvement. To this end, we\npropose CoF (Coarse to Fine), a novel pipeline that utilizes off-the-shelf LLMs\nto automatically generate long-context QA instances with precise sentence-level\ncitations, and leverage this pipeline to construct LongCite-45k, a large-scale\nSFT dataset for LQAC. Finally, we train LongCite-8B and LongCite-9B using the\nLongCite-45k dataset, successfully enabling their generation of accurate\nresponses and fine-grained sentence-level citations in a single output. The\nevaluation results on LongBench-Cite show that our trained models achieve\nstate-of-the-art citation quality, surpassing advanced proprietary models\nincluding GPT-4o."
                },
                "authors": [
                    {
                        "name": "Jiajie Zhang"
                    },
                    {
                        "name": "Yushi Bai"
                    },
                    {
                        "name": "Xin Lv"
                    },
                    {
                        "name": "Wanjun Gu"
                    },
                    {
                        "name": "Danqing Liu"
                    },
                    {
                        "name": "Minhao Zou"
                    },
                    {
                        "name": "Shulin Cao"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Ling Feng"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02894v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02894v1",
                "updated": "2024-09-04T17:38:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    38,
                    13,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T17:38:13Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    38,
                    13,
                    2,
                    248,
                    0
                ],
                "title": "Follow the Mass -- A Concordance Picture of Tidal Disruption Events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow the Mass -- A Concordance Picture of Tidal Disruption Events"
                },
                "summary": "Three recent global simulations of tidal disruption events (TDEs) have\nproduced, using different numerical techniques and parameters, very similar\npictures of their dynamics. In typical TDEs, after the star is disrupted by a\nsupermassive black hole, the bound portion of the stellar debris follows highly\neccentric trajectories, reaching apocenters of several thousand gravitational\nradii. Only a very small fraction is captured upon returning to the vicinity of\nthe supermassive black hole. Nearly all the debris returns to the apocenter,\nwhere shocks produce a thick irregular cloud on this radial scale and power the\noptical/UV flare. These simulation results imply that over a few years, the\nthick cloud settles into an accretion flow responsible for the long term\nemission. Despite not being designed to match observations, the dynamical\npicture given by the three simulations aligns well with observations of typical\nevents, correctly predicting the flares' total radiated energy, luminosity,\ntemperature and emission line width. On the basis of these predictions, we\nprovide an updated method ({\\sc TDEmass}) to infer the stellar and black hole\nmasses from a flare's peak luminosity and temperature. This picture also\ncorrectly predicts the luminosity observed years after the flare. In addition,\nwe show that in a magnitude-limited survey, if the intrinsic rate of TDEs is\nindependent of black hole mass, the detected events will preferentially have\nblack hole masses $\\sim 10^{6 \\pm 0.3} M_\\odot$ and stellar masses of $\\sim\n1-1.5 M_\\odot$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Three recent global simulations of tidal disruption events (TDEs) have\nproduced, using different numerical techniques and parameters, very similar\npictures of their dynamics. In typical TDEs, after the star is disrupted by a\nsupermassive black hole, the bound portion of the stellar debris follows highly\neccentric trajectories, reaching apocenters of several thousand gravitational\nradii. Only a very small fraction is captured upon returning to the vicinity of\nthe supermassive black hole. Nearly all the debris returns to the apocenter,\nwhere shocks produce a thick irregular cloud on this radial scale and power the\noptical/UV flare. These simulation results imply that over a few years, the\nthick cloud settles into an accretion flow responsible for the long term\nemission. Despite not being designed to match observations, the dynamical\npicture given by the three simulations aligns well with observations of typical\nevents, correctly predicting the flares' total radiated energy, luminosity,\ntemperature and emission line width. On the basis of these predictions, we\nprovide an updated method ({\\sc TDEmass}) to infer the stellar and black hole\nmasses from a flare's peak luminosity and temperature. This picture also\ncorrectly predicts the luminosity observed years after the flare. In addition,\nwe show that in a magnitude-limited survey, if the intrinsic rate of TDEs is\nindependent of black hole mass, the detected events will preferentially have\nblack hole masses $\\sim 10^{6 \\pm 0.3} M_\\odot$ and stellar masses of $\\sim\n1-1.5 M_\\odot$."
                },
                "authors": [
                    {
                        "name": "Julian Krolik"
                    },
                    {
                        "name": "Tsvi Piran"
                    },
                    {
                        "name": "Taeho Ryu"
                    }
                ],
                "author_detail": {
                    "name": "Taeho Ryu"
                },
                "author": "Taeho Ryu",
                "arxiv_comment": "20 pages, 4 figures, comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02894v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02894v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07832v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07832v3",
                "updated": "2024-09-04T17:31:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    31,
                    0,
                    2,
                    248,
                    0
                ],
                "published": "2024-07-31T14:49:35Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    49,
                    35,
                    2,
                    213,
                    0
                ],
                "title": "LADDER: Language Driven Slice Discovery and Error Rectification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LADDER: Language Driven Slice Discovery and Error Rectification"
                },
                "summary": "Error slice discovery associates structured patterns with model errors.\nExisting methods discover error slices by clustering the error-prone samples\nwith similar patterns or assigning discrete attributes to each sample for\npost-hoc analysis. While these methods aim for interpretability and easier\nmitigation through reweighting or rebalancing, they may not capture the full\ncomplexity of error patterns due to incomplete or missing attributes. Contrary\nto the existing approach, this paper utilizes the reasoning capabilities of the\nLarge Language Model (LLM) to analyze complex error patterns and generate\ntestable hypotheses. This paper proposes LADDER: Language Driven slice\nDiscovery and Error Rectification. It first projects the model's representation\ninto a language-aligned feature space (eg CLIP) to preserve semantics in the\noriginal model feature space. This ensures the accurate retrieval of sentences\nthat highlight the model's errors. Next, the LLM utilizes the sentences and\ngenerates hypotheses to discover error slices. Finally, we mitigate the error\nby fine-tuning the classification head by creating a group-balanced dataset\nusing the hypotheses. Our entire method does not require any attribute\nannotation, either explicitly or through external tagging models. We validate\nour method with \\textbf{five} image classification datasets. The code is\navailable (https://github.com/batmanlab/Ladder).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Error slice discovery associates structured patterns with model errors.\nExisting methods discover error slices by clustering the error-prone samples\nwith similar patterns or assigning discrete attributes to each sample for\npost-hoc analysis. While these methods aim for interpretability and easier\nmitigation through reweighting or rebalancing, they may not capture the full\ncomplexity of error patterns due to incomplete or missing attributes. Contrary\nto the existing approach, this paper utilizes the reasoning capabilities of the\nLarge Language Model (LLM) to analyze complex error patterns and generate\ntestable hypotheses. This paper proposes LADDER: Language Driven slice\nDiscovery and Error Rectification. It first projects the model's representation\ninto a language-aligned feature space (eg CLIP) to preserve semantics in the\noriginal model feature space. This ensures the accurate retrieval of sentences\nthat highlight the model's errors. Next, the LLM utilizes the sentences and\ngenerates hypotheses to discover error slices. Finally, we mitigate the error\nby fine-tuning the classification head by creating a group-balanced dataset\nusing the hypotheses. Our entire method does not require any attribute\nannotation, either explicitly or through external tagging models. We validate\nour method with \\textbf{five} image classification datasets. The code is\navailable (https://github.com/batmanlab/Ladder)."
                },
                "authors": [
                    {
                        "name": "Shantanu Ghosh"
                    },
                    {
                        "name": "Rayan Syed"
                    },
                    {
                        "name": "Chenyu Wang"
                    },
                    {
                        "name": "Clare B. Poynton"
                    },
                    {
                        "name": "Kayhan Batmanghelich"
                    }
                ],
                "author_detail": {
                    "name": "Kayhan Batmanghelich"
                },
                "author": "Kayhan Batmanghelich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07832v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07832v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02889v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02889v1",
                "updated": "2024-09-04T17:25:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    25,
                    21,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T17:25:21Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    25,
                    21,
                    2,
                    248,
                    0
                ],
                "title": "LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via\n  Hybrid Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via\n  Hybrid Architecture"
                },
                "summary": "Expanding the long-context capabilities of Multi-modal Large Language\nModels~(MLLMs) is crucial for video understanding, high-resolution image\nunderstanding, and multi-modal agents. This involves a series of systematic\noptimizations, including model architecture, data construction and training\nstrategy, particularly addressing challenges such as \\textit{degraded\nperformance with more images} and \\textit{high computational costs}. In this\npaper, we adapt the model architecture to a hybrid of Mamba and Transformer\nblocks, approach data construction with both temporal and spatial dependencies\namong multiple images and employ a progressive training strategy. The released\nmodel \\textbf{LongLLaVA}~(\\textbf{Long}-Context \\textbf{L}arge\n\\textbf{L}anguage \\textbf{a}nd \\textbf{V}ision \\textbf{A}ssistant) is the first\nhybrid MLLM, which achieved a better balance between efficiency and\neffectiveness. LongLLaVA not only achieves competitive results across various\nbenchmarks, but also maintains high throughput and low memory consumption.\nEspecially, it could process nearly a thousand images on a single A100 80GB\nGPU, showing promising application prospects for a wide range of tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expanding the long-context capabilities of Multi-modal Large Language\nModels~(MLLMs) is crucial for video understanding, high-resolution image\nunderstanding, and multi-modal agents. This involves a series of systematic\noptimizations, including model architecture, data construction and training\nstrategy, particularly addressing challenges such as \\textit{degraded\nperformance with more images} and \\textit{high computational costs}. In this\npaper, we adapt the model architecture to a hybrid of Mamba and Transformer\nblocks, approach data construction with both temporal and spatial dependencies\namong multiple images and employ a progressive training strategy. The released\nmodel \\textbf{LongLLaVA}~(\\textbf{Long}-Context \\textbf{L}arge\n\\textbf{L}anguage \\textbf{a}nd \\textbf{V}ision \\textbf{A}ssistant) is the first\nhybrid MLLM, which achieved a better balance between efficiency and\neffectiveness. LongLLaVA not only achieves competitive results across various\nbenchmarks, but also maintains high throughput and low memory consumption.\nEspecially, it could process nearly a thousand images on a single A100 80GB\nGPU, showing promising application prospects for a wide range of tasks."
                },
                "authors": [
                    {
                        "name": "Xidong Wang"
                    },
                    {
                        "name": "Dingjie Song"
                    },
                    {
                        "name": "Shunian Chen"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "arxiv_comment": "19 pages, 7 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02889v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05790v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05790v2",
                "updated": "2024-09-04T17:23:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    23,
                    39,
                    2,
                    248,
                    0
                ],
                "published": "2024-07-08T09:52:46Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    9,
                    52,
                    46,
                    0,
                    190,
                    0
                ],
                "title": "Kinetic Interacting Particle Langevin Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kinetic Interacting Particle Langevin Monte Carlo"
                },
                "summary": "This paper introduces and analyses interacting underdamped Langevin\nalgorithms, termed Kinetic Interacting Particle Langevin Monte Carlo (KIPLMC)\nmethods, for statistical inference in latent variable models. We propose a\ndiffusion process that evolves jointly in the space of parameters and latent\nvariables and exploit the fact that the stationary distribution of this\ndiffusion concentrates around the maximum marginal likelihood estimate of the\nparameters. We then provide two explicit discretisations of this diffusion as\npractical algorithms to estimate parameters of statistical models. For each\nalgorithm, we obtain nonasymptotic rates of convergence for the case where the\njoint log-likelihood is strongly concave with respect to latent variables and\nparameters. In particular, we provide convergence analysis for the diffusion\ntogether with the discretisation error, providing convergence rate estimates\nfor the algorithms in Wasserstein-2 distance. We achieve accelerated\nconvergence rates clearly demonstrating improvement in dimension dependence,\nsimilar to the underdamped samplers. To demonstrate the utility of the\nintroduced methodology, we provide numerical experiments that demonstrate the\neffectiveness of the proposed diffusion for statistical inference and the\nstability of the numerical integrators utilised for discretisation. Our setting\ncovers a broad number of applications, including unsupervised learning,\nstatistical inference, and inverse problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces and analyses interacting underdamped Langevin\nalgorithms, termed Kinetic Interacting Particle Langevin Monte Carlo (KIPLMC)\nmethods, for statistical inference in latent variable models. We propose a\ndiffusion process that evolves jointly in the space of parameters and latent\nvariables and exploit the fact that the stationary distribution of this\ndiffusion concentrates around the maximum marginal likelihood estimate of the\nparameters. We then provide two explicit discretisations of this diffusion as\npractical algorithms to estimate parameters of statistical models. For each\nalgorithm, we obtain nonasymptotic rates of convergence for the case where the\njoint log-likelihood is strongly concave with respect to latent variables and\nparameters. In particular, we provide convergence analysis for the diffusion\ntogether with the discretisation error, providing convergence rate estimates\nfor the algorithms in Wasserstein-2 distance. We achieve accelerated\nconvergence rates clearly demonstrating improvement in dimension dependence,\nsimilar to the underdamped samplers. To demonstrate the utility of the\nintroduced methodology, we provide numerical experiments that demonstrate the\neffectiveness of the proposed diffusion for statistical inference and the\nstability of the numerical integrators utilised for discretisation. Our setting\ncovers a broad number of applications, including unsupervised learning,\nstatistical inference, and inverse problems."
                },
                "authors": [
                    {
                        "name": "Paul Felix Valsecchi Oliva"
                    },
                    {
                        "name": "O. Deniz Akyildiz"
                    }
                ],
                "author_detail": {
                    "name": "O. Deniz Akyildiz"
                },
                "author": "O. Deniz Akyildiz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05790v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05790v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18322v2",
                "updated": "2024-09-04T17:16:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    16,
                    5,
                    2,
                    248,
                    0
                ],
                "published": "2024-07-01T19:52:41Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    19,
                    52,
                    41,
                    0,
                    183,
                    0
                ],
                "title": "The Need for Guardrails with Large Language Models in Medical\n  Safety-Critical Settings: An Artificial Intelligence Application in the\n  Pharmacovigilance Ecosystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Need for Guardrails with Large Language Models in Medical\n  Safety-Critical Settings: An Artificial Intelligence Application in the\n  Pharmacovigilance Ecosystem"
                },
                "summary": "Large language models (LLMs) are useful tools with the capacity for\nperforming specific types of knowledge work at an effective scale. However, LLM\ndeployments in high-risk and safety-critical domains pose unique challenges,\nnotably the issue of ``hallucination,'' where LLMs can generate fabricated\ninformation. This is particularly concerning in settings such as drug safety,\nwhere inaccuracies could lead to patient harm. To mitigate these risks, we have\ndeveloped and demonstrated a proof of concept suite of guardrails specifically\ndesigned to mitigate certain types of hallucinations and errors for drug\nsafety, and potentially applicable to other medical safety-critical contexts.\nThese guardrails include mechanisms to detect anomalous documents to prevent\nthe ingestion of inappropriate data, identify incorrect drug names or adverse\nevent terms, and convey uncertainty in generated content. We integrated these\nguardrails with an LLM fine-tuned for a text-to-text task, which involves\nconverting both structured and unstructured data within adverse event reports\ninto natural language. This method was applied to translate individual case\nsafety reports, demonstrating effective application in a pharmacovigilance\nprocessing task. Our guardrail framework offers a set of tools with broad\napplicability across various domains, ensuring LLMs can be safely used in\nhigh-risk situations by eliminating the occurrence of key errors, including the\ngeneration of incorrect pharmacovigilance-related terms, thus adhering to\nstringent regulatory and quality standards in medical safety-critical\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are useful tools with the capacity for\nperforming specific types of knowledge work at an effective scale. However, LLM\ndeployments in high-risk and safety-critical domains pose unique challenges,\nnotably the issue of ``hallucination,'' where LLMs can generate fabricated\ninformation. This is particularly concerning in settings such as drug safety,\nwhere inaccuracies could lead to patient harm. To mitigate these risks, we have\ndeveloped and demonstrated a proof of concept suite of guardrails specifically\ndesigned to mitigate certain types of hallucinations and errors for drug\nsafety, and potentially applicable to other medical safety-critical contexts.\nThese guardrails include mechanisms to detect anomalous documents to prevent\nthe ingestion of inappropriate data, identify incorrect drug names or adverse\nevent terms, and convey uncertainty in generated content. We integrated these\nguardrails with an LLM fine-tuned for a text-to-text task, which involves\nconverting both structured and unstructured data within adverse event reports\ninto natural language. This method was applied to translate individual case\nsafety reports, demonstrating effective application in a pharmacovigilance\nprocessing task. Our guardrail framework offers a set of tools with broad\napplicability across various domains, ensuring LLMs can be safely used in\nhigh-risk situations by eliminating the occurrence of key errors, including the\ngeneration of incorrect pharmacovigilance-related terms, thus adhering to\nstringent regulatory and quality standards in medical safety-critical\nenvironments."
                },
                "authors": [
                    {
                        "name": "Joe B Hakim"
                    },
                    {
                        "name": "Jeffery L Painter"
                    },
                    {
                        "name": "Darmendra Ramcharran"
                    },
                    {
                        "name": "Vijay Kara"
                    },
                    {
                        "name": "Greg Powell"
                    },
                    {
                        "name": "Paulina Sobczak"
                    },
                    {
                        "name": "Chiho Sato"
                    },
                    {
                        "name": "Andrew Bate"
                    },
                    {
                        "name": "Andrew Beam"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Beam"
                },
                "author": "Andrew Beam",
                "arxiv_comment": "27 pages, 6 figures, 4 tables and supplementary material provided",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1; I.2.7; I.7.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.13290v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.13290v2",
                "updated": "2024-09-04T17:11:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    11,
                    4,
                    2,
                    248,
                    0
                ],
                "published": "2023-11-22T10:19:40Z",
                "published_parsed": [
                    2023,
                    11,
                    22,
                    10,
                    19,
                    40,
                    2,
                    326,
                    0
                ],
                "title": "Hyft: A Reconfigurable Softmax Accelerator with Hybrid Numeric Format\n  for both Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyft: A Reconfigurable Softmax Accelerator with Hybrid Numeric Format\n  for both Training and Inference"
                },
                "summary": "The attention mechanism is a pivotal element within the transformer\narchitecture, making a substantial contribution to its exceptional performance.\nWithin this attention mechanism, Softmax is an imperative component that\nenables the model to assess the degree of correlation between various segments\nof the input. Yet, prior research has shown that Softmax operations can\nsignificantly increase processing latency and energy consumption in the\ntransformer network due to their internal nonlinear operations and data\ndependencies. In this work, we proposed Hyft, a hardware efficient floating\npoint Softmax accelerator for both training and inference. Hyft aims to reduce\nthe implementation cost of different nonlinear arithmetic operations within\nsoftmax by adaptively converting intermediate results into the most suitable\nnumeric format for each specific operation, leading to reconfigurable\naccelerator with hybrid numeric format. The evaluation results highlight that\nHyft achieves a remarkable 10x reduction in hardware resource utilization and a\n6x reduction in processing latency, all while maintaining a negligible impact\non transformer accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The attention mechanism is a pivotal element within the transformer\narchitecture, making a substantial contribution to its exceptional performance.\nWithin this attention mechanism, Softmax is an imperative component that\nenables the model to assess the degree of correlation between various segments\nof the input. Yet, prior research has shown that Softmax operations can\nsignificantly increase processing latency and energy consumption in the\ntransformer network due to their internal nonlinear operations and data\ndependencies. In this work, we proposed Hyft, a hardware efficient floating\npoint Softmax accelerator for both training and inference. Hyft aims to reduce\nthe implementation cost of different nonlinear arithmetic operations within\nsoftmax by adaptively converting intermediate results into the most suitable\nnumeric format for each specific operation, leading to reconfigurable\naccelerator with hybrid numeric format. The evaluation results highlight that\nHyft achieves a remarkable 10x reduction in hardware resource utilization and a\n6x reduction in processing latency, all while maintaining a negligible impact\non transformer accuracy."
                },
                "authors": [
                    {
                        "name": "Tianhua Xia"
                    },
                    {
                        "name": "Sai Qian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Sai Qian Zhang"
                },
                "author": "Sai Qian Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.13290v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.13290v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02877v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02877v1",
                "updated": "2024-09-04T17:01:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    1,
                    2,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T17:01:02Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    1,
                    2,
                    2,
                    248,
                    0
                ],
                "title": "Configurable Foundation Models: Building LLMs from a Modular Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Configurable Foundation Models: Building LLMs from a Modular Perspective"
                },
                "summary": "Advancements in LLMs have recently unveiled challenges tied to computational\nefficiency and continual scalability due to their requirements of huge\nparameters, making the applications and evolution of these models on devices\nwith limited computation resources and scenarios requiring various abilities\nincreasingly cumbersome. Inspired by modularity within the human brain, there\nis a growing tendency to decompose LLMs into numerous functional modules,\nallowing for inference with part of modules and dynamic assembly of modules to\ntackle complex tasks, such as mixture-of-experts. To highlight the inherent\nefficiency and composability of the modular approach, we coin the term brick to\nrepresent each functional module, designating the modularized structure as\nconfigurable foundation models. In this paper, we offer a comprehensive\noverview and investigation of the construction, utilization, and limitation of\nconfigurable foundation models. We first formalize modules into emergent bricks\n- functional neuron partitions that emerge during the pre-training phase, and\ncustomized bricks - bricks constructed via additional post-training to improve\nthe capabilities and knowledge of LLMs. Based on diverse functional bricks, we\nfurther present four brick-oriented operations: retrieval and routing, merging,\nupdating, and growing. These operations allow for dynamic configuration of LLMs\nbased on instructions to handle complex tasks. To verify our perspective, we\nconduct an empirical analysis on widely-used LLMs. We find that the FFN layers\nfollow modular patterns with functional specialization of neurons and\nfunctional neuron partitions. Finally, we highlight several open issues and\ndirections for future research. Overall, this paper aims to offer a fresh\nmodular perspective on existing LLM research and inspire the future creation of\nmore efficient and scalable foundational models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in LLMs have recently unveiled challenges tied to computational\nefficiency and continual scalability due to their requirements of huge\nparameters, making the applications and evolution of these models on devices\nwith limited computation resources and scenarios requiring various abilities\nincreasingly cumbersome. Inspired by modularity within the human brain, there\nis a growing tendency to decompose LLMs into numerous functional modules,\nallowing for inference with part of modules and dynamic assembly of modules to\ntackle complex tasks, such as mixture-of-experts. To highlight the inherent\nefficiency and composability of the modular approach, we coin the term brick to\nrepresent each functional module, designating the modularized structure as\nconfigurable foundation models. In this paper, we offer a comprehensive\noverview and investigation of the construction, utilization, and limitation of\nconfigurable foundation models. We first formalize modules into emergent bricks\n- functional neuron partitions that emerge during the pre-training phase, and\ncustomized bricks - bricks constructed via additional post-training to improve\nthe capabilities and knowledge of LLMs. Based on diverse functional bricks, we\nfurther present four brick-oriented operations: retrieval and routing, merging,\nupdating, and growing. These operations allow for dynamic configuration of LLMs\nbased on instructions to handle complex tasks. To verify our perspective, we\nconduct an empirical analysis on widely-used LLMs. We find that the FFN layers\nfollow modular patterns with functional specialization of neurons and\nfunctional neuron partitions. Finally, we highlight several open issues and\ndirections for future research. Overall, this paper aims to offer a fresh\nmodular perspective on existing LLM research and inspire the future creation of\nmore efficient and scalable foundational models."
                },
                "authors": [
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Zhengyan Zhang"
                    },
                    {
                        "name": "Chenyang Song"
                    },
                    {
                        "name": "Dazhi Jiang"
                    },
                    {
                        "name": "Feng Yao"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Xiaozhi Wang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Guanyu Lin"
                    },
                    {
                        "name": "Yingfa Chen"
                    },
                    {
                        "name": "Weilin Zhao"
                    },
                    {
                        "name": "Yuge Tu"
                    },
                    {
                        "name": "Zexuan Zhong"
                    },
                    {
                        "name": "Ao Zhang"
                    },
                    {
                        "name": "Chenglei Si"
                    },
                    {
                        "name": "Khai Hao Moo"
                    },
                    {
                        "name": "Chenyang Zhao"
                    },
                    {
                        "name": "Huimin Chen"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Jingbo Shang"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02877v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02877v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.17293v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.17293v4",
                "updated": "2024-09-04T16:59:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    59,
                    27,
                    2,
                    248,
                    0
                ],
                "published": "2023-12-28T13:59:43Z",
                "published_parsed": [
                    2023,
                    12,
                    28,
                    13,
                    59,
                    43,
                    3,
                    362,
                    0
                ],
                "title": "$$GUIDE: a framework for quantitative imaging via generalized\n  uncertainty-driven inference using deep learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$$GUIDE: a framework for quantitative imaging via generalized\n  uncertainty-driven inference using deep learning"
                },
                "summary": "This work proposes $\\mu$GUIDE: a general Bayesian framework to estimate\nposterior distributions of tissue microstructure parameters from any given\nbiophysical model or MRI signal representation, with exemplar demonstration in\ndiffusion-weighted MRI. Harnessing a new deep learning architecture for\nautomatic signal feature selection combined with simulation-based inference and\nefficient sampling of the posterior distributions, $\\mu$GUIDE bypasses the high\ncomputational and time cost of conventional Bayesian approaches and does not\nrely on acquisition constraints to define model-specific summary statistics.\nThe obtained posterior distributions allow to highlight degeneracies present in\nthe model definition and quantify the uncertainty and ambiguity of the\nestimated parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work proposes $\\mu$GUIDE: a general Bayesian framework to estimate\nposterior distributions of tissue microstructure parameters from any given\nbiophysical model or MRI signal representation, with exemplar demonstration in\ndiffusion-weighted MRI. Harnessing a new deep learning architecture for\nautomatic signal feature selection combined with simulation-based inference and\nefficient sampling of the posterior distributions, $\\mu$GUIDE bypasses the high\ncomputational and time cost of conventional Bayesian approaches and does not\nrely on acquisition constraints to define model-specific summary statistics.\nThe obtained posterior distributions allow to highlight degeneracies present in\nthe model definition and quantify the uncertainty and ambiguity of the\nestimated parameters."
                },
                "authors": [
                    {
                        "name": "Maliss Jallais"
                    },
                    {
                        "name": "Marco Palombo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Palombo"
                },
                "author": "Marco Palombo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.17293v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.17293v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10690v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10690v3",
                "updated": "2024-09-04T16:58:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    58,
                    25,
                    2,
                    248,
                    0
                ],
                "published": "2024-06-15T17:07:31Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    17,
                    7,
                    31,
                    5,
                    167,
                    0
                ],
                "title": "Automating Pharmacovigilance Evidence Generation: Using Large Language\n  Models to Produce Context-Aware SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Pharmacovigilance Evidence Generation: Using Large Language\n  Models to Produce Context-Aware SQL"
                },
                "summary": "Objective: To enhance the efficiency and accuracy of information retrieval\nfrom pharmacovigilance (PV) databases by employing Large Language Models (LLMs)\nto convert natural language queries (NLQs) into Structured Query Language (SQL)\nqueries, leveraging a business context document.\n  Materials and Methods: We utilized OpenAI's GPT-4 model within a\nretrieval-augmented generation (RAG) framework, enriched with a business\ncontext document, to transform NLQs into syntactically precise SQL queries.\nEach NLQ was presented to the LLM randomly and independently to prevent\nmemorization. The study was conducted in three phases, varying query\ncomplexity, and assessing the LLM's performance both with and without the\nbusiness context document.\n  Results: Our approach significantly improved NLQ-to-SQL accuracy, increasing\nfrom 8.3\\% with the database schema alone to 78.3\\% with the business context\ndocument. This enhancement was consistent across low, medium, and high\ncomplexity queries, indicating the critical role of contextual knowledge in\nquery generation.\n  Discussion: The integration of a business context document markedly improved\nthe LLM's ability to generate accurate and contextually relevant SQL queries.\nPerformance achieved a maximum of 85\\% when high complexity queries are\nexcluded, suggesting promise for routine deployment.\n  Conclusion: This study presents a novel approach to employing LLMs for safety\ndata retrieval and analysis, demonstrating significant advancements in query\ngeneration accuracy. The methodology offers a framework applicable to various\ndata-intensive domains, enhancing the accessibility and efficiency of\ninformation retrieval for non-technical users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objective: To enhance the efficiency and accuracy of information retrieval\nfrom pharmacovigilance (PV) databases by employing Large Language Models (LLMs)\nto convert natural language queries (NLQs) into Structured Query Language (SQL)\nqueries, leveraging a business context document.\n  Materials and Methods: We utilized OpenAI's GPT-4 model within a\nretrieval-augmented generation (RAG) framework, enriched with a business\ncontext document, to transform NLQs into syntactically precise SQL queries.\nEach NLQ was presented to the LLM randomly and independently to prevent\nmemorization. The study was conducted in three phases, varying query\ncomplexity, and assessing the LLM's performance both with and without the\nbusiness context document.\n  Results: Our approach significantly improved NLQ-to-SQL accuracy, increasing\nfrom 8.3\\% with the database schema alone to 78.3\\% with the business context\ndocument. This enhancement was consistent across low, medium, and high\ncomplexity queries, indicating the critical role of contextual knowledge in\nquery generation.\n  Discussion: The integration of a business context document markedly improved\nthe LLM's ability to generate accurate and contextually relevant SQL queries.\nPerformance achieved a maximum of 85\\% when high complexity queries are\nexcluded, suggesting promise for routine deployment.\n  Conclusion: This study presents a novel approach to employing LLMs for safety\ndata retrieval and analysis, demonstrating significant advancements in query\ngeneration accuracy. The methodology offers a framework applicable to various\ndata-intensive domains, enhancing the accessibility and efficiency of\ninformation retrieval for non-technical users."
                },
                "authors": [
                    {
                        "name": "Jeffery L. Painter"
                    },
                    {
                        "name": "Venkateswara Rao Chalamalasetti"
                    },
                    {
                        "name": "Raymond Kassekert"
                    },
                    {
                        "name": "Andrew Bate"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Bate"
                },
                "author": "Andrew Bate",
                "arxiv_comment": "15 pages, 3 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10690v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10690v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02862v1",
                "updated": "2024-09-04T16:41:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    41,
                    56,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T16:41:56Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    41,
                    56,
                    2,
                    248,
                    0
                ],
                "title": "Four-dimensional phase space tomography from one-dimensional\n  measurements in a high-power hadron ring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Four-dimensional phase space tomography from one-dimensional\n  measurements in a high-power hadron ring"
                },
                "summary": "In this paper, we use one-dimensional measurements to infer the\nfour-dimensional phase space density of an accumulated 1 GeV proton beam in the\nSpallation Neutron Source (SNS) accelerator. The reconstruction was performed\nusing MENT, an exact maximum-entropy tomography algorithm, and thus represents\nthe most reasonable inference from the data. The reconstructed distribution\nreproduces the measured profiles with the same dynamic range as the measurement\ndevices, and simulations indicate that the problem is well-constrained. Similar\nmeasurements could serve as benchmarks for simulations of intense, coupled beam\ndynamics in the SNS or other hadron rings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we use one-dimensional measurements to infer the\nfour-dimensional phase space density of an accumulated 1 GeV proton beam in the\nSpallation Neutron Source (SNS) accelerator. The reconstruction was performed\nusing MENT, an exact maximum-entropy tomography algorithm, and thus represents\nthe most reasonable inference from the data. The reconstructed distribution\nreproduces the measured profiles with the same dynamic range as the measurement\ndevices, and simulations indicate that the problem is well-constrained. Similar\nmeasurements could serve as benchmarks for simulations of intense, coupled beam\ndynamics in the SNS or other hadron rings."
                },
                "authors": [
                    {
                        "name": "Austin Hoover"
                    }
                ],
                "author_detail": {
                    "name": "Austin Hoover"
                },
                "author": "Austin Hoover",
                "arxiv_comment": "7 pages, 8 figures, submitted to PRAB",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00847v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00847v2",
                "updated": "2024-09-04T16:39:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    39,
                    22,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-01T21:30:14Z",
                "published_parsed": [
                    2024,
                    9,
                    1,
                    21,
                    30,
                    14,
                    6,
                    245,
                    0
                ],
                "title": "The Design of an LLM-powered Unstructured Analytics System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Design of an LLM-powered Unstructured Analytics System"
                },
                "summary": "LLMs demonstrate an uncanny ability to process unstructured data, and as\nsuch, have the potential to go beyond search and run complex, semantic analyses\nat scale. We describe the design of an unstructured analytics system, Aryn, and\nthe tenets and use cases that motivate its design. With Aryn, users can specify\nqueries in natural language and the system automatically determines a semantic\nplan and executes it to compute an answer from a large collection of\nunstructured documents using LLMs. At the core of Aryn is Sycamore, a\ndeclarative document processing engine, built using Ray, that provides a\nreliable distributed abstraction called DocSets. Sycamore allows users to\nanalyze, enrich, and transform complex documents at scale. Aryn also comprises\nLuna, a query planner that translates natural language queries to Sycamore\nscripts, and the Aryn Partitioner, which takes raw PDFs and document images,\nand converts them to DocSets for downstream processing. Using Aryn, we\ndemonstrate a real world use case for analyzing accident reports from the\nNational Transportation Safety Board (NTSB), and discuss some of the major\nchallenges we encountered in deploying Aryn in the wild.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs demonstrate an uncanny ability to process unstructured data, and as\nsuch, have the potential to go beyond search and run complex, semantic analyses\nat scale. We describe the design of an unstructured analytics system, Aryn, and\nthe tenets and use cases that motivate its design. With Aryn, users can specify\nqueries in natural language and the system automatically determines a semantic\nplan and executes it to compute an answer from a large collection of\nunstructured documents using LLMs. At the core of Aryn is Sycamore, a\ndeclarative document processing engine, built using Ray, that provides a\nreliable distributed abstraction called DocSets. Sycamore allows users to\nanalyze, enrich, and transform complex documents at scale. Aryn also comprises\nLuna, a query planner that translates natural language queries to Sycamore\nscripts, and the Aryn Partitioner, which takes raw PDFs and document images,\nand converts them to DocSets for downstream processing. Using Aryn, we\ndemonstrate a real world use case for analyzing accident reports from the\nNational Transportation Safety Board (NTSB), and discuss some of the major\nchallenges we encountered in deploying Aryn in the wild."
                },
                "authors": [
                    {
                        "name": "Eric Anderson"
                    },
                    {
                        "name": "Jonathan Fritz"
                    },
                    {
                        "name": "Austin Lee"
                    },
                    {
                        "name": "Bohou Li"
                    },
                    {
                        "name": "Mark Lindblad"
                    },
                    {
                        "name": "Henry Lindeman"
                    },
                    {
                        "name": "Alex Meyer"
                    },
                    {
                        "name": "Parth Parmar"
                    },
                    {
                        "name": "Tanvi Ranade"
                    },
                    {
                        "name": "Mehul A. Shah"
                    },
                    {
                        "name": "Benjamin Sowell"
                    },
                    {
                        "name": "Dan Tecuci"
                    },
                    {
                        "name": "Vinayak Thapliyal"
                    },
                    {
                        "name": "Matt Welsh"
                    }
                ],
                "author_detail": {
                    "name": "Matt Welsh"
                },
                "author": "Matt Welsh",
                "arxiv_comment": "6 pages, 3 figures, fixed typos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00847v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00847v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01433v2",
                "updated": "2024-09-04T16:28:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    28,
                    36,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-02T19:20:26Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    19,
                    20,
                    26,
                    0,
                    246,
                    0
                ],
                "title": "Domain Decomposition-based coupling of Operator Inference reduced order\n  models via the Schwarz alternating method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain Decomposition-based coupling of Operator Inference reduced order\n  models via the Schwarz alternating method"
                },
                "summary": "This paper presents and evaluates an approach for coupling together\nsubdomain-local reduced order models (ROMs) constructed via non-intrusive\noperator inference (OpInf) with each other and with subdomain-local full order\nmodels (FOMs), following a domain decomposition of the spatial geometry on\nwhich a given partial differential equation (PDE) is posed. Joining\nsubdomain-local models is accomplished using the overlapping Schwarz\nalternating method, a minimally-intrusive multiscale coupling technique that\nworks by transforming a monolithic problem into a sequence of subdomain-local\nproblems, which communicate through transmission boundary conditions imposed on\nthe subdomain interfaces. After formulating the overlapping Schwarz alternating\nmethod for OpInf ROMs, termed OpInf-Schwarz, we evaluate the method's accuracy\nand efficiency on several test cases involving the heat equation in two spatial\ndimensions. We demonstrate that the method is capable of coupling together\narbitrary combinations of OpInf ROMs and FOMs, and that speed-ups over a\nmonolithic FOM are possible when performing OpInf ROM coupling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents and evaluates an approach for coupling together\nsubdomain-local reduced order models (ROMs) constructed via non-intrusive\noperator inference (OpInf) with each other and with subdomain-local full order\nmodels (FOMs), following a domain decomposition of the spatial geometry on\nwhich a given partial differential equation (PDE) is posed. Joining\nsubdomain-local models is accomplished using the overlapping Schwarz\nalternating method, a minimally-intrusive multiscale coupling technique that\nworks by transforming a monolithic problem into a sequence of subdomain-local\nproblems, which communicate through transmission boundary conditions imposed on\nthe subdomain interfaces. After formulating the overlapping Schwarz alternating\nmethod for OpInf ROMs, termed OpInf-Schwarz, we evaluate the method's accuracy\nand efficiency on several test cases involving the heat equation in two spatial\ndimensions. We demonstrate that the method is capable of coupling together\narbitrary combinations of OpInf ROMs and FOMs, and that speed-ups over a\nmonolithic FOM are possible when performing OpInf ROM coupling."
                },
                "authors": [
                    {
                        "name": "Ian Moore"
                    },
                    {
                        "name": "Christopher Wentland"
                    },
                    {
                        "name": "Anthony Gruber"
                    },
                    {
                        "name": "Irina Tezaur"
                    }
                ],
                "author_detail": {
                    "name": "Irina Tezaur"
                },
                "author": "Irina Tezaur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08763v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08763v4",
                "updated": "2024-09-04T16:13:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    13,
                    18,
                    2,
                    248,
                    0
                ],
                "published": "2024-03-13T17:58:57Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    17,
                    58,
                    57,
                    2,
                    73,
                    0
                ],
                "title": "Simple and Scalable Strategies to Continually Pre-train Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple and Scalable Strategies to Continually Pre-train Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by the final loss\nand the average score on several language model (LM) evaluation benchmarks.\nSpecifically, we show this for a weak but realistic distribution shift between\ntwo commonly used LLM pre-training datasets (English$\\rightarrow$English) and a\nstronger distribution shift (English$\\rightarrow$German) at the $405$M\nparameter model scale with large dataset sizes (hundreds of billions of\ntokens). Selecting the weak but realistic shift for larger-scale experiments,\nwe also find that our continual learning strategies match the re-training\nbaseline for a 10B parameter LLM. Our results demonstrate that LLMs can be\nsuccessfully updated via simple and scalable continual learning strategies,\nmatching the re-training baseline using only a fraction of the compute.\nFinally, inspired by previous work, we propose alternatives to the cosine\nlearning rate schedule that help circumvent forgetting induced by LR re-warming\nand that are not bound to a fixed token budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by the final loss\nand the average score on several language model (LM) evaluation benchmarks.\nSpecifically, we show this for a weak but realistic distribution shift between\ntwo commonly used LLM pre-training datasets (English$\\rightarrow$English) and a\nstronger distribution shift (English$\\rightarrow$German) at the $405$M\nparameter model scale with large dataset sizes (hundreds of billions of\ntokens). Selecting the weak but realistic shift for larger-scale experiments,\nwe also find that our continual learning strategies match the re-training\nbaseline for a 10B parameter LLM. Our results demonstrate that LLMs can be\nsuccessfully updated via simple and scalable continual learning strategies,\nmatching the re-training baseline using only a fraction of the compute.\nFinally, inspired by previous work, we propose alternatives to the cosine\nlearning rate schedule that help circumvent forgetting induced by LR re-warming\nand that are not bound to a fixed token budget."
                },
                "authors": [
                    {
                        "name": "Adam Ibrahim"
                    },
                    {
                        "name": "Benjamin Thrien"
                    },
                    {
                        "name": "Kshitij Gupta"
                    },
                    {
                        "name": "Mats L. Richter"
                    },
                    {
                        "name": "Quentin Anthony"
                    },
                    {
                        "name": "Timothe Lesort"
                    },
                    {
                        "name": "Eugene Belilovsky"
                    },
                    {
                        "name": "Irina Rish"
                    }
                ],
                "author_detail": {
                    "name": "Irina Rish"
                },
                "author": "Irina Rish",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08763v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08763v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02834v1",
                "updated": "2024-09-04T16:00:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    0,
                    21,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T16:00:21Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    0,
                    21,
                    2,
                    248,
                    0
                ],
                "title": "CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the\n  Mathematics Reasoning of Large Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the\n  Mathematics Reasoning of Large Multimodal Models"
                },
                "summary": "Large language models (LLMs) have obtained promising results in mathematical\nreasoning, which is a foundational skill for human intelligence. Most previous\nstudies focus on improving and measuring the performance of LLMs based on\ntextual math reasoning datasets (e.g., MATH, GSM8K). Recently, a few\nresearchers have released English multimodal math datasets (e.g., MATHVISTA and\nMATH-V) to evaluate the effectiveness of large multimodal models (LMMs). In\nthis paper, we release a Chinese multimodal math (CMM-Math) dataset, including\nbenchmark and training parts, to evaluate and enhance the mathematical\nreasoning of LMMs. CMM-Math contains over 28,000 high-quality samples,\nfeaturing a variety of problem types (e.g., multiple-choice, fill-in-the-blank,\nand so on) with detailed solutions across 12 grade levels from elementary to\nhigh school in China. Specifically, the visual context may be present in the\nquestions or opinions, which makes this dataset more challenging. Through\ncomprehensive analysis, we discover that state-of-the-art LMMs on the CMM-Math\ndataset face challenges, emphasizing the necessity for further improvements in\nLMM development. We also propose a Multimodal Mathematical LMM (Math-LMM) to\nhandle the problems with mixed input of multiple images and text segments. We\ntrain our model using three stages, including foundational pre-training,\nfoundational fine-tuning, and mathematical fine-tuning. The extensive\nexperiments indicate that our model effectively improves math reasoning\nperformance by comparing it with the SOTA LMMs over three multimodal\nmathematical datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have obtained promising results in mathematical\nreasoning, which is a foundational skill for human intelligence. Most previous\nstudies focus on improving and measuring the performance of LLMs based on\ntextual math reasoning datasets (e.g., MATH, GSM8K). Recently, a few\nresearchers have released English multimodal math datasets (e.g., MATHVISTA and\nMATH-V) to evaluate the effectiveness of large multimodal models (LMMs). In\nthis paper, we release a Chinese multimodal math (CMM-Math) dataset, including\nbenchmark and training parts, to evaluate and enhance the mathematical\nreasoning of LMMs. CMM-Math contains over 28,000 high-quality samples,\nfeaturing a variety of problem types (e.g., multiple-choice, fill-in-the-blank,\nand so on) with detailed solutions across 12 grade levels from elementary to\nhigh school in China. Specifically, the visual context may be present in the\nquestions or opinions, which makes this dataset more challenging. Through\ncomprehensive analysis, we discover that state-of-the-art LMMs on the CMM-Math\ndataset face challenges, emphasizing the necessity for further improvements in\nLMM development. We also propose a Multimodal Mathematical LMM (Math-LMM) to\nhandle the problems with mixed input of multiple images and text segments. We\ntrain our model using three stages, including foundational pre-training,\nfoundational fine-tuning, and mathematical fine-tuning. The extensive\nexperiments indicate that our model effectively improves math reasoning\nperformance by comparing it with the SOTA LMMs over three multimodal\nmathematical datasets."
                },
                "authors": [
                    {
                        "name": "Wentao Liu"
                    },
                    {
                        "name": "Qianjun Pan"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Zhuo Liu"
                    },
                    {
                        "name": "Ji Wu"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Aimin Zhou"
                    },
                    {
                        "name": "Qin Chen"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Liang He"
                    }
                ],
                "author_detail": {
                    "name": "Liang He"
                },
                "author": "Liang He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00509v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00509v2",
                "updated": "2024-09-04T15:55:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    15,
                    55,
                    22,
                    2,
                    248,
                    0
                ],
                "published": "2024-08-31T17:19:30Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    17,
                    19,
                    30,
                    5,
                    244,
                    0
                ],
                "title": "LongRecipe: Recipe for Efficient Long Context Generalization in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongRecipe: Recipe for Efficient Long Context Generalization in Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) face significant challenges in handling\nlong-context tasks because of their limited effective context window size\nduring pretraining, which restricts their ability to generalize over extended\nsequences. Meanwhile, extending the context window in LLMs through\npost-pretraining is highly resource-intensive. To address this, we introduce\nLongRecipe, an efficient training strategy for extending the context window of\nLLMs, including impactful token analysis, position index transformation, and\ntraining optimization strategies. It simulates long-sequence inputs while\nmaintaining training efficiency and significantly improves the model's\nunderstanding of long-range dependencies. Experiments on three types of LLMs\nshow that LongRecipe can utilize long sequences while requiring only 30% of the\ntarget context window size, and reduces computational training resource over\n85% compared to full sequence training. Furthermore, LongRecipe also preserves\nthe original LLM's capabilities in general tasks. Ultimately, we can extend the\neffective context window of open-source LLMs from 8k to 128k, achieving\nperformance close to GPT-4 with just one day of dedicated training using a\nsingle GPU with 80G memory. Our code is released at\nhttps://github.com/zhiyuanhubj/LongRecipe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face significant challenges in handling\nlong-context tasks because of their limited effective context window size\nduring pretraining, which restricts their ability to generalize over extended\nsequences. Meanwhile, extending the context window in LLMs through\npost-pretraining is highly resource-intensive. To address this, we introduce\nLongRecipe, an efficient training strategy for extending the context window of\nLLMs, including impactful token analysis, position index transformation, and\ntraining optimization strategies. It simulates long-sequence inputs while\nmaintaining training efficiency and significantly improves the model's\nunderstanding of long-range dependencies. Experiments on three types of LLMs\nshow that LongRecipe can utilize long sequences while requiring only 30% of the\ntarget context window size, and reduces computational training resource over\n85% compared to full sequence training. Furthermore, LongRecipe also preserves\nthe original LLM's capabilities in general tasks. Ultimately, we can extend the\neffective context window of open-source LLMs from 8k to 128k, achieving\nperformance close to GPT-4 with just one day of dedicated training using a\nsingle GPU with 80G memory. Our code is released at\nhttps://github.com/zhiyuanhubj/LongRecipe."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Hu"
                    },
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Jinman Zhao"
                    },
                    {
                        "name": "Suyuchen Wang"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Wei Shen"
                    },
                    {
                        "name": "Qing Gu"
                    },
                    {
                        "name": "Anh Tuan Luu"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Zhiwei Jiang"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00509v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00509v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16039v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16039v2",
                "updated": "2024-09-04T15:52:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    15,
                    52,
                    3,
                    2,
                    248,
                    0
                ],
                "published": "2024-08-28T17:38:35Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    38,
                    35,
                    2,
                    241,
                    0
                ],
                "title": "Group Difference in Differences can Identify Effect Heterogeneity in\n  Non-Canonical Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group Difference in Differences can Identify Effect Heterogeneity in\n  Non-Canonical Settings"
                },
                "summary": "Consider a very general setting in which data on an outcome of interest is\ncollected in two `groups' at two time periods, with certain group-periods\ndeemed `treated' and others `untreated'. A special case is the canonical\nDifference-in-Differences (DiD) setting in which one group is treated only in\nthe second period while the other is treated in neither period. Then it is well\nknown that under a parallel trends assumption across the two groups the classic\nDiD formula (subtracting the average change in the outcome across periods in\nthe treated group by the average change in the outcome across periods in the\nuntreated group) identifies the average treatment effect on the treated in the\nsecond period. But other relations between group, period, and treatment are\npossible. For example, the groups might be demographic (or other baseline\ncovariate) categories with all units in both groups treated in the second\nperiod and none treated in the first, i.e. a pre-post design. Or one group\nmight be treated in both periods while the other is treated in neither. In\nthese non-canonical settings (lacking a control group or a pre-period), some\nresearchers still compute DiD estimators, while others avoid causal inference\naltogether. In this paper, we will elucidate the group-period-treatment\nscenarios and corresponding parallel trends assumptions under which a DiD\nformula identifies meaningful causal estimands and what those causal estimands\nare. We find that in non-canonical settings, under a group parallel trends\nassumption the DiD formula identifies effect heterogeneity in the treated\nacross groups or across time periods (depending on the setting).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consider a very general setting in which data on an outcome of interest is\ncollected in two `groups' at two time periods, with certain group-periods\ndeemed `treated' and others `untreated'. A special case is the canonical\nDifference-in-Differences (DiD) setting in which one group is treated only in\nthe second period while the other is treated in neither period. Then it is well\nknown that under a parallel trends assumption across the two groups the classic\nDiD formula (subtracting the average change in the outcome across periods in\nthe treated group by the average change in the outcome across periods in the\nuntreated group) identifies the average treatment effect on the treated in the\nsecond period. But other relations between group, period, and treatment are\npossible. For example, the groups might be demographic (or other baseline\ncovariate) categories with all units in both groups treated in the second\nperiod and none treated in the first, i.e. a pre-post design. Or one group\nmight be treated in both periods while the other is treated in neither. In\nthese non-canonical settings (lacking a control group or a pre-period), some\nresearchers still compute DiD estimators, while others avoid causal inference\naltogether. In this paper, we will elucidate the group-period-treatment\nscenarios and corresponding parallel trends assumptions under which a DiD\nformula identifies meaningful causal estimands and what those causal estimands\nare. We find that in non-canonical settings, under a group parallel trends\nassumption the DiD formula identifies effect heterogeneity in the treated\nacross groups or across time periods (depending on the setting)."
                },
                "authors": [
                    {
                        "name": "Zach Shahn"
                    },
                    {
                        "name": "Laura Hatfield"
                    }
                ],
                "author_detail": {
                    "name": "Laura Hatfield"
                },
                "author": "Laura Hatfield",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16039v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16039v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02823v1",
                "updated": "2024-09-04T15:42:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    15,
                    42,
                    59,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T15:42:59Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    15,
                    42,
                    59,
                    2,
                    248,
                    0
                ],
                "title": "Design Contradictions: Help or Hindrance?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design Contradictions: Help or Hindrance?"
                },
                "summary": "The need for innovative ideas in data visualisation drives us to explore new\ncreative approaches. Combining two or more creative words, particularly those\nthat contradict each other, can positively impact the creative process,\nsparking novel ideas and designs. As we move towards AI-driven design, an open\nquestion arises: do these design contradictions work positively with AI tools?\nCurrently, the answer is no. AI systems, like large language models (LLMs),\nrely on algorithms that engender similarity, whereas creativity often requires\ndivergence and novelty. This poster initiates a conversation on how to drive AI\nsystems to be more creative and generate new ideas. This research invites us to\nreconsider traditional design methods and explore new approaches in an\nAI-driven world. Can we apply the same techniques used in traditional design,\nlike the double diamond model, or do we need new methods for design\nengineering? How can we quickly design visualisations and craft new ideas with\ngenerative AI? This paper seeks to start this critical conversation and offers\npractical insights into the potential of AI in driving creativity in data\nvisualisation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The need for innovative ideas in data visualisation drives us to explore new\ncreative approaches. Combining two or more creative words, particularly those\nthat contradict each other, can positively impact the creative process,\nsparking novel ideas and designs. As we move towards AI-driven design, an open\nquestion arises: do these design contradictions work positively with AI tools?\nCurrently, the answer is no. AI systems, like large language models (LLMs),\nrely on algorithms that engender similarity, whereas creativity often requires\ndivergence and novelty. This poster initiates a conversation on how to drive AI\nsystems to be more creative and generate new ideas. This research invites us to\nreconsider traditional design methods and explore new approaches in an\nAI-driven world. Can we apply the same techniques used in traditional design,\nlike the double diamond model, or do we need new methods for design\nengineering? How can we quickly design visualisations and craft new ideas with\ngenerative AI? This paper seeks to start this critical conversation and offers\npractical insights into the potential of AI in driving creativity in data\nvisualisation."
                },
                "authors": [
                    {
                        "name": "Aron E. Owen"
                    },
                    {
                        "name": "Jonathan C. Roberts"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan C. Roberts"
                },
                "author": "Jonathan C. Roberts",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02822v1",
                "updated": "2024-09-04T15:42:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    15,
                    42,
                    29,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T15:42:29Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    15,
                    42,
                    29,
                    2,
                    248,
                    0
                ],
                "title": "Language Understanding as a Constraint on Consensus Size in LLM\n  Societies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Understanding as a Constraint on Consensus Size in LLM\n  Societies"
                },
                "summary": "The applications of Large Language Models (LLMs) are going towards\ncollaborative tasks where several agents interact with each other like in an\nLLM society. In such a setting, large groups of LLMs could reach consensus\nabout arbitrary norms for which there is no information supporting one option\nover another, regulating their own behavior in a self-organized way. In human\nsocieties, the ability to reach consensus without institutions has a limit in\nthe cognitive capacities of humans. To understand if a similar phenomenon\ncharacterizes also LLMs, we apply methods from complexity science and\nprinciples from behavioral sciences in a new approach of AI anthropology. We\nfind that LLMs are able to reach consensus in groups and that the opinion\ndynamics of LLMs can be understood with a function parametrized by a majority\nforce coefficient that determines whether consensus is possible. This majority\nforce is stronger for models with higher language understanding capabilities\nand decreases for larger groups, leading to a critical group size beyond which,\nfor a given LLM, consensus is unfeasible. This critical group size grows\nexponentially with the language understanding capabilities of models and for\nthe most advanced models, it can reach an order of magnitude beyond the typical\nsize of informal human groups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The applications of Large Language Models (LLMs) are going towards\ncollaborative tasks where several agents interact with each other like in an\nLLM society. In such a setting, large groups of LLMs could reach consensus\nabout arbitrary norms for which there is no information supporting one option\nover another, regulating their own behavior in a self-organized way. In human\nsocieties, the ability to reach consensus without institutions has a limit in\nthe cognitive capacities of humans. To understand if a similar phenomenon\ncharacterizes also LLMs, we apply methods from complexity science and\nprinciples from behavioral sciences in a new approach of AI anthropology. We\nfind that LLMs are able to reach consensus in groups and that the opinion\ndynamics of LLMs can be understood with a function parametrized by a majority\nforce coefficient that determines whether consensus is possible. This majority\nforce is stronger for models with higher language understanding capabilities\nand decreases for larger groups, leading to a critical group size beyond which,\nfor a given LLM, consensus is unfeasible. This critical group size grows\nexponentially with the language understanding capabilities of models and for\nthe most advanced models, it can reach an order of magnitude beyond the typical\nsize of informal human groups."
                },
                "authors": [
                    {
                        "name": "Giordano De Marzo"
                    },
                    {
                        "name": "Claudio Castellano"
                    },
                    {
                        "name": "David Garcia"
                    }
                ],
                "author_detail": {
                    "name": "David Garcia"
                },
                "author": "David Garcia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16961v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16961v2",
                "updated": "2024-09-04T15:39:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    15,
                    39,
                    47,
                    2,
                    248,
                    0
                ],
                "published": "2024-08-15T17:59:14Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    17,
                    59,
                    14,
                    3,
                    228,
                    0
                ],
                "title": "The Future of Open Human Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Future of Open Human Feedback"
                },
                "summary": "Human feedback on conversations with language language models (LLMs) is\ncentral to how these systems learn about the world, improve their capabilities,\nand are steered toward desirable and safe behaviors. However, this feedback is\nmostly collected by frontier AI labs and kept behind closed doors. In this\nwork, we bring together interdisciplinary experts to assess the opportunities\nand challenges to realizing an open ecosystem of human feedback for AI. We\nfirst look for successful practices in peer production, open source, and\ncitizen science communities. We then characterize the main challenges for open\nhuman feedback. For each, we survey current approaches and offer\nrecommendations. We end by envisioning the components needed to underpin a\nsustainable and open human feedback ecosystem. In the center of this ecosystem\nare mutually beneficial feedback loops, between users and specialized models,\nincentivizing a diverse stakeholders community of model trainers and feedback\nproviders to support a general open feedback pool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human feedback on conversations with language language models (LLMs) is\ncentral to how these systems learn about the world, improve their capabilities,\nand are steered toward desirable and safe behaviors. However, this feedback is\nmostly collected by frontier AI labs and kept behind closed doors. In this\nwork, we bring together interdisciplinary experts to assess the opportunities\nand challenges to realizing an open ecosystem of human feedback for AI. We\nfirst look for successful practices in peer production, open source, and\ncitizen science communities. We then characterize the main challenges for open\nhuman feedback. For each, we survey current approaches and offer\nrecommendations. We end by envisioning the components needed to underpin a\nsustainable and open human feedback ecosystem. In the center of this ecosystem\nare mutually beneficial feedback loops, between users and specialized models,\nincentivizing a diverse stakeholders community of model trainers and feedback\nproviders to support a general open feedback pool."
                },
                "authors": [
                    {
                        "name": "Shachar Don-Yehiya"
                    },
                    {
                        "name": "Ben Burtenshaw"
                    },
                    {
                        "name": "Ramon Fernandez Astudillo"
                    },
                    {
                        "name": "Cailean Osborne"
                    },
                    {
                        "name": "Mimansa Jaiswal"
                    },
                    {
                        "name": "Tzu-Sheng Kuo"
                    },
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Idan Shenfeld"
                    },
                    {
                        "name": "Andi Peng"
                    },
                    {
                        "name": "Mikhail Yurochkin"
                    },
                    {
                        "name": "Atoosa Kasirzadeh"
                    },
                    {
                        "name": "Yangsibo Huang"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    },
                    {
                        "name": "Yacine Jernite"
                    },
                    {
                        "name": "Daniel Vila-Suero"
                    },
                    {
                        "name": "Omri Abend"
                    },
                    {
                        "name": "Jennifer Ding"
                    },
                    {
                        "name": "Sara Hooker"
                    },
                    {
                        "name": "Hannah Rose Kirk"
                    },
                    {
                        "name": "Leshem Choshen"
                    }
                ],
                "author_detail": {
                    "name": "Leshem Choshen"
                },
                "author": "Leshem Choshen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16961v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16961v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.05244v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.05244v2",
                "updated": "2024-09-04T15:37:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    15,
                    37,
                    9,
                    2,
                    248,
                    0
                ],
                "published": "2023-04-11T14:31:53Z",
                "published_parsed": [
                    2023,
                    4,
                    11,
                    14,
                    31,
                    53,
                    1,
                    101,
                    0
                ],
                "title": "Bayesian Analysis of Generalized Hierarchical Indian Buffet Processes\n  for Within and Across Group Sharing of Latent Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Analysis of Generalized Hierarchical Indian Buffet Processes\n  for Within and Across Group Sharing of Latent Features"
                },
                "summary": "Bayesian nonparametric hierarchical priors are highly effective in providing\nflexible models for latent data structures exhibiting sharing of information\nwithin and across groups. In this work, we focus on latent feature allocation\nmodels, where the data structures correspond to multi-sets or unbounded sparse\nmatrices, which we refer to as generalized hierarchical Indian Buffet processes\n(HIBP). These are based on hierarchical versions of generalized spike and slab\nIndian Buffet processes (IBP), where the fundamental development in this regard\nis the Bernoulli-based HIBP, devised by Thibaux-Jordan (2007), as a\nhierarchical extension of the IBP devised by Griffiths-Ghahramani (2005). With\na focus on Bayesian inference, we provide novel explicit descriptions of the\njoint, marginal, and posterior distributions of the HIBP, significantly\nadvancing our understanding of these processes. Our results allow for exact\nsampling for the otherwise complex joint marginal distributions. We provide a\ngeneral characterization of their posterior distributions as well as highlight\nbottlenecks for practical implementation. Our main focus then shifts to\nspecific tractable results for the remarkable case of Poisson HIBP, which\ncorrespond to generalizations of mixed Poisson random count models arising in\ngenetics, imaging, topic modeling, random occupancy, and species sampling\nmodels. We show they also have important relations to Bayesian nonparametric\nlatent class models appearing in the literature. Furthermore, we show that all\ngeneral HIBP may be coupled to Poisson HIBP, allowing for further analysis of\nsuch processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian nonparametric hierarchical priors are highly effective in providing\nflexible models for latent data structures exhibiting sharing of information\nwithin and across groups. In this work, we focus on latent feature allocation\nmodels, where the data structures correspond to multi-sets or unbounded sparse\nmatrices, which we refer to as generalized hierarchical Indian Buffet processes\n(HIBP). These are based on hierarchical versions of generalized spike and slab\nIndian Buffet processes (IBP), where the fundamental development in this regard\nis the Bernoulli-based HIBP, devised by Thibaux-Jordan (2007), as a\nhierarchical extension of the IBP devised by Griffiths-Ghahramani (2005). With\na focus on Bayesian inference, we provide novel explicit descriptions of the\njoint, marginal, and posterior distributions of the HIBP, significantly\nadvancing our understanding of these processes. Our results allow for exact\nsampling for the otherwise complex joint marginal distributions. We provide a\ngeneral characterization of their posterior distributions as well as highlight\nbottlenecks for practical implementation. Our main focus then shifts to\nspecific tractable results for the remarkable case of Poisson HIBP, which\ncorrespond to generalizations of mixed Poisson random count models arising in\ngenetics, imaging, topic modeling, random occupancy, and species sampling\nmodels. We show they also have important relations to Bayesian nonparametric\nlatent class models appearing in the literature. Furthermore, we show that all\ngeneral HIBP may be coupled to Poisson HIBP, allowing for further analysis of\nsuch processes."
                },
                "authors": [
                    {
                        "name": "Lancelot Fitzgerald James"
                    },
                    {
                        "name": "Juho Lee"
                    },
                    {
                        "name": "Abhinav Pandey"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Pandey"
                },
                "author": "Abhinav Pandey",
                "arxiv_comment": "This an extensive rewrite of the earlier version ArXiv 2304.05244\n  v1., New results and new focus on Poisson HIBP cases",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.05244v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.05244v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60C05, 60G09, 60G57, 60E99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02817v1",
                "updated": "2024-09-04T15:35:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    15,
                    35,
                    18,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T15:35:18Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    15,
                    35,
                    18,
                    2,
                    248,
                    0
                ],
                "title": "Obsidian: Cooperative State-Space Exploration for Performant Inference\n  on Secure ML Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Obsidian: Cooperative State-Space Exploration for Performant Inference\n  on Secure ML Accelerators"
                },
                "summary": "Trusted execution environments (TEEs) for machine learning accelerators are\nindispensable in secure and efficient ML inference. Optimizing workloads\nthrough state-space exploration for the accelerator architectures improves\nperformance and energy consumption. However, such explorations are expensive\nand slow due to the large search space. Current research has to use fast\nanalytical models that forego critical hardware details and cross-layer\nopportunities unique to the hardware security primitives. While cycle-accurate\nmodels can theoretically reach better designs, their high runtime cost\nrestricts them to a smaller state space.\n  We present Obsidian, an optimization framework for finding the optimal\nmapping from ML kernels to a secure ML accelerator. Obsidian addresses the\nabove challenge by exploring the state space using analytical and\ncycle-accurate models cooperatively. The two main exploration components\ninclude: (1) A secure accelerator analytical model, that includes the effect of\nsecure hardware while traversing the large mapping state space and produce the\nbest m model mappings; (2) A compiler profiling step on a cycle-accurate model,\nthat captures runtime bottlenecks to further improve execution runtime, energy\nand resource utilization and find the optimal model mapping.\n  We compare our results to a baseline secure accelerator, comprising of the\nstate-of-the-art security schemes obtained from guardnn [ 33 ] and sesame [11].\nThe analytical model reduces the inference latency by 20.5% for a cloud and\n8.4% for an edge deployment with an energy improvement of 24% and 19%\nrespectively. The cycle-accurate model, further reduces the latency by 9.1% for\na cloud and 12.2% for an edge with an energy improvement of 13.8% and 13.1%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trusted execution environments (TEEs) for machine learning accelerators are\nindispensable in secure and efficient ML inference. Optimizing workloads\nthrough state-space exploration for the accelerator architectures improves\nperformance and energy consumption. However, such explorations are expensive\nand slow due to the large search space. Current research has to use fast\nanalytical models that forego critical hardware details and cross-layer\nopportunities unique to the hardware security primitives. While cycle-accurate\nmodels can theoretically reach better designs, their high runtime cost\nrestricts them to a smaller state space.\n  We present Obsidian, an optimization framework for finding the optimal\nmapping from ML kernels to a secure ML accelerator. Obsidian addresses the\nabove challenge by exploring the state space using analytical and\ncycle-accurate models cooperatively. The two main exploration components\ninclude: (1) A secure accelerator analytical model, that includes the effect of\nsecure hardware while traversing the large mapping state space and produce the\nbest m model mappings; (2) A compiler profiling step on a cycle-accurate model,\nthat captures runtime bottlenecks to further improve execution runtime, energy\nand resource utilization and find the optimal model mapping.\n  We compare our results to a baseline secure accelerator, comprising of the\nstate-of-the-art security schemes obtained from guardnn [ 33 ] and sesame [11].\nThe analytical model reduces the inference latency by 20.5% for a cloud and\n8.4% for an edge deployment with an energy improvement of 24% and 19%\nrespectively. The cycle-accurate model, further reduces the latency by 9.1% for\na cloud and 12.2% for an edge with an energy improvement of 13.8% and 13.1%."
                },
                "authors": [
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Shijia Wei"
                    },
                    {
                        "name": "Prakash Ramrakhyani"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15778v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15778v3",
                "updated": "2024-09-05T10:30:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    10,
                    30,
                    39,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-28T13:16:41Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    16,
                    41,
                    2,
                    241,
                    0
                ],
                "title": "LogicGame: Benchmarking Rule-Based Reasoning Abilities of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogicGame: Benchmarking Rule-Based Reasoning Abilities of Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated notable capabilities across\nvarious tasks, showcasing complex problem-solving abilities. Understanding and\nexecuting complex rules, along with multi-step planning, are fundamental to\nlogical reasoning and critical for practical LLM agents and decision-making\nsystems. However, evaluating LLMs as effective rule-based executors and\nplanners remains underexplored. In this paper, we introduce LogicGame, a novel\nbenchmark designed to evaluate the comprehensive rule understanding, execution,\nand planning capabilities of LLMs. Unlike traditional benchmarks, LogicGame\nprovides diverse games that contain a series of rules with an initial state,\nrequiring models to comprehend and apply predefined regulations to solve\nproblems. We create simulated scenarios in which models execute or plan\noperations to achieve specific outcomes. These game scenarios are specifically\ndesigned to distinguish logical reasoning from mere knowledge by relying\nexclusively on predefined rules. This separation allows for a pure assessment\nof rule-based reasoning capabilities. The evaluation considers not only final\noutcomes but also intermediate steps, providing a comprehensive assessment of\nmodel performance. Moreover, these intermediate steps are deterministic and can\nbe automatically verified. LogicGame defines game scenarios with varying\ndifficulty levels, from simple rule applications to complex reasoning chains,\nin order to offer a precise evaluation of model performance on rule\nunderstanding and multi-step execution. Utilizing LogicGame, we test various\nLLMs and identify notable shortcomings in their rule-based logical reasoning\nabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated notable capabilities across\nvarious tasks, showcasing complex problem-solving abilities. Understanding and\nexecuting complex rules, along with multi-step planning, are fundamental to\nlogical reasoning and critical for practical LLM agents and decision-making\nsystems. However, evaluating LLMs as effective rule-based executors and\nplanners remains underexplored. In this paper, we introduce LogicGame, a novel\nbenchmark designed to evaluate the comprehensive rule understanding, execution,\nand planning capabilities of LLMs. Unlike traditional benchmarks, LogicGame\nprovides diverse games that contain a series of rules with an initial state,\nrequiring models to comprehend and apply predefined regulations to solve\nproblems. We create simulated scenarios in which models execute or plan\noperations to achieve specific outcomes. These game scenarios are specifically\ndesigned to distinguish logical reasoning from mere knowledge by relying\nexclusively on predefined rules. This separation allows for a pure assessment\nof rule-based reasoning capabilities. The evaluation considers not only final\noutcomes but also intermediate steps, providing a comprehensive assessment of\nmodel performance. Moreover, these intermediate steps are deterministic and can\nbe automatically verified. LogicGame defines game scenarios with varying\ndifficulty levels, from simple rule applications to complex reasoning chains,\nin order to offer a precise evaluation of model performance on rule\nunderstanding and multi-step execution. Utilizing LogicGame, we test various\nLLMs and identify notable shortcomings in their rule-based logical reasoning\nabilities."
                },
                "authors": [
                    {
                        "name": "Jiayi Gui"
                    },
                    {
                        "name": "Yiming Liu"
                    },
                    {
                        "name": "Jiale Cheng"
                    },
                    {
                        "name": "Xiaotao Gu"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15778v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15778v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02795v1",
                "updated": "2024-09-04T15:11:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    15,
                    11,
                    55,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T15:11:55Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    15,
                    11,
                    55,
                    2,
                    248,
                    0
                ],
                "title": "Towards a Unified View of Preference Learning for Large Language Models:\n  A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Unified View of Preference Learning for Large Language Models:\n  A Survey"
                },
                "summary": "Large Language Models (LLMs) exhibit remarkably powerful capabilities. One of\nthe crucial factors to achieve success is aligning the LLM's output with human\npreferences. This alignment process often requires only a small amount of data\nto efficiently enhance the LLM's performance. While effective, research in this\narea spans multiple domains, and the methods involved are relatively complex to\nunderstand. The relationships between different methods have been\nunder-explored, limiting the development of the preference alignment. In light\nof this, we break down the existing popular alignment strategies into different\ncomponents and provide a unified framework to study the current alignment\nstrategies, thereby establishing connections among them. In this survey, we\ndecompose all the strategies in preference learning into four components:\nmodel, data, feedback, and algorithm. This unified view offers an in-depth\nunderstanding of existing alignment algorithms and also opens up possibilities\nto synergize the strengths of different strategies. Furthermore, we present\ndetailed working examples of prevalent existing algorithms to facilitate a\ncomprehensive understanding for the readers. Finally, based on our unified\nperspective, we explore the challenges and future research directions for\naligning large language models with human preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit remarkably powerful capabilities. One of\nthe crucial factors to achieve success is aligning the LLM's output with human\npreferences. This alignment process often requires only a small amount of data\nto efficiently enhance the LLM's performance. While effective, research in this\narea spans multiple domains, and the methods involved are relatively complex to\nunderstand. The relationships between different methods have been\nunder-explored, limiting the development of the preference alignment. In light\nof this, we break down the existing popular alignment strategies into different\ncomponents and provide a unified framework to study the current alignment\nstrategies, thereby establishing connections among them. In this survey, we\ndecompose all the strategies in preference learning into four components:\nmodel, data, feedback, and algorithm. This unified view offers an in-depth\nunderstanding of existing alignment algorithms and also opens up possibilities\nto synergize the strengths of different strategies. Furthermore, we present\ndetailed working examples of prevalent existing algorithms to facilitate a\ncomprehensive understanding for the readers. Finally, based on our unified\nperspective, we explore the challenges and future research directions for\naligning large language models with human preferences."
                },
                "authors": [
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Feifan Song"
                    },
                    {
                        "name": "Yibo Miao"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Zhe Yang"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Helan Hu"
                    },
                    {
                        "name": "Runxin Xu"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Ce Zheng"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Daoguang Zan"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Zeyu Cui"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Lei Sha"
                    },
                    {
                        "name": "Houfeng Wang"
                    },
                    {
                        "name": "Zhifang Sui"
                    },
                    {
                        "name": "Peiyi Wang"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Baobao Chang"
                    }
                ],
                "author_detail": {
                    "name": "Baobao Chang"
                },
                "author": "Baobao Chang",
                "arxiv_comment": "Initial Commit, 21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00105v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00105v2",
                "updated": "2024-09-04T14:40:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    14,
                    40,
                    14,
                    2,
                    248,
                    0
                ],
                "published": "2024-08-27T14:40:16Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    40,
                    16,
                    1,
                    240,
                    0
                ],
                "title": "Negation Blindness in Large Language Models: Unveiling the NO Syndrome\n  in Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Negation Blindness in Large Language Models: Unveiling the NO Syndrome\n  in Image Generation"
                },
                "summary": "Foundational Large Language Models (LLMs) have changed the way we perceive\ntechnology. They have been shown to excel in tasks ranging from poem writing\nand coding to essay generation and puzzle solving. With the incorporation of\nimage generation capability, they have become more comprehensive and versatile\nAI tools. At the same time, researchers are striving to identify the\nlimitations of these tools to improve them further. Currently identified flaws\ninclude hallucination, biases, and bypassing restricted commands to generate\nharmful content. In the present work, we have identified a fundamental\nlimitation related to the image generation ability of LLMs, and termed it The\nNO Syndrome. This negation blindness refers to LLMs inability to correctly\ncomprehend NO related natural language prompts to generate the desired images.\nInterestingly, all tested LLMs including GPT-4, Gemini, and Copilot were found\nto be suffering from this syndrome. To demonstrate the generalization of this\nlimitation, we carried out simulation experiments and conducted entropy-based\nand benchmark statistical analysis tests on various LLMs in multiple languages,\nincluding English, Hindi, and French. We conclude that the NO syndrome is a\nsignificant flaw in current LLMs that needs to be addressed. A related finding\nof this study showed a consistent discrepancy between image and textual\nresponses as a result of this NO syndrome. We posit that the introduction of a\nnegation context-aware reinforcement learning based feedback loop between the\nLLMs textual response and generated image could help ensure the generated text\nis based on both the LLMs correct contextual understanding of the negation\nquery and the generated visual output.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundational Large Language Models (LLMs) have changed the way we perceive\ntechnology. They have been shown to excel in tasks ranging from poem writing\nand coding to essay generation and puzzle solving. With the incorporation of\nimage generation capability, they have become more comprehensive and versatile\nAI tools. At the same time, researchers are striving to identify the\nlimitations of these tools to improve them further. Currently identified flaws\ninclude hallucination, biases, and bypassing restricted commands to generate\nharmful content. In the present work, we have identified a fundamental\nlimitation related to the image generation ability of LLMs, and termed it The\nNO Syndrome. This negation blindness refers to LLMs inability to correctly\ncomprehend NO related natural language prompts to generate the desired images.\nInterestingly, all tested LLMs including GPT-4, Gemini, and Copilot were found\nto be suffering from this syndrome. To demonstrate the generalization of this\nlimitation, we carried out simulation experiments and conducted entropy-based\nand benchmark statistical analysis tests on various LLMs in multiple languages,\nincluding English, Hindi, and French. We conclude that the NO syndrome is a\nsignificant flaw in current LLMs that needs to be addressed. A related finding\nof this study showed a consistent discrepancy between image and textual\nresponses as a result of this NO syndrome. We posit that the introduction of a\nnegation context-aware reinforcement learning based feedback loop between the\nLLMs textual response and generated image could help ensure the generated text\nis based on both the LLMs correct contextual understanding of the negation\nquery and the generated visual output."
                },
                "authors": [
                    {
                        "name": "Mohammad Nadeem"
                    },
                    {
                        "name": "Shahab Saquib Sohail"
                    },
                    {
                        "name": "Erik Cambria"
                    },
                    {
                        "name": "Bjrn W. Schuller"
                    },
                    {
                        "name": "Amir Hussain"
                    }
                ],
                "author_detail": {
                    "name": "Amir Hussain"
                },
                "author": "Amir Hussain",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00105v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00105v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02763v1",
                "updated": "2024-09-04T14:39:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    14,
                    39,
                    11,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T14:39:11Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    14,
                    39,
                    11,
                    2,
                    248,
                    0
                ],
                "title": "Federated Quantum-Train with Batched Parameter Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Quantum-Train with Batched Parameter Generation"
                },
                "summary": "In this work, we introduce the Federated Quantum-Train (QT) framework, which\nintegrates the QT model into federated learning to leverage quantum computing\nfor distributed learning systems. Quantum client nodes employ Quantum Neural\nNetworks (QNNs) and a mapping model to generate local target model parameters,\nwhich are updated and aggregated at a central node. Testing with a VGG-like\nconvolutional neural network on the CIFAR-10 dataset, our approach\nsignificantly reduces qubit usage from 19 to as low as 8 qubits while reducing\ngeneralization error. The QT method mitigates overfitting observed in classical\nmodels, aligning training and testing accuracy and improving performance in\nhighly compressed models. Notably, the Federated QT framework does not require\na quantum computer during inference, enhancing practicality given current\nquantum hardware limitations. This work highlights the potential of integrating\nquantum techniques into federated learning, paving the way for advancements in\nquantum machine learning and distributed learning systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce the Federated Quantum-Train (QT) framework, which\nintegrates the QT model into federated learning to leverage quantum computing\nfor distributed learning systems. Quantum client nodes employ Quantum Neural\nNetworks (QNNs) and a mapping model to generate local target model parameters,\nwhich are updated and aggregated at a central node. Testing with a VGG-like\nconvolutional neural network on the CIFAR-10 dataset, our approach\nsignificantly reduces qubit usage from 19 to as low as 8 qubits while reducing\ngeneralization error. The QT method mitigates overfitting observed in classical\nmodels, aligning training and testing accuracy and improving performance in\nhighly compressed models. Notably, the Federated QT framework does not require\na quantum computer during inference, enhancing practicality given current\nquantum hardware limitations. This work highlights the potential of integrating\nquantum techniques into federated learning, paving the way for advancements in\nquantum machine learning and distributed learning systems."
                },
                "authors": [
                    {
                        "name": "Chen-Yu Liu"
                    },
                    {
                        "name": "Samuel Yen-Chi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Yen-Chi Chen"
                },
                "author": "Samuel Yen-Chi Chen",
                "arxiv_comment": "6 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02751v1",
                "updated": "2024-09-04T14:30:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    14,
                    30,
                    13,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T14:30:13Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    14,
                    30,
                    13,
                    2,
                    248,
                    0
                ],
                "title": "A Comparative Study of Pre-training and Self-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparative Study of Pre-training and Self-training"
                },
                "summary": "Pre-training and self-training are two approaches to semi-supervised\nlearning. The comparison between pre-training and self-training has been\nexplored. However, the previous works led to confusing findings: self-training\noutperforms pre-training experienced on some tasks in computer vision, and\ncontrarily, pre-training outperforms self-training experienced on some tasks in\nnatural language processing, under certain conditions of incomparable settings.\nWe propose, comparatively and exhaustively, an ensemble method to empirical\nstudy all feasible training paradigms combining pre-training, self-training,\nand fine-tuning within consistent foundational settings comparable to data\naugmentation. We conduct experiments on six datasets, four data augmentation,\nand imbalanced data for sentiment analysis and natural language inference\ntasks. Our findings confirm that the pre-training and fine-tuning paradigm\nyields the best overall performances. Moreover, self-training offers no\nadditional benefits when combined with semi-supervised pre-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-training and self-training are two approaches to semi-supervised\nlearning. The comparison between pre-training and self-training has been\nexplored. However, the previous works led to confusing findings: self-training\noutperforms pre-training experienced on some tasks in computer vision, and\ncontrarily, pre-training outperforms self-training experienced on some tasks in\nnatural language processing, under certain conditions of incomparable settings.\nWe propose, comparatively and exhaustively, an ensemble method to empirical\nstudy all feasible training paradigms combining pre-training, self-training,\nand fine-tuning within consistent foundational settings comparable to data\naugmentation. We conduct experiments on six datasets, four data augmentation,\nand imbalanced data for sentiment analysis and natural language inference\ntasks. Our findings confirm that the pre-training and fine-tuning paradigm\nyields the best overall performances. Moreover, self-training offers no\nadditional benefits when combined with semi-supervised pre-training."
                },
                "authors": [
                    {
                        "name": "Yiheng Wang"
                    },
                    {
                        "name": "Jiayu Lin"
                    },
                    {
                        "name": "Zuoquan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zuoquan Lin"
                },
                "author": "Zuoquan Lin",
                "arxiv_comment": "19 pages, 2 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.17439v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.17439v4",
                "updated": "2024-09-04T14:20:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    14,
                    20,
                    21,
                    2,
                    248,
                    0
                ],
                "published": "2023-09-29T17:51:25Z",
                "published_parsed": [
                    2023,
                    9,
                    29,
                    17,
                    51,
                    25,
                    4,
                    272,
                    0
                ],
                "title": "Self-intercalation as origin of high-temperature ferromagnetism in\n  epitaxially grown Fe5GeTe2 thin films",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-intercalation as origin of high-temperature ferromagnetism in\n  epitaxially grown Fe5GeTe2 thin films"
                },
                "summary": "The role of self-intercalation in 2D van der Waals materials is key to the\nunderstanding of many of their properties. Here we show that the magnetic\nordering temperature of thin films of the 2D ferromagnet Fe5GeTe2 is\nsubstantially increased by self-intercalated Fe that resides in the van der\nWaals gaps. The epitaxial films were prepared by molecular beam epitaxy and\ntheir magnetic properties explored by element-specific x-ray magnetic circular\ndichroism that showed ferromagnetic ordering up to 375 K. Both surface and bulk\nsensitive x-ray absorption modes were used to confirm that the magnetic signal\nis of an intrinsic nature. Fe occupation within the van der Waals gap was\ndetermined by x-ray diffraction which showed a notably higher occupation with\nrespect to bulk crystals. We thus infer, supported by first-principles\ncalculations, that the higher magnetic ordering temperature results from an\nincreased exchange interaction between the individual Fe5GeTe2 layers mediated\nby Fe atoms residing within the van der Waals gaps. Our findings establish\nself-intercalation during epitaxial growth as an efficient mechanism to achieve\nhigh-temperature magnetism in a broad class of van der Waals materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The role of self-intercalation in 2D van der Waals materials is key to the\nunderstanding of many of their properties. Here we show that the magnetic\nordering temperature of thin films of the 2D ferromagnet Fe5GeTe2 is\nsubstantially increased by self-intercalated Fe that resides in the van der\nWaals gaps. The epitaxial films were prepared by molecular beam epitaxy and\ntheir magnetic properties explored by element-specific x-ray magnetic circular\ndichroism that showed ferromagnetic ordering up to 375 K. Both surface and bulk\nsensitive x-ray absorption modes were used to confirm that the magnetic signal\nis of an intrinsic nature. Fe occupation within the van der Waals gap was\ndetermined by x-ray diffraction which showed a notably higher occupation with\nrespect to bulk crystals. We thus infer, supported by first-principles\ncalculations, that the higher magnetic ordering temperature results from an\nincreased exchange interaction between the individual Fe5GeTe2 layers mediated\nby Fe atoms residing within the van der Waals gaps. Our findings establish\nself-intercalation during epitaxial growth as an efficient mechanism to achieve\nhigh-temperature magnetism in a broad class of van der Waals materials."
                },
                "authors": [
                    {
                        "name": "M. Silinskas"
                    },
                    {
                        "name": "S. Senz"
                    },
                    {
                        "name": "P. Gargiani"
                    },
                    {
                        "name": "A. M. Ruiz"
                    },
                    {
                        "name": "B. Kalkofen"
                    },
                    {
                        "name": "I. Kostanovskiy"
                    },
                    {
                        "name": "K. Mohseni"
                    },
                    {
                        "name": "J. J. Baldov"
                    },
                    {
                        "name": "H. L. Meyerheim"
                    },
                    {
                        "name": "S. S. P. Parkin"
                    },
                    {
                        "name": "A. Bedoya-Pinto"
                    }
                ],
                "author_detail": {
                    "name": "A. Bedoya-Pinto"
                },
                "author": "A. Bedoya-Pinto",
                "arxiv_comment": "10 pages, 3 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.17439v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.17439v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12965v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12965v2",
                "updated": "2024-09-04T14:10:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    14,
                    10,
                    43,
                    2,
                    248,
                    0
                ],
                "published": "2024-05-21T17:45:36Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    17,
                    45,
                    36,
                    1,
                    142,
                    0
                ],
                "title": "The future of cosmological likelihood-based inference: accelerated\n  high-dimensional parameter estimation and model comparison",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The future of cosmological likelihood-based inference: accelerated\n  high-dimensional parameter estimation and model comparison"
                },
                "summary": "We advocate for a new paradigm of cosmological likelihood-based inference,\nleveraging recent developments in machine learning and its underlying\ntechnology, to accelerate Bayesian inference in high-dimensional settings.\nSpecifically, we combine (i) emulation, where a machine learning model is\ntrained to mimic cosmological observables, e.g. CosmoPower-JAX; (ii)\ndifferentiable and probabilistic programming, e.g. JAX and NumPyro,\nrespectively; (iii) scalable Markov chain Monte Carlo (MCMC) sampling\ntechniques that exploit gradients, e.g. Hamiltonian Monte Carlo; and (iv)\ndecoupled and scalable Bayesian model selection techniques that compute the\nBayesian evidence purely from posterior samples, e.g. the learned harmonic mean\nimplemented in harmonic. This paradigm allows us to carry out a complete\nBayesian analysis, including both parameter estimation and model selection, in\na fraction of the time of traditional approaches. First, we demonstrate the\napplication of this paradigm on a simulated cosmic shear analysis for a Stage\nIV survey in 37- and 39-dimensional parameter spaces, comparing $\\Lambda$CDM\nand a dynamical dark energy model ($w_0w_a$CDM). We recover posterior contours\nand evidence estimates that are in excellent agreement with those computed by\nthe traditional nested sampling approach while reducing the computational cost\nfrom 8 months on 48 CPU cores to 2 days on 12 GPUs. Second, we consider a joint\nanalysis between three simulated next-generation surveys, each performing a\n3x2pt analysis, resulting in 157- and 159-dimensional parameter spaces.\nStandard nested sampling techniques are simply unlikely to be feasible in this\nhigh-dimensional setting, requiring a projected 12 years of compute time on 48\nCPU cores; on the other hand, the proposed approach only requires 8 days of\ncompute time on 24 GPUs. All packages used in our analyses are publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We advocate for a new paradigm of cosmological likelihood-based inference,\nleveraging recent developments in machine learning and its underlying\ntechnology, to accelerate Bayesian inference in high-dimensional settings.\nSpecifically, we combine (i) emulation, where a machine learning model is\ntrained to mimic cosmological observables, e.g. CosmoPower-JAX; (ii)\ndifferentiable and probabilistic programming, e.g. JAX and NumPyro,\nrespectively; (iii) scalable Markov chain Monte Carlo (MCMC) sampling\ntechniques that exploit gradients, e.g. Hamiltonian Monte Carlo; and (iv)\ndecoupled and scalable Bayesian model selection techniques that compute the\nBayesian evidence purely from posterior samples, e.g. the learned harmonic mean\nimplemented in harmonic. This paradigm allows us to carry out a complete\nBayesian analysis, including both parameter estimation and model selection, in\na fraction of the time of traditional approaches. First, we demonstrate the\napplication of this paradigm on a simulated cosmic shear analysis for a Stage\nIV survey in 37- and 39-dimensional parameter spaces, comparing $\\Lambda$CDM\nand a dynamical dark energy model ($w_0w_a$CDM). We recover posterior contours\nand evidence estimates that are in excellent agreement with those computed by\nthe traditional nested sampling approach while reducing the computational cost\nfrom 8 months on 48 CPU cores to 2 days on 12 GPUs. Second, we consider a joint\nanalysis between three simulated next-generation surveys, each performing a\n3x2pt analysis, resulting in 157- and 159-dimensional parameter spaces.\nStandard nested sampling techniques are simply unlikely to be feasible in this\nhigh-dimensional setting, requiring a projected 12 years of compute time on 48\nCPU cores; on the other hand, the proposed approach only requires 8 days of\ncompute time on 24 GPUs. All packages used in our analyses are publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Davide Piras"
                    },
                    {
                        "name": "Alicja Polanska"
                    },
                    {
                        "name": "Alessio Spurio Mancini"
                    },
                    {
                        "name": "Matthew A. Price"
                    },
                    {
                        "name": "Jason D. McEwen"
                    }
                ],
                "author_detail": {
                    "name": "Jason D. McEwen"
                },
                "author": "Jason D. McEwen",
                "arxiv_doi": "10.33232/001c.123368",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.33232/001c.123368",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.12965v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12965v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 6 figures. Accepted for publication in the Open Journal of\n  Astrophysics. Codes available at\n  https://github.com/alessiospuriomancini/cosmopower,\n  https://github.com/dpiras/cosmopower-jax,\n  https://github.com/astro-informatics/harmonic/",
                "arxiv_journal_ref": "Open Journal of Astrophysics, Vol. 7, September 5th 2024",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04183v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04183v2",
                "updated": "2024-09-04T14:07:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    14,
                    7,
                    7,
                    2,
                    248,
                    0
                ],
                "published": "2024-07-04T23:05:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    23,
                    5,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "Seeing Like an AI: How LLMs Apply (and Misapply) Wikipedia Neutrality\n  Norms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seeing Like an AI: How LLMs Apply (and Misapply) Wikipedia Neutrality\n  Norms"
                },
                "summary": "Large language models (LLMs) are trained on broad corpora and then used in\ncommunities with specialized norms. Is providing LLMs with community rules\nenough for models to follow these norms? We evaluate LLMs' capacity to detect\n(Task 1) and correct (Task 2) biased Wikipedia edits according to Wikipedia's\nNeutral Point of View (NPOV) policy. LLMs struggled with bias detection,\nachieving only 64% accuracy on a balanced dataset. Models exhibited contrasting\nbiases (some under- and others over-predicted bias), suggesting distinct priors\nabout neutrality. LLMs performed better at generation, removing 79% of words\nremoved by Wikipedia editors. However, LLMs made additional changes beyond\nWikipedia editors' simpler neutralizations, resulting in high-recall but\nlow-precision editing. Interestingly, crowdworkers rated AI rewrites as more\nneutral (70%) and fluent (61%) than Wikipedia-editor rewrites. Qualitative\nanalysis found LLMs sometimes applied NPOV more comprehensively than Wikipedia\neditors but often made extraneous non-NPOV-related changes (such as grammar).\nLLMs may apply rules in ways that resonate with the public but diverge from\ncommunity experts. While potentially effective for generation, LLMs may reduce\neditor agency and increase moderation workload (e.g., verifying additions).\nEven when rules are easy to articulate, having LLMs apply them like community\nmembers may still be difficult.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are trained on broad corpora and then used in\ncommunities with specialized norms. Is providing LLMs with community rules\nenough for models to follow these norms? We evaluate LLMs' capacity to detect\n(Task 1) and correct (Task 2) biased Wikipedia edits according to Wikipedia's\nNeutral Point of View (NPOV) policy. LLMs struggled with bias detection,\nachieving only 64% accuracy on a balanced dataset. Models exhibited contrasting\nbiases (some under- and others over-predicted bias), suggesting distinct priors\nabout neutrality. LLMs performed better at generation, removing 79% of words\nremoved by Wikipedia editors. However, LLMs made additional changes beyond\nWikipedia editors' simpler neutralizations, resulting in high-recall but\nlow-precision editing. Interestingly, crowdworkers rated AI rewrites as more\nneutral (70%) and fluent (61%) than Wikipedia-editor rewrites. Qualitative\nanalysis found LLMs sometimes applied NPOV more comprehensively than Wikipedia\neditors but often made extraneous non-NPOV-related changes (such as grammar).\nLLMs may apply rules in ways that resonate with the public but diverge from\ncommunity experts. While potentially effective for generation, LLMs may reduce\neditor agency and increase moderation workload (e.g., verifying additions).\nEven when rules are easy to articulate, having LLMs apply them like community\nmembers may still be difficult."
                },
                "authors": [
                    {
                        "name": "Joshua Ashkinaze"
                    },
                    {
                        "name": "Ruijia Guan"
                    },
                    {
                        "name": "Laura Kurek"
                    },
                    {
                        "name": "Eytan Adar"
                    },
                    {
                        "name": "Ceren Budak"
                    },
                    {
                        "name": "Eric Gilbert"
                    }
                ],
                "author_detail": {
                    "name": "Eric Gilbert"
                },
                "author": "Eric Gilbert",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04183v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04183v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02727v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02727v2",
                "updated": "2024-09-05T07:17:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    7,
                    17,
                    59,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-04T14:01:48Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    14,
                    1,
                    48,
                    2,
                    248,
                    0
                ],
                "title": "Pooling And Attention: What Are Effective Designs For LLM-Based\n  Embedding Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pooling And Attention: What Are Effective Designs For LLM-Based\n  Embedding Models?"
                },
                "summary": "The significant advancements of Large Language Models (LLMs) in generative\ntasks have led to a growing body of work exploring LLM-based embedding models.\nWhile these models, employing different pooling and attention strategies, have\nachieved state-of-the-art performance on public embedding benchmarks, questions\nstill arise about what constitutes an effective design for LLM-based embedding\nmodels. However, these models are often trained on different datasets, using\ndifferent LLM base models or training settings. Moreover, evaluations on public\nembedding benchmarks often fail to report statistical significance, making it\ndifficult to determine which designs truly contribute to final performance.\nThis complicates the process for practitioners seeking optimal training recipes\nfor LLM-based embedding models. In this study, we conduct a large-scale\nexperiment by training a series of LLM-based embedding models using the same\ntraining data and base model but differing in their pooling and attention\nstrategies. The results show that there is no one-size-fits-all solution: while\nbidirectional attention and an additional trainable pooling layer outperform in\ntext similarity and information retrieval tasks, they do not significantly\nsurpass simpler designs like EOS-last token pooling and default causal\nattention in clustering and classification tasks. Furthermore, we propose a new\npooling strategy, Multi-Layers Trainable Pooling, which transforms the outputs\nof all hidden layers, rather than just the last layer, using a cross-attention\nnetwork. This method proves to be statistically superior in text similarity and\nretrieval tasks compared to existing pooling methods. Overall, this paper sheds\nlight on effective training strategies for LLM-based embedding models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The significant advancements of Large Language Models (LLMs) in generative\ntasks have led to a growing body of work exploring LLM-based embedding models.\nWhile these models, employing different pooling and attention strategies, have\nachieved state-of-the-art performance on public embedding benchmarks, questions\nstill arise about what constitutes an effective design for LLM-based embedding\nmodels. However, these models are often trained on different datasets, using\ndifferent LLM base models or training settings. Moreover, evaluations on public\nembedding benchmarks often fail to report statistical significance, making it\ndifficult to determine which designs truly contribute to final performance.\nThis complicates the process for practitioners seeking optimal training recipes\nfor LLM-based embedding models. In this study, we conduct a large-scale\nexperiment by training a series of LLM-based embedding models using the same\ntraining data and base model but differing in their pooling and attention\nstrategies. The results show that there is no one-size-fits-all solution: while\nbidirectional attention and an additional trainable pooling layer outperform in\ntext similarity and information retrieval tasks, they do not significantly\nsurpass simpler designs like EOS-last token pooling and default causal\nattention in clustering and classification tasks. Furthermore, we propose a new\npooling strategy, Multi-Layers Trainable Pooling, which transforms the outputs\nof all hidden layers, rather than just the last layer, using a cross-attention\nnetwork. This method proves to be statistically superior in text similarity and\nretrieval tasks compared to existing pooling methods. Overall, this paper sheds\nlight on effective training strategies for LLM-based embedding models."
                },
                "authors": [
                    {
                        "name": "Yixuan Tang"
                    },
                    {
                        "name": "Yi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Yang"
                },
                "author": "Yi Yang",
                "arxiv_comment": "https://github.com/yixuantt/PoolingAndAttn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02727v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02727v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02718v1",
                "updated": "2024-09-04T13:54:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    54,
                    38,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T13:54:38Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    54,
                    38,
                    2,
                    248,
                    0
                ],
                "title": "Alignment-Aware Model Extraction Attacks on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment-Aware Model Extraction Attacks on Large Language Models"
                },
                "summary": "Model extraction attacks (MEAs) on large language models (LLMs) have received\nincreasing research attention lately. Existing attack methods on LLMs inherit\nthe extraction strategies from those designed for deep neural networks (DNNs)\nyet neglect the inconsistency of training tasks between MEA and LLMs'\nalignments. As such, they result in poor attack performances. To tackle this\nissue, we present Locality Reinforced Distillation (LoRD), a novel model\nextraction attack algorithm specifically for LLMs. In particular, we design a\npolicy-gradient-style training task, which utilizes victim models' responses as\na signal to guide the crafting of preference for the local model. Theoretical\nanalysis has shown that i) LoRD's convergence procedure in MEAs is consistent\nwith the alignments of LLMs, and ii) LoRD can reduce query complexity while\nmitigating watermark protection through exploration-based stealing. Extensive\nexperiments on domain-specific extractions demonstrate the superiority of our\nmethod by examining the extraction of various state-of-the-art commercial LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model extraction attacks (MEAs) on large language models (LLMs) have received\nincreasing research attention lately. Existing attack methods on LLMs inherit\nthe extraction strategies from those designed for deep neural networks (DNNs)\nyet neglect the inconsistency of training tasks between MEA and LLMs'\nalignments. As such, they result in poor attack performances. To tackle this\nissue, we present Locality Reinforced Distillation (LoRD), a novel model\nextraction attack algorithm specifically for LLMs. In particular, we design a\npolicy-gradient-style training task, which utilizes victim models' responses as\na signal to guide the crafting of preference for the local model. Theoretical\nanalysis has shown that i) LoRD's convergence procedure in MEAs is consistent\nwith the alignments of LLMs, and ii) LoRD can reduce query complexity while\nmitigating watermark protection through exploration-based stealing. Extensive\nexperiments on domain-specific extractions demonstrate the superiority of our\nmethod by examining the extraction of various state-of-the-art commercial LLMs."
                },
                "authors": [
                    {
                        "name": "Zi Liang"
                    },
                    {
                        "name": "Qingqing Ye"
                    },
                    {
                        "name": "Yanyun Wang"
                    },
                    {
                        "name": "Sen Zhang"
                    },
                    {
                        "name": "Yaxin Xiao"
                    },
                    {
                        "name": "Ronghua Li"
                    },
                    {
                        "name": "Jianliang Xu"
                    },
                    {
                        "name": "Haibo Hu"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Hu"
                },
                "author": "Haibo Hu",
                "arxiv_comment": "Source code: https://github.com/liangzid/alignmentExtraction",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02711v1",
                "updated": "2024-09-04T13:49:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    49,
                    19,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T13:49:19Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    49,
                    19,
                    2,
                    248,
                    0
                ],
                "title": "Creating a Gen-AI based Track and Trace Assistant MVP (SuperTracy) for\n  PostNL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating a Gen-AI based Track and Trace Assistant MVP (SuperTracy) for\n  PostNL"
                },
                "summary": "The developments in the field of generative AI has brought a lot of\nopportunities for companies, for instance to improve efficiency in customer\nservice and automating tasks. PostNL, the biggest parcel and E-commerce\ncorporation of the Netherlands wants to use generative AI to enhance the\ncommunication around track and trace of parcels. During the internship a\nMinimal Viable Product (MVP) is created to showcase the value of using\ngenerative AI technologies, to enhance parcel tracking, analyzing the parcel's\njourney and being able to communicate about it in an easy to understand manner.\nThe primary goal was to develop an in-house LLM-based system, reducing\ndependency on external platforms and establishing the feasibility of a\ndedicated generative AI team within the company. This multi-agent LLM based\nsystem aimed to construct parcel journey stories and identify logistical\ndisruptions with heightened efficiency and accuracy. The research involved\ndeploying a sophisticated AI-driven communication system, employing\nRetrieval-Augmented Generation (RAG) for enhanced response precision, and\noptimizing large language models (LLMs) tailored to domain specific tasks.\n  The MVP successfully implemented a multi-agent open-source LLM system, called\nSuperTracy. SuperTracy is capable of autonomously managing a broad spectrum of\nuser inquiries and improving internal knowledge handling. Results and\nevaluation demonstrated technological innovation and feasibility, notably in\ncommunication about the track and trace of a parcel, which exceeded initial\nexpectations. These advancements highlight the potential of AI-driven solutions\nin logistics, suggesting many opportunities for further refinement and broader\nimplementation within PostNL operational framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The developments in the field of generative AI has brought a lot of\nopportunities for companies, for instance to improve efficiency in customer\nservice and automating tasks. PostNL, the biggest parcel and E-commerce\ncorporation of the Netherlands wants to use generative AI to enhance the\ncommunication around track and trace of parcels. During the internship a\nMinimal Viable Product (MVP) is created to showcase the value of using\ngenerative AI technologies, to enhance parcel tracking, analyzing the parcel's\njourney and being able to communicate about it in an easy to understand manner.\nThe primary goal was to develop an in-house LLM-based system, reducing\ndependency on external platforms and establishing the feasibility of a\ndedicated generative AI team within the company. This multi-agent LLM based\nsystem aimed to construct parcel journey stories and identify logistical\ndisruptions with heightened efficiency and accuracy. The research involved\ndeploying a sophisticated AI-driven communication system, employing\nRetrieval-Augmented Generation (RAG) for enhanced response precision, and\noptimizing large language models (LLMs) tailored to domain specific tasks.\n  The MVP successfully implemented a multi-agent open-source LLM system, called\nSuperTracy. SuperTracy is capable of autonomously managing a broad spectrum of\nuser inquiries and improving internal knowledge handling. Results and\nevaluation demonstrated technological innovation and feasibility, notably in\ncommunication about the track and trace of a parcel, which exceeded initial\nexpectations. These advancements highlight the potential of AI-driven solutions\nin logistics, suggesting many opportunities for further refinement and broader\nimplementation within PostNL operational framework."
                },
                "authors": [
                    {
                        "name": "Mohammad Reshadati"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Reshadati"
                },
                "author": "Mohammad Reshadati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02705v1",
                "updated": "2024-09-04T13:41:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    41,
                    46,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T13:41:46Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    41,
                    46,
                    2,
                    248,
                    0
                ],
                "title": "A family of toroidal diffusions with exact likelihood inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A family of toroidal diffusions with exact likelihood inference"
                },
                "summary": "We provide a class of diffusion processes for continuous time-varying\nmultivariate angular data with explicit transition probability densities,\nenabling exact likelihood inference. The presented diffusions are\ntime-reversible and can be constructed for any pre-specified stationary\ndistribution on the torus, including highly-multimodal mixtures. We give\nresults on asymptotic likelihood theory allowing one-sample inference and tests\nof linear hypotheses for $k$ groups of diffusions, including homogeneity. We\nshow that exact and direct diffusion bridge simulation is possible too. A class\nof circular jump processes with similar properties is also proposed. Several\nnumerical experiments illustrate the methodology for the circular and\ntwo-dimensional torus cases. The new family of diffusions is applied (i) to\ntest several homogeneity hypotheses on the movement of ants and (ii) to\nsimulate bridges between the three-dimensional backbones of two related\nproteins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We provide a class of diffusion processes for continuous time-varying\nmultivariate angular data with explicit transition probability densities,\nenabling exact likelihood inference. The presented diffusions are\ntime-reversible and can be constructed for any pre-specified stationary\ndistribution on the torus, including highly-multimodal mixtures. We give\nresults on asymptotic likelihood theory allowing one-sample inference and tests\nof linear hypotheses for $k$ groups of diffusions, including homogeneity. We\nshow that exact and direct diffusion bridge simulation is possible too. A class\nof circular jump processes with similar properties is also proposed. Several\nnumerical experiments illustrate the methodology for the circular and\ntwo-dimensional torus cases. The new family of diffusions is applied (i) to\ntest several homogeneity hypotheses on the movement of ants and (ii) to\nsimulate bridges between the three-dimensional backbones of two related\nproteins."
                },
                "authors": [
                    {
                        "name": "Eduardo Garca-Portugus"
                    },
                    {
                        "name": "Michael Srensen"
                    }
                ],
                "author_detail": {
                    "name": "Michael Srensen"
                },
                "author": "Michael Srensen",
                "arxiv_comment": "23 pages, 7 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60J60, 62H11, 62M02",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02697v1",
                "updated": "2024-09-04T13:33:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    33,
                    38,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T13:33:38Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    33,
                    38,
                    2,
                    248,
                    0
                ],
                "title": "Decision Transformer for Enhancing Neural Local Search on the Job Shop\n  Scheduling Problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision Transformer for Enhancing Neural Local Search on the Job Shop\n  Scheduling Problem"
                },
                "summary": "The job shop scheduling problem (JSSP) and its solution algorithms have been\nof enduring interest in both academia and industry for decades. In recent\nyears, machine learning (ML) is playing an increasingly important role in\nadvancing existing and building new heuristic solutions for the JSSP, aiming to\nfind better solutions in shorter computation times. In this paper we build on\ntop of a state-of-the-art deep reinforcement learning (DRL) agent, called\nNeural Local Search (NLS), which can efficiently and effectively control a\nlarge local neighborhood search on the JSSP. In particular, we develop a method\nfor training the decision transformer (DT) algorithm on search trajectories\ntaken by a trained NLS agent to further improve upon the learned\ndecision-making sequences. Our experiments show that the DT successfully learns\nlocal search strategies that are different and, in many cases, more effective\nthan those of the NLS agent itself. In terms of the tradeoff between solution\nquality and acceptable computational time needed for the search, the DT is\nparticularly superior in application scenarios where longer computational times\nare acceptable. In this case, it makes up for the longer inference times\nrequired per search step, which are caused by the larger neural network\narchitecture, through better quality decisions per step. Thereby, the DT\nachieves state-of-the-art results for solving the JSSP with ML-enhanced search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The job shop scheduling problem (JSSP) and its solution algorithms have been\nof enduring interest in both academia and industry for decades. In recent\nyears, machine learning (ML) is playing an increasingly important role in\nadvancing existing and building new heuristic solutions for the JSSP, aiming to\nfind better solutions in shorter computation times. In this paper we build on\ntop of a state-of-the-art deep reinforcement learning (DRL) agent, called\nNeural Local Search (NLS), which can efficiently and effectively control a\nlarge local neighborhood search on the JSSP. In particular, we develop a method\nfor training the decision transformer (DT) algorithm on search trajectories\ntaken by a trained NLS agent to further improve upon the learned\ndecision-making sequences. Our experiments show that the DT successfully learns\nlocal search strategies that are different and, in many cases, more effective\nthan those of the NLS agent itself. In terms of the tradeoff between solution\nquality and acceptable computational time needed for the search, the DT is\nparticularly superior in application scenarios where longer computational times\nare acceptable. In this case, it makes up for the longer inference times\nrequired per search step, which are caused by the larger neural network\narchitecture, through better quality decisions per step. Thereby, the DT\nachieves state-of-the-art results for solving the JSSP with ML-enhanced search."
                },
                "authors": [
                    {
                        "name": "Constantin Waubert de Puiseau"
                    },
                    {
                        "name": "Fabian Wolz"
                    },
                    {
                        "name": "Merlin Montag"
                    },
                    {
                        "name": "Jannik Peters"
                    },
                    {
                        "name": "Hasan Tercan"
                    },
                    {
                        "name": "Tobias Meisen"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Meisen"
                },
                "author": "Tobias Meisen",
                "arxiv_comment": "currently under review for IEEE Transactions on Cybernetics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10502v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10502v2",
                "updated": "2024-09-04T13:31:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    31,
                    31,
                    2,
                    248,
                    0
                ],
                "published": "2024-02-16T08:21:43Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    8,
                    21,
                    43,
                    4,
                    47,
                    0
                ],
                "title": "A possible late-time transition of $M_B$ inferred via neural networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A possible late-time transition of $M_B$ inferred via neural networks"
                },
                "summary": "The strengthening of tensions in the cosmological parameters has led to a\nreconsideration of fundamental aspects of standard cosmology. The tension in\nthe Hubble constant can also be viewed as a tension between local and early\nUniverse constraints on the absolute magnitude $M_B$ of Type Ia supernova. In\nthis work, we reconsider the possibility of a variation of this parameter in a\nmodel-independent way. We employ neural networks to agnostically constrain the\nvalue of the absolute magnitude as well as assess the impact and statistical\nsignificance of a variation in $M_B$ with redshift from the Pantheon+\ncompilation, together with a thorough analysis of the neural network\narchitecture. We find an indication for a possible transition redshift at the\n$z\\approx 1$ region.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strengthening of tensions in the cosmological parameters has led to a\nreconsideration of fundamental aspects of standard cosmology. The tension in\nthe Hubble constant can also be viewed as a tension between local and early\nUniverse constraints on the absolute magnitude $M_B$ of Type Ia supernova. In\nthis work, we reconsider the possibility of a variation of this parameter in a\nmodel-independent way. We employ neural networks to agnostically constrain the\nvalue of the absolute magnitude as well as assess the impact and statistical\nsignificance of a variation in $M_B$ with redshift from the Pantheon+\ncompilation, together with a thorough analysis of the neural network\narchitecture. We find an indication for a possible transition redshift at the\n$z\\approx 1$ region."
                },
                "authors": [
                    {
                        "name": "Purba Mukherjee"
                    },
                    {
                        "name": "Konstantinos F. Dialektopoulos"
                    },
                    {
                        "name": "Jackson Levi Said"
                    },
                    {
                        "name": "Jurgen Mifsud"
                    }
                ],
                "author_detail": {
                    "name": "Jurgen Mifsud"
                },
                "author": "Jurgen Mifsud",
                "arxiv_comment": "13 pages, 9 sets of figures, 2 tables. To appear in JCAP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10502v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10502v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04160v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04160v2",
                "updated": "2024-09-04T13:29:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    29,
                    56,
                    2,
                    248,
                    0
                ],
                "published": "2024-05-07T09:55:05Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    9,
                    55,
                    5,
                    1,
                    128,
                    0
                ],
                "title": "A Causal Explainable Guardrails for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Causal Explainable Guardrails for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have shown impressive performance in natural\nlanguage tasks, but their outputs can exhibit undesirable attributes or biases.\nExisting methods for steering LLMs toward desired attributes often assume\nunbiased representations and rely solely on steering prompts. However, the\nrepresentations learned from pre-training can introduce semantic biases that\ninfluence the steering process, leading to suboptimal results. We propose\nLLMGuardrail, a novel framework that incorporates causal analysis and\nadversarial learning to obtain unbiased steering representations in LLMs.\nLLMGuardrail systematically identifies and blocks the confounding effects of\nbiases, enabling the extraction of unbiased steering representations.\nAdditionally, it includes an explainable component that provides insights into\nthe alignment between the generated output and the desired direction.\nExperiments demonstrate LLMGuardrail's effectiveness in steering LLMs toward\ndesired attributes while mitigating biases. Our work contributes to the\ndevelopment of safe and reliable LLMs that align with desired attributes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive performance in natural\nlanguage tasks, but their outputs can exhibit undesirable attributes or biases.\nExisting methods for steering LLMs toward desired attributes often assume\nunbiased representations and rely solely on steering prompts. However, the\nrepresentations learned from pre-training can introduce semantic biases that\ninfluence the steering process, leading to suboptimal results. We propose\nLLMGuardrail, a novel framework that incorporates causal analysis and\nadversarial learning to obtain unbiased steering representations in LLMs.\nLLMGuardrail systematically identifies and blocks the confounding effects of\nbiases, enabling the extraction of unbiased steering representations.\nAdditionally, it includes an explainable component that provides insights into\nthe alignment between the generated output and the desired direction.\nExperiments demonstrate LLMGuardrail's effectiveness in steering LLMs toward\ndesired attributes while mitigating biases. Our work contributes to the\ndevelopment of safe and reliable LLMs that align with desired attributes."
                },
                "authors": [
                    {
                        "name": "Zhixuan Chu"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Longfei Li"
                    },
                    {
                        "name": "Zhibo Wang"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04160v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04160v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02313v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02313v2",
                "updated": "2024-09-04T13:25:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    25,
                    0,
                    2,
                    248,
                    0
                ],
                "published": "2024-04-02T21:19:50Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    21,
                    19,
                    50,
                    1,
                    93,
                    0
                ],
                "title": "Optimal combination of composite likelihoods using approximate Bayesian\n  computation with application to state-space models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal combination of composite likelihoods using approximate Bayesian\n  computation with application to state-space models"
                },
                "summary": "Composite likelihood provides approximate inference when the full likelihood\nis intractable and sub-likelihood functions of marginal events can be evaluated\nrelatively easily. It has been successfully applied for many complex models.\nHowever, its wider application is limited by two issues. First, weight\nselection of marginal likelihood can have a significant impact on the\ninformation efficiency and is currently an open question. Second, calibrated\nBayesian inference with composite likelihood requires curvature adjustment\nwhich is difficult for dependent data. This work shows that approximate\nBayesian computation (ABC) can properly address these two issues by using\nmultiple composite score functions as summary statistics. First, the\nsummary-based posterior distribution gives the optimal Godambe information\namong a wide class of estimators defined by linear combinations of estimating\nfunctions. Second, to make ABC computationally feasible for models where\nmarginal likelihoods have no closed form, a novel approach is proposed to\nestimate all simulated marginal scores using a Monte Carlo sample with size N.\nSufficient conditions are given for the additional noise to be negligible with\nN fixed as the data size n goes to infinity, and the computational cost is\nO(n). Third, asymptotic properties of ABC with summary statistics having\nheterogeneous convergence rates is derived, and an adaptive scheme to choose\nthe component composite scores is proposed. Numerical studies show that the new\nmethod significantly outperforms the existing Bayesian composite likelihood\nmethods, and the efficiency of adaptively combined composite scores well\napproximates the efficiency of particle MCMC using the full likelihood.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Composite likelihood provides approximate inference when the full likelihood\nis intractable and sub-likelihood functions of marginal events can be evaluated\nrelatively easily. It has been successfully applied for many complex models.\nHowever, its wider application is limited by two issues. First, weight\nselection of marginal likelihood can have a significant impact on the\ninformation efficiency and is currently an open question. Second, calibrated\nBayesian inference with composite likelihood requires curvature adjustment\nwhich is difficult for dependent data. This work shows that approximate\nBayesian computation (ABC) can properly address these two issues by using\nmultiple composite score functions as summary statistics. First, the\nsummary-based posterior distribution gives the optimal Godambe information\namong a wide class of estimators defined by linear combinations of estimating\nfunctions. Second, to make ABC computationally feasible for models where\nmarginal likelihoods have no closed form, a novel approach is proposed to\nestimate all simulated marginal scores using a Monte Carlo sample with size N.\nSufficient conditions are given for the additional noise to be negligible with\nN fixed as the data size n goes to infinity, and the computational cost is\nO(n). Third, asymptotic properties of ABC with summary statistics having\nheterogeneous convergence rates is derived, and an adaptive scheme to choose\nthe component composite scores is proposed. Numerical studies show that the new\nmethod significantly outperforms the existing Bayesian composite likelihood\nmethods, and the efficiency of adaptively combined composite scores well\napproximates the efficiency of particle MCMC using the full likelihood."
                },
                "authors": [
                    {
                        "name": "Wentao Li"
                    },
                    {
                        "name": "Rosabeth White"
                    },
                    {
                        "name": "Dennis Prangle"
                    }
                ],
                "author_detail": {
                    "name": "Dennis Prangle"
                },
                "author": "Dennis Prangle",
                "arxiv_comment": "56 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02313v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02313v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02691v1",
                "updated": "2024-09-04T13:24:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    24,
                    3,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T13:24:03Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    24,
                    3,
                    2,
                    248,
                    0
                ],
                "title": "LLM-Assisted Visual Analytics: Opportunities and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Assisted Visual Analytics: Opportunities and Challenges"
                },
                "summary": "We explore the integration of large language models (LLMs) into visual\nanalytics (VA) systems to transform their capabilities through intuitive\nnatural language interactions. We survey current research directions in this\nemerging field, examining how LLMs are integrated into data management,\nlanguage interaction, visualisation generation, and language generation\nprocesses. We highlight the new possibilities that LLMs bring to VA, especially\nhow they can change VA processes beyond the usual use cases. We especially\nhighlight building new visualisation-language models, allowing access of a\nbreadth of domain knowledge, multimodal interaction, and opportunities with\nguidance. Finally, we carefully consider the prominent challenges of using\ncurrent LLMs in VA tasks. Our discussions in this paper aim to guide future\nresearchers working on LLM-assisted VA systems and help them navigate common\nobstacles when developing these systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the integration of large language models (LLMs) into visual\nanalytics (VA) systems to transform their capabilities through intuitive\nnatural language interactions. We survey current research directions in this\nemerging field, examining how LLMs are integrated into data management,\nlanguage interaction, visualisation generation, and language generation\nprocesses. We highlight the new possibilities that LLMs bring to VA, especially\nhow they can change VA processes beyond the usual use cases. We especially\nhighlight building new visualisation-language models, allowing access of a\nbreadth of domain knowledge, multimodal interaction, and opportunities with\nguidance. Finally, we carefully consider the prominent challenges of using\ncurrent LLMs in VA tasks. Our discussions in this paper aim to guide future\nresearchers working on LLM-assisted VA systems and help them navigate common\nobstacles when developing these systems."
                },
                "authors": [
                    {
                        "name": "Maeve Hutchinson"
                    },
                    {
                        "name": "Radu Jianu"
                    },
                    {
                        "name": "Aidan Slingsby"
                    },
                    {
                        "name": "Pranava Madhyastha"
                    }
                ],
                "author_detail": {
                    "name": "Pranava Madhyastha"
                },
                "author": "Pranava Madhyastha",
                "arxiv_comment": "Accepted at EG UK Computer Graphics & Visual Computing 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02686v1",
                "updated": "2024-09-04T13:17:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    17,
                    9,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T13:17:09Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    17,
                    9,
                    2,
                    248,
                    0
                ],
                "title": "Deconfounded Causality-aware Parameter-Efficient Fine-Tuning for\n  Problem-Solving Improvement of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deconfounded Causality-aware Parameter-Efficient Fine-Tuning for\n  Problem-Solving Improvement of LLMs"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable efficiency in\ntackling various tasks based on human instructions, but recent studies reveal\nthat these models often fail to achieve satisfactory results on questions\ninvolving reasoning, such as mathematics or physics questions. This phenomenon\nis usually attributed to the uncertainty regarding whether these models could\ngenuinely comprehend the knowledge embedded in the text or merely learn to\nreplicate the token distribution without a true understanding of the content.\nIn this paper, we delve into this problem and aim to enhance the reasoning\ncapabilities of LLMs. First, we investigate if the model has genuine reasoning\ncapabilities by visualizing the text generation process at the attention and\nrepresentation level. Then, we formulate the reasoning process of LLMs into a\ncausal framework, which provides a formal explanation of the problems we\nobserve in the visualization. Finally, building upon this causal framework, we\npropose Deconfounded Causal Adaptation (DCA), a novel parameter-efficient\nfine-tuning (PEFT) method to enhance the model's reasoning capabilities by\nencouraging the model to extract the general problem-solving skills and apply\nthese skills to different questions. Experiments show that our method\noutperforms the baseline consistently across multiple benchmarks, and with only\n1.2M tunable parameters, we achieve better or comparable results to other\nfine-tuning methods. This demonstrates the effectiveness and efficiency of our\nmethod in improving the overall accuracy and reliability of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable efficiency in\ntackling various tasks based on human instructions, but recent studies reveal\nthat these models often fail to achieve satisfactory results on questions\ninvolving reasoning, such as mathematics or physics questions. This phenomenon\nis usually attributed to the uncertainty regarding whether these models could\ngenuinely comprehend the knowledge embedded in the text or merely learn to\nreplicate the token distribution without a true understanding of the content.\nIn this paper, we delve into this problem and aim to enhance the reasoning\ncapabilities of LLMs. First, we investigate if the model has genuine reasoning\ncapabilities by visualizing the text generation process at the attention and\nrepresentation level. Then, we formulate the reasoning process of LLMs into a\ncausal framework, which provides a formal explanation of the problems we\nobserve in the visualization. Finally, building upon this causal framework, we\npropose Deconfounded Causal Adaptation (DCA), a novel parameter-efficient\nfine-tuning (PEFT) method to enhance the model's reasoning capabilities by\nencouraging the model to extract the general problem-solving skills and apply\nthese skills to different questions. Experiments show that our method\noutperforms the baseline consistently across multiple benchmarks, and with only\n1.2M tunable parameters, we achieve better or comparable results to other\nfine-tuning methods. This demonstrates the effectiveness and efficiency of our\nmethod in improving the overall accuracy and reliability of LLMs."
                },
                "authors": [
                    {
                        "name": "Ruoyu Wang"
                    },
                    {
                        "name": "Xiaoxuan Li"
                    },
                    {
                        "name": "Lina Yao"
                    }
                ],
                "author_detail": {
                    "name": "Lina Yao"
                },
                "author": "Lina Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11850v2",
                "updated": "2024-09-04T13:14:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    14,
                    57,
                    2,
                    248,
                    0
                ],
                "published": "2024-08-13T08:32:06Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    8,
                    32,
                    6,
                    1,
                    226,
                    0
                ],
                "title": "Parallel Speculative Decoding with Adaptive Draft Length",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Speculative Decoding with Adaptive Draft Length"
                },
                "summary": "Speculative decoding (SD), where an extra draft model is employed to provide\nmultiple \\textit{draft} tokens first and then the original target model\nverifies these tokens in parallel, has shown great power for LLM inference\nacceleration. However, existing SD methods suffer from the mutual waiting\nproblem, i.e., the target model gets stuck when the draft model is\n\\textit{guessing} tokens, and vice versa. This problem is directly incurred by\nthe asynchronous execution of the draft model and the target model, and is\nexacerbated due to the fixed draft length in speculative decoding. To address\nthese challenges, we propose a conceptually simple, flexible, and general\nframework to boost speculative decoding, namely \\textbf{P}arallel\nsp\\textbf{E}culative decoding with \\textbf{A}daptive d\\textbf{R}aft\n\\textbf{L}ength (PEARL). Specifically, PEARL proposes \\textit{pre-verify} to\nverify the first draft token in advance during the drafting phase, and\n\\textit{post-verify} to generate more draft tokens during the verification\nphase. PEARL parallels the drafting phase and the verification phase via\napplying the two strategies, and achieves adaptive draft length for different\nscenarios, which effectively alleviates the mutual waiting problem. Moreover,\nwe theoretically demonstrate that the mean accepted tokens of PEARL is more\nthan existing \\textit{draft-then-verify} works. Experiments on various text\ngeneration benchmarks demonstrate the effectiveness of our \\name, leading to a\nsuperior speedup performance up to \\textbf{3.79$\\times$} and\n\\textbf{1.52$\\times$}, compared to auto-regressive decoding and vanilla\nspeculative decoding, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD), where an extra draft model is employed to provide\nmultiple \\textit{draft} tokens first and then the original target model\nverifies these tokens in parallel, has shown great power for LLM inference\nacceleration. However, existing SD methods suffer from the mutual waiting\nproblem, i.e., the target model gets stuck when the draft model is\n\\textit{guessing} tokens, and vice versa. This problem is directly incurred by\nthe asynchronous execution of the draft model and the target model, and is\nexacerbated due to the fixed draft length in speculative decoding. To address\nthese challenges, we propose a conceptually simple, flexible, and general\nframework to boost speculative decoding, namely \\textbf{P}arallel\nsp\\textbf{E}culative decoding with \\textbf{A}daptive d\\textbf{R}aft\n\\textbf{L}ength (PEARL). Specifically, PEARL proposes \\textit{pre-verify} to\nverify the first draft token in advance during the drafting phase, and\n\\textit{post-verify} to generate more draft tokens during the verification\nphase. PEARL parallels the drafting phase and the verification phase via\napplying the two strategies, and achieves adaptive draft length for different\nscenarios, which effectively alleviates the mutual waiting problem. Moreover,\nwe theoretically demonstrate that the mean accepted tokens of PEARL is more\nthan existing \\textit{draft-then-verify} works. Experiments on various text\ngeneration benchmarks demonstrate the effectiveness of our \\name, leading to a\nsuperior speedup performance up to \\textbf{3.79$\\times$} and\n\\textbf{1.52$\\times$}, compared to auto-regressive decoding and vanilla\nspeculative decoding, respectively."
                },
                "authors": [
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Yun Li"
                    },
                    {
                        "name": "Qitan Lv"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Winston Hu"
                    }
                ],
                "author_detail": {
                    "name": "Winston Hu"
                },
                "author": "Winston Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02682v1",
                "updated": "2024-09-04T13:12:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    12,
                    58,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T13:12:58Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    12,
                    58,
                    2,
                    248,
                    0
                ],
                "title": "Symmetries and synchronization from whole-neural activity in {\\it C.\n  elegans} connectome: Integration of functional and structural networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetries and synchronization from whole-neural activity in {\\it C.\n  elegans} connectome: Integration of functional and structural networks"
                },
                "summary": "Understanding the dynamical behavior of complex systems from their underlying\nnetwork architectures is a long-standing question in complexity theory.\nTherefore, many metrics have been devised to extract network features like\nmotifs, centrality, and modularity measures. It has previously been proposed\nthat network symmetries are of particular importance since they are expected to\nunderly the synchronization of a system's units, which is ubiquitously observed\nin nervous system activity patterns. However, perfectly symmetrical structures\nare difficult to assess in noisy measurements of biological systems, like\nneuronal connectomes. Here, we devise a principled method to infer network\nsymmetries from combined connectome and neuronal activity data. Using nervous\nsystem-wide population activity recordings of the \\textit{C.elegans} backward\nlocomotor system, we infer structures in the connectome called fibration\nsymmetries, which can explain which group of neurons synchronize their\nactivity. Our analysis suggests functional building blocks in the animal's\nmotor periphery, providing new testable hypotheses on how descending\ninterneuron circuits communicate with the motor periphery to control behavior.\nOur approach opens a new door to exploring the structure-function relations in\nother complex systems, like the nervous systems of larger animals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the dynamical behavior of complex systems from their underlying\nnetwork architectures is a long-standing question in complexity theory.\nTherefore, many metrics have been devised to extract network features like\nmotifs, centrality, and modularity measures. It has previously been proposed\nthat network symmetries are of particular importance since they are expected to\nunderly the synchronization of a system's units, which is ubiquitously observed\nin nervous system activity patterns. However, perfectly symmetrical structures\nare difficult to assess in noisy measurements of biological systems, like\nneuronal connectomes. Here, we devise a principled method to infer network\nsymmetries from combined connectome and neuronal activity data. Using nervous\nsystem-wide population activity recordings of the \\textit{C.elegans} backward\nlocomotor system, we infer structures in the connectome called fibration\nsymmetries, which can explain which group of neurons synchronize their\nactivity. Our analysis suggests functional building blocks in the animal's\nmotor periphery, providing new testable hypotheses on how descending\ninterneuron circuits communicate with the motor periphery to control behavior.\nOur approach opens a new door to exploring the structure-function relations in\nother complex systems, like the nervous systems of larger animals."
                },
                "authors": [
                    {
                        "name": "Bryant Avila"
                    },
                    {
                        "name": "Pedro Augusto"
                    },
                    {
                        "name": "David Phillips"
                    },
                    {
                        "name": "Tommaso Gili"
                    },
                    {
                        "name": "Manuel Zimmer"
                    },
                    {
                        "name": "Hernn A. Makse"
                    }
                ],
                "author_detail": {
                    "name": "Hernn A. Makse"
                },
                "author": "Hernn A. Makse",
                "arxiv_comment": "32 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2211.12267v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2211.12267v3",
                "updated": "2024-09-04T13:12:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    12,
                    53,
                    2,
                    248,
                    0
                ],
                "published": "2022-11-22T13:34:53Z",
                "published_parsed": [
                    2022,
                    11,
                    22,
                    13,
                    34,
                    53,
                    1,
                    326,
                    0
                ],
                "title": "Nonparametric Bayesian estimation in a multidimensional diffusion model\n  with high frequency data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonparametric Bayesian estimation in a multidimensional diffusion model\n  with high frequency data"
                },
                "summary": "We consider nonparametric Bayesian inference in a multidimensional diffusion\nmodel with reflecting boundary conditions based on discrete high-frequency\nobservations. We prove a general posterior contraction rate theorem in\n$L^2$-loss, which is applied to Gaussian priors. The resulting posteriors, as\nwell as their posterior means, are shown to converge to the ground truth at the\nminimax optimal rate over H\\\"older smoothness classes in any dimension. Of\nindependent interest and as part of our proofs, we show that certain\nfrequentist penalized least squares estimators are also minimax optimal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider nonparametric Bayesian inference in a multidimensional diffusion\nmodel with reflecting boundary conditions based on discrete high-frequency\nobservations. We prove a general posterior contraction rate theorem in\n$L^2$-loss, which is applied to Gaussian priors. The resulting posteriors, as\nwell as their posterior means, are shown to converge to the ground truth at the\nminimax optimal rate over H\\\"older smoothness classes in any dimension. Of\nindependent interest and as part of our proofs, we show that certain\nfrequentist penalized least squares estimators are also minimax optimal."
                },
                "authors": [
                    {
                        "name": "Marc Hoffmann"
                    },
                    {
                        "name": "Kolyan Ray"
                    }
                ],
                "author_detail": {
                    "name": "Kolyan Ray"
                },
                "author": "Kolyan Ray",
                "arxiv_comment": "61 pages, 1 figure, to appear in Probability Theory and Related\n  Fields",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2211.12267v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2211.12267v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62G20, 62F15, 60J60",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02676v1",
                "updated": "2024-09-04T13:06:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    6,
                    40,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T13:06:40Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    6,
                    40,
                    2,
                    248,
                    0
                ],
                "title": "Improved Single Camera BEV Perception Using Multi-Camera Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Single Camera BEV Perception Using Multi-Camera Training"
                },
                "summary": "Bird's Eye View (BEV) map prediction is essential for downstream autonomous\ndriving tasks like trajectory prediction. In the past, this was accomplished\nthrough the use of a sophisticated sensor configuration that captured a\nsurround view from multiple cameras. However, in large-scale production, cost\nefficiency is an optimization goal, so that using fewer cameras becomes more\nrelevant. But the consequence of fewer input images correlates with a\nperformance drop. This raises the problem of developing a BEV perception model\nthat provides a sufficient performance on a low-cost sensor setup. Although,\nprimarily relevant for inference time on production cars, this cost restriction\nis less problematic on a test vehicle during training. Therefore, the objective\nof our approach is to reduce the aforementioned performance drop as much as\npossible using a modern multi-camera surround view model reduced for\nsingle-camera inference. The approach includes three features, a modern masking\ntechnique, a cyclic Learning Rate (LR) schedule, and a feature reconstruction\nloss for supervising the transition from six-camera inputs to one-camera input\nduring training. Our method outperforms versions trained strictly with one\ncamera or strictly with six-camera surround view for single-camera inference\nresulting in reduced hallucination and better quality of the BEV map.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bird's Eye View (BEV) map prediction is essential for downstream autonomous\ndriving tasks like trajectory prediction. In the past, this was accomplished\nthrough the use of a sophisticated sensor configuration that captured a\nsurround view from multiple cameras. However, in large-scale production, cost\nefficiency is an optimization goal, so that using fewer cameras becomes more\nrelevant. But the consequence of fewer input images correlates with a\nperformance drop. This raises the problem of developing a BEV perception model\nthat provides a sufficient performance on a low-cost sensor setup. Although,\nprimarily relevant for inference time on production cars, this cost restriction\nis less problematic on a test vehicle during training. Therefore, the objective\nof our approach is to reduce the aforementioned performance drop as much as\npossible using a modern multi-camera surround view model reduced for\nsingle-camera inference. The approach includes three features, a modern masking\ntechnique, a cyclic Learning Rate (LR) schedule, and a feature reconstruction\nloss for supervising the transition from six-camera inputs to one-camera input\nduring training. Our method outperforms versions trained strictly with one\ncamera or strictly with six-camera surround view for single-camera inference\nresulting in reduced hallucination and better quality of the BEV map."
                },
                "authors": [
                    {
                        "name": "Daniel Busch"
                    },
                    {
                        "name": "Ido Freeman"
                    },
                    {
                        "name": "Richard Meyes"
                    },
                    {
                        "name": "Tobias Meisen"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Meisen"
                },
                "author": "Tobias Meisen",
                "arxiv_comment": "This Paper has been accepted to the 27th IEEE International\n  Conference on Intelligent Transportation Systems (ITSC 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2103.05909v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2103.05909v4",
                "updated": "2024-09-04T13:05:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    5,
                    21,
                    2,
                    248,
                    0
                ],
                "published": "2021-03-10T07:37:20Z",
                "published_parsed": [
                    2021,
                    3,
                    10,
                    7,
                    37,
                    20,
                    2,
                    69,
                    0
                ],
                "title": "A variational inference framework for inverse problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A variational inference framework for inverse problems"
                },
                "summary": "A framework is presented for fitting inverse problem models via variational\nBayes approximations. This methodology guarantees flexibility to statistical\nmodel specification for a broad range of applications, good accuracy and\nreduced model fitting times. The message passing and factor graph fragment\napproach to variational Bayes that is also described facilitates streamlined\nimplementation of approximate inference algorithms and allows for supple\ninclusion of numerous response distributions and penalizations into the inverse\nproblem model. Models for one- and two-dimensional response variables are\nexamined and an infrastructure is laid down where efficient algorithm updates\nbased on nullifying weak interactions between variables can also be derived for\ninverse problems in higher dimensions. An image processing application and a\nsimulation exercise motivated by biomedical problems reveal the computational\nadvantage offered by efficient implementation of variational Bayes over Markov\nchain Monte Carlo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A framework is presented for fitting inverse problem models via variational\nBayes approximations. This methodology guarantees flexibility to statistical\nmodel specification for a broad range of applications, good accuracy and\nreduced model fitting times. The message passing and factor graph fragment\napproach to variational Bayes that is also described facilitates streamlined\nimplementation of approximate inference algorithms and allows for supple\ninclusion of numerous response distributions and penalizations into the inverse\nproblem model. Models for one- and two-dimensional response variables are\nexamined and an infrastructure is laid down where efficient algorithm updates\nbased on nullifying weak interactions between variables can also be derived for\ninverse problems in higher dimensions. An image processing application and a\nsimulation exercise motivated by biomedical problems reveal the computational\nadvantage offered by efficient implementation of variational Bayes over Markov\nchain Monte Carlo."
                },
                "authors": [
                    {
                        "name": "Luca Maestrini"
                    },
                    {
                        "name": "Robert G. Aykroyd"
                    },
                    {
                        "name": "Matt P. Wand"
                    }
                ],
                "author_detail": {
                    "name": "Matt P. Wand"
                },
                "author": "Matt P. Wand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2103.05909v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2103.05909v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12347v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12347v4",
                "updated": "2024-09-04T12:49:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    12,
                    49,
                    34,
                    2,
                    248,
                    0
                ],
                "published": "2024-08-22T12:43:14Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    43,
                    14,
                    3,
                    235,
                    0
                ],
                "title": "Preregistration does not improve the transparent evaluation of severity\n  in Popper's philosophy of science or when deviations are allowed",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preregistration does not improve the transparent evaluation of severity\n  in Popper's philosophy of science or when deviations are allowed"
                },
                "summary": "One justification for preregistering research hypotheses, methods, and\nanalyses is that it improves the transparent evaluation of the severity of\nhypothesis tests. In this article, I consider two cases in which\npreregistration does not improve this evaluation. First, I argue that, although\npreregistration can facilitate the transparent evaluation of severity in Mayo's\nerror statistical philosophy of science, it does not facilitate this evaluation\nin Popper's theory-centric approach. To illustrate, I show that associated\nconcerns about Type I error rate inflation are only relevant in the error\nstatistical approach and not in a theory-centric approach. Second, I argue that\na preregistered test procedure that allows deviations in its implementation\ndoes not provide a more transparent evaluation of Mayoian severity than a\nnon-preregistered procedure. In particular, I argue that sample-based\nvalidity-enhancing deviations cause an unknown inflation of the test\nprocedure's Type I (familywise) error rate and, consequently, an unknown\nreduction in its capability to license inferences severely. I conclude that\npreregistration does not improve the transparent evaluation of severity in\nPopper's philosophy of science or when deviations are allowed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One justification for preregistering research hypotheses, methods, and\nanalyses is that it improves the transparent evaluation of the severity of\nhypothesis tests. In this article, I consider two cases in which\npreregistration does not improve this evaluation. First, I argue that, although\npreregistration can facilitate the transparent evaluation of severity in Mayo's\nerror statistical philosophy of science, it does not facilitate this evaluation\nin Popper's theory-centric approach. To illustrate, I show that associated\nconcerns about Type I error rate inflation are only relevant in the error\nstatistical approach and not in a theory-centric approach. Second, I argue that\na preregistered test procedure that allows deviations in its implementation\ndoes not provide a more transparent evaluation of Mayoian severity than a\nnon-preregistered procedure. In particular, I argue that sample-based\nvalidity-enhancing deviations cause an unknown inflation of the test\nprocedure's Type I (familywise) error rate and, consequently, an unknown\nreduction in its capability to license inferences severely. I conclude that\npreregistration does not improve the transparent evaluation of severity in\nPopper's philosophy of science or when deviations are allowed."
                },
                "authors": [
                    {
                        "name": "Mark Rubin"
                    }
                ],
                "author_detail": {
                    "name": "Mark Rubin"
                },
                "author": "Mark Rubin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12347v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12347v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09979v2",
                "updated": "2024-09-04T12:33:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    12,
                    33,
                    24,
                    2,
                    248,
                    0
                ],
                "published": "2024-06-14T12:41:07Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    12,
                    41,
                    7,
                    4,
                    166,
                    0
                ],
                "title": "HIRO: Hierarchical Information Retrieval Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HIRO: Hierarchical Information Retrieval Optimization"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has revolutionized natural language\nprocessing by dynamically integrating external knowledge into Large Language\nModels (LLMs), addressing their limitation of static training datasets. Recent\nimplementations of RAG leverage hierarchical data structures, which organize\ndocuments at various levels of summarization and information density. This\ncomplexity, however, can cause LLMs to \"choke\" on information overload,\nnecessitating more sophisticated querying mechanisms. In this context, we\nintroduce Hierarchical Information Retrieval Optimization (HIRO), a novel\nquerying approach that employs a Depth-First Search (DFS)-based recursive\nsimilarity score calculation and branch pruning. This method uniquely minimizes\nthe context delivered to the LLM without informational loss, effectively\nmanaging the challenge of excessive data. HIRO's refined approach is validated\nby a 10.85% improvement in performance on the NarrativeQA dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has revolutionized natural language\nprocessing by dynamically integrating external knowledge into Large Language\nModels (LLMs), addressing their limitation of static training datasets. Recent\nimplementations of RAG leverage hierarchical data structures, which organize\ndocuments at various levels of summarization and information density. This\ncomplexity, however, can cause LLMs to \"choke\" on information overload,\nnecessitating more sophisticated querying mechanisms. In this context, we\nintroduce Hierarchical Information Retrieval Optimization (HIRO), a novel\nquerying approach that employs a Depth-First Search (DFS)-based recursive\nsimilarity score calculation and branch pruning. This method uniquely minimizes\nthe context delivered to the LLM without informational loss, effectively\nmanaging the challenge of excessive data. HIRO's refined approach is validated\nby a 10.85% improvement in performance on the NarrativeQA dataset."
                },
                "authors": [
                    {
                        "name": "Krish Goel"
                    },
                    {
                        "name": "Mahek Chandak"
                    }
                ],
                "author_detail": {
                    "name": "Mahek Chandak"
                },
                "author": "Mahek Chandak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02644v1",
                "updated": "2024-09-04T12:20:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    12,
                    20,
                    27,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T12:20:27Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    12,
                    20,
                    27,
                    2,
                    248,
                    0
                ],
                "title": "Conformal Prediction in Dynamic Biological Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal Prediction in Dynamic Biological Systems"
                },
                "summary": "Uncertainty quantification (UQ) is the process of systematically determining\nand characterizing the degree of confidence in computational model predictions.\nIn the context of systems biology, especially with dynamic models, UQ is\ncrucial because it addresses the challenges posed by nonlinearity and parameter\nsensitivity, allowing us to properly understand and extrapolate the behavior of\ncomplex biological systems. Here, we focus on dynamic models represented by\ndeterministic nonlinear ordinary differential equations. Many current UQ\napproaches in this field rely on Bayesian statistical methods. While powerful,\nthese methods often require strong prior specifications and make parametric\nassumptions that may not always hold in biological systems. Additionally, these\nmethods face challenges in domains where sample sizes are limited, and\nstatistical inference becomes constrained, with computational speed being a\nbottleneck in large models of biological systems. As an alternative, we propose\nthe use of conformal inference methods, introducing two novel algorithms that,\nin some instances, offer non-asymptotic guarantees, enhancing robustness and\nscalability across various applications. We demonstrate the efficacy of our\nproposed algorithms through several scenarios, highlighting their advantages\nover traditional Bayesian approaches. The proposed methods show promising\nresults for diverse biological data structures and scenarios, offering a\ngeneral framework to quantify uncertainty for dynamic models of biological\nsystems.The software for the methodology and the reproduction of the results is\navailable at https://zenodo.org/doi/10.5281/zenodo.13644870.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty quantification (UQ) is the process of systematically determining\nand characterizing the degree of confidence in computational model predictions.\nIn the context of systems biology, especially with dynamic models, UQ is\ncrucial because it addresses the challenges posed by nonlinearity and parameter\nsensitivity, allowing us to properly understand and extrapolate the behavior of\ncomplex biological systems. Here, we focus on dynamic models represented by\ndeterministic nonlinear ordinary differential equations. Many current UQ\napproaches in this field rely on Bayesian statistical methods. While powerful,\nthese methods often require strong prior specifications and make parametric\nassumptions that may not always hold in biological systems. Additionally, these\nmethods face challenges in domains where sample sizes are limited, and\nstatistical inference becomes constrained, with computational speed being a\nbottleneck in large models of biological systems. As an alternative, we propose\nthe use of conformal inference methods, introducing two novel algorithms that,\nin some instances, offer non-asymptotic guarantees, enhancing robustness and\nscalability across various applications. We demonstrate the efficacy of our\nproposed algorithms through several scenarios, highlighting their advantages\nover traditional Bayesian approaches. The proposed methods show promising\nresults for diverse biological data structures and scenarios, offering a\ngeneral framework to quantify uncertainty for dynamic models of biological\nsystems.The software for the methodology and the reproduction of the results is\navailable at https://zenodo.org/doi/10.5281/zenodo.13644870."
                },
                "authors": [
                    {
                        "name": "Alberto Portela"
                    },
                    {
                        "name": "Julio R. Banga"
                    },
                    {
                        "name": "Marcos Matabuena"
                    }
                ],
                "author_detail": {
                    "name": "Marcos Matabuena"
                },
                "author": "Marcos Matabuena",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12333v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12333v2",
                "updated": "2024-09-04T12:00:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    12,
                    0,
                    25,
                    2,
                    248,
                    0
                ],
                "published": "2024-08-22T12:21:22Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    21,
                    22,
                    3,
                    235,
                    0
                ],
                "title": "Graph Retrieval Augmented Trustworthiness Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Retrieval Augmented Trustworthiness Reasoning"
                },
                "summary": "Trustworthiness reasoning is crucial in multiplayer games with incomplete\ninformation, enabling agents to identify potential allies and adversaries,\nthereby enhancing reasoning and decision-making processes. Traditional\napproaches relying on pre-trained models necessitate extensive domain-specific\ndata and considerable reward feedback, with their lack of real-time\nadaptability hindering their effectiveness in dynamic environments. In this\npaper, we introduce the Graph Retrieval Augmented Reasoning (GRATR) framework,\nleveraging the Retrieval-Augmented Generation (RAG) technique to bolster\ntrustworthiness reasoning in agents. GRATR constructs a dynamic trustworthiness\ngraph, updating it in real-time with evidential information, and retrieves\nrelevant trust data to augment the reasoning capabilities of Large Language\nModels (LLMs). We validate our approach through experiments on the multiplayer\ngame \"Werewolf,\" comparing GRATR against baseline LLM and LLM enhanced with\nNative RAG and Rerank RAG. Our results demonstrate that GRATR surpasses the\nbaseline methods by over 30\\% in winning rate, with superior reasoning\nperformance. Moreover, GRATR effectively mitigates LLM hallucinations, such as\nidentity and objective amnesia, and crucially, it renders the reasoning process\nmore transparent and traceable through the use of the trustworthiness graph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthiness reasoning is crucial in multiplayer games with incomplete\ninformation, enabling agents to identify potential allies and adversaries,\nthereby enhancing reasoning and decision-making processes. Traditional\napproaches relying on pre-trained models necessitate extensive domain-specific\ndata and considerable reward feedback, with their lack of real-time\nadaptability hindering their effectiveness in dynamic environments. In this\npaper, we introduce the Graph Retrieval Augmented Reasoning (GRATR) framework,\nleveraging the Retrieval-Augmented Generation (RAG) technique to bolster\ntrustworthiness reasoning in agents. GRATR constructs a dynamic trustworthiness\ngraph, updating it in real-time with evidential information, and retrieves\nrelevant trust data to augment the reasoning capabilities of Large Language\nModels (LLMs). We validate our approach through experiments on the multiplayer\ngame \"Werewolf,\" comparing GRATR against baseline LLM and LLM enhanced with\nNative RAG and Rerank RAG. Our results demonstrate that GRATR surpasses the\nbaseline methods by over 30\\% in winning rate, with superior reasoning\nperformance. Moreover, GRATR effectively mitigates LLM hallucinations, such as\nidentity and objective amnesia, and crucially, it renders the reasoning process\nmore transparent and traceable through the use of the trustworthiness graph."
                },
                "authors": [
                    {
                        "name": "Ying Zhu"
                    },
                    {
                        "name": "Shengchang Li"
                    },
                    {
                        "name": "Ziqian Kong"
                    },
                    {
                        "name": "Peilan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Peilan Xu"
                },
                "author": "Peilan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12333v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12333v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02636v1",
                "updated": "2024-09-04T11:59:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    11,
                    59,
                    53,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T11:59:53Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    11,
                    59,
                    53,
                    2,
                    248,
                    0
                ],
                "title": "Mamba as a motion encoder for robotic imitation learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mamba as a motion encoder for robotic imitation learning"
                },
                "summary": "Recent advancements in imitation learning, particularly with the integration\nof LLM techniques, are set to significantly improve robots' dexterity and\nadaptability. In this study, we propose using Mamba, a state-of-the-art\narchitecture with potential applications in LLMs, for robotic imitation\nlearning, highlighting its ability to function as an encoder that effectively\ncaptures contextual information. By reducing the dimensionality of the state\nspace, Mamba operates similarly to an autoencoder. It effectively compresses\nthe sequential information into state variables while preserving the essential\ntemporal dynamics necessary for accurate motion prediction. Experimental\nresults in tasks such as cup placing and case loading demonstrate that despite\nexhibiting higher estimation errors, Mamba achieves superior success rates\ncompared to Transformers in practical task execution. This performance is\nattributed to Mamba's structure, which encompasses the state space model.\nAdditionally, the study investigates Mamba's capacity to serve as a real-time\nmotion generator with a limited amount of training data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in imitation learning, particularly with the integration\nof LLM techniques, are set to significantly improve robots' dexterity and\nadaptability. In this study, we propose using Mamba, a state-of-the-art\narchitecture with potential applications in LLMs, for robotic imitation\nlearning, highlighting its ability to function as an encoder that effectively\ncaptures contextual information. By reducing the dimensionality of the state\nspace, Mamba operates similarly to an autoencoder. It effectively compresses\nthe sequential information into state variables while preserving the essential\ntemporal dynamics necessary for accurate motion prediction. Experimental\nresults in tasks such as cup placing and case loading demonstrate that despite\nexhibiting higher estimation errors, Mamba achieves superior success rates\ncompared to Transformers in practical task execution. This performance is\nattributed to Mamba's structure, which encompasses the state space model.\nAdditionally, the study investigates Mamba's capacity to serve as a real-time\nmotion generator with a limited amount of training data."
                },
                "authors": [
                    {
                        "name": "Toshiaki Tsuji"
                    }
                ],
                "author_detail": {
                    "name": "Toshiaki Tsuji"
                },
                "author": "Toshiaki Tsuji",
                "arxiv_comment": "7 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02634v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02634v2",
                "updated": "2024-09-05T09:11:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    9,
                    11,
                    25,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-04T11:55:14Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    11,
                    55,
                    14,
                    2,
                    248,
                    0
                ],
                "title": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion\n  Dependency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion\n  Dependency"
                },
                "summary": "With the introduction of diffusion-based video generation techniques,\naudio-conditioned human video generation has recently achieved significant\nbreakthroughs in both the naturalness of motion and the synthesis of portrait\ndetails. Due to the limited control of audio signals in driving human motion,\nexisting methods often add auxiliary spatial signals to stabilize movements,\nwhich may compromise the naturalness and freedom of motion. In this paper, we\npropose an end-to-end audio-only conditioned video diffusion model named Loopy.\nSpecifically, we designed an inter- and intra-clip temporal module and an\naudio-to-latents module, enabling the model to leverage long-term motion\ninformation from the data to learn natural motion patterns and improving\naudio-portrait movement correlation. This method removes the need for manually\nspecified spatial motion templates used in existing methods to constrain motion\nduring inference. Extensive experiments show that Loopy outperforms recent\naudio-driven portrait diffusion models, delivering more lifelike and\nhigh-quality results across various scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the introduction of diffusion-based video generation techniques,\naudio-conditioned human video generation has recently achieved significant\nbreakthroughs in both the naturalness of motion and the synthesis of portrait\ndetails. Due to the limited control of audio signals in driving human motion,\nexisting methods often add auxiliary spatial signals to stabilize movements,\nwhich may compromise the naturalness and freedom of motion. In this paper, we\npropose an end-to-end audio-only conditioned video diffusion model named Loopy.\nSpecifically, we designed an inter- and intra-clip temporal module and an\naudio-to-latents module, enabling the model to leverage long-term motion\ninformation from the data to learn natural motion patterns and improving\naudio-portrait movement correlation. This method removes the need for manually\nspecified spatial motion templates used in existing methods to constrain motion\nduring inference. Extensive experiments show that Loopy outperforms recent\naudio-driven portrait diffusion models, delivering more lifelike and\nhigh-quality results across various scenarios."
                },
                "authors": [
                    {
                        "name": "Jianwen Jiang"
                    },
                    {
                        "name": "Chao Liang"
                    },
                    {
                        "name": "Jiaqi Yang"
                    },
                    {
                        "name": "Gaojie Lin"
                    },
                    {
                        "name": "Tianyun Zhong"
                    },
                    {
                        "name": "Yanbo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Yanbo Zheng"
                },
                "author": "Yanbo Zheng",
                "arxiv_comment": "Homepage: https://loopyavatar.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02634v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02634v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07986v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07986v2",
                "updated": "2024-09-04T11:50:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    11,
                    50,
                    23,
                    2,
                    248,
                    0
                ],
                "published": "2024-03-12T18:00:24Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    18,
                    0,
                    24,
                    1,
                    72,
                    0
                ],
                "title": "EIGER VI. The Correlation Function, Host Halo Mass and Duty Cycle of\n  Luminous Quasars at $z\\gtrsim6$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EIGER VI. The Correlation Function, Host Halo Mass and Duty Cycle of\n  Luminous Quasars at $z\\gtrsim6$"
                },
                "summary": "We expect luminous ($M_{1450}\\lesssim-26.5$) high-redshift quasars to trace\nthe highest density peaks in the early universe. Here, we present observations\nof four $z\\gtrsim6$ quasar fields using JWST/NIRCam in imaging and widefield\nslitless spectroscopy mode and report a wide range in the number of detected\n[OIII]-emitting galaxies in the quasars' environments, ranging between a\ndensity enhancement of $\\delta\\approx65$ within a $2$ cMpc radius - one of the\nlargest proto-clusters during the Epoch of Reionization discovered to date - to\na density contrast consistent with zero, indicating the presence of a\nUV-luminous quasar in a region comparable to the average density of the\nuniverse. By measuring the two-point cross-correlation function of quasars and\ntheir surrounding galaxies, as well as the galaxy auto-correlation function, we\ninfer a correlation length of quasars at $\\langle z\\rangle=6.25$ of $r_0^{\\rm\nQQ}=22.0^{+3.0}_{-2.9}~{\\rm cMpc}\\,h^{-1}$, while we obtain a correlation\nlength of the [OIII]-emitting galaxies of $r_0^{\\rm GG}=4.1\\pm0.3~{\\rm\ncMpc}\\,h^{-1}$. By comparing the correlation functions to dark-matter-only\nsimulations we estimate the minimum mass of the quasars' host dark matter halos\nto be $\\log_{10}(M_{\\rm halo, min}/M_\\odot)=12.43^{+0.13}_{-0.15}$ (and\n$\\log_{10}(M_{\\rm halo, min}^{\\rm [OIII]}/M_\\odot) = 10.56^{+0.05}_{-0.03}$ for\nthe [OIII]-emitters), indicating that (a) luminous quasars do not necessarily\nreside within the most overdense regions in the early universe, and that (b)\nthe UV-luminous duty cycle of quasar activity at these redshifts is $f_{\\rm\nduty}\\ll1$. Such short quasar activity timescales challenge our understanding\nof early supermassive black hole growth and provide evidence for highly\ndust-obscured growth phases or episodic, radiatively inefficient accretion\nrates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We expect luminous ($M_{1450}\\lesssim-26.5$) high-redshift quasars to trace\nthe highest density peaks in the early universe. Here, we present observations\nof four $z\\gtrsim6$ quasar fields using JWST/NIRCam in imaging and widefield\nslitless spectroscopy mode and report a wide range in the number of detected\n[OIII]-emitting galaxies in the quasars' environments, ranging between a\ndensity enhancement of $\\delta\\approx65$ within a $2$ cMpc radius - one of the\nlargest proto-clusters during the Epoch of Reionization discovered to date - to\na density contrast consistent with zero, indicating the presence of a\nUV-luminous quasar in a region comparable to the average density of the\nuniverse. By measuring the two-point cross-correlation function of quasars and\ntheir surrounding galaxies, as well as the galaxy auto-correlation function, we\ninfer a correlation length of quasars at $\\langle z\\rangle=6.25$ of $r_0^{\\rm\nQQ}=22.0^{+3.0}_{-2.9}~{\\rm cMpc}\\,h^{-1}$, while we obtain a correlation\nlength of the [OIII]-emitting galaxies of $r_0^{\\rm GG}=4.1\\pm0.3~{\\rm\ncMpc}\\,h^{-1}$. By comparing the correlation functions to dark-matter-only\nsimulations we estimate the minimum mass of the quasars' host dark matter halos\nto be $\\log_{10}(M_{\\rm halo, min}/M_\\odot)=12.43^{+0.13}_{-0.15}$ (and\n$\\log_{10}(M_{\\rm halo, min}^{\\rm [OIII]}/M_\\odot) = 10.56^{+0.05}_{-0.03}$ for\nthe [OIII]-emitters), indicating that (a) luminous quasars do not necessarily\nreside within the most overdense regions in the early universe, and that (b)\nthe UV-luminous duty cycle of quasar activity at these redshifts is $f_{\\rm\nduty}\\ll1$. Such short quasar activity timescales challenge our understanding\nof early supermassive black hole growth and provide evidence for highly\ndust-obscured growth phases or episodic, radiatively inefficient accretion\nrates."
                },
                "authors": [
                    {
                        "name": "Anna-Christina Eilers"
                    },
                    {
                        "name": "Ruari Mackenzie"
                    },
                    {
                        "name": "Elia Pizzati"
                    },
                    {
                        "name": "Jorryt Matthee"
                    },
                    {
                        "name": "Joseph F. Hennawi"
                    },
                    {
                        "name": "Haowen Zhang"
                    },
                    {
                        "name": "Rongmon Bordoloi"
                    },
                    {
                        "name": "Daichi Kashino"
                    },
                    {
                        "name": "Simon J. Lilly"
                    },
                    {
                        "name": "Rohan P. Naidu"
                    },
                    {
                        "name": "Robert A. Simcoe"
                    },
                    {
                        "name": "Minghao Yue"
                    },
                    {
                        "name": "Carlos S. Frenk"
                    },
                    {
                        "name": "John C. Helly"
                    },
                    {
                        "name": "Matthieu Schaller"
                    },
                    {
                        "name": "Joop Schaye"
                    }
                ],
                "author_detail": {
                    "name": "Joop Schaye"
                },
                "author": "Joop Schaye",
                "arxiv_comment": "ApJ accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07986v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07986v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.07107v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.07107v4",
                "updated": "2024-09-04T11:39:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    11,
                    39,
                    56,
                    2,
                    248,
                    0
                ],
                "published": "2023-08-14T12:47:22Z",
                "published_parsed": [
                    2023,
                    8,
                    14,
                    12,
                    47,
                    22,
                    0,
                    226,
                    0
                ],
                "title": "Large Language Models for Information Retrieval: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Information Retrieval: A Survey"
                },
                "summary": "As a primary means of information acquisition, information retrieval (IR)\nsystems, such as search engines, have integrated themselves into our daily\nlives. These systems also serve as components of dialogue, question-answering,\nand recommender systems. The trajectory of IR has evolved dynamically from its\norigins in term-based methods to its integration with advanced neural models.\nWhile the neural models excel at capturing complex contextual signals and\nsemantic nuances, thereby reshaping the IR landscape, they still face\nchallenges such as data scarcity, interpretability, and the generation of\ncontextually plausible yet potentially inaccurate responses. This evolution\nrequires a combination of both traditional methods (such as term-based sparse\nretrieval methods with rapid response) and modern neural architectures (such as\nlanguage models with powerful language understanding capacity). Meanwhile, the\nemergence of large language models (LLMs), typified by ChatGPT and GPT-4, has\nrevolutionized natural language processing due to their remarkable language\nunderstanding, generation, generalization, and reasoning abilities.\nConsequently, recent research has sought to leverage LLMs to improve IR\nsystems. Given the rapid evolution of this research trajectory, it is necessary\nto consolidate existing methodologies and provide nuanced insights through a\ncomprehensive overview. In this survey, we delve into the confluence of LLMs\nand IR systems, including crucial aspects such as query rewriters, retrievers,\nrerankers, and readers. Additionally, we explore promising directions, such as\nsearch agents, within this expanding field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a primary means of information acquisition, information retrieval (IR)\nsystems, such as search engines, have integrated themselves into our daily\nlives. These systems also serve as components of dialogue, question-answering,\nand recommender systems. The trajectory of IR has evolved dynamically from its\norigins in term-based methods to its integration with advanced neural models.\nWhile the neural models excel at capturing complex contextual signals and\nsemantic nuances, thereby reshaping the IR landscape, they still face\nchallenges such as data scarcity, interpretability, and the generation of\ncontextually plausible yet potentially inaccurate responses. This evolution\nrequires a combination of both traditional methods (such as term-based sparse\nretrieval methods with rapid response) and modern neural architectures (such as\nlanguage models with powerful language understanding capacity). Meanwhile, the\nemergence of large language models (LLMs), typified by ChatGPT and GPT-4, has\nrevolutionized natural language processing due to their remarkable language\nunderstanding, generation, generalization, and reasoning abilities.\nConsequently, recent research has sought to leverage LLMs to improve IR\nsystems. Given the rapid evolution of this research trajectory, it is necessary\nto consolidate existing methodologies and provide nuanced insights through a\ncomprehensive overview. In this survey, we delve into the confluence of LLMs\nand IR systems, including crucial aspects such as query rewriters, retrievers,\nrerankers, and readers. Additionally, we explore promising directions, such as\nsearch agents, within this expanding field."
                },
                "authors": [
                    {
                        "name": "Yutao Zhu"
                    },
                    {
                        "name": "Huaying Yuan"
                    },
                    {
                        "name": "Shuting Wang"
                    },
                    {
                        "name": "Jiongnan Liu"
                    },
                    {
                        "name": "Wenhan Liu"
                    },
                    {
                        "name": "Chenlong Deng"
                    },
                    {
                        "name": "Haonan Chen"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "updated to version 3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.07107v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.07107v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07569v2",
                "updated": "2024-09-04T11:34:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    11,
                    34,
                    33,
                    2,
                    248,
                    0
                ],
                "published": "2024-04-11T08:57:48Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    8,
                    57,
                    48,
                    3,
                    102,
                    0
                ],
                "title": "Can Vehicle Motion Planning Generalize to Realistic Long-tail Scenarios?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Vehicle Motion Planning Generalize to Realistic Long-tail Scenarios?"
                },
                "summary": "Real-world autonomous driving systems must make safe decisions in the face of\nrare and diverse traffic scenarios. Current state-of-the-art planners are\nmostly evaluated on real-world datasets like nuScenes (open-loop) or nuPlan\n(closed-loop). In particular, nuPlan seems to be an expressive evaluation\nmethod since it is based on real-world data and closed-loop, yet it mostly\ncovers basic driving scenarios. This makes it difficult to judge a planner's\ncapabilities to generalize to rarely-seen situations. Therefore, we propose a\nnovel closed-loop benchmark interPlan containing several edge cases and\nchallenging driving scenarios. We assess existing state-of-the-art planners on\nour benchmark and show that neither rule-based nor learning-based planners can\nsafely navigate the interPlan scenarios. A recently evolving direction is the\nusage of foundation models like large language models (LLM) to handle\ngeneralization. We evaluate an LLM-only planner and introduce a novel hybrid\nplanner that combines an LLM-based behavior planner with a rule-based motion\nplanner that achieves state-of-the-art performance on our benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world autonomous driving systems must make safe decisions in the face of\nrare and diverse traffic scenarios. Current state-of-the-art planners are\nmostly evaluated on real-world datasets like nuScenes (open-loop) or nuPlan\n(closed-loop). In particular, nuPlan seems to be an expressive evaluation\nmethod since it is based on real-world data and closed-loop, yet it mostly\ncovers basic driving scenarios. This makes it difficult to judge a planner's\ncapabilities to generalize to rarely-seen situations. Therefore, we propose a\nnovel closed-loop benchmark interPlan containing several edge cases and\nchallenging driving scenarios. We assess existing state-of-the-art planners on\nour benchmark and show that neither rule-based nor learning-based planners can\nsafely navigate the interPlan scenarios. A recently evolving direction is the\nusage of foundation models like large language models (LLM) to handle\ngeneralization. We evaluate an LLM-only planner and introduce a novel hybrid\nplanner that combines an LLM-based behavior planner with a rule-based motion\nplanner that achieves state-of-the-art performance on our benchmark."
                },
                "authors": [
                    {
                        "name": "Marcel Hallgarten"
                    },
                    {
                        "name": "Julian Zapata"
                    },
                    {
                        "name": "Martin Stoll"
                    },
                    {
                        "name": "Katrin Renz"
                    },
                    {
                        "name": "Andreas Zell"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Zell"
                },
                "author": "Andreas Zell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02617v1",
                "updated": "2024-09-04T11:19:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    11,
                    19,
                    17,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T11:19:17Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    11,
                    19,
                    17,
                    2,
                    248,
                    0
                ],
                "title": "PUB: Plot Understanding Benchmark and Dataset for Evaluating Large\n  Language Models on Synthetic Visual Data Interpretation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PUB: Plot Understanding Benchmark and Dataset for Evaluating Large\n  Language Models on Synthetic Visual Data Interpretation"
                },
                "summary": "The ability of large language models (LLMs) to interpret visual\nrepresentations of data is crucial for advancing their application in data\nanalysis and decision-making processes. This paper presents a novel synthetic\ndataset designed to evaluate the proficiency of LLMs in interpreting various\nforms of data visualizations, including plots like time series, histograms,\nviolins, boxplots, and clusters. Our dataset is generated using controlled\nparameters to ensure comprehensive coverage of potential real-world scenarios.\nWe employ multimodal text prompts with questions related to visual data in\nimages to benchmark several state-of-the-art models like ChatGPT or Gemini,\nassessing their understanding and interpretative accuracy.\n  To ensure data integrity, our benchmark dataset is generated automatically,\nmaking it entirely new and free from prior exposure to the models being tested.\nThis strategy allows us to evaluate the models' ability to truly interpret and\nunderstand the data, eliminating possibility of pre-learned responses, and\nallowing for an unbiased evaluation of the models' capabilities. We also\nintroduce quantitative metrics to assess the performance of the models,\nproviding a robust and comprehensive evaluation tool.\n  Benchmarking several state-of-the-art LLMs with this dataset reveals varying\ndegrees of success, highlighting specific strengths and weaknesses in\ninterpreting diverse types of visual data. The results provide valuable\ninsights into the current capabilities of LLMs and identify key areas for\nimprovement. This work establishes a foundational benchmark for future research\nand development aimed at enhancing the visual interpretative abilities of\nlanguage models. In the future, improved LLMs with robust visual interpretation\nskills can significantly aid in automated data analysis, scientific research,\neducational tools, and business intelligence applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability of large language models (LLMs) to interpret visual\nrepresentations of data is crucial for advancing their application in data\nanalysis and decision-making processes. This paper presents a novel synthetic\ndataset designed to evaluate the proficiency of LLMs in interpreting various\nforms of data visualizations, including plots like time series, histograms,\nviolins, boxplots, and clusters. Our dataset is generated using controlled\nparameters to ensure comprehensive coverage of potential real-world scenarios.\nWe employ multimodal text prompts with questions related to visual data in\nimages to benchmark several state-of-the-art models like ChatGPT or Gemini,\nassessing their understanding and interpretative accuracy.\n  To ensure data integrity, our benchmark dataset is generated automatically,\nmaking it entirely new and free from prior exposure to the models being tested.\nThis strategy allows us to evaluate the models' ability to truly interpret and\nunderstand the data, eliminating possibility of pre-learned responses, and\nallowing for an unbiased evaluation of the models' capabilities. We also\nintroduce quantitative metrics to assess the performance of the models,\nproviding a robust and comprehensive evaluation tool.\n  Benchmarking several state-of-the-art LLMs with this dataset reveals varying\ndegrees of success, highlighting specific strengths and weaknesses in\ninterpreting diverse types of visual data. The results provide valuable\ninsights into the current capabilities of LLMs and identify key areas for\nimprovement. This work establishes a foundational benchmark for future research\nand development aimed at enhancing the visual interpretative abilities of\nlanguage models. In the future, improved LLMs with robust visual interpretation\nskills can significantly aid in automated data analysis, scientific research,\neducational tools, and business intelligence applications."
                },
                "authors": [
                    {
                        "name": "Aneta Pawelec"
                    },
                    {
                        "name": "Victoria Sara Wesoowska"
                    },
                    {
                        "name": "Zuzanna Bczek"
                    },
                    {
                        "name": "Piotr Sankowski"
                    }
                ],
                "author_detail": {
                    "name": "Piotr Sankowski"
                },
                "author": "Piotr Sankowski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00960v2",
                "updated": "2024-09-04T10:58:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    58,
                    26,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-02T06:01:20Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    6,
                    1,
                    20,
                    0,
                    246,
                    0
                ],
                "title": "Unveiling the Vulnerability of Private Fine-Tuning in Split-Based\n  Frameworks for Large Language Models: A Bidirectionally Enhanced Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Vulnerability of Private Fine-Tuning in Split-Based\n  Frameworks for Large Language Models: A Bidirectionally Enhanced Attack"
                },
                "summary": "Recent advancements in pre-trained large language models (LLMs) have\nsignificantly influenced various domains. Adapting these models for specific\ntasks often involves fine-tuning (FT) with private, domain-specific data.\nHowever, privacy concerns keep this data undisclosed, and the computational\ndemands for deploying LLMs pose challenges for resource-limited data holders.\nThis has sparked interest in split learning (SL), a Model-as-a-Service (MaaS)\nparadigm that divides LLMs into smaller segments for distributed training and\ndeployment, transmitting only intermediate activations instead of raw data. SL\nhas garnered substantial interest in both industry and academia as it aims to\nbalance user data privacy, model ownership, and resource challenges in the\nprivate fine-tuning of LLMs. Despite its privacy claims, this paper reveals\nsignificant vulnerabilities arising from the combination of SL and LLM-FT: the\nNot-too-far property of fine-tuning and the auto-regressive nature of LLMs.\nExploiting these vulnerabilities, we propose Bidirectional Semi-white-box\nReconstruction (BiSR), the first data reconstruction attack (DRA) designed to\ntarget both the forward and backward propagation processes of SL. BiSR utilizes\npre-trained weights as prior knowledge, combining a learning-based attack with\na bidirectional optimization-based approach for highly effective data\nreconstruction. Additionally, it incorporates a Noise-adaptive Mixture of\nExperts (NaMoE) model to enhance reconstruction performance under perturbation.\nWe conducted systematic experiments on various mainstream LLMs and different\nsetups, empirically demonstrating BiSR's state-of-the-art performance.\nFurthermore, we thoroughly examined three representative defense mechanisms,\nshowcasing our method's capability to reconstruct private data even in the\npresence of these defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in pre-trained large language models (LLMs) have\nsignificantly influenced various domains. Adapting these models for specific\ntasks often involves fine-tuning (FT) with private, domain-specific data.\nHowever, privacy concerns keep this data undisclosed, and the computational\ndemands for deploying LLMs pose challenges for resource-limited data holders.\nThis has sparked interest in split learning (SL), a Model-as-a-Service (MaaS)\nparadigm that divides LLMs into smaller segments for distributed training and\ndeployment, transmitting only intermediate activations instead of raw data. SL\nhas garnered substantial interest in both industry and academia as it aims to\nbalance user data privacy, model ownership, and resource challenges in the\nprivate fine-tuning of LLMs. Despite its privacy claims, this paper reveals\nsignificant vulnerabilities arising from the combination of SL and LLM-FT: the\nNot-too-far property of fine-tuning and the auto-regressive nature of LLMs.\nExploiting these vulnerabilities, we propose Bidirectional Semi-white-box\nReconstruction (BiSR), the first data reconstruction attack (DRA) designed to\ntarget both the forward and backward propagation processes of SL. BiSR utilizes\npre-trained weights as prior knowledge, combining a learning-based attack with\na bidirectional optimization-based approach for highly effective data\nreconstruction. Additionally, it incorporates a Noise-adaptive Mixture of\nExperts (NaMoE) model to enhance reconstruction performance under perturbation.\nWe conducted systematic experiments on various mainstream LLMs and different\nsetups, empirically demonstrating BiSR's state-of-the-art performance.\nFurthermore, we thoroughly examined three representative defense mechanisms,\nshowcasing our method's capability to reconstruct private data even in the\npresence of these defenses."
                },
                "authors": [
                    {
                        "name": "Guanzhong Chen"
                    },
                    {
                        "name": "Zhenghan Qin"
                    },
                    {
                        "name": "Mingxin Yang"
                    },
                    {
                        "name": "Yajie Zhou"
                    },
                    {
                        "name": "Tao Fan"
                    },
                    {
                        "name": "Tianyu Du"
                    },
                    {
                        "name": "Zenglin Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zenglin Xu"
                },
                "author": "Zenglin Xu",
                "arxiv_comment": "ACM Conference on Computer and Communications Security 2024 (CCS 24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15031v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15031v2",
                "updated": "2024-09-04T10:42:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    42,
                    36,
                    2,
                    248,
                    0
                ],
                "published": "2024-01-26T17:44:53Z",
                "published_parsed": [
                    2024,
                    1,
                    26,
                    17,
                    44,
                    53,
                    4,
                    26,
                    0
                ],
                "title": "Tensor product algorithms for inference of contact network from\n  epidemiological data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor product algorithms for inference of contact network from\n  epidemiological data"
                },
                "summary": "We consider a problem of inferring contact network from nodal states observed\nduring an epidemiological process. In a black--box Bayesian optimisation\nframework this problem reduces to a discrete likelihood optimisation over the\nset of possible networks. The cardinality of this set grows combinatorially\nwith the number of network nodes, which makes this optimisation computationally\nchallenging. For each network, its likelihood is the probability for the\nobserved data to appear during the evolution of the epidemiological process on\nthis network. This probability can be very small, particularly if the network\nis significantly different from the ground truth network, from which the\nobserved data actually appear. A commonly used stochastic simulation algorithm\nstruggles to recover rare events and hence to estimate small probabilities and\nlikelihoods. In this paper we replace the stochastic simulation with solving\nthe chemical master equation for the probabilities of all network states. Since\nthis equation also suffers from the curse of dimensionality, we apply tensor\ntrain approximations to overcome it and enable fast and accurate computations.\nNumerical simulations demonstrate efficient black--box Bayesian inference of\nthe network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a problem of inferring contact network from nodal states observed\nduring an epidemiological process. In a black--box Bayesian optimisation\nframework this problem reduces to a discrete likelihood optimisation over the\nset of possible networks. The cardinality of this set grows combinatorially\nwith the number of network nodes, which makes this optimisation computationally\nchallenging. For each network, its likelihood is the probability for the\nobserved data to appear during the evolution of the epidemiological process on\nthis network. This probability can be very small, particularly if the network\nis significantly different from the ground truth network, from which the\nobserved data actually appear. A commonly used stochastic simulation algorithm\nstruggles to recover rare events and hence to estimate small probabilities and\nlikelihoods. In this paper we replace the stochastic simulation with solving\nthe chemical master equation for the probabilities of all network states. Since\nthis equation also suffers from the curse of dimensionality, we apply tensor\ntrain approximations to overcome it and enable fast and accurate computations.\nNumerical simulations demonstrate efficient black--box Bayesian inference of\nthe network."
                },
                "authors": [
                    {
                        "name": "Sergey Dolgov"
                    },
                    {
                        "name": "Dmitry Savostyanov"
                    }
                ],
                "author_detail": {
                    "name": "Dmitry Savostyanov"
                },
                "author": "Dmitry Savostyanov",
                "arxiv_doi": "10.1186/s12859-024-05910-7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1186/s12859-024-05910-7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.15031v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15031v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "BMC Bioinformatics 25, 285 (2024)",
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "15A69, 34A30, 37N25, 60J28, 62F15, 65F55, 90B15, 95C42",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02604v1",
                "updated": "2024-09-04T10:37:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    37,
                    44,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T10:37:44Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    37,
                    44,
                    2,
                    248,
                    0
                ],
                "title": "Hypothesizing Missing Causal Variables with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hypothesizing Missing Causal Variables with LLMs"
                },
                "summary": "Scientific discovery is a catalyst for human intellectual advances, driven by\nthe cycle of hypothesis generation, experimental design, data evaluation, and\niterative assumption refinement. This process, while crucial, is expensive and\nheavily dependent on the domain knowledge of scientists to generate hypotheses\nand navigate the scientific cycle. Central to this is causality, the ability to\nestablish the relationship between the cause and the effect. Motivated by the\nscientific discovery process, in this work, we formulate a novel task where the\ninput is a partial causal graph with missing variables, and the output is a\nhypothesis about the missing variables to complete the partial graph. We design\na benchmark with varying difficulty levels and knowledge assumptions about the\ncausal graph. With the growing interest in using Large Language Models (LLMs)\nto assist in scientific discovery, we benchmark open-source and closed models\non our testbed. We show the strong ability of LLMs to hypothesize the mediation\nvariables between a cause and its effect. In contrast, they underperform in\nhypothesizing the cause and effect variables themselves. We also observe\nsurprising results where some of the open-source models outperform the closed\nGPT-4 model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific discovery is a catalyst for human intellectual advances, driven by\nthe cycle of hypothesis generation, experimental design, data evaluation, and\niterative assumption refinement. This process, while crucial, is expensive and\nheavily dependent on the domain knowledge of scientists to generate hypotheses\nand navigate the scientific cycle. Central to this is causality, the ability to\nestablish the relationship between the cause and the effect. Motivated by the\nscientific discovery process, in this work, we formulate a novel task where the\ninput is a partial causal graph with missing variables, and the output is a\nhypothesis about the missing variables to complete the partial graph. We design\na benchmark with varying difficulty levels and knowledge assumptions about the\ncausal graph. With the growing interest in using Large Language Models (LLMs)\nto assist in scientific discovery, we benchmark open-source and closed models\non our testbed. We show the strong ability of LLMs to hypothesize the mediation\nvariables between a cause and its effect. In contrast, they underperform in\nhypothesizing the cause and effect variables themselves. We also observe\nsurprising results where some of the open-source models outperform the closed\nGPT-4 model."
                },
                "authors": [
                    {
                        "name": "Ivaxi Sheth"
                    },
                    {
                        "name": "Sahar Abdelnabi"
                    },
                    {
                        "name": "Mario Fritz"
                    }
                ],
                "author_detail": {
                    "name": "Mario Fritz"
                },
                "author": "Mario Fritz",
                "arxiv_comment": "Code - https://github.com/ivaxi0s/hypothesizing-causal-variable-llm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02601v1",
                "updated": "2024-09-04T10:33:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    33,
                    37,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T10:33:37Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    33,
                    37,
                    2,
                    248,
                    0
                ],
                "title": "ChatGPT vs Social Surveys: Probing the Objective and Subjective Human\n  Society",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatGPT vs Social Surveys: Probing the Objective and Subjective Human\n  Society"
                },
                "summary": "The extent to which Large Language Models (LLMs) can simulate the\ndata-generating process for social surveys remains unclear. Current research\nhas not thoroughly assessed potential biases in the sociodemographic population\nrepresented within the language model's framework. Additionally, the subjective\nworlds of LLMs often show inconsistencies in how closely their responses match\nthose of groups of human respondents. In this paper, we used ChatGPT-3.5 to\nsimulate the sampling process and generated six socioeconomic characteristics\nfrom the 2020 US population. We also analyzed responses to questions about\nincome inequality and gender roles to explore GPT's subjective attitudes. By\nusing repeated random sampling, we created a sampling distribution to identify\nthe parameters of the GPT-generated population and compared these with Census\ndata. Our findings show some alignment in gender and age means with the actual\n2020 US population, but we also found mismatches in the distributions of racial\nand educational groups. Furthermore, there were significant differences between\nthe distribution of GPT's responses and human self-reported attitudes. While\nthe overall point estimates of GPT's income attitudinal responses seem to align\nwith the mean of the population occasionally, their response distributions\nfollow a normal distribution that diverges from human responses. In terms of\ngender relations, GPT's answers tend to cluster in the most frequently answered\ncategory, demonstrating a deterministic pattern. We conclude by emphasizing the\ndistinct design philosophies of LLMs and social surveys: LLMs aim to predict\nthe most suitable answers, while social surveys seek to reveal the\nheterogeneity among social groups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The extent to which Large Language Models (LLMs) can simulate the\ndata-generating process for social surveys remains unclear. Current research\nhas not thoroughly assessed potential biases in the sociodemographic population\nrepresented within the language model's framework. Additionally, the subjective\nworlds of LLMs often show inconsistencies in how closely their responses match\nthose of groups of human respondents. In this paper, we used ChatGPT-3.5 to\nsimulate the sampling process and generated six socioeconomic characteristics\nfrom the 2020 US population. We also analyzed responses to questions about\nincome inequality and gender roles to explore GPT's subjective attitudes. By\nusing repeated random sampling, we created a sampling distribution to identify\nthe parameters of the GPT-generated population and compared these with Census\ndata. Our findings show some alignment in gender and age means with the actual\n2020 US population, but we also found mismatches in the distributions of racial\nand educational groups. Furthermore, there were significant differences between\nthe distribution of GPT's responses and human self-reported attitudes. While\nthe overall point estimates of GPT's income attitudinal responses seem to align\nwith the mean of the population occasionally, their response distributions\nfollow a normal distribution that diverges from human responses. In terms of\ngender relations, GPT's answers tend to cluster in the most frequently answered\ncategory, demonstrating a deterministic pattern. We conclude by emphasizing the\ndistinct design philosophies of LLMs and social surveys: LLMs aim to predict\nthe most suitable answers, while social surveys seek to reveal the\nheterogeneity among social groups."
                },
                "authors": [
                    {
                        "name": "Muzhi Zhou"
                    },
                    {
                        "name": "Lu Yu"
                    },
                    {
                        "name": "Xiaomin Geng"
                    },
                    {
                        "name": "Lan Luo"
                    }
                ],
                "author_detail": {
                    "name": "Lan Luo"
                },
                "author": "Lan Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02597v1",
                "updated": "2024-09-04T10:28:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    28,
                    5,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T10:28:05Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    28,
                    5,
                    2,
                    248,
                    0
                ],
                "title": "Rate-Adaptive Generative Semantic Communication Using Conditional\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rate-Adaptive Generative Semantic Communication Using Conditional\n  Diffusion Models"
                },
                "summary": "Recent advances in deep learning-based joint source-channel coding (DJSCC)\nhave shown promise for end-to-end semantic image transmission. However, most\nexisting schemes primarily focus on optimizing pixel-wise metrics, which often\nfail to align with human perception, leading to lower perceptual quality. In\nthis letter, we propose a novel generative DJSCC approach using conditional\ndiffusion models to enhance the perceptual quality of transmitted images.\nSpecifically, by utilizing entropy models, we effectively manage transmission\nbandwidth based on the estimated entropy of transmitted sym-bols. These symbols\nare then used at the receiver as conditional information to guide a conditional\ndiffusion decoder in image reconstruction. Our model is built upon the emerging\nadvanced mamba-like linear attention (MLLA) skeleton, which excels in image\nprocessing tasks while also offering fast inference speed. Besides, we\nintroduce a multi-stage training strategy to ensure the stability and improve\nthe overall performance of the model. Simulation results demonstrate that our\nproposed method significantly outperforms existing approaches in terms of\nperceptual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in deep learning-based joint source-channel coding (DJSCC)\nhave shown promise for end-to-end semantic image transmission. However, most\nexisting schemes primarily focus on optimizing pixel-wise metrics, which often\nfail to align with human perception, leading to lower perceptual quality. In\nthis letter, we propose a novel generative DJSCC approach using conditional\ndiffusion models to enhance the perceptual quality of transmitted images.\nSpecifically, by utilizing entropy models, we effectively manage transmission\nbandwidth based on the estimated entropy of transmitted sym-bols. These symbols\nare then used at the receiver as conditional information to guide a conditional\ndiffusion decoder in image reconstruction. Our model is built upon the emerging\nadvanced mamba-like linear attention (MLLA) skeleton, which excels in image\nprocessing tasks while also offering fast inference speed. Besides, we\nintroduce a multi-stage training strategy to ensure the stability and improve\nthe overall performance of the model. Simulation results demonstrate that our\nproposed method significantly outperforms existing approaches in terms of\nperceptual quality."
                },
                "authors": [
                    {
                        "name": "Pujing Yang"
                    },
                    {
                        "name": "Guangyi Zhang"
                    },
                    {
                        "name": "Yunlong Cai"
                    }
                ],
                "author_detail": {
                    "name": "Yunlong Cai"
                },
                "author": "Yunlong Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01227v2",
                "updated": "2024-09-04T10:20:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    20,
                    59,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-02T13:02:51Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    13,
                    2,
                    51,
                    0,
                    246,
                    0
                ],
                "title": "Prompt Compression with Context-Aware Sentence Encoding for Fast and\n  Improved LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Compression with Context-Aware Sentence Encoding for Fast and\n  Improved LLM Inference"
                },
                "summary": "Large language models (LLMs) have triggered a new stream of research focusing\non compressing the context length to reduce the computational cost while\nensuring the retention of helpful information for LLMs to answer the given\nquestion. Token-based removal methods are one of the most prominent approaches\nin this direction, but risk losing the semantics of the context caused by\nintermediate token removal, especially under high compression ratios, while\nalso facing challenges in computational efficiency. In this work, we propose\ncontext-aware prompt compression (CPC), a sentence-level prompt compression\ntechnique where its key innovation is a novel context-aware sentence encoder\nthat provides a relevance score for each sentence for a given question. To\ntrain this encoder, we generate a new dataset consisting of questions,\npositives, and negative pairs where positives are sentences relevant to the\nquestion, while negatives are irrelevant context sentences. We train the\nencoder in a contrastive setup to learn context-aware sentence representations.\nOur method considerably outperforms prior works on prompt compression on\nbenchmark datasets and is up to 10.93x faster at inference compared to the best\ntoken-level compression method. We also find better improvement for shorter\nlength constraints in most benchmarks, showing the effectiveness of our\nproposed solution in the compression of relevant information in a shorter\ncontext. Finally, we release the code and the dataset for quick reproducibility\nand further development: https://github.com/Workday/cpc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have triggered a new stream of research focusing\non compressing the context length to reduce the computational cost while\nensuring the retention of helpful information for LLMs to answer the given\nquestion. Token-based removal methods are one of the most prominent approaches\nin this direction, but risk losing the semantics of the context caused by\nintermediate token removal, especially under high compression ratios, while\nalso facing challenges in computational efficiency. In this work, we propose\ncontext-aware prompt compression (CPC), a sentence-level prompt compression\ntechnique where its key innovation is a novel context-aware sentence encoder\nthat provides a relevance score for each sentence for a given question. To\ntrain this encoder, we generate a new dataset consisting of questions,\npositives, and negative pairs where positives are sentences relevant to the\nquestion, while negatives are irrelevant context sentences. We train the\nencoder in a contrastive setup to learn context-aware sentence representations.\nOur method considerably outperforms prior works on prompt compression on\nbenchmark datasets and is up to 10.93x faster at inference compared to the best\ntoken-level compression method. We also find better improvement for shorter\nlength constraints in most benchmarks, showing the effectiveness of our\nproposed solution in the compression of relevant information in a shorter\ncontext. Finally, we release the code and the dataset for quick reproducibility\nand further development: https://github.com/Workday/cpc."
                },
                "authors": [
                    {
                        "name": "Barys Liskavets"
                    },
                    {
                        "name": "Maxim Ushakov"
                    },
                    {
                        "name": "Shuvendu Roy"
                    },
                    {
                        "name": "Mark Klibanov"
                    },
                    {
                        "name": "Ali Etemad"
                    },
                    {
                        "name": "Shane Luke"
                    }
                ],
                "author_detail": {
                    "name": "Shane Luke"
                },
                "author": "Shane Luke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04985v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04985v6",
                "updated": "2024-09-04T10:04:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    4,
                    52,
                    2,
                    248,
                    0
                ],
                "published": "2023-12-08T11:47:35Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    11,
                    47,
                    35,
                    4,
                    342,
                    0
                ],
                "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQ Attention: Bandwidth-Efficient LLM Inference"
                },
                "summary": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Luka Ribar"
                    },
                    {
                        "name": "Ivan Chelombiev"
                    },
                    {
                        "name": "Luke Hudlass-Galley"
                    },
                    {
                        "name": "Charlie Blake"
                    },
                    {
                        "name": "Carlo Luschi"
                    },
                    {
                        "name": "Douglas Orr"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Orr"
                },
                "author": "Douglas Orr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04985v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04985v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02580v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02580v1",
                "updated": "2024-09-04T10:03:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    3,
                    9,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T10:03:09Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    3,
                    9,
                    2,
                    248,
                    0
                ],
                "title": "AlignGroup: Learning and Aligning Group Consensus with Member\n  Preferences for Group Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignGroup: Learning and Aligning Group Consensus with Member\n  Preferences for Group Recommendation"
                },
                "summary": "Group activities are important behaviors in human society, providing\npersonalized recommendations for groups is referred to as the group\nrecommendation task. Existing methods can usually be categorized into two\nstrategies to infer group preferences: 1) determining group preferences by\naggregating members' personalized preferences, and 2) inferring group consensus\nby capturing group members' coherent decisions after common compromises.\nHowever, the former would suffer from the lack of group-level considerations,\nand the latter overlooks the fine-grained preferences of individual users. To\nthis end, we propose a novel group recommendation method AlignGroup, which\nfocuses on both group consensus and individual preferences of group members to\ninfer the group decision-making. Specifically, AlignGroup explores group\nconsensus through a well-designed hypergraph neural network that efficiently\nlearns intra- and inter-group relationships. Moreover, AlignGroup innovatively\nutilizes a self-supervised alignment task to capture fine-grained group\ndecision-making by aligning the group consensus with members' common\npreferences. Extensive experiments on two real-world datasets validate that our\nAlignGroup outperforms the state-of-the-art on both the group recommendation\ntask and the user recommendation task, as well as outperforms the efficiency of\nmost baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group activities are important behaviors in human society, providing\npersonalized recommendations for groups is referred to as the group\nrecommendation task. Existing methods can usually be categorized into two\nstrategies to infer group preferences: 1) determining group preferences by\naggregating members' personalized preferences, and 2) inferring group consensus\nby capturing group members' coherent decisions after common compromises.\nHowever, the former would suffer from the lack of group-level considerations,\nand the latter overlooks the fine-grained preferences of individual users. To\nthis end, we propose a novel group recommendation method AlignGroup, which\nfocuses on both group consensus and individual preferences of group members to\ninfer the group decision-making. Specifically, AlignGroup explores group\nconsensus through a well-designed hypergraph neural network that efficiently\nlearns intra- and inter-group relationships. Moreover, AlignGroup innovatively\nutilizes a self-supervised alignment task to capture fine-grained group\ndecision-making by aligning the group consensus with members' common\npreferences. Extensive experiments on two real-world datasets validate that our\nAlignGroup outperforms the state-of-the-art on both the group recommendation\ntask and the user recommendation task, as well as outperforms the efficiency of\nmost baselines."
                },
                "authors": [
                    {
                        "name": "Jinfeng Xu"
                    },
                    {
                        "name": "Zheyu Chen"
                    },
                    {
                        "name": "Jinze Li"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Hewei Wang"
                    },
                    {
                        "name": "Edith C. -H. Ngai"
                    }
                ],
                "author_detail": {
                    "name": "Edith C. -H. Ngai"
                },
                "author": "Edith C. -H. Ngai",
                "arxiv_doi": "10.1145/3627673.3679697",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3627673.3679697",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02580v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02580v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "10 pages, accepted by CIKM 2024",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02572v1",
                "updated": "2024-09-04T09:46:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    9,
                    46,
                    33,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T09:46:33Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    9,
                    46,
                    33,
                    2,
                    248,
                    0
                ],
                "title": "Advancing Cyber Incident Timeline Analysis Through Rule Based AI and\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Cyber Incident Timeline Analysis Through Rule Based AI and\n  Large Language Models"
                },
                "summary": "Timeline Analysis (TA) is a key part of Timeline Forensics (TF) in Digital\nForensics (DF), focusing primarily on examining and analysing temporal digital\nartefacts such as timestamps, derived from event logs, file metadata, and other\nrelated data to correlate events resulting from cyber incidents and reconstruct\ntheir chronological timeline. Traditional tools often struggle to efficiently\nprocess the vast volume and variety of data acquired during DF investigations\nand Incident Response (IR) processes. This paper presents a novel framework,\nGenDFIR, that combines Rule-Based Artificial Intelligence (R-BAI) algorithms\nwith Large Language Models (LLMs) to advance and automate the TA process. Our\napproach consists of two main stages (1) We use R-BAI to identify and select\nanomalous digital artefacts based on predefined rules. (2) The selected\nartefacts are then converted into embeddings for processing by an LLM with the\nhelp of a Retrieval-Augmented Generation (RAG) agent. The LLM consequently\nleverages its capabilities to perform automated TA on the artefacts and predict\npotential incident scenarios. To validate our framework, we evaluate GenDFIR\nperformance, efficiency, and reliability using various metrics across synthetic\ncyber incident simulation scenarios. This paper presents a proof of concept,\nwhere the findings demonstrate the significant potential of integrating R-BAI\nand LLMs for TA. This novel approach highlights the power of Generative AI\n(GenAI), specifically LLMs, and opens new avenues for advanced threat detection\nand incident reconstruction, representing a significant step forward in the\nfield.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timeline Analysis (TA) is a key part of Timeline Forensics (TF) in Digital\nForensics (DF), focusing primarily on examining and analysing temporal digital\nartefacts such as timestamps, derived from event logs, file metadata, and other\nrelated data to correlate events resulting from cyber incidents and reconstruct\ntheir chronological timeline. Traditional tools often struggle to efficiently\nprocess the vast volume and variety of data acquired during DF investigations\nand Incident Response (IR) processes. This paper presents a novel framework,\nGenDFIR, that combines Rule-Based Artificial Intelligence (R-BAI) algorithms\nwith Large Language Models (LLMs) to advance and automate the TA process. Our\napproach consists of two main stages (1) We use R-BAI to identify and select\nanomalous digital artefacts based on predefined rules. (2) The selected\nartefacts are then converted into embeddings for processing by an LLM with the\nhelp of a Retrieval-Augmented Generation (RAG) agent. The LLM consequently\nleverages its capabilities to perform automated TA on the artefacts and predict\npotential incident scenarios. To validate our framework, we evaluate GenDFIR\nperformance, efficiency, and reliability using various metrics across synthetic\ncyber incident simulation scenarios. This paper presents a proof of concept,\nwhere the findings demonstrate the significant potential of integrating R-BAI\nand LLMs for TA. This novel approach highlights the power of Generative AI\n(GenAI), specifically LLMs, and opens new avenues for advanced threat detection\nand incident reconstruction, representing a significant step forward in the\nfield."
                },
                "authors": [
                    {
                        "name": "Fatma Yasmine Loumachi"
                    },
                    {
                        "name": "Mohamed Chahine Ghanem"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Chahine Ghanem"
                },
                "author": "Mohamed Chahine Ghanem",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02569v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02569v1",
                "updated": "2024-09-04T09:39:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    9,
                    39,
                    7,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T09:39:07Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    9,
                    39,
                    7,
                    2,
                    248,
                    0
                ],
                "title": "More is More: Addition Bias in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More is More: Addition Bias in Large Language Models"
                },
                "summary": "In this paper, we investigate the presence of additive bias in Large Language\nModels (LLMs), drawing a parallel to the cognitive bias observed in humans\nwhere individuals tend to favor additive over subtractive changes. Using a\nseries of controlled experiments, we tested various LLMs, including GPT-3.5\nTurbo, Claude 3.5 Sonnet, Mistral, Math$\\Sigma$tral, and Llama 3.1, on tasks\ndesigned to measure their propensity for additive versus subtractive\nmodifications. Our findings demonstrate a significant preference for additive\nchanges across all tested models. For example, in a palindrome creation task,\nLlama 3.1 favored adding letters 97.85% of the time over removing them.\nSimilarly, in a Lego tower balancing task, GPT-3.5 Turbo chose to add a brick\n76.38% of the time rather than remove one. In a text summarization task,\nMistral 7B produced longer summaries in 59.40% to 75.10% of cases when asked to\nimprove its own or others' writing. These results indicate that, similar to\nhumans, LLMs exhibit a marked additive bias, which might have implications when\nLLMs are used on a large scale. Addittive bias might increase resource use and\nenvironmental impact, leading to higher economic costs due to overconsumption\nand waste. This bias should be considered in the development and application of\nLLMs to ensure balanced and efficient problem-solving approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate the presence of additive bias in Large Language\nModels (LLMs), drawing a parallel to the cognitive bias observed in humans\nwhere individuals tend to favor additive over subtractive changes. Using a\nseries of controlled experiments, we tested various LLMs, including GPT-3.5\nTurbo, Claude 3.5 Sonnet, Mistral, Math$\\Sigma$tral, and Llama 3.1, on tasks\ndesigned to measure their propensity for additive versus subtractive\nmodifications. Our findings demonstrate a significant preference for additive\nchanges across all tested models. For example, in a palindrome creation task,\nLlama 3.1 favored adding letters 97.85% of the time over removing them.\nSimilarly, in a Lego tower balancing task, GPT-3.5 Turbo chose to add a brick\n76.38% of the time rather than remove one. In a text summarization task,\nMistral 7B produced longer summaries in 59.40% to 75.10% of cases when asked to\nimprove its own or others' writing. These results indicate that, similar to\nhumans, LLMs exhibit a marked additive bias, which might have implications when\nLLMs are used on a large scale. Addittive bias might increase resource use and\nenvironmental impact, leading to higher economic costs due to overconsumption\nand waste. This bias should be considered in the development and application of\nLLMs to ensure balanced and efficient problem-solving approaches."
                },
                "authors": [
                    {
                        "name": "Luca Santagata"
                    },
                    {
                        "name": "Cristiano De Nobili"
                    }
                ],
                "author_detail": {
                    "name": "Cristiano De Nobili"
                },
                "author": "Cristiano De Nobili",
                "arxiv_comment": "25 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02569v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.00109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.00109v2",
                "updated": "2024-09-04T09:27:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    9,
                    27,
                    5,
                    2,
                    248,
                    0
                ],
                "published": "2023-07-26T18:58:53Z",
                "published_parsed": [
                    2023,
                    7,
                    26,
                    18,
                    58,
                    53,
                    2,
                    207,
                    0
                ],
                "title": "A Sentence is Worth a Thousand Pictures: Can Large Language Models\n  Understand Hum4n L4ngu4ge and the W0rld behind W0rds?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Sentence is Worth a Thousand Pictures: Can Large Language Models\n  Understand Hum4n L4ngu4ge and the W0rld behind W0rds?"
                },
                "summary": "Modern Artificial Intelligence applications show great potential for\nlanguage-related tasks that rely on next-word prediction. The current\ngeneration of Large Language Models (LLMs) have been linked to claims about\nhuman-like linguistic performance and their applications are hailed both as a\nstep towards artificial general intelligence and as a major advance in\nunderstanding the cognitive, and even neural basis of human language. To assess\nthese claims, first we analyze the contribution of LLMs as theoretically\ninformative representations of a target cognitive system vs. atheoretical\nmechanistic tools. Second, we evaluate the models' ability to see the bigger\npicture, through top-down feedback from higher levels of processing, which\nrequires grounding in previous expectations and past world experience. We\nhypothesize that since models lack grounded cognition, they cannot take\nadvantage of these features and instead solely rely on fixed associations\nbetween represented words and word vectors. To assess this, we designed and ran\na novel 'leet task' (l33t t4sk), which requires decoding sentences in which\nletters are systematically replaced by numbers. The results suggest that humans\nexcel in this task whereas models struggle, confirming our hypothesis. We\ninterpret the results by identifying the key abilities that are still missing\nfrom the current state of development of these models, which require solutions\nthat go beyond increased system scaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Artificial Intelligence applications show great potential for\nlanguage-related tasks that rely on next-word prediction. The current\ngeneration of Large Language Models (LLMs) have been linked to claims about\nhuman-like linguistic performance and their applications are hailed both as a\nstep towards artificial general intelligence and as a major advance in\nunderstanding the cognitive, and even neural basis of human language. To assess\nthese claims, first we analyze the contribution of LLMs as theoretically\ninformative representations of a target cognitive system vs. atheoretical\nmechanistic tools. Second, we evaluate the models' ability to see the bigger\npicture, through top-down feedback from higher levels of processing, which\nrequires grounding in previous expectations and past world experience. We\nhypothesize that since models lack grounded cognition, they cannot take\nadvantage of these features and instead solely rely on fixed associations\nbetween represented words and word vectors. To assess this, we designed and ran\na novel 'leet task' (l33t t4sk), which requires decoding sentences in which\nletters are systematically replaced by numbers. The results suggest that humans\nexcel in this task whereas models struggle, confirming our hypothesis. We\ninterpret the results by identifying the key abilities that are still missing\nfrom the current state of development of these models, which require solutions\nthat go beyond increased system scaling."
                },
                "authors": [
                    {
                        "name": "Evelina Leivada"
                    },
                    {
                        "name": "Gary Marcus"
                    },
                    {
                        "name": "Fritz Gnther"
                    },
                    {
                        "name": "Elliot Murphy"
                    }
                ],
                "author_detail": {
                    "name": "Elliot Murphy"
                },
                "author": "Elliot Murphy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.00109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.00109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03222v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03222v2",
                "updated": "2024-09-04T09:25:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    9,
                    25,
                    21,
                    2,
                    248,
                    0
                ],
                "published": "2024-05-06T07:31:41Z",
                "published_parsed": [
                    2024,
                    5,
                    6,
                    7,
                    31,
                    41,
                    0,
                    127,
                    0
                ],
                "title": "Computational Efficient Width-Wise Early Exiting in Wireless\n  Communication Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Efficient Width-Wise Early Exiting in Wireless\n  Communication Systems"
                },
                "summary": "Deep learning (DL) techniques are increasingly pervasive across various\ndomains, including wireless communication, where they extract insights from raw\nradio signals. However, the computational demands of DL pose significant\nchallenges, particularly in distributed wireless networks like Cell-free\nnetworks, where deploying DL models on edge devices becomes hard due to\nheightened computational loads. These computational loads escalate with larger\ninput sizes, often correlating with improved model performance. To mitigate\nthis challenge, Early Exiting (EE) techniques have been introduced in DL,\nprimarily targeting the depth of the model. This approach enables models to\nexit during inference based on specified criteria, leveraging entropy measures\nat intermediate exits. Doing so makes less complex samples exit early, reducing\nthe average computational load and inference time. In our contribution, we\npropose a novel width-wise exiting strategy for Convolutional Neural Network\n(CNN)-based architectures. By selectively adjusting the input size, we aim to\nregulate computational demands effectively. Our approach aims to decrease the\naverage computational load during inference while maintaining performance\nlevels comparable to conventional models. We specifically investigate\nModulation Classification, a well-established application of DL in wireless\ncommunication. Our experimental results show substantial reductions in\ncomputational load, with an average decrease of 26%, and particularly notable\nreductions of 60% in high-SNR scenarios. Through this work, we present a\npractical solution for reducing computational demands in deep learning\napplications, particularly within the domain of wireless communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning (DL) techniques are increasingly pervasive across various\ndomains, including wireless communication, where they extract insights from raw\nradio signals. However, the computational demands of DL pose significant\nchallenges, particularly in distributed wireless networks like Cell-free\nnetworks, where deploying DL models on edge devices becomes hard due to\nheightened computational loads. These computational loads escalate with larger\ninput sizes, often correlating with improved model performance. To mitigate\nthis challenge, Early Exiting (EE) techniques have been introduced in DL,\nprimarily targeting the depth of the model. This approach enables models to\nexit during inference based on specified criteria, leveraging entropy measures\nat intermediate exits. Doing so makes less complex samples exit early, reducing\nthe average computational load and inference time. In our contribution, we\npropose a novel width-wise exiting strategy for Convolutional Neural Network\n(CNN)-based architectures. By selectively adjusting the input size, we aim to\nregulate computational demands effectively. Our approach aims to decrease the\naverage computational load during inference while maintaining performance\nlevels comparable to conventional models. We specifically investigate\nModulation Classification, a well-established application of DL in wireless\ncommunication. Our experimental results show substantial reductions in\ncomputational load, with an average decrease of 26%, and particularly notable\nreductions of 60% in high-SNR scenarios. Through this work, we present a\npractical solution for reducing computational demands in deep learning\napplications, particularly within the domain of wireless communication."
                },
                "authors": [
                    {
                        "name": "Dieter Verbruggen"
                    },
                    {
                        "name": "Hazem Sallouha"
                    },
                    {
                        "name": "Sofie Pollin"
                    }
                ],
                "author_detail": {
                    "name": "Sofie Pollin"
                },
                "author": "Sofie Pollin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03222v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03222v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08956v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08956v2",
                "updated": "2024-09-04T08:52:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    8,
                    52,
                    47,
                    2,
                    248,
                    0
                ],
                "published": "2024-08-16T18:01:02Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    18,
                    1,
                    2,
                    4,
                    229,
                    0
                ],
                "title": "Theory-agnostic searches for non-gravitational modes in black hole\n  ringdown",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theory-agnostic searches for non-gravitational modes in black hole\n  ringdown"
                },
                "summary": "In any extension of General Relativity (GR), extra fundamental degrees of\nfreedom couple to gravity. Besides deforming GR forecasts in a theory-dependent\nway, this coupling generically introduces extra modes in the gravitational-wave\nsignal. We propose a novel theory-agnostic test of gravity to search for these\nnongravitational modes in black hole merger ringdown signals. To leading order\nin the GR deviations, their frequencies and damping times match those of a test\nscalar or vector field in a Kerr background, with only amplitudes and phases as\nfree parameters. This test will be highly valuable for future detectors, which\nwill achieve signal-to-noise ratios higher than 100 (and as high as 1000 for\nspace-based detectors such as LISA). Such sensitivity will allow measurement of\nthese modes with amplitude ratios as low as 0.05 for ground-based detectors\n(and as low as 0.008 for LISA), relative to the fundamental mode, enabling\nstringent agnostic constraints or detection of scalar/vector modes. By applying\nthis test to GW150914, GW190521, and GW200129, we find that the current\nevidence for an extra mode is comparable to that for the first gravitational\novertone, but its inclusion modifies the inferred remnant spin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In any extension of General Relativity (GR), extra fundamental degrees of\nfreedom couple to gravity. Besides deforming GR forecasts in a theory-dependent\nway, this coupling generically introduces extra modes in the gravitational-wave\nsignal. We propose a novel theory-agnostic test of gravity to search for these\nnongravitational modes in black hole merger ringdown signals. To leading order\nin the GR deviations, their frequencies and damping times match those of a test\nscalar or vector field in a Kerr background, with only amplitudes and phases as\nfree parameters. This test will be highly valuable for future detectors, which\nwill achieve signal-to-noise ratios higher than 100 (and as high as 1000 for\nspace-based detectors such as LISA). Such sensitivity will allow measurement of\nthese modes with amplitude ratios as low as 0.05 for ground-based detectors\n(and as low as 0.008 for LISA), relative to the fundamental mode, enabling\nstringent agnostic constraints or detection of scalar/vector modes. By applying\nthis test to GW150914, GW190521, and GW200129, we find that the current\nevidence for an extra mode is comparable to that for the first gravitational\novertone, but its inclusion modifies the inferred remnant spin."
                },
                "authors": [
                    {
                        "name": "Francesco Crescimbeni"
                    },
                    {
                        "name": "Xisco Jimenez Forteza"
                    },
                    {
                        "name": "Swetha Bhagwat"
                    },
                    {
                        "name": "Julian Westerweck"
                    },
                    {
                        "name": "Paolo Pani"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Pani"
                },
                "author": "Paolo Pani",
                "arxiv_comment": "10 pages, 9 figures. v2: minor modifications, new explicit example,\n  and new results on precessing binaries. Submitted version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08956v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02530v1",
                "updated": "2024-09-04T08:44:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    8,
                    44,
                    36,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T08:44:36Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    8,
                    44,
                    36,
                    2,
                    248,
                    0
                ],
                "title": "Understanding eGFR Trajectories and Kidney Function Decline via Large\n  Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding eGFR Trajectories and Kidney Function Decline via Large\n  Multimodal Models"
                },
                "summary": "The estimated Glomerular Filtration Rate (eGFR) is an essential indicator of\nkidney function in clinical practice. Although traditional equations and\nMachine Learning (ML) models using clinical and laboratory data can estimate\neGFR, accurately predicting future eGFR levels remains a significant challenge\nfor nephrologists and ML researchers. Recent advances demonstrate that Large\nLanguage Models (LLMs) and Large Multimodal Models (LMMs) can serve as robust\nfoundation models for diverse applications. This study investigates the\npotential of LMMs to predict future eGFR levels with a dataset consisting of\nlaboratory and clinical values from 50 patients. By integrating various\nprompting techniques and ensembles of LMMs, our findings suggest that these\nmodels, when combined with precise prompts and visual representations of eGFR\ntrajectories, offer predictive performance comparable to existing ML models.\nThis research extends the application of foundation models and suggests avenues\nfor future studies to harness these models in addressing complex medical\nforecasting challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The estimated Glomerular Filtration Rate (eGFR) is an essential indicator of\nkidney function in clinical practice. Although traditional equations and\nMachine Learning (ML) models using clinical and laboratory data can estimate\neGFR, accurately predicting future eGFR levels remains a significant challenge\nfor nephrologists and ML researchers. Recent advances demonstrate that Large\nLanguage Models (LLMs) and Large Multimodal Models (LMMs) can serve as robust\nfoundation models for diverse applications. This study investigates the\npotential of LMMs to predict future eGFR levels with a dataset consisting of\nlaboratory and clinical values from 50 patients. By integrating various\nprompting techniques and ensembles of LMMs, our findings suggest that these\nmodels, when combined with precise prompts and visual representations of eGFR\ntrajectories, offer predictive performance comparable to existing ML models.\nThis research extends the application of foundation models and suggests avenues\nfor future studies to harness these models in addressing complex medical\nforecasting challenges."
                },
                "authors": [
                    {
                        "name": "Chih-Yuan Li"
                    },
                    {
                        "name": "Jun-Ting Wu"
                    },
                    {
                        "name": "Chan Hsu"
                    },
                    {
                        "name": "Ming-Yen Lin"
                    },
                    {
                        "name": "Yihuang Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yihuang Kang"
                },
                "author": "Yihuang Kang",
                "arxiv_comment": "This preprint version includes corrections of typographical errors\n  related to numerical values in Table 2, which were present in the version\n  published at the BDH workshop in MIPR 2024. These corrections do not affect\n  the overall conclusions of the study",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02529v1",
                "updated": "2024-09-04T08:42:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    8,
                    42,
                    42,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T08:42:42Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    8,
                    42,
                    42,
                    2,
                    248,
                    0
                ],
                "title": "Sample what you cant compress",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sample what you cant compress"
                },
                "summary": "For learned image representations, basic autoencoders often produce blurry\nresults. Reconstruction quality can be improved by incorporating additional\npenalties such as adversarial (GAN) and perceptual losses. Arguably, these\napproaches lack a principled interpretation. Concurrently, in generative\nsettings diffusion has demonstrated a remarkable ability to create crisp, high\nquality results and has solid theoretical underpinnings (from variational\ninference to direct study as the Fisher Divergence). Our work combines\nautoencoder representation learning with diffusion and is, to our knowledge,\nthe first to demonstrate the efficacy of jointly learning a continuous encoder\nand decoder under a diffusion-based loss. We demonstrate that this approach\nyields better reconstruction quality as compared to GAN-based autoencoders\nwhile being easier to tune. We also show that the resulting representation is\neasier to model with a latent diffusion model as compared to the representation\nobtained from a state-of-the-art GAN-based loss. Since our decoder is\nstochastic, it can generate details not encoded in the otherwise deterministic\nlatent representation; we therefore name our approach \"Sample what you can't\ncompress\", or SWYCC for short.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For learned image representations, basic autoencoders often produce blurry\nresults. Reconstruction quality can be improved by incorporating additional\npenalties such as adversarial (GAN) and perceptual losses. Arguably, these\napproaches lack a principled interpretation. Concurrently, in generative\nsettings diffusion has demonstrated a remarkable ability to create crisp, high\nquality results and has solid theoretical underpinnings (from variational\ninference to direct study as the Fisher Divergence). Our work combines\nautoencoder representation learning with diffusion and is, to our knowledge,\nthe first to demonstrate the efficacy of jointly learning a continuous encoder\nand decoder under a diffusion-based loss. We demonstrate that this approach\nyields better reconstruction quality as compared to GAN-based autoencoders\nwhile being easier to tune. We also show that the resulting representation is\neasier to model with a latent diffusion model as compared to the representation\nobtained from a state-of-the-art GAN-based loss. Since our decoder is\nstochastic, it can generate details not encoded in the otherwise deterministic\nlatent representation; we therefore name our approach \"Sample what you can't\ncompress\", or SWYCC for short."
                },
                "authors": [
                    {
                        "name": "Vighnesh Birodkar"
                    },
                    {
                        "name": "Gabriel Barcik"
                    },
                    {
                        "name": "James Lyon"
                    },
                    {
                        "name": "Sergey Ioffe"
                    },
                    {
                        "name": "David Minnen"
                    },
                    {
                        "name": "Joshua V. Dillon"
                    }
                ],
                "author_detail": {
                    "name": "Joshua V. Dillon"
                },
                "author": "Joshua V. Dillon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02522v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02522v1",
                "updated": "2024-09-04T08:30:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    8,
                    30,
                    3,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T08:30:03Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    8,
                    30,
                    3,
                    2,
                    248,
                    0
                ],
                "title": "Cog-GA: A Large Language Models-based Generative Agent for\n  Vision-Language Navigation in Continuous Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cog-GA: A Large Language Models-based Generative Agent for\n  Vision-Language Navigation in Continuous Environments"
                },
                "summary": "Vision Language Navigation in Continuous Environments (VLN-CE) represents a\nfrontier in embodied AI, demanding agents to navigate freely in unbounded 3D\nspaces solely guided by natural language instructions. This task introduces\ndistinct challenges in multimodal comprehension, spatial reasoning, and\ndecision-making. To address these challenges, we introduce Cog-GA, a generative\nagent founded on large language models (LLMs) tailored for VLN-CE tasks. Cog-GA\nemploys a dual-pronged strategy to emulate human-like cognitive processes.\nFirstly, it constructs a cognitive map, integrating temporal, spatial, and\nsemantic elements, thereby facilitating the development of spatial memory\nwithin LLMs. Secondly, Cog-GA employs a predictive mechanism for waypoints,\nstrategically optimizing the exploration trajectory to maximize navigational\nefficiency. Each waypoint is accompanied by a dual-channel scene description,\ncategorizing environmental cues into 'what' and 'where' streams as the brain.\nThis segregation enhances the agent's attentional focus, enabling it to discern\npertinent spatial information for navigation. A reflective mechanism\ncomplements these strategies by capturing feedback from prior navigation\nexperiences, facilitating continual learning and adaptive replanning. Extensive\nevaluations conducted on VLN-CE benchmarks validate Cog-GA's state-of-the-art\nperformance and ability to simulate human-like navigation behaviors. This\nresearch significantly contributes to the development of strategic and\ninterpretable VLN-CE agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Navigation in Continuous Environments (VLN-CE) represents a\nfrontier in embodied AI, demanding agents to navigate freely in unbounded 3D\nspaces solely guided by natural language instructions. This task introduces\ndistinct challenges in multimodal comprehension, spatial reasoning, and\ndecision-making. To address these challenges, we introduce Cog-GA, a generative\nagent founded on large language models (LLMs) tailored for VLN-CE tasks. Cog-GA\nemploys a dual-pronged strategy to emulate human-like cognitive processes.\nFirstly, it constructs a cognitive map, integrating temporal, spatial, and\nsemantic elements, thereby facilitating the development of spatial memory\nwithin LLMs. Secondly, Cog-GA employs a predictive mechanism for waypoints,\nstrategically optimizing the exploration trajectory to maximize navigational\nefficiency. Each waypoint is accompanied by a dual-channel scene description,\ncategorizing environmental cues into 'what' and 'where' streams as the brain.\nThis segregation enhances the agent's attentional focus, enabling it to discern\npertinent spatial information for navigation. A reflective mechanism\ncomplements these strategies by capturing feedback from prior navigation\nexperiences, facilitating continual learning and adaptive replanning. Extensive\nevaluations conducted on VLN-CE benchmarks validate Cog-GA's state-of-the-art\nperformance and ability to simulate human-like navigation behaviors. This\nresearch significantly contributes to the development of strategic and\ninterpretable VLN-CE agents."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Li"
                    },
                    {
                        "name": "Yanfeng Lu"
                    },
                    {
                        "name": "Yao Mu"
                    },
                    {
                        "name": "Hong Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Hong Qiao"
                },
                "author": "Hong Qiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02522v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02522v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02519v1",
                "updated": "2024-09-04T08:27:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    8,
                    27,
                    43,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T08:27:43Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    8,
                    27,
                    43,
                    2,
                    248,
                    0
                ],
                "title": "Language is Scary when Over-Analyzed: Unpacking Implied Misogynistic\n  Reasoning with Argumentation Theory-Driven Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language is Scary when Over-Analyzed: Unpacking Implied Misogynistic\n  Reasoning with Argumentation Theory-Driven Prompts"
                },
                "summary": "We propose misogyny detection as an Argumentative Reasoning task and we\ninvestigate the capacity of large language models (LLMs) to understand the\nimplicit reasoning used to convey misogyny in both Italian and English. The\ncentral aim is to generate the missing reasoning link between a message and the\nimplied meanings encoding the misogyny. Our study uses argumentation theory as\na foundation to form a collection of prompts in both zero-shot and few-shot\nsettings. These prompts integrate different techniques, including\nchain-of-thought reasoning and augmented knowledge. Our findings show that LLMs\nfall short on reasoning capabilities about misogynistic comments and that they\nmostly rely on their implicit knowledge derived from internalized common\nstereotypes about women to generate implied assumptions, rather than on\ninductive reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose misogyny detection as an Argumentative Reasoning task and we\ninvestigate the capacity of large language models (LLMs) to understand the\nimplicit reasoning used to convey misogyny in both Italian and English. The\ncentral aim is to generate the missing reasoning link between a message and the\nimplied meanings encoding the misogyny. Our study uses argumentation theory as\na foundation to form a collection of prompts in both zero-shot and few-shot\nsettings. These prompts integrate different techniques, including\nchain-of-thought reasoning and augmented knowledge. Our findings show that LLMs\nfall short on reasoning capabilities about misogynistic comments and that they\nmostly rely on their implicit knowledge derived from internalized common\nstereotypes about women to generate implied assumptions, rather than on\ninductive reasoning."
                },
                "authors": [
                    {
                        "name": "Arianna Muti"
                    },
                    {
                        "name": "Federico Ruggeri"
                    },
                    {
                        "name": "Khalid Al-Khatib"
                    },
                    {
                        "name": "Alberto Barrn-Cedeo"
                    },
                    {
                        "name": "Tommaso Caselli"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Caselli"
                },
                "author": "Tommaso Caselli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02517v1",
                "updated": "2024-09-04T08:25:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    8,
                    25,
                    54,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T08:25:54Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    8,
                    25,
                    54,
                    2,
                    248,
                    0
                ],
                "title": "Training Universal Vocoders with Feature Smoothing-Based Augmentation\n  Methods for High-Quality TTS Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Universal Vocoders with Feature Smoothing-Based Augmentation\n  Methods for High-Quality TTS Systems"
                },
                "summary": "While universal vocoders have achieved proficient waveform generation across\ndiverse voices, their integration into text-to-speech (TTS) tasks often results\nin degraded synthetic quality. To address this challenge, we present a novel\naugmentation technique for training universal vocoders. Our training scheme\nrandomly applies linear smoothing filters to input acoustic features,\nfacilitating vocoder generalization across a wide range of smoothings. It\nsignificantly mitigates the training-inference mismatch, enhancing the\nnaturalness of synthetic output even when the acoustic model produces overly\nsmoothed features. Notably, our method is applicable to any vocoder without\nrequiring architectural modifications or dependencies on specific acoustic\nmodels. The experimental results validate the superiority of our vocoder over\nconventional methods, achieving 11.99% and 12.05% improvements in mean opinion\nscores when integrated with Tacotron 2 and FastSpeech 2 TTS acoustic models,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While universal vocoders have achieved proficient waveform generation across\ndiverse voices, their integration into text-to-speech (TTS) tasks often results\nin degraded synthetic quality. To address this challenge, we present a novel\naugmentation technique for training universal vocoders. Our training scheme\nrandomly applies linear smoothing filters to input acoustic features,\nfacilitating vocoder generalization across a wide range of smoothings. It\nsignificantly mitigates the training-inference mismatch, enhancing the\nnaturalness of synthetic output even when the acoustic model produces overly\nsmoothed features. Notably, our method is applicable to any vocoder without\nrequiring architectural modifications or dependencies on specific acoustic\nmodels. The experimental results validate the superiority of our vocoder over\nconventional methods, achieving 11.99% and 12.05% improvements in mean opinion\nscores when integrated with Tacotron 2 and FastSpeech 2 TTS acoustic models,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jeongmin Liu"
                    },
                    {
                        "name": "Eunwoo Song"
                    }
                ],
                "author_detail": {
                    "name": "Eunwoo Song"
                },
                "author": "Eunwoo Song",
                "arxiv_comment": "4 pages, 4 figures, for demo samples, see\n  https://sytronik.github.io/demos/voc_smth_aug/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00084v2",
                "updated": "2024-09-04T08:22:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    8,
                    22,
                    28,
                    2,
                    248,
                    0
                ],
                "published": "2024-08-25T14:50:47Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    14,
                    50,
                    47,
                    6,
                    238,
                    0
                ],
                "title": "Vision-Language and Large Language Model Performance in\n  Gastroenterology: GPT, Claude, Llama, Phi, Mistral, Gemma, and Quantized\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language and Large Language Model Performance in\n  Gastroenterology: GPT, Claude, Llama, Phi, Mistral, Gemma, and Quantized\n  Models"
                },
                "summary": "Background and Aims: This study evaluates the medical reasoning performance\nof large language models (LLMs) and vision language models (VLMs) in\ngastroenterology.\n  Methods: We used 300 gastroenterology board exam-style multiple-choice\nquestions, 138 of which contain images to systematically assess the impact of\nmodel configurations and parameters and prompt engineering strategies utilizing\nGPT-3.5. Next, we assessed the performance of proprietary and open-source LLMs\n(versions), including GPT (3.5, 4, 4o, 4omini), Claude (3, 3.5), Gemini (1.0),\nMistral, Llama (2, 3, 3.1), Mixtral, and Phi (3), across different interfaces\n(web and API), computing environments (cloud and local), and model precisions\n(with and without quantization). Finally, we assessed accuracy using a\nsemiautomated pipeline.\n  Results: Among the proprietary models, GPT-4o (73.7%) and Claude3.5-Sonnet\n(74.0%) achieved the highest accuracy, outperforming the top open-source\nmodels: Llama3.1-405b (64%), Llama3.1-70b (58.3%), and Mixtral-8x7b (54.3%).\nAmong the quantized open-source models, the 6-bit quantized Phi3-14b (48.7%)\nperformed best. The scores of the quantized models were comparable to those of\nthe full-precision models Llama2-7b, Llama2--13b, and Gemma2-9b. Notably, VLM\nperformance on image-containing questions did not improve when the images were\nprovided and worsened when LLM-generated captions were provided. In contrast, a\n10% increase in accuracy was observed when images were accompanied by\nhuman-crafted image descriptions.\n  Conclusion: In conclusion, while LLMs exhibit robust zero-shot performance in\nmedical reasoning, the integration of visual data remains a challenge for VLMs.\nEffective deployment involves carefully determining optimal model\nconfigurations, encouraging users to consider either the high performance of\nproprietary models or the flexible adaptability of open-source models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background and Aims: This study evaluates the medical reasoning performance\nof large language models (LLMs) and vision language models (VLMs) in\ngastroenterology.\n  Methods: We used 300 gastroenterology board exam-style multiple-choice\nquestions, 138 of which contain images to systematically assess the impact of\nmodel configurations and parameters and prompt engineering strategies utilizing\nGPT-3.5. Next, we assessed the performance of proprietary and open-source LLMs\n(versions), including GPT (3.5, 4, 4o, 4omini), Claude (3, 3.5), Gemini (1.0),\nMistral, Llama (2, 3, 3.1), Mixtral, and Phi (3), across different interfaces\n(web and API), computing environments (cloud and local), and model precisions\n(with and without quantization). Finally, we assessed accuracy using a\nsemiautomated pipeline.\n  Results: Among the proprietary models, GPT-4o (73.7%) and Claude3.5-Sonnet\n(74.0%) achieved the highest accuracy, outperforming the top open-source\nmodels: Llama3.1-405b (64%), Llama3.1-70b (58.3%), and Mixtral-8x7b (54.3%).\nAmong the quantized open-source models, the 6-bit quantized Phi3-14b (48.7%)\nperformed best. The scores of the quantized models were comparable to those of\nthe full-precision models Llama2-7b, Llama2--13b, and Gemma2-9b. Notably, VLM\nperformance on image-containing questions did not improve when the images were\nprovided and worsened when LLM-generated captions were provided. In contrast, a\n10% increase in accuracy was observed when images were accompanied by\nhuman-crafted image descriptions.\n  Conclusion: In conclusion, while LLMs exhibit robust zero-shot performance in\nmedical reasoning, the integration of visual data remains a challenge for VLMs.\nEffective deployment involves carefully determining optimal model\nconfigurations, encouraging users to consider either the high performance of\nproprietary models or the flexible adaptability of open-source models."
                },
                "authors": [
                    {
                        "name": "Seyed Amir Ahmad Safavi-Naini"
                    },
                    {
                        "name": "Shuhaib Ali"
                    },
                    {
                        "name": "Omer Shahab"
                    },
                    {
                        "name": "Zahra Shahhoseini"
                    },
                    {
                        "name": "Thomas Savage"
                    },
                    {
                        "name": "Sara Rafiee"
                    },
                    {
                        "name": "Jamil S Samaan"
                    },
                    {
                        "name": "Reem Al Shabeeb"
                    },
                    {
                        "name": "Farah Ladak"
                    },
                    {
                        "name": "Jamie O Yang"
                    },
                    {
                        "name": "Juan Echavarria"
                    },
                    {
                        "name": "Sumbal Babar"
                    },
                    {
                        "name": "Aasma Shaukat"
                    },
                    {
                        "name": "Samuel Margolis"
                    },
                    {
                        "name": "Nicholas P Tatonetti"
                    },
                    {
                        "name": "Girish Nadkarni"
                    },
                    {
                        "name": "Bara El Kurdi"
                    },
                    {
                        "name": "Ali Soroush"
                    }
                ],
                "author_detail": {
                    "name": "Ali Soroush"
                },
                "author": "Ali Soroush",
                "arxiv_comment": "Manuscript Pages: 34, Figures: 7, Tables: 2, Supplementary File\n  Pages: 35, Data Transparency Statement: Code is available at:\n  https://github.com/Sdamirsa/LLM-VLM-in-Gastroenterology . Study data from\n  American College of Gastroenterology (ACG) are restricted and available upon\n  request with ACG permission. Correction: updated abstract considering\n  Llama3.1 results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92C50, 68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00426v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00426v2",
                "updated": "2024-09-04T08:21:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    8,
                    21,
                    48,
                    2,
                    248,
                    0
                ],
                "published": "2024-08-31T11:59:42Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    11,
                    59,
                    42,
                    5,
                    244,
                    0
                ],
                "title": "Is Difficulty Calibration All We Need? Towards More Practical Membership\n  Inference Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Difficulty Calibration All We Need? Towards More Practical Membership\n  Inference Attacks"
                },
                "summary": "The vulnerability of machine learning models to Membership Inference Attacks\n(MIAs) has garnered considerable attention in recent years. These attacks\ndetermine whether a data sample belongs to the model's training set or not.\nRecent research has focused on reference-based attacks, which leverage\ndifficulty calibration with independently trained reference models. While\nempirical studies have demonstrated its effectiveness, there is a notable gap\nin our understanding of the circumstances under which it succeeds or fails. In\nthis paper, we take a further step towards a deeper understanding of the role\nof difficulty calibration. Our observations reveal inherent limitations in\ncalibration methods, leading to the misclassification of non-members and\nsuboptimal performance, particularly on high-loss samples. We further identify\nthat these errors stem from an imperfect sampling of the potential distribution\nand a strong dependence of membership scores on the model parameters. By\nshedding light on these issues, we propose RAPID: a query-efficient and\ncomputation-efficient MIA that directly \\textbf{R}e-lever\\textbf{A}ges the\noriginal membershi\\textbf{P} scores to m\\textbf{I}tigate the errors in\n\\textbf{D}ifficulty calibration. Our experimental results, spanning 9 datasets\nand 5 model architectures, demonstrate that RAPID outperforms previous\nstate-of-the-art attacks (e.g., LiRA and Canary offline) across different\nmetrics while remaining computationally efficient. Our observations and\nanalysis challenge the current de facto paradigm of difficulty calibration in\nhigh-precision inference, encouraging greater attention to the persistent risks\nposed by MIAs in more practical scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vulnerability of machine learning models to Membership Inference Attacks\n(MIAs) has garnered considerable attention in recent years. These attacks\ndetermine whether a data sample belongs to the model's training set or not.\nRecent research has focused on reference-based attacks, which leverage\ndifficulty calibration with independently trained reference models. While\nempirical studies have demonstrated its effectiveness, there is a notable gap\nin our understanding of the circumstances under which it succeeds or fails. In\nthis paper, we take a further step towards a deeper understanding of the role\nof difficulty calibration. Our observations reveal inherent limitations in\ncalibration methods, leading to the misclassification of non-members and\nsuboptimal performance, particularly on high-loss samples. We further identify\nthat these errors stem from an imperfect sampling of the potential distribution\nand a strong dependence of membership scores on the model parameters. By\nshedding light on these issues, we propose RAPID: a query-efficient and\ncomputation-efficient MIA that directly \\textbf{R}e-lever\\textbf{A}ges the\noriginal membershi\\textbf{P} scores to m\\textbf{I}tigate the errors in\n\\textbf{D}ifficulty calibration. Our experimental results, spanning 9 datasets\nand 5 model architectures, demonstrate that RAPID outperforms previous\nstate-of-the-art attacks (e.g., LiRA and Canary offline) across different\nmetrics while remaining computationally efficient. Our observations and\nanalysis challenge the current de facto paradigm of difficulty calibration in\nhigh-precision inference, encouraging greater attention to the persistent risks\nposed by MIAs in more practical scenarios."
                },
                "authors": [
                    {
                        "name": "Yu He"
                    },
                    {
                        "name": "Boheng Li"
                    },
                    {
                        "name": "Yao Wang"
                    },
                    {
                        "name": "Mengda Yang"
                    },
                    {
                        "name": "Juan Wang"
                    },
                    {
                        "name": "Hongxin Hu"
                    },
                    {
                        "name": "Xingyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xingyu Zhao"
                },
                "author": "Xingyu Zhao",
                "arxiv_comment": "Accepted by ACM CCS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00426v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00426v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10093v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10093v2",
                "updated": "2024-09-04T08:20:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    8,
                    20,
                    40,
                    2,
                    248,
                    0
                ],
                "published": "2024-06-14T14:49:12Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    14,
                    49,
                    12,
                    4,
                    166,
                    0
                ],
                "title": "BiKC: Keypose-Conditioned Consistency Policy for Bimanual Robotic\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BiKC: Keypose-Conditioned Consistency Policy for Bimanual Robotic\n  Manipulation"
                },
                "summary": "Bimanual manipulation tasks typically involve multiple stages which require\nefficient interactions between two arms, posing step-wise and stage-wise\nchallenges for imitation learning systems. Specifically, failure and delay of\none step will broadcast through time, hinder success and efficiency of each\nsub-stage task, and thereby overall task performance. Although recent works\nhave made strides in addressing certain challenges, few approaches explicitly\nconsider the multi-stage nature of bimanual tasks while simultaneously\nemphasizing the importance of inference speed. In this paper, we introduce a\nnovel keypose-conditioned consistency policy tailored for bimanual\nmanipulation. It is a hierarchical imitation learning framework that consists\nof a high-level keypose predictor and a low-level trajectory generator. The\npredicted keyposes provide guidance for trajectory generation and also mark the\ncompletion of one sub-stage task. The trajectory generator is designed as a\nconsistency model trained from scratch without distillation, which generates\naction sequences conditioning on current observations and predicted keyposes\nwith fast inference speed. Simulated and real-world experimental results\ndemonstrate that the proposed approach surpasses baseline methods in terms of\nsuccess rate and operational efficiency. Codes are available at\nhttps://github.com/ManUtdMoon/BiKC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bimanual manipulation tasks typically involve multiple stages which require\nefficient interactions between two arms, posing step-wise and stage-wise\nchallenges for imitation learning systems. Specifically, failure and delay of\none step will broadcast through time, hinder success and efficiency of each\nsub-stage task, and thereby overall task performance. Although recent works\nhave made strides in addressing certain challenges, few approaches explicitly\nconsider the multi-stage nature of bimanual tasks while simultaneously\nemphasizing the importance of inference speed. In this paper, we introduce a\nnovel keypose-conditioned consistency policy tailored for bimanual\nmanipulation. It is a hierarchical imitation learning framework that consists\nof a high-level keypose predictor and a low-level trajectory generator. The\npredicted keyposes provide guidance for trajectory generation and also mark the\ncompletion of one sub-stage task. The trajectory generator is designed as a\nconsistency model trained from scratch without distillation, which generates\naction sequences conditioning on current observations and predicted keyposes\nwith fast inference speed. Simulated and real-world experimental results\ndemonstrate that the proposed approach surpasses baseline methods in terms of\nsuccess rate and operational efficiency. Codes are available at\nhttps://github.com/ManUtdMoon/BiKC."
                },
                "authors": [
                    {
                        "name": "Dongjie Yu"
                    },
                    {
                        "name": "Hang Xu"
                    },
                    {
                        "name": "Yizhou Chen"
                    },
                    {
                        "name": "Yi Ren"
                    },
                    {
                        "name": "Jia Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jia Pan"
                },
                "author": "Jia Pan",
                "arxiv_comment": "Accepted by The 16th International Workshop on the Algorithmic\n  Foundations of Robotics (WAFR 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10093v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10093v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10305v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10305v2",
                "updated": "2024-09-04T08:16:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    8,
                    16,
                    1,
                    2,
                    248,
                    0
                ],
                "published": "2024-08-19T18:00:01Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    18,
                    0,
                    1,
                    0,
                    232,
                    0
                ],
                "title": "Not-so-little Red Dots: Two massive and dusty starbursts at z~5-7\n  pushing the limits of star formation discovered by JWST in the COSMOS-Web\n  survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not-so-little Red Dots: Two massive and dusty starbursts at z~5-7\n  pushing the limits of star formation discovered by JWST in the COSMOS-Web\n  survey"
                },
                "summary": "We present the properties of two candidate massive\n($M_\\star\\sim10^{11}M_\\odot$) and dusty ($A_{\\rm v}>2.5$ mag) galaxies at\n$z=5-7$ in the first 0.28 deg$^2$ of the COSMOS-Web survey. One object is\nspectroscopically confirmed at $z_{\\rm spec}=5.051$, while the other has a\nrobust $z_{\\rm phot}=6.7\\pm0.3$. Thanks to their extremely red colors\n($F277W-F444W\\sim1.7$ mag), these galaxies satisfy the nominal color-selection\nfor the widely-studied ``little red dot\" (LRD) population with the exception of\ntheir spatially-resolved morphologies. The morphology of our targets allows us\nto conclude that their red continuum is dominated by highly obscured stellar\nemission and not by reddened nuclear activity. Using a variety of SED-fitting\ntools and star formation histories, we estimate the stellar masses to be\n$\\log(M_\\star)=11.32^{+0.07}_{-0.15}$ $M_\\odot$ and\n$\\log(M_\\star)=11.2^{+0.1}_{-0.2}$ $M_\\odot$, respectively, with a red\ncontinuum emission dominated by a recent episode of star formation. We then\ncompare their number density to the halo mass function to infer stellar baryon\nfractions of $\\epsilon_\\star\\sim0.25$ and $\\epsilon_\\star\\sim0.5$. Both are\nsignificantly higher than what is commonly observed in lower-z galaxies or more\ndust-obscured galaxies at similar redshifts. With very bright ultra-high-z\nLyman-Break Galaxies and some non-AGN dominated LRDs, such ``extended\" LRDs\nrepresent another population that may require very efficient star formation at\nearly times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the properties of two candidate massive\n($M_\\star\\sim10^{11}M_\\odot$) and dusty ($A_{\\rm v}>2.5$ mag) galaxies at\n$z=5-7$ in the first 0.28 deg$^2$ of the COSMOS-Web survey. One object is\nspectroscopically confirmed at $z_{\\rm spec}=5.051$, while the other has a\nrobust $z_{\\rm phot}=6.7\\pm0.3$. Thanks to their extremely red colors\n($F277W-F444W\\sim1.7$ mag), these galaxies satisfy the nominal color-selection\nfor the widely-studied ``little red dot\" (LRD) population with the exception of\ntheir spatially-resolved morphologies. The morphology of our targets allows us\nto conclude that their red continuum is dominated by highly obscured stellar\nemission and not by reddened nuclear activity. Using a variety of SED-fitting\ntools and star formation histories, we estimate the stellar masses to be\n$\\log(M_\\star)=11.32^{+0.07}_{-0.15}$ $M_\\odot$ and\n$\\log(M_\\star)=11.2^{+0.1}_{-0.2}$ $M_\\odot$, respectively, with a red\ncontinuum emission dominated by a recent episode of star formation. We then\ncompare their number density to the halo mass function to infer stellar baryon\nfractions of $\\epsilon_\\star\\sim0.25$ and $\\epsilon_\\star\\sim0.5$. Both are\nsignificantly higher than what is commonly observed in lower-z galaxies or more\ndust-obscured galaxies at similar redshifts. With very bright ultra-high-z\nLyman-Break Galaxies and some non-AGN dominated LRDs, such ``extended\" LRDs\nrepresent another population that may require very efficient star formation at\nearly times."
                },
                "authors": [
                    {
                        "name": "Fabrizio Gentile"
                    },
                    {
                        "name": "Caitlin M. Casey"
                    },
                    {
                        "name": "Hollis B. Akins"
                    },
                    {
                        "name": "Maximilien Franco"
                    },
                    {
                        "name": "Jed McKinney"
                    },
                    {
                        "name": "Edward Berman"
                    },
                    {
                        "name": "Olivia R. Cooper"
                    },
                    {
                        "name": "Nicole E. Drakos"
                    },
                    {
                        "name": "Michaela Hirschmann"
                    },
                    {
                        "name": "Arianna S. Long"
                    },
                    {
                        "name": "Georgios Magdis"
                    },
                    {
                        "name": "Anton M. Koekemoer"
                    },
                    {
                        "name": "Vasily Kokorev"
                    },
                    {
                        "name": "Marko Shuntov"
                    },
                    {
                        "name": "Margherita Talia"
                    },
                    {
                        "name": "Natalie Allen"
                    },
                    {
                        "name": "Santosh Harish"
                    },
                    {
                        "name": "Olivier Ilbert"
                    },
                    {
                        "name": "Henry J. McCracken"
                    },
                    {
                        "name": "Jeyhan S. Kartaltepe"
                    },
                    {
                        "name": "Daizhong Liu"
                    },
                    {
                        "name": "Louise Paquereau"
                    },
                    {
                        "name": "Jason Rhodes"
                    },
                    {
                        "name": "Michael R. Rich"
                    },
                    {
                        "name": "Brant Robertson"
                    },
                    {
                        "name": "Sune Toft"
                    },
                    {
                        "name": "Ghassem Gozaliasl"
                    }
                ],
                "author_detail": {
                    "name": "Ghassem Gozaliasl"
                },
                "author": "Ghassem Gozaliasl",
                "arxiv_doi": "10.3847/2041-8213/ad738a",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/2041-8213/ad738a",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.10305v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10305v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 4 figures, 1 table. Accepted for publication in ApJL",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.13595v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.13595v2",
                "updated": "2024-09-04T08:06:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    8,
                    6,
                    27,
                    2,
                    248,
                    0
                ],
                "published": "2023-08-25T18:00:01Z",
                "published_parsed": [
                    2023,
                    8,
                    25,
                    18,
                    0,
                    1,
                    4,
                    237,
                    0
                ],
                "title": "Ubiquitous Late Radio Emission from Tidal Disruption Events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ubiquitous Late Radio Emission from Tidal Disruption Events"
                },
                "summary": "We present radio observations of 23 optically discovered tidal disruption\nevents (TDEs) on timescales of 500-3200 days post discovery. We detect nine new\nTDEs that did not have detectable radio emission at earlier times, indicating a\nlate-time brightening after several hundred (and up to 2300) days; an\nadditional seven TDEs exhibit radio emission whose origin is ambiguous or may\nbe attributed to the host galaxy or an active galactic nucleus. We also report\na new rising component in one TDE previously detected in the radio at 10^3\ndays. While the radio emission in some of the detected TDEs peaked on a\ntimescale 2-4 yr, over half of the sample still show rising emission. The range\nof luminosities for the sample is 10^37-10^39 erg/s, about 2 orders of\nmagnitude below the radio luminosity of the relativistic TDE Sw J1644+57. Our\ndata set indicates 40% of all optical TDEs are detected in radio hundreds to\nthousands of days after discovery, and that this is probably more common than\nearly radio emission peaking at 10^2 days. Using an equipartition analysis, we\nfind evidence for a delayed launch of the radio-emitting outflows, with delay\ntimescales of 500-2000 days, inferred velocities of 0.02-0.15c, and kinetic\nenergies of 10^47-10^49 erg. We rule out off axis relativistic jets as a viable\nexplanation for this population, and conclude delayed outflows are a more\nlikely explanation, possibly from delayed disk formation. We conclude late\nradio emission marks a fairly ubiquitous but heretofore overlooked phase of TDE\nevolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present radio observations of 23 optically discovered tidal disruption\nevents (TDEs) on timescales of 500-3200 days post discovery. We detect nine new\nTDEs that did not have detectable radio emission at earlier times, indicating a\nlate-time brightening after several hundred (and up to 2300) days; an\nadditional seven TDEs exhibit radio emission whose origin is ambiguous or may\nbe attributed to the host galaxy or an active galactic nucleus. We also report\na new rising component in one TDE previously detected in the radio at 10^3\ndays. While the radio emission in some of the detected TDEs peaked on a\ntimescale 2-4 yr, over half of the sample still show rising emission. The range\nof luminosities for the sample is 10^37-10^39 erg/s, about 2 orders of\nmagnitude below the radio luminosity of the relativistic TDE Sw J1644+57. Our\ndata set indicates 40% of all optical TDEs are detected in radio hundreds to\nthousands of days after discovery, and that this is probably more common than\nearly radio emission peaking at 10^2 days. Using an equipartition analysis, we\nfind evidence for a delayed launch of the radio-emitting outflows, with delay\ntimescales of 500-2000 days, inferred velocities of 0.02-0.15c, and kinetic\nenergies of 10^47-10^49 erg. We rule out off axis relativistic jets as a viable\nexplanation for this population, and conclude delayed outflows are a more\nlikely explanation, possibly from delayed disk formation. We conclude late\nradio emission marks a fairly ubiquitous but heretofore overlooked phase of TDE\nevolution."
                },
                "authors": [
                    {
                        "name": "Yvette Cendes"
                    },
                    {
                        "name": "Edo Berger"
                    },
                    {
                        "name": "Kate D. Alexander"
                    },
                    {
                        "name": "Ryan Chornock"
                    },
                    {
                        "name": "Raffaella Margutti"
                    },
                    {
                        "name": "Brian Metzger"
                    },
                    {
                        "name": "Mark H. Wieringa"
                    },
                    {
                        "name": "Michael F. Bietenholz"
                    },
                    {
                        "name": "Aprajita Hajela"
                    },
                    {
                        "name": "Tanmoy Laskar"
                    },
                    {
                        "name": "Michael C. Stroh"
                    },
                    {
                        "name": "Giacomo Terreran"
                    }
                ],
                "author_detail": {
                    "name": "Giacomo Terreran"
                },
                "author": "Giacomo Terreran",
                "arxiv_comment": "30 pages. Published in ApJ",
                "arxiv_journal_ref": "The Astrophysical Journal, Volume 971, Number 2, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.13595v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.13595v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11449v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11449v3",
                "updated": "2024-09-04T08:03:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    8,
                    3,
                    27,
                    2,
                    248,
                    0
                ],
                "published": "2024-05-19T04:58:53Z",
                "published_parsed": [
                    2024,
                    5,
                    19,
                    4,
                    58,
                    53,
                    6,
                    140,
                    0
                ],
                "title": "NetMamba: Efficient Network Traffic Classification via Pre-training\n  Unidirectional Mamba",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NetMamba: Efficient Network Traffic Classification via Pre-training\n  Unidirectional Mamba"
                },
                "summary": "Network traffic classification is a crucial research area aiming to enhance\nservice quality, streamline network management, and bolster cybersecurity. To\naddress the growing complexity of transmission encryption techniques, various\nmachine learning and deep learning methods have been proposed. However,\nexisting approaches face two main challenges. Firstly, they struggle with model\ninefficiency due to the quadratic complexity of the widely used Transformer\narchitecture. Secondly, they suffer from inadequate traffic representation\nbecause of discarding important byte information while retaining unwanted\nbiases. To address these challenges, we propose NetMamba, an efficient\nlinear-time state space model equipped with a comprehensive traffic\nrepresentation scheme. We adopt a specially selected and improved\nunidirectional Mamba architecture for the networking field, instead of the\nTransformer, to address efficiency issues. In addition, we design a traffic\nrepresentation scheme to extract valid information from massive traffic data\nwhile removing biased information. Evaluation experiments on six public\ndatasets encompassing three main classification tasks showcase NetMamba's\nsuperior classification performance compared to state-of-the-art baselines. It\nachieves an accuracy rate of nearly 99% (some over 99%) in all tasks.\nAdditionally, NetMamba demonstrates excellent efficiency, improving inference\nspeed by up to 60 times while maintaining comparably low memory usage.\nFurthermore, NetMamba exhibits superior few-shot learning abilities, achieving\nbetter classification performance with fewer labeled data. To the best of our\nknowledge, NetMamba is the first model to tailor the Mamba architecture for\nnetworking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network traffic classification is a crucial research area aiming to enhance\nservice quality, streamline network management, and bolster cybersecurity. To\naddress the growing complexity of transmission encryption techniques, various\nmachine learning and deep learning methods have been proposed. However,\nexisting approaches face two main challenges. Firstly, they struggle with model\ninefficiency due to the quadratic complexity of the widely used Transformer\narchitecture. Secondly, they suffer from inadequate traffic representation\nbecause of discarding important byte information while retaining unwanted\nbiases. To address these challenges, we propose NetMamba, an efficient\nlinear-time state space model equipped with a comprehensive traffic\nrepresentation scheme. We adopt a specially selected and improved\nunidirectional Mamba architecture for the networking field, instead of the\nTransformer, to address efficiency issues. In addition, we design a traffic\nrepresentation scheme to extract valid information from massive traffic data\nwhile removing biased information. Evaluation experiments on six public\ndatasets encompassing three main classification tasks showcase NetMamba's\nsuperior classification performance compared to state-of-the-art baselines. It\nachieves an accuracy rate of nearly 99% (some over 99%) in all tasks.\nAdditionally, NetMamba demonstrates excellent efficiency, improving inference\nspeed by up to 60 times while maintaining comparably low memory usage.\nFurthermore, NetMamba exhibits superior few-shot learning abilities, achieving\nbetter classification performance with fewer labeled data. To the best of our\nknowledge, NetMamba is the first model to tailor the Mamba architecture for\nnetworking."
                },
                "authors": [
                    {
                        "name": "Tongze Wang"
                    },
                    {
                        "name": "Xiaohui Xie"
                    },
                    {
                        "name": "Wenduo Wang"
                    },
                    {
                        "name": "Chuyi Wang"
                    },
                    {
                        "name": "Youjian Zhao"
                    },
                    {
                        "name": "Yong Cui"
                    }
                ],
                "author_detail": {
                    "name": "Yong Cui"
                },
                "author": "Yong Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11449v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11449v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01833v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01833v2",
                "updated": "2024-09-04T07:50:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    50,
                    40,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-03T12:30:57Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    12,
                    30,
                    57,
                    1,
                    247,
                    0
                ],
                "title": "Growth of nonconvex functionals at strict local minimizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Growth of nonconvex functionals at strict local minimizers"
                },
                "summary": "In this paper, we present new equivalent conditions for the growth of proper\nlower semicontinuous functionals at strict local minimizers. The main\nconditions are a variant of the so-called tilt stability property of local\nminimizers and an analog of the classic Polyak-{\\L}ojasiewicz condition, where\nthe gradient is replaced by linear perturbations. We derive the following\ntilting principle: stability of minimizers under linear perturbations can infer\ntheir stability under nonlinear ones. We show how growth conditions can be used\nto give convergence rates for the proximal point algorithm. Finally, we give an\napplication to elliptic tracking problems, establishing a novel equivalence\nbetween second-order conditions and the sensitivity of solutions with respect\nto uncertainty in data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present new equivalent conditions for the growth of proper\nlower semicontinuous functionals at strict local minimizers. The main\nconditions are a variant of the so-called tilt stability property of local\nminimizers and an analog of the classic Polyak-{\\L}ojasiewicz condition, where\nthe gradient is replaced by linear perturbations. We derive the following\ntilting principle: stability of minimizers under linear perturbations can infer\ntheir stability under nonlinear ones. We show how growth conditions can be used\nto give convergence rates for the proximal point algorithm. Finally, we give an\napplication to elliptic tracking problems, establishing a novel equivalence\nbetween second-order conditions and the sensitivity of solutions with respect\nto uncertainty in data."
                },
                "authors": [
                    {
                        "name": "Alberto Domnguez Corella"
                    },
                    {
                        "name": "Tr Minh L"
                    }
                ],
                "author_detail": {
                    "name": "Tr Minh L"
                },
                "author": "Tr Minh L",
                "arxiv_comment": "24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01833v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01833v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "49J52, 49K40, 90C31, 90C48",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02494v1",
                "updated": "2024-09-04T07:45:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    45,
                    6,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T07:45:06Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    45,
                    6,
                    2,
                    248,
                    0
                ],
                "title": "Plane2Depth: Hierarchical Adaptive Plane Guidance for Monocular Depth\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plane2Depth: Hierarchical Adaptive Plane Guidance for Monocular Depth\n  Estimation"
                },
                "summary": "Monocular depth estimation aims to infer a dense depth map from a single\nimage, which is a fundamental and prevalent task in computer vision. Many\nprevious works have shown impressive depth estimation results through carefully\ndesigned network structures, but they usually ignore the planar information and\ntherefore perform poorly in low-texture areas of indoor scenes. In this paper,\nwe propose Plane2Depth, which adaptively utilizes plane information to improve\ndepth prediction within a hierarchical framework. Specifically, in the proposed\nplane guided depth generator (PGDG), we design a set of plane queries as\nprototypes to softly model planes in the scene and predict per-pixel plane\ncoefficients. Then the predicted plane coefficients can be converted into\nmetric depth values with the pinhole camera model. In the proposed adaptive\nplane query aggregation (APGA) module, we introduce a novel feature interaction\napproach to improve the aggregation of multi-scale plane features in a top-down\nmanner. Extensive experiments show that our method can achieve outstanding\nperformance, especially in low-texture or repetitive areas. Furthermore, under\nthe same backbone network, our method outperforms the state-of-the-art methods\non the NYU-Depth-v2 dataset, achieves competitive results with state-of-the-art\nmethods KITTI dataset and can be generalized to unseen scenes effectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monocular depth estimation aims to infer a dense depth map from a single\nimage, which is a fundamental and prevalent task in computer vision. Many\nprevious works have shown impressive depth estimation results through carefully\ndesigned network structures, but they usually ignore the planar information and\ntherefore perform poorly in low-texture areas of indoor scenes. In this paper,\nwe propose Plane2Depth, which adaptively utilizes plane information to improve\ndepth prediction within a hierarchical framework. Specifically, in the proposed\nplane guided depth generator (PGDG), we design a set of plane queries as\nprototypes to softly model planes in the scene and predict per-pixel plane\ncoefficients. Then the predicted plane coefficients can be converted into\nmetric depth values with the pinhole camera model. In the proposed adaptive\nplane query aggregation (APGA) module, we introduce a novel feature interaction\napproach to improve the aggregation of multi-scale plane features in a top-down\nmanner. Extensive experiments show that our method can achieve outstanding\nperformance, especially in low-texture or repetitive areas. Furthermore, under\nthe same backbone network, our method outperforms the state-of-the-art methods\non the NYU-Depth-v2 dataset, achieves competitive results with state-of-the-art\nmethods KITTI dataset and can be generalized to unseen scenes effectively."
                },
                "authors": [
                    {
                        "name": "Li Liu"
                    },
                    {
                        "name": "Ruijie Zhu"
                    },
                    {
                        "name": "Jiacheng Deng"
                    },
                    {
                        "name": "Ziyang Song"
                    },
                    {
                        "name": "Wenfei Yang"
                    },
                    {
                        "name": "Tianzhu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tianzhu Zhang"
                },
                "author": "Tianzhu Zhang",
                "arxiv_comment": "14 pages, 12 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02486v1",
                "updated": "2024-09-04T07:25:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    25,
                    50,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T07:25:50Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    25,
                    50,
                    2,
                    248,
                    0
                ],
                "title": "Boosting Generalizability towards Zero-Shot Cross-Dataset Single-Image\n  Indoor Depth by Meta-Initialization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Generalizability towards Zero-Shot Cross-Dataset Single-Image\n  Indoor Depth by Meta-Initialization"
                },
                "summary": "Indoor robots rely on depth to perform tasks like navigation or obstacle\ndetection, and single-image depth estimation is widely used to assist\nperception. Most indoor single-image depth prediction focuses less on model\ngeneralizability to unseen datasets, concerned with in-the-wild robustness for\nsystem deployment. This work leverages gradient-based meta-learning to gain\nhigher generalizability on zero-shot cross-dataset inference. Unlike the\nmost-studied meta-learning of image classification associated with explicit\nclass labels, no explicit task boundaries exist for continuous depth values\ntied to highly varying indoor environments regarding object arrangement and\nscene composition. We propose fine-grained task that treats each RGB-D\nmini-batch as a task in our meta-learning formulation. We first show that our\nmethod on limited data induces a much better prior (max 27.8% in RMSE). Then,\nfinetuning on meta-learned initialization consistently outperforms baselines\nwithout the meta approach. Aiming at generalization, we propose zero-shot\ncross-dataset protocols and validate higher generalizability induced by our\nmeta-initialization, as a simple and useful plugin to many existing depth\nestimation methods. The work at the intersection of depth and meta-learning\npotentially drives both research to step closer to practical robotic and\nmachine perception usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indoor robots rely on depth to perform tasks like navigation or obstacle\ndetection, and single-image depth estimation is widely used to assist\nperception. Most indoor single-image depth prediction focuses less on model\ngeneralizability to unseen datasets, concerned with in-the-wild robustness for\nsystem deployment. This work leverages gradient-based meta-learning to gain\nhigher generalizability on zero-shot cross-dataset inference. Unlike the\nmost-studied meta-learning of image classification associated with explicit\nclass labels, no explicit task boundaries exist for continuous depth values\ntied to highly varying indoor environments regarding object arrangement and\nscene composition. We propose fine-grained task that treats each RGB-D\nmini-batch as a task in our meta-learning formulation. We first show that our\nmethod on limited data induces a much better prior (max 27.8% in RMSE). Then,\nfinetuning on meta-learned initialization consistently outperforms baselines\nwithout the meta approach. Aiming at generalization, we propose zero-shot\ncross-dataset protocols and validate higher generalizability induced by our\nmeta-initialization, as a simple and useful plugin to many existing depth\nestimation methods. The work at the intersection of depth and meta-learning\npotentially drives both research to step closer to practical robotic and\nmachine perception usage."
                },
                "authors": [
                    {
                        "name": "Cho-Ying Wu"
                    },
                    {
                        "name": "Yiqi Zhong"
                    },
                    {
                        "name": "Junying Wang"
                    },
                    {
                        "name": "Ulrich Neumann"
                    }
                ],
                "author_detail": {
                    "name": "Ulrich Neumann"
                },
                "author": "Ulrich Neumann",
                "arxiv_comment": "IROS 2024. The version supersedes 2305.07269. arXiv admin note: text\n  overlap with arXiv:2305.07269",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02485v1",
                "updated": "2024-09-04T07:23:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    23,
                    12,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T07:23:12Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    23,
                    12,
                    2,
                    248,
                    0
                ],
                "title": "Adversarial Attacks on Machine Learning-Aided Visualizations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Attacks on Machine Learning-Aided Visualizations"
                },
                "summary": "Research in ML4VIS investigates how to use machine learning (ML) techniques\nto generate visualizations, and the field is rapidly growing with high societal\nimpact. However, as with any computational pipeline that employs ML processes,\nML4VIS approaches are susceptible to a range of ML-specific adversarial\nattacks. These attacks can manipulate visualization generations, causing\nanalysts to be tricked and their judgments to be impaired. Due to a lack of\nsynthesis from both visualization and ML perspectives, this security aspect is\nlargely overlooked by the current ML4VIS literature. To bridge this gap, we\ninvestigate the potential vulnerabilities of ML-aided visualizations from\nadversarial attacks using a holistic lens of both visualization and ML\nperspectives. We first identify the attack surface (i.e., attack entry points)\nthat is unique in ML-aided visualizations. We then exemplify five different\nadversarial attacks. These examples highlight the range of possible attacks\nwhen considering the attack surface and multiple different adversary\ncapabilities. Our results show that adversaries can induce various attacks,\nsuch as creating arbitrary and deceptive visualizations, by systematically\nidentifying input attributes that are influential in ML inferences. Based on\nour observations of the attack surface characteristics and the attack examples,\nwe underline the importance of comprehensive studies of security issues and\ndefense mechanisms as a call of urgency for the ML4VIS community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research in ML4VIS investigates how to use machine learning (ML) techniques\nto generate visualizations, and the field is rapidly growing with high societal\nimpact. However, as with any computational pipeline that employs ML processes,\nML4VIS approaches are susceptible to a range of ML-specific adversarial\nattacks. These attacks can manipulate visualization generations, causing\nanalysts to be tricked and their judgments to be impaired. Due to a lack of\nsynthesis from both visualization and ML perspectives, this security aspect is\nlargely overlooked by the current ML4VIS literature. To bridge this gap, we\ninvestigate the potential vulnerabilities of ML-aided visualizations from\nadversarial attacks using a holistic lens of both visualization and ML\nperspectives. We first identify the attack surface (i.e., attack entry points)\nthat is unique in ML-aided visualizations. We then exemplify five different\nadversarial attacks. These examples highlight the range of possible attacks\nwhen considering the attack surface and multiple different adversary\ncapabilities. Our results show that adversaries can induce various attacks,\nsuch as creating arbitrary and deceptive visualizations, by systematically\nidentifying input attributes that are influential in ML inferences. Based on\nour observations of the attack surface characteristics and the attack examples,\nwe underline the importance of comprehensive studies of security issues and\ndefense mechanisms as a call of urgency for the ML4VIS community."
                },
                "authors": [
                    {
                        "name": "Takanori Fujiwara"
                    },
                    {
                        "name": "Kostiantyn Kucher"
                    },
                    {
                        "name": "Junpeng Wang"
                    },
                    {
                        "name": "Rafael M. Martins"
                    },
                    {
                        "name": "Andreas Kerren"
                    },
                    {
                        "name": "Anders Ynnerman"
                    }
                ],
                "author_detail": {
                    "name": "Anders Ynnerman"
                },
                "author": "Anders Ynnerman",
                "arxiv_comment": "This is the author's version of the article that has been accepted by\n  the Journal of Visualization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02474v1",
                "updated": "2024-09-04T06:46:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    6,
                    46,
                    31,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T06:46:31Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    6,
                    46,
                    31,
                    2,
                    248,
                    0
                ],
                "title": "A Comparative Study on Large Language Models for Log Parsing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparative Study on Large Language Models for Log Parsing"
                },
                "summary": "Background: Log messages provide valuable information about the status of\nsoftware systems. This information is provided in an unstructured fashion and\nautomated approaches are applied to extract relevant parameters. To ease this\nprocess, log parsing can be applied, which transforms log messages into\nstructured log templates. Recent advances in language models have led to\nseveral studies that apply ChatGPT to the task of log parsing with promising\nresults. However, the performance of other state-of-the-art large language\nmodels (LLMs) on the log parsing task remains unclear.\n  Aims: In this study, we investigate the current capability of\nstate-of-the-art LLMs to perform log parsing.\n  Method: We select six recent LLMs, including both paid proprietary (GPT-3.5,\nClaude 2.1) and four free-to-use open models, and compare their performance on\nsystem logs obtained from a selection of mature open-source projects. We design\ntwo different prompting approaches and apply the LLMs on 1, 354 log templates\nacross 16 different projects. We evaluate their effectiveness, in the number of\ncorrectly identified templates, and the syntactic similarity between the\ngenerated templates and the ground truth.\n  Results: We found that free-to-use models are able to compete with paid\nmodels, with CodeLlama extracting 10% more log templates correctly than\nGPT-3.5. Moreover, we provide qualitative insights into the usability of\nlanguage models (e.g., how easy it is to use their responses).\n  Conclusions: Our results reveal that some of the smaller, free-to-use LLMs\ncan considerably assist log parsing compared to their paid proprietary\ncompetitors, especially code-specialized models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Log messages provide valuable information about the status of\nsoftware systems. This information is provided in an unstructured fashion and\nautomated approaches are applied to extract relevant parameters. To ease this\nprocess, log parsing can be applied, which transforms log messages into\nstructured log templates. Recent advances in language models have led to\nseveral studies that apply ChatGPT to the task of log parsing with promising\nresults. However, the performance of other state-of-the-art large language\nmodels (LLMs) on the log parsing task remains unclear.\n  Aims: In this study, we investigate the current capability of\nstate-of-the-art LLMs to perform log parsing.\n  Method: We select six recent LLMs, including both paid proprietary (GPT-3.5,\nClaude 2.1) and four free-to-use open models, and compare their performance on\nsystem logs obtained from a selection of mature open-source projects. We design\ntwo different prompting approaches and apply the LLMs on 1, 354 log templates\nacross 16 different projects. We evaluate their effectiveness, in the number of\ncorrectly identified templates, and the syntactic similarity between the\ngenerated templates and the ground truth.\n  Results: We found that free-to-use models are able to compete with paid\nmodels, with CodeLlama extracting 10% more log templates correctly than\nGPT-3.5. Moreover, we provide qualitative insights into the usability of\nlanguage models (e.g., how easy it is to use their responses).\n  Conclusions: Our results reveal that some of the smaller, free-to-use LLMs\ncan considerably assist log parsing compared to their paid proprietary\ncompetitors, especially code-specialized models."
                },
                "authors": [
                    {
                        "name": "Merve Astekin"
                    },
                    {
                        "name": "Max Hort"
                    },
                    {
                        "name": "Leon Moonen"
                    }
                ],
                "author_detail": {
                    "name": "Leon Moonen"
                },
                "author": "Leon Moonen",
                "arxiv_doi": "10.1145/3674805.3686684",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3674805.3686684",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication in the 18th ACM/IEEE International Symposium\n  on Empirical Software Engineering and Measurement (ESEM '24)",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15796v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15796v2",
                "updated": "2024-09-04T06:36:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    6,
                    36,
                    22,
                    2,
                    248,
                    0
                ],
                "published": "2024-08-28T13:42:28Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    42,
                    28,
                    2,
                    241,
                    0
                ],
                "title": "Evaluating Named Entity Recognition Using Few-Shot Prompting with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Named Entity Recognition Using Few-Shot Prompting with Large\n  Language Models"
                },
                "summary": "This paper evaluates Few-Shot Prompting with Large Language Models for Named\nEntity Recognition (NER). Traditional NER systems rely on extensive labeled\ndatasets, which are costly and time-consuming to obtain. Few-Shot Prompting or\nin-context learning enables models to recognize entities with minimal examples.\nWe assess state-of-the-art models like GPT-4 in NER tasks, comparing their\nfew-shot performance to fully supervised benchmarks. Results show that while\nthere is a performance gap, large models excel in adapting to new entity types\nand domains with very limited data. We also explore the effects of prompt\nengineering, guided output format and context length on performance. This study\nunderscores Few-Shot Learning's potential to reduce the need for large labeled\ndatasets, enhancing NER scalability and accessibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper evaluates Few-Shot Prompting with Large Language Models for Named\nEntity Recognition (NER). Traditional NER systems rely on extensive labeled\ndatasets, which are costly and time-consuming to obtain. Few-Shot Prompting or\nin-context learning enables models to recognize entities with minimal examples.\nWe assess state-of-the-art models like GPT-4 in NER tasks, comparing their\nfew-shot performance to fully supervised benchmarks. Results show that while\nthere is a performance gap, large models excel in adapting to new entity types\nand domains with very limited data. We also explore the effects of prompt\nengineering, guided output format and context length on performance. This study\nunderscores Few-Shot Learning's potential to reduce the need for large labeled\ndatasets, enhancing NER scalability and accessibility."
                },
                "authors": [
                    {
                        "name": "Hdi Zeghidi"
                    },
                    {
                        "name": "Ludovic Moncla"
                    }
                ],
                "author_detail": {
                    "name": "Ludovic Moncla"
                },
                "author": "Ludovic Moncla",
                "arxiv_comment": "Github repo: https://github.com/GEODE-project/ner-llm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15796v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15796v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02465v1",
                "updated": "2024-09-04T06:28:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    6,
                    28,
                    22,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T06:28:22Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    6,
                    28,
                    22,
                    2,
                    248,
                    0
                ],
                "title": "DetectiveQA: Evaluating Long-Context Reasoning on Detective Novels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DetectiveQA: Evaluating Long-Context Reasoning on Detective Novels"
                },
                "summary": "With the rapid advancement of Large Language Models (LLMs), long-context\ninformation understanding and processing have become a hot topic in academia\nand industry. However, benchmarks for evaluating the ability of LLMs to handle\nlong-context information do not seem to have kept pace with the development of\nLLMs. Despite the emergence of various long-context evaluation benchmarks, the\ntypes of capability assessed are still limited, without new capability\ndimensions. In this paper, we introduce DetectiveQA, a narrative reasoning\nbenchmark featured with an average context length of over 100K tokens.\nDetectiveQA focuses on evaluating the long-context reasoning ability of LLMs,\nwhich not only requires a full understanding of context but also requires\nextracting important evidences from the context and reasoning according to\nextracted evidences to answer the given questions. This is a new dimension of\ncapability evaluation, which is more in line with the current intelligence\nlevel of LLMs. We use detective novels as data sources, which naturally have\nvarious reasoning elements. Finally, we manually annotated 600 questions in\nChinese and then also provided an English edition of the context information\nand questions. We evaluate many long-context LLMs on DetectiveQA, including\ncommercial and open-sourced models, and the results indicate that existing\nlong-context LLMs still require significant advancements to effectively process\ntrue long-context dependency questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of Large Language Models (LLMs), long-context\ninformation understanding and processing have become a hot topic in academia\nand industry. However, benchmarks for evaluating the ability of LLMs to handle\nlong-context information do not seem to have kept pace with the development of\nLLMs. Despite the emergence of various long-context evaluation benchmarks, the\ntypes of capability assessed are still limited, without new capability\ndimensions. In this paper, we introduce DetectiveQA, a narrative reasoning\nbenchmark featured with an average context length of over 100K tokens.\nDetectiveQA focuses on evaluating the long-context reasoning ability of LLMs,\nwhich not only requires a full understanding of context but also requires\nextracting important evidences from the context and reasoning according to\nextracted evidences to answer the given questions. This is a new dimension of\ncapability evaluation, which is more in line with the current intelligence\nlevel of LLMs. We use detective novels as data sources, which naturally have\nvarious reasoning elements. Finally, we manually annotated 600 questions in\nChinese and then also provided an English edition of the context information\nand questions. We evaluate many long-context LLMs on DetectiveQA, including\ncommercial and open-sourced models, and the results indicate that existing\nlong-context LLMs still require significant advancements to effectively process\ntrue long-context dependency questions."
                },
                "authors": [
                    {
                        "name": "Zhe Xu"
                    },
                    {
                        "name": "Jiasheng Ye"
                    },
                    {
                        "name": "Xiangyang Liu"
                    },
                    {
                        "name": "Tianxiang Sun"
                    },
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00899v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00899v2",
                "updated": "2024-09-04T06:19:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    6,
                    19,
                    8,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-02T02:24:38Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    24,
                    38,
                    0,
                    246,
                    0
                ],
                "title": "MarsCode Agent: AI-native Automated Bug Fixing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MarsCode Agent: AI-native Automated Bug Fixing"
                },
                "summary": "Recent advances in large language models (LLMs) have shown significant\npotential to automate various software development tasks, including code\ncompletion, test generation, and bug fixing. However, the application of LLMs\nfor automated bug fixing remains challenging due to the complexity and\ndiversity of real-world software systems. In this paper, we introduce MarsCode\nAgent, a novel framework that leverages LLMs to automatically identify and\nrepair bugs in software code. MarsCode Agent combines the power of LLMs with\nadvanced code analysis techniques to accurately localize faults and generate\npatches. Our approach follows a systematic process of planning, bug\nreproduction, fault localization, candidate patch generation, and validation to\nensure high-quality bug fixes. We evaluated MarsCode Agent on SWE-bench, a\ncomprehensive benchmark of real-world software projects, and our results show\nthat MarsCode Agent achieves a high success rate in bug fixing compared to most\nof the existing automated approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have shown significant\npotential to automate various software development tasks, including code\ncompletion, test generation, and bug fixing. However, the application of LLMs\nfor automated bug fixing remains challenging due to the complexity and\ndiversity of real-world software systems. In this paper, we introduce MarsCode\nAgent, a novel framework that leverages LLMs to automatically identify and\nrepair bugs in software code. MarsCode Agent combines the power of LLMs with\nadvanced code analysis techniques to accurately localize faults and generate\npatches. Our approach follows a systematic process of planning, bug\nreproduction, fault localization, candidate patch generation, and validation to\nensure high-quality bug fixes. We evaluated MarsCode Agent on SWE-bench, a\ncomprehensive benchmark of real-world software projects, and our results show\nthat MarsCode Agent achieves a high success rate in bug fixing compared to most\nof the existing automated approaches."
                },
                "authors": [
                    {
                        "name": "Yizhou Liu"
                    },
                    {
                        "name": "Pengfei Gao"
                    },
                    {
                        "name": "Xinchen Wang"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Yexuan Shi"
                    },
                    {
                        "name": "Zhao Zhang"
                    },
                    {
                        "name": "Chao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Chao Peng"
                },
                "author": "Chao Peng",
                "arxiv_comment": "Yizhou Liu and Pengfei Gao contributed equally and the order is\n  determined by rolling the dice. Chao Peng is the corresponding author",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00899v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00899v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10155v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10155v3",
                "updated": "2024-09-04T06:10:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    6,
                    10,
                    47,
                    2,
                    248,
                    0
                ],
                "published": "2024-04-15T22:02:58Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    22,
                    2,
                    58,
                    0,
                    106,
                    0
                ],
                "title": "The Fault in our Stars: Quality Assessment of Code Generation Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Fault in our Stars: Quality Assessment of Code Generation Benchmarks"
                },
                "summary": "Large Language Models (LLMs) are gaining popularity among software engineers.\nA crucial aspect of developing effective code generation LLMs is to evaluate\nthese models using a robust benchmark. Evaluation benchmarks with quality\nissues can provide a false sense of performance. In this work, we conduct the\nfirst-of-its-kind study of the quality of prompts within benchmarks used to\ncompare the performance of different code generation models. To conduct this\nstudy, we analyzed 3,566 prompts from 9 code generation benchmarks to identify\nquality issues in them. We also investigated whether fixing the identified\nquality issues in the benchmarks' prompts affects a model's performance. We\nalso studied memorization issues of the evaluation dataset, which can put into\nquestion a benchmark's trustworthiness. We found that code generation\nevaluation benchmarks mainly focused on Python and coding exercises and had\nvery limited contextual dependencies to challenge the model. These datasets and\nthe developers' prompts suffer from quality issues like spelling and\ngrammatical errors, unclear sentences to express developers' intent, and not\nusing proper documentation style. Fixing all these issues in the benchmarks can\nlead to a better performance for Python code generation, but not a significant\nimprovement was observed for Java code generation. We also found evidence that\nGPT-3.5-Turbo and CodeGen-2.5 models may have data contamination issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are gaining popularity among software engineers.\nA crucial aspect of developing effective code generation LLMs is to evaluate\nthese models using a robust benchmark. Evaluation benchmarks with quality\nissues can provide a false sense of performance. In this work, we conduct the\nfirst-of-its-kind study of the quality of prompts within benchmarks used to\ncompare the performance of different code generation models. To conduct this\nstudy, we analyzed 3,566 prompts from 9 code generation benchmarks to identify\nquality issues in them. We also investigated whether fixing the identified\nquality issues in the benchmarks' prompts affects a model's performance. We\nalso studied memorization issues of the evaluation dataset, which can put into\nquestion a benchmark's trustworthiness. We found that code generation\nevaluation benchmarks mainly focused on Python and coding exercises and had\nvery limited contextual dependencies to challenge the model. These datasets and\nthe developers' prompts suffer from quality issues like spelling and\ngrammatical errors, unclear sentences to express developers' intent, and not\nusing proper documentation style. Fixing all these issues in the benchmarks can\nlead to a better performance for Python code generation, but not a significant\nimprovement was observed for Java code generation. We also found evidence that\nGPT-3.5-Turbo and CodeGen-2.5 models may have data contamination issues."
                },
                "authors": [
                    {
                        "name": "Mohammed Latif Siddiq"
                    },
                    {
                        "name": "Simantika Dristi"
                    },
                    {
                        "name": "Joy Saha"
                    },
                    {
                        "name": "Joanna C. S. Santos"
                    }
                ],
                "author_detail": {
                    "name": "Joanna C. S. Santos"
                },
                "author": "Joanna C. S. Santos",
                "arxiv_comment": "Accepted at the 24th IEEE International Conference on Source Code\n  Analysis and Manipulation(SCAM 2024) Research Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10155v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10155v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14411v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14411v2",
                "updated": "2024-09-04T06:00:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    6,
                    0,
                    56,
                    2,
                    248,
                    0
                ],
                "published": "2024-05-23T10:32:38Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    10,
                    32,
                    38,
                    3,
                    144,
                    0
                ],
                "title": "Large Language Models for Explainable Decisions in Dynamic Digital Twins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Explainable Decisions in Dynamic Digital Twins"
                },
                "summary": "Dynamic data-driven Digital Twins (DDTs) can enable informed decision-making\nand provide an optimisation platform for the underlying system. By leveraging\nprinciples of Dynamic Data-Driven Applications Systems (DDDAS), DDTs can\nformulate computational modalities for feedback loops, model updates and\ndecision-making, including autonomous ones. However, understanding autonomous\ndecision-making often requires technical and domain-specific knowledge. This\npaper explores using large language models (LLMs) to provide an explainability\nplatform for DDTs, generating natural language explanations of the system's\ndecision-making by leveraging domain-specific knowledge bases. A case study\nfrom smart agriculture is presented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic data-driven Digital Twins (DDTs) can enable informed decision-making\nand provide an optimisation platform for the underlying system. By leveraging\nprinciples of Dynamic Data-Driven Applications Systems (DDDAS), DDTs can\nformulate computational modalities for feedback loops, model updates and\ndecision-making, including autonomous ones. However, understanding autonomous\ndecision-making often requires technical and domain-specific knowledge. This\npaper explores using large language models (LLMs) to provide an explainability\nplatform for DDTs, generating natural language explanations of the system's\ndecision-making by leveraging domain-specific knowledge bases. A case study\nfrom smart agriculture is presented."
                },
                "authors": [
                    {
                        "name": "Nan Zhang"
                    },
                    {
                        "name": "Christian Vergara-Marcillo"
                    },
                    {
                        "name": "Georgios Diamantopoulos"
                    },
                    {
                        "name": "Jingran Shen"
                    },
                    {
                        "name": "Nikos Tziritas"
                    },
                    {
                        "name": "Rami Bahsoon"
                    },
                    {
                        "name": "Georgios Theodoropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Theodoropoulos"
                },
                "author": "Georgios Theodoropoulos",
                "arxiv_comment": "9 pages, 3 figures, accepted by DDDAS2024 -- the 5th International\n  Conference on Dynamic Data Driven Applications Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14411v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14411v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02454v1",
                "updated": "2024-09-04T05:34:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    5,
                    34,
                    27,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T05:34:27Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    5,
                    34,
                    27,
                    2,
                    248,
                    0
                ],
                "title": "Multi-Sources Fusion Learning for Multi-Points NLOS Localization in OFDM\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Sources Fusion Learning for Multi-Points NLOS Localization in OFDM\n  System"
                },
                "summary": "Accurate localization of mobile terminals is a pivotal aspect of integrated\nsensing and communication systems. Traditional fingerprint-based localization\nmethods, which infer coordinates from channel information within pre-set\nrectangular areas, often face challenges due to the heterogeneous distribution\nof fingerprints inherent in non-line-of-sight (NLOS) scenarios, particularly\nwithin orthogonal frequency division multiplexing systems. To overcome this\nlimitation, we develop a novel multi-sources information fusion learning\nframework referred to as the Autosync Multi-Domains NLOS Localization\n(AMDNLoc). Specifically, AMDNLoc employs a two-stage matched filter fused with\na target tracking algorithm and iterative centroid-based clustering to\nautomatically and irregularly segment NLOS regions, ensuring uniform\ndistribution within channel state information across frequency, power, and\ntime-delay domains. Additionally, the framework utilizes a segment-specific\nlinear classifier array, coupled with deep residual network-based feature\nextraction and fusion, to establish the correlation function between\nfingerprint features and coordinates within these regions. Simulation results\nreveal that AMDNLoc achieves an impressive NLOS localization accuracy of 1.46\nmeters on typical wireless artificial intelligence research datasets and\ndemonstrates significant improvements in interpretability, adaptability, and\nscalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate localization of mobile terminals is a pivotal aspect of integrated\nsensing and communication systems. Traditional fingerprint-based localization\nmethods, which infer coordinates from channel information within pre-set\nrectangular areas, often face challenges due to the heterogeneous distribution\nof fingerprints inherent in non-line-of-sight (NLOS) scenarios, particularly\nwithin orthogonal frequency division multiplexing systems. To overcome this\nlimitation, we develop a novel multi-sources information fusion learning\nframework referred to as the Autosync Multi-Domains NLOS Localization\n(AMDNLoc). Specifically, AMDNLoc employs a two-stage matched filter fused with\na target tracking algorithm and iterative centroid-based clustering to\nautomatically and irregularly segment NLOS regions, ensuring uniform\ndistribution within channel state information across frequency, power, and\ntime-delay domains. Additionally, the framework utilizes a segment-specific\nlinear classifier array, coupled with deep residual network-based feature\nextraction and fusion, to establish the correlation function between\nfingerprint features and coordinates within these regions. Simulation results\nreveal that AMDNLoc achieves an impressive NLOS localization accuracy of 1.46\nmeters on typical wireless artificial intelligence research datasets and\ndemonstrates significant improvements in interpretability, adaptability, and\nscalability."
                },
                "authors": [
                    {
                        "name": "Bohao Wang"
                    },
                    {
                        "name": "Zitao Shuai"
                    },
                    {
                        "name": "Chongwen Huang"
                    },
                    {
                        "name": "Qianqian Yang"
                    },
                    {
                        "name": "Zhaohui Yang"
                    },
                    {
                        "name": "Richeng Jin"
                    },
                    {
                        "name": "Ahmed Al Hammadi"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Chau Yuen"
                    },
                    {
                        "name": "Mrouane Debbah"
                    }
                ],
                "author_detail": {
                    "name": "Mrouane Debbah"
                },
                "author": "Mrouane Debbah",
                "arxiv_doi": "10.1109/JSTSP.2024.3453548",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JSTSP.2024.3453548",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 14 figures, accepted by IEEE Journal of Selected Topics in\n  Signal Processing (JSTSP). arXiv admin note: substantial text overlap with\n  arXiv:2401.12538",
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15412v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15412v5",
                "updated": "2024-09-04T05:12:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    5,
                    12,
                    54,
                    2,
                    248,
                    0
                ],
                "published": "2024-03-05T08:29:36Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    8,
                    29,
                    36,
                    1,
                    65,
                    0
                ],
                "title": "Towards Measuring and Modeling \"Culture\" in LLMs: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Measuring and Modeling \"Culture\" in LLMs: A Survey"
                },
                "summary": "We present a survey of more than 90 recent papers that aim to study cultural\nrepresentation and inclusion in large language models (LLMs). We observe that\nnone of the studies explicitly define \"culture, which is a complex,\nmultifaceted concept; instead, they probe the models on some specially designed\ndatasets which represent certain aspects of \"culture\". We call these aspects\nthe proxies of culture, and organize them across two dimensions of demographic\nand semantic proxies. We also categorize the probing methods employed. Our\nanalysis indicates that only certain aspects of ``culture,'' such as values and\nobjectives, have been studied, leaving several other interesting and important\nfacets, especially the multitude of semantic domains (Thompson et al., 2020)\nand aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps\nare the lack of robustness of probing techniques and situated studies on the\nimpact of cultural mis- and under-representation in LLM-based applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a survey of more than 90 recent papers that aim to study cultural\nrepresentation and inclusion in large language models (LLMs). We observe that\nnone of the studies explicitly define \"culture, which is a complex,\nmultifaceted concept; instead, they probe the models on some specially designed\ndatasets which represent certain aspects of \"culture\". We call these aspects\nthe proxies of culture, and organize them across two dimensions of demographic\nand semantic proxies. We also categorize the probing methods employed. Our\nanalysis indicates that only certain aspects of ``culture,'' such as values and\nobjectives, have been studied, leaving several other interesting and important\nfacets, especially the multitude of semantic domains (Thompson et al., 2020)\nand aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps\nare the lack of robustness of probing techniques and situated studies on the\nimpact of cultural mis- and under-representation in LLM-based applications."
                },
                "authors": [
                    {
                        "name": "Muhammad Farid Adilazuarda"
                    },
                    {
                        "name": "Sagnik Mukherjee"
                    },
                    {
                        "name": "Pradhyumna Lavania"
                    },
                    {
                        "name": "Siddhant Singh"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    },
                    {
                        "name": "Jacki O'Neill"
                    },
                    {
                        "name": "Ashutosh Modi"
                    },
                    {
                        "name": "Monojit Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Monojit Choudhury"
                },
                "author": "Monojit Choudhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15412v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15412v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02451v1",
                "updated": "2024-09-04T05:12:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    5,
                    12,
                    15,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T05:12:15Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    5,
                    12,
                    15,
                    2,
                    248,
                    0
                ],
                "title": "Fast, High-Quality and Parameter-Efficient Articulatory Synthesis using\n  Differentiable DSP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast, High-Quality and Parameter-Efficient Articulatory Synthesis using\n  Differentiable DSP"
                },
                "summary": "Articulatory trajectories like electromagnetic articulography (EMA) provide a\nlow-dimensional representation of the vocal tract filter and have been used as\nnatural, grounded features for speech synthesis. Differentiable digital signal\nprocessing (DDSP) is a parameter-efficient framework for audio synthesis.\nTherefore, integrating low-dimensional EMA features with DDSP can significantly\nenhance the computational efficiency of speech synthesis. In this paper, we\npropose a fast, high-quality, and parameter-efficient DDSP articulatory vocoder\nthat can synthesize speech from EMA, F0, and loudness. We incorporate several\ntechniques to solve the harmonics / noise imbalance problem, and add a\nmulti-resolution adversarial loss for better synthesis quality. Our model\nachieves a transcription word error rate (WER) of 6.67% and a mean opinion\nscore (MOS) of 3.74, with an improvement of 1.63% and 0.16 compared to the\nstate-of-the-art (SOTA) baseline. Our DDSP vocoder is 4.9x faster than the\nbaseline on CPU during inference, and can generate speech of comparable quality\nwith only 0.4M parameters, in contrast to the 9M parameters required by the\nSOTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Articulatory trajectories like electromagnetic articulography (EMA) provide a\nlow-dimensional representation of the vocal tract filter and have been used as\nnatural, grounded features for speech synthesis. Differentiable digital signal\nprocessing (DDSP) is a parameter-efficient framework for audio synthesis.\nTherefore, integrating low-dimensional EMA features with DDSP can significantly\nenhance the computational efficiency of speech synthesis. In this paper, we\npropose a fast, high-quality, and parameter-efficient DDSP articulatory vocoder\nthat can synthesize speech from EMA, F0, and loudness. We incorporate several\ntechniques to solve the harmonics / noise imbalance problem, and add a\nmulti-resolution adversarial loss for better synthesis quality. Our model\nachieves a transcription word error rate (WER) of 6.67% and a mean opinion\nscore (MOS) of 3.74, with an improvement of 1.63% and 0.16 compared to the\nstate-of-the-art (SOTA) baseline. Our DDSP vocoder is 4.9x faster than the\nbaseline on CPU during inference, and can generate speech of comparable quality\nwith only 0.4M parameters, in contrast to the 9M parameters required by the\nSOTA."
                },
                "authors": [
                    {
                        "name": "Yisi Liu"
                    },
                    {
                        "name": "Bohan Yu"
                    },
                    {
                        "name": "Drake Lin"
                    },
                    {
                        "name": "Peter Wu"
                    },
                    {
                        "name": "Cheol Jun Cho"
                    },
                    {
                        "name": "Gopala Krishna Anumanchipalli"
                    }
                ],
                "author_detail": {
                    "name": "Gopala Krishna Anumanchipalli"
                },
                "author": "Gopala Krishna Anumanchipalli",
                "arxiv_comment": "accepted for Spoken Language Technology Workshop 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16672v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16672v3",
                "updated": "2024-09-04T05:09:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    5,
                    9,
                    0,
                    2,
                    248,
                    0
                ],
                "published": "2024-08-29T16:21:00Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    21,
                    0,
                    3,
                    242,
                    0
                ],
                "title": "Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction\n  Retriever",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction\n  Retriever"
                },
                "summary": "Multi-vector dense models, such as ColBERT, have proven highly effective in\ninformation retrieval. ColBERT's late interaction scoring approximates the\njoint query-document attention seen in cross-encoders while maintaining\ninference efficiency closer to traditional dense retrieval models, thanks to\nits bi-encoder architecture and recent optimizations in indexing and search. In\nthis paper, we introduce a novel architecture and a training framework to\nsupport long context window and multilingual retrieval. Our new model,\nJina-ColBERT-v2, demonstrates strong performance across a range of English and\nmultilingual retrieval tasks,",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-vector dense models, such as ColBERT, have proven highly effective in\ninformation retrieval. ColBERT's late interaction scoring approximates the\njoint query-document attention seen in cross-encoders while maintaining\ninference efficiency closer to traditional dense retrieval models, thanks to\nits bi-encoder architecture and recent optimizations in indexing and search. In\nthis paper, we introduce a novel architecture and a training framework to\nsupport long context window and multilingual retrieval. Our new model,\nJina-ColBERT-v2, demonstrates strong performance across a range of English and\nmultilingual retrieval tasks,"
                },
                "authors": [
                    {
                        "name": "Rohan Jha"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Michael Gnther"
                    },
                    {
                        "name": "Georgios Mastrapas"
                    },
                    {
                        "name": "Saba Sturua"
                    },
                    {
                        "name": "Isabelle Mohr"
                    },
                    {
                        "name": "Andreas Koukounas"
                    },
                    {
                        "name": "Mohammad Kalim Akram"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Han Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Han Xiao"
                },
                "author": "Han Xiao",
                "arxiv_comment": "8 pages, references at pp7,8; EMNLP workshop submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16672v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16672v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.03520v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.03520v2",
                "updated": "2024-09-04T04:51:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    4,
                    51,
                    36,
                    2,
                    248,
                    0
                ],
                "published": "2023-10-05T13:09:28Z",
                "published_parsed": [
                    2023,
                    10,
                    5,
                    13,
                    9,
                    28,
                    3,
                    278,
                    0
                ],
                "title": "Identification of Gravitational-waves from Extreme Mass Ratio Inspirals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identification of Gravitational-waves from Extreme Mass Ratio Inspirals"
                },
                "summary": "Space-based gravitational wave detectors like TianQin or LISA could observe\nextreme-mass-ratio-inspirals (EMRIs) at millihertz frequencies. The accurate\nidentification of these EMRI signals from the data plays a crucial role in\nenabling in-depth study of astronomy and physics. We aim at the identification\nstage of the data analysis, with the aim to extract key features of the signal\nfrom the data, such as the evolution of the orbital frequency, as well as to\npinpoint the parameter range that can fit the data well for the subsequent\nparameter inference stage. In this manuscript, we demonstrated the\nidentification of EMRI signals without any additional prior information on\nphysical parameters. High-precision measurements of EMRI signals have been\nachieved, using a hierarchical search. It combines the search for physical\nparameters that guide the subsequent parameter inference, and a semi-coherent\nsearch with phenomenological waveforms that reaches precision levels down to\n$10^{-4}$ for the phenomenological waveform parameters $\\omega_{0}$,\n$\\dot{\\omega}_{0}$, and $\\ddot{\\omega}_{0}$. As a result, we obtain measurement\nrelative errors of less than 4% for the mass of the massive black hole, while\nkeeping the relative errors of the other parameters within as small as 0.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Space-based gravitational wave detectors like TianQin or LISA could observe\nextreme-mass-ratio-inspirals (EMRIs) at millihertz frequencies. The accurate\nidentification of these EMRI signals from the data plays a crucial role in\nenabling in-depth study of astronomy and physics. We aim at the identification\nstage of the data analysis, with the aim to extract key features of the signal\nfrom the data, such as the evolution of the orbital frequency, as well as to\npinpoint the parameter range that can fit the data well for the subsequent\nparameter inference stage. In this manuscript, we demonstrated the\nidentification of EMRI signals without any additional prior information on\nphysical parameters. High-precision measurements of EMRI signals have been\nachieved, using a hierarchical search. It combines the search for physical\nparameters that guide the subsequent parameter inference, and a semi-coherent\nsearch with phenomenological waveforms that reaches precision levels down to\n$10^{-4}$ for the phenomenological waveform parameters $\\omega_{0}$,\n$\\dot{\\omega}_{0}$, and $\\ddot{\\omega}_{0}$. As a result, we obtain measurement\nrelative errors of less than 4% for the mass of the massive black hole, while\nkeeping the relative errors of the other parameters within as small as 0.5%."
                },
                "authors": [
                    {
                        "name": "Chang-Qing Ye"
                    },
                    {
                        "name": "Hui-Min Fan"
                    },
                    {
                        "name": "Alejandro Torres-Orjuela"
                    },
                    {
                        "name": "Jian-dong Zhang"
                    },
                    {
                        "name": "Yi-Ming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Yi-Ming Hu"
                },
                "author": "Yi-Ming Hu",
                "arxiv_doi": "10.1103/PhysRevD.109.124034",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.109.124034",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.03520v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.03520v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 7 figures, comments welcom",
                "arxiv_journal_ref": "Physical Review D 109.12 (2024): 124034",
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02443v1",
                "updated": "2024-09-04T04:41:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    4,
                    41,
                    15,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T04:41:15Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    4,
                    41,
                    15,
                    2,
                    248,
                    0
                ],
                "title": "Exploring the applicability of Large Language Models to citation context\n  analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the applicability of Large Language Models to citation context\n  analysis"
                },
                "summary": "Unlike traditional citation analysis -- which assumes that all citations in a\npaper are equivalent -- citation context analysis considers the contextual\ninformation of individual citations. However, citation context analysis\nrequires creating large amounts of data through annotation, which hinders the\nwidespread use of this methodology. This study explored the applicability of\nLarge Language Models (LLMs) -- particularly ChatGPT -- to citation context\nanalysis by comparing LLMs and human annotation results. The results show that\nthe LLMs annotation is as good as or better than the human annotation in terms\nof consistency but poor in terms of predictive performance. Thus, having LLMs\nimmediately replace human annotators in citation context analysis is\ninappropriate. However, the annotation results obtained by LLMs can be used as\nreference information when narrowing the annotation results obtained by\nmultiple human annotators to one, or LLMs can be used as one of the annotators\nwhen it is difficult to prepare sufficient human annotators. This study\nprovides basic findings important for the future development of citation\ncontext analyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlike traditional citation analysis -- which assumes that all citations in a\npaper are equivalent -- citation context analysis considers the contextual\ninformation of individual citations. However, citation context analysis\nrequires creating large amounts of data through annotation, which hinders the\nwidespread use of this methodology. This study explored the applicability of\nLarge Language Models (LLMs) -- particularly ChatGPT -- to citation context\nanalysis by comparing LLMs and human annotation results. The results show that\nthe LLMs annotation is as good as or better than the human annotation in terms\nof consistency but poor in terms of predictive performance. Thus, having LLMs\nimmediately replace human annotators in citation context analysis is\ninappropriate. However, the annotation results obtained by LLMs can be used as\nreference information when narrowing the annotation results obtained by\nmultiple human annotators to one, or LLMs can be used as one of the annotators\nwhen it is difficult to prepare sufficient human annotators. This study\nprovides basic findings important for the future development of citation\ncontext analyses."
                },
                "authors": [
                    {
                        "name": "Kai Nishikawa"
                    },
                    {
                        "name": "Hitoshi Koshiba"
                    }
                ],
                "author_detail": {
                    "name": "Hitoshi Koshiba"
                },
                "author": "Hitoshi Koshiba",
                "arxiv_doi": "10.1007/s11192-024-05142-9",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s11192-024-05142-9",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.02739v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.02739v3",
                "updated": "2024-09-04T04:39:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    4,
                    39,
                    30,
                    2,
                    248,
                    0
                ],
                "published": "2023-01-06T22:35:34Z",
                "published_parsed": [
                    2023,
                    1,
                    6,
                    22,
                    35,
                    34,
                    4,
                    6,
                    0
                ],
                "title": "Rank-transformed subsampling: inference for multiple data splitting and\n  exchangeable p-values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rank-transformed subsampling: inference for multiple data splitting and\n  exchangeable p-values"
                },
                "summary": "Many testing problems are readily amenable to randomised tests such as those\nemploying data splitting. However despite their usefulness in principle,\nrandomised tests have obvious drawbacks. Firstly, two analyses of the same\ndataset may lead to different results. Secondly, the test typically loses power\nbecause it does not fully utilise the entire sample. As a remedy to these\ndrawbacks, we study how to combine the test statistics or p-values resulting\nfrom multiple random realisations such as through random data splits. We\ndevelop rank-transformed subsampling as a general method for delivering large\nsample inference about the combined statistic or p-value under mild\nassumptions. We apply our methodology to a wide range of problems, including\ntesting unimodality in high-dimensional data, testing goodness-of-fit of\nparametric quantile regression models, testing no direct effect in a\nsequentially randomised trial and calibrating cross-fit double machine learning\nconfidence intervals. In contrast to existing p-value aggregation schemes that\ncan be highly conservative, our method enjoys type-I error control that\nasymptotically approaches the nominal level. Moreover, compared to using the\nordinary subsampling, we show that our rank transform can remove the\nfirst-order bias in approximating the null under alternatives and greatly\nimprove power.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many testing problems are readily amenable to randomised tests such as those\nemploying data splitting. However despite their usefulness in principle,\nrandomised tests have obvious drawbacks. Firstly, two analyses of the same\ndataset may lead to different results. Secondly, the test typically loses power\nbecause it does not fully utilise the entire sample. As a remedy to these\ndrawbacks, we study how to combine the test statistics or p-values resulting\nfrom multiple random realisations such as through random data splits. We\ndevelop rank-transformed subsampling as a general method for delivering large\nsample inference about the combined statistic or p-value under mild\nassumptions. We apply our methodology to a wide range of problems, including\ntesting unimodality in high-dimensional data, testing goodness-of-fit of\nparametric quantile regression models, testing no direct effect in a\nsequentially randomised trial and calibrating cross-fit double machine learning\nconfidence intervals. In contrast to existing p-value aggregation schemes that\ncan be highly conservative, our method enjoys type-I error control that\nasymptotically approaches the nominal level. Moreover, compared to using the\nordinary subsampling, we show that our rank transform can remove the\nfirst-order bias in approximating the null under alternatives and greatly\nimprove power."
                },
                "authors": [
                    {
                        "name": "F. Richard Guo"
                    },
                    {
                        "name": "Rajen D. Shah"
                    }
                ],
                "author_detail": {
                    "name": "Rajen D. Shah"
                },
                "author": "Rajen D. Shah",
                "arxiv_doi": "10.1093/jrsssb/qkae091",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/jrsssb/qkae091",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2301.02739v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.02739v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "83 pages; typo correction and bibliography updates",
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02437v1",
                "updated": "2024-09-04T04:27:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    4,
                    27,
                    14,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T04:27:14Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    4,
                    27,
                    14,
                    2,
                    248,
                    0
                ],
                "title": "Fuzzy Logic Control for Indoor Navigation of Mobile Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fuzzy Logic Control for Indoor Navigation of Mobile Robots"
                },
                "summary": "Autonomous mobile robots have many applications in indoor unstructured\nenvironment, wherein optimal movement of the robot is needed. The robot\ntherefore needs to navigate in unknown and dynamic environments. This paper\npresents an implementation of fuzzy logic controller for navigation of mobile\nrobot in an unknown dynamically cluttered environment. Fuzzy logic controller\nis used here as it is capable of making inferences even under uncertainties. It\nhelps in rule generation and decision making process in order to reach the goal\nposition under various situations. Sensor readings from the robot and the\ndesired direction of motion are inputs to the fuzz logic controllers and the\nacceleration of the respective wheels are the output of the controller. Hence,\nthe mobile robot avoids obstacles and reaches the goal position. Keywords:\nFuzzy Logic Controller, Membership Functions, Takagi-Sugeno-Kang FIS, Centroid\nDefuzzification",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous mobile robots have many applications in indoor unstructured\nenvironment, wherein optimal movement of the robot is needed. The robot\ntherefore needs to navigate in unknown and dynamic environments. This paper\npresents an implementation of fuzzy logic controller for navigation of mobile\nrobot in an unknown dynamically cluttered environment. Fuzzy logic controller\nis used here as it is capable of making inferences even under uncertainties. It\nhelps in rule generation and decision making process in order to reach the goal\nposition under various situations. Sensor readings from the robot and the\ndesired direction of motion are inputs to the fuzz logic controllers and the\nacceleration of the respective wheels are the output of the controller. Hence,\nthe mobile robot avoids obstacles and reaches the goal position. Keywords:\nFuzzy Logic Controller, Membership Functions, Takagi-Sugeno-Kang FIS, Centroid\nDefuzzification"
                },
                "authors": [
                    {
                        "name": "Akshay Kumar"
                    },
                    {
                        "name": "Ashwin Sahasrabudhe"
                    },
                    {
                        "name": "Sanjuksha Nirgude"
                    }
                ],
                "author_detail": {
                    "name": "Sanjuksha Nirgude"
                },
                "author": "Sanjuksha Nirgude",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2407.13989v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13989v3",
                "updated": "2024-09-04T17:52:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    52,
                    37,
                    2,
                    248,
                    0
                ],
                "published": "2024-07-19T02:34:10Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    2,
                    34,
                    10,
                    4,
                    201,
                    0
                ],
                "title": "Enhancing Graph Neural Networks with Limited Labeled Data by Actively\n  Distilling Knowledge from Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Graph Neural Networks with Limited Labeled Data by Actively\n  Distilling Knowledge from Large Language Models"
                },
                "summary": "Graphs are pervasive in the real-world, such as social network analysis,\nbioinformatics, and knowledge graphs. Graph neural networks (GNNs) have great\nability in node classification, a fundamental task on graphs. Unfortunately,\nconventional GNNs still face challenges in scenarios with few labeled nodes,\ndespite the prevalence of few-shot node classification tasks in real-world\napplications. To address this challenge, various approaches have been proposed,\nincluding graph meta-learning, transfer learning, and methods based on Large\nLanguage Models (LLMs). However, traditional meta-learning and transfer\nlearning methods often require prior knowledge from base classes or fail to\nexploit the potential advantages of unlabeled nodes. Meanwhile, LLM-based\nmethods may overlook the zero-shot capabilities of LLMs and rely heavily on the\nquality of generated contexts. In this paper, we propose a novel approach that\nintegrates LLMs and GNNs, leveraging the zero-shot inference and reasoning\ncapabilities of LLMs and employing a Graph-LLM-based active learning paradigm\nto enhance GNNs' performance. Extensive experiments demonstrate the\neffectiveness of our model in improving node classification accuracy with\nconsiderably limited labeled data, surpassing state-of-the-art baselines by\nsignificant margins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphs are pervasive in the real-world, such as social network analysis,\nbioinformatics, and knowledge graphs. Graph neural networks (GNNs) have great\nability in node classification, a fundamental task on graphs. Unfortunately,\nconventional GNNs still face challenges in scenarios with few labeled nodes,\ndespite the prevalence of few-shot node classification tasks in real-world\napplications. To address this challenge, various approaches have been proposed,\nincluding graph meta-learning, transfer learning, and methods based on Large\nLanguage Models (LLMs). However, traditional meta-learning and transfer\nlearning methods often require prior knowledge from base classes or fail to\nexploit the potential advantages of unlabeled nodes. Meanwhile, LLM-based\nmethods may overlook the zero-shot capabilities of LLMs and rely heavily on the\nquality of generated contexts. In this paper, we propose a novel approach that\nintegrates LLMs and GNNs, leveraging the zero-shot inference and reasoning\ncapabilities of LLMs and employing a Graph-LLM-based active learning paradigm\nto enhance GNNs' performance. Extensive experiments demonstrate the\neffectiveness of our model in improving node classification accuracy with\nconsiderably limited labeled data, surpassing state-of-the-art baselines by\nsignificant margins."
                },
                "authors": [
                    {
                        "name": "Quan Li"
                    },
                    {
                        "name": "Tianxiang Zhao"
                    },
                    {
                        "name": "Lingwei Chen"
                    },
                    {
                        "name": "Junjie Xu"
                    },
                    {
                        "name": "Suhang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Suhang Wang"
                },
                "author": "Suhang Wang",
                "arxiv_comment": "10 pages, 3 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13989v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13989v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02912v1",
                "updated": "2024-09-04T17:51:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    51,
                    18,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T17:51:18Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    51,
                    18,
                    2,
                    248,
                    0
                ],
                "title": "Design of a Standard-Compliant Real-Time Neural Receiver for 5G NR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design of a Standard-Compliant Real-Time Neural Receiver for 5G NR"
                },
                "summary": "We detail the steps required to deploy a multi-user multiple-input\nmultiple-output (MU-MIMO) neural receiver (NRX) in an actual cellular\ncommunication system. This raises several exciting research challenges,\nincluding the need for real-time inference and compatibility with the 5G NR\nstandard. As the network configuration in a practical setup can change\ndynamically within milliseconds, we propose an adaptive NRX architecture\ncapable of supporting dynamic modulation and coding scheme (MCS) configurations\nwithout the need for any re-training and without additional inference cost. We\noptimize the latency of the neural network (NN) architecture to achieve\ninference times of less than 1ms on an NVIDIA A100 GPU using the TensorRT\ninference library. These latency constraints effectively limit the size of the\nNN and we quantify the resulting signal-to-noise ratio (SNR) degradation as\nless than 0.7 dB when compared to a preliminary non-real-time NRX architecture.\nFinally, we explore the potential for site-specific adaptation of the receiver\nby investigating the required size of the training dataset and the number of\nfine-tuning iterations to optimize the NRX for specific radio environments\nusing a ray tracing-based channel model. The resulting NRX is ready for\ndeployment in a real-time 5G NR system and the source code including the\nTensorRT experiments is available online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We detail the steps required to deploy a multi-user multiple-input\nmultiple-output (MU-MIMO) neural receiver (NRX) in an actual cellular\ncommunication system. This raises several exciting research challenges,\nincluding the need for real-time inference and compatibility with the 5G NR\nstandard. As the network configuration in a practical setup can change\ndynamically within milliseconds, we propose an adaptive NRX architecture\ncapable of supporting dynamic modulation and coding scheme (MCS) configurations\nwithout the need for any re-training and without additional inference cost. We\noptimize the latency of the neural network (NN) architecture to achieve\ninference times of less than 1ms on an NVIDIA A100 GPU using the TensorRT\ninference library. These latency constraints effectively limit the size of the\nNN and we quantify the resulting signal-to-noise ratio (SNR) degradation as\nless than 0.7 dB when compared to a preliminary non-real-time NRX architecture.\nFinally, we explore the potential for site-specific adaptation of the receiver\nby investigating the required size of the training dataset and the number of\nfine-tuning iterations to optimize the NRX for specific radio environments\nusing a ray tracing-based channel model. The resulting NRX is ready for\ndeployment in a real-time 5G NR system and the source code including the\nTensorRT experiments is available online."
                },
                "authors": [
                    {
                        "name": "Reinhard Wiesmayr"
                    },
                    {
                        "name": "Sebastian Cammerer"
                    },
                    {
                        "name": "Fayal At Aoudia"
                    },
                    {
                        "name": "Jakob Hoydis"
                    },
                    {
                        "name": "Jakub Zakrzewski"
                    },
                    {
                        "name": "Alexander Keller"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Keller"
                },
                "author": "Alexander Keller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02897v2",
                "updated": "2024-09-05T03:53:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    3,
                    53,
                    13,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-04T17:41:19Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    41,
                    19,
                    2,
                    248,
                    0
                ],
                "title": "LongCite: Enabling LLMs to Generate Fine-grained Citations in\n  Long-context QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongCite: Enabling LLMs to Generate Fine-grained Citations in\n  Long-context QA"
                },
                "summary": "Though current long-context large language models (LLMs) have demonstrated\nimpressive capacities in answering user questions based on extensive text, the\nlack of citations in their responses makes user verification difficult, leading\nto concerns about their trustworthiness due to their potential hallucinations.\nIn this work, we aim to enable long-context LLMs to generate responses with\nfine-grained sentence-level citations, improving their faithfulness and\nverifiability. We first introduce LongBench-Cite, an automated benchmark for\nassessing current LLMs' performance in Long-Context Question Answering with\nCitations (LQAC), revealing considerable room for improvement. To this end, we\npropose CoF (Coarse to Fine), a novel pipeline that utilizes off-the-shelf LLMs\nto automatically generate long-context QA instances with precise sentence-level\ncitations, and leverage this pipeline to construct LongCite-45k, a large-scale\nSFT dataset for LQAC. Finally, we train LongCite-8B and LongCite-9B using the\nLongCite-45k dataset, successfully enabling their generation of accurate\nresponses and fine-grained sentence-level citations in a single output. The\nevaluation results on LongBench-Cite show that our trained models achieve\nstate-of-the-art citation quality, surpassing advanced proprietary models\nincluding GPT-4o.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Though current long-context large language models (LLMs) have demonstrated\nimpressive capacities in answering user questions based on extensive text, the\nlack of citations in their responses makes user verification difficult, leading\nto concerns about their trustworthiness due to their potential hallucinations.\nIn this work, we aim to enable long-context LLMs to generate responses with\nfine-grained sentence-level citations, improving their faithfulness and\nverifiability. We first introduce LongBench-Cite, an automated benchmark for\nassessing current LLMs' performance in Long-Context Question Answering with\nCitations (LQAC), revealing considerable room for improvement. To this end, we\npropose CoF (Coarse to Fine), a novel pipeline that utilizes off-the-shelf LLMs\nto automatically generate long-context QA instances with precise sentence-level\ncitations, and leverage this pipeline to construct LongCite-45k, a large-scale\nSFT dataset for LQAC. Finally, we train LongCite-8B and LongCite-9B using the\nLongCite-45k dataset, successfully enabling their generation of accurate\nresponses and fine-grained sentence-level citations in a single output. The\nevaluation results on LongBench-Cite show that our trained models achieve\nstate-of-the-art citation quality, surpassing advanced proprietary models\nincluding GPT-4o."
                },
                "authors": [
                    {
                        "name": "Jiajie Zhang"
                    },
                    {
                        "name": "Yushi Bai"
                    },
                    {
                        "name": "Xin Lv"
                    },
                    {
                        "name": "Wanjun Gu"
                    },
                    {
                        "name": "Danqing Liu"
                    },
                    {
                        "name": "Minhao Zou"
                    },
                    {
                        "name": "Shulin Cao"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Ling Feng"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07832v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07832v3",
                "updated": "2024-09-04T17:31:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    31,
                    0,
                    2,
                    248,
                    0
                ],
                "published": "2024-07-31T14:49:35Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    49,
                    35,
                    2,
                    213,
                    0
                ],
                "title": "LADDER: Language Driven Slice Discovery and Error Rectification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LADDER: Language Driven Slice Discovery and Error Rectification"
                },
                "summary": "Error slice discovery associates structured patterns with model errors.\nExisting methods discover error slices by clustering the error-prone samples\nwith similar patterns or assigning discrete attributes to each sample for\npost-hoc analysis. While these methods aim for interpretability and easier\nmitigation through reweighting or rebalancing, they may not capture the full\ncomplexity of error patterns due to incomplete or missing attributes. Contrary\nto the existing approach, this paper utilizes the reasoning capabilities of the\nLarge Language Model (LLM) to analyze complex error patterns and generate\ntestable hypotheses. This paper proposes LADDER: Language Driven slice\nDiscovery and Error Rectification. It first projects the model's representation\ninto a language-aligned feature space (eg CLIP) to preserve semantics in the\noriginal model feature space. This ensures the accurate retrieval of sentences\nthat highlight the model's errors. Next, the LLM utilizes the sentences and\ngenerates hypotheses to discover error slices. Finally, we mitigate the error\nby fine-tuning the classification head by creating a group-balanced dataset\nusing the hypotheses. Our entire method does not require any attribute\nannotation, either explicitly or through external tagging models. We validate\nour method with \\textbf{five} image classification datasets. The code is\navailable (https://github.com/batmanlab/Ladder).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Error slice discovery associates structured patterns with model errors.\nExisting methods discover error slices by clustering the error-prone samples\nwith similar patterns or assigning discrete attributes to each sample for\npost-hoc analysis. While these methods aim for interpretability and easier\nmitigation through reweighting or rebalancing, they may not capture the full\ncomplexity of error patterns due to incomplete or missing attributes. Contrary\nto the existing approach, this paper utilizes the reasoning capabilities of the\nLarge Language Model (LLM) to analyze complex error patterns and generate\ntestable hypotheses. This paper proposes LADDER: Language Driven slice\nDiscovery and Error Rectification. It first projects the model's representation\ninto a language-aligned feature space (eg CLIP) to preserve semantics in the\noriginal model feature space. This ensures the accurate retrieval of sentences\nthat highlight the model's errors. Next, the LLM utilizes the sentences and\ngenerates hypotheses to discover error slices. Finally, we mitigate the error\nby fine-tuning the classification head by creating a group-balanced dataset\nusing the hypotheses. Our entire method does not require any attribute\nannotation, either explicitly or through external tagging models. We validate\nour method with \\textbf{five} image classification datasets. The code is\navailable (https://github.com/batmanlab/Ladder)."
                },
                "authors": [
                    {
                        "name": "Shantanu Ghosh"
                    },
                    {
                        "name": "Rayan Syed"
                    },
                    {
                        "name": "Chenyu Wang"
                    },
                    {
                        "name": "Clare B. Poynton"
                    },
                    {
                        "name": "Kayhan Batmanghelich"
                    }
                ],
                "author_detail": {
                    "name": "Kayhan Batmanghelich"
                },
                "author": "Kayhan Batmanghelich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07832v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07832v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02889v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02889v1",
                "updated": "2024-09-04T17:25:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    25,
                    21,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T17:25:21Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    25,
                    21,
                    2,
                    248,
                    0
                ],
                "title": "LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via\n  Hybrid Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via\n  Hybrid Architecture"
                },
                "summary": "Expanding the long-context capabilities of Multi-modal Large Language\nModels~(MLLMs) is crucial for video understanding, high-resolution image\nunderstanding, and multi-modal agents. This involves a series of systematic\noptimizations, including model architecture, data construction and training\nstrategy, particularly addressing challenges such as \\textit{degraded\nperformance with more images} and \\textit{high computational costs}. In this\npaper, we adapt the model architecture to a hybrid of Mamba and Transformer\nblocks, approach data construction with both temporal and spatial dependencies\namong multiple images and employ a progressive training strategy. The released\nmodel \\textbf{LongLLaVA}~(\\textbf{Long}-Context \\textbf{L}arge\n\\textbf{L}anguage \\textbf{a}nd \\textbf{V}ision \\textbf{A}ssistant) is the first\nhybrid MLLM, which achieved a better balance between efficiency and\neffectiveness. LongLLaVA not only achieves competitive results across various\nbenchmarks, but also maintains high throughput and low memory consumption.\nEspecially, it could process nearly a thousand images on a single A100 80GB\nGPU, showing promising application prospects for a wide range of tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expanding the long-context capabilities of Multi-modal Large Language\nModels~(MLLMs) is crucial for video understanding, high-resolution image\nunderstanding, and multi-modal agents. This involves a series of systematic\noptimizations, including model architecture, data construction and training\nstrategy, particularly addressing challenges such as \\textit{degraded\nperformance with more images} and \\textit{high computational costs}. In this\npaper, we adapt the model architecture to a hybrid of Mamba and Transformer\nblocks, approach data construction with both temporal and spatial dependencies\namong multiple images and employ a progressive training strategy. The released\nmodel \\textbf{LongLLaVA}~(\\textbf{Long}-Context \\textbf{L}arge\n\\textbf{L}anguage \\textbf{a}nd \\textbf{V}ision \\textbf{A}ssistant) is the first\nhybrid MLLM, which achieved a better balance between efficiency and\neffectiveness. LongLLaVA not only achieves competitive results across various\nbenchmarks, but also maintains high throughput and low memory consumption.\nEspecially, it could process nearly a thousand images on a single A100 80GB\nGPU, showing promising application prospects for a wide range of tasks."
                },
                "authors": [
                    {
                        "name": "Xidong Wang"
                    },
                    {
                        "name": "Dingjie Song"
                    },
                    {
                        "name": "Shunian Chen"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "arxiv_comment": "19 pages, 7 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02889v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18322v2",
                "updated": "2024-09-04T17:16:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    16,
                    5,
                    2,
                    248,
                    0
                ],
                "published": "2024-07-01T19:52:41Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    19,
                    52,
                    41,
                    0,
                    183,
                    0
                ],
                "title": "The Need for Guardrails with Large Language Models in Medical\n  Safety-Critical Settings: An Artificial Intelligence Application in the\n  Pharmacovigilance Ecosystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Need for Guardrails with Large Language Models in Medical\n  Safety-Critical Settings: An Artificial Intelligence Application in the\n  Pharmacovigilance Ecosystem"
                },
                "summary": "Large language models (LLMs) are useful tools with the capacity for\nperforming specific types of knowledge work at an effective scale. However, LLM\ndeployments in high-risk and safety-critical domains pose unique challenges,\nnotably the issue of ``hallucination,'' where LLMs can generate fabricated\ninformation. This is particularly concerning in settings such as drug safety,\nwhere inaccuracies could lead to patient harm. To mitigate these risks, we have\ndeveloped and demonstrated a proof of concept suite of guardrails specifically\ndesigned to mitigate certain types of hallucinations and errors for drug\nsafety, and potentially applicable to other medical safety-critical contexts.\nThese guardrails include mechanisms to detect anomalous documents to prevent\nthe ingestion of inappropriate data, identify incorrect drug names or adverse\nevent terms, and convey uncertainty in generated content. We integrated these\nguardrails with an LLM fine-tuned for a text-to-text task, which involves\nconverting both structured and unstructured data within adverse event reports\ninto natural language. This method was applied to translate individual case\nsafety reports, demonstrating effective application in a pharmacovigilance\nprocessing task. Our guardrail framework offers a set of tools with broad\napplicability across various domains, ensuring LLMs can be safely used in\nhigh-risk situations by eliminating the occurrence of key errors, including the\ngeneration of incorrect pharmacovigilance-related terms, thus adhering to\nstringent regulatory and quality standards in medical safety-critical\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are useful tools with the capacity for\nperforming specific types of knowledge work at an effective scale. However, LLM\ndeployments in high-risk and safety-critical domains pose unique challenges,\nnotably the issue of ``hallucination,'' where LLMs can generate fabricated\ninformation. This is particularly concerning in settings such as drug safety,\nwhere inaccuracies could lead to patient harm. To mitigate these risks, we have\ndeveloped and demonstrated a proof of concept suite of guardrails specifically\ndesigned to mitigate certain types of hallucinations and errors for drug\nsafety, and potentially applicable to other medical safety-critical contexts.\nThese guardrails include mechanisms to detect anomalous documents to prevent\nthe ingestion of inappropriate data, identify incorrect drug names or adverse\nevent terms, and convey uncertainty in generated content. We integrated these\nguardrails with an LLM fine-tuned for a text-to-text task, which involves\nconverting both structured and unstructured data within adverse event reports\ninto natural language. This method was applied to translate individual case\nsafety reports, demonstrating effective application in a pharmacovigilance\nprocessing task. Our guardrail framework offers a set of tools with broad\napplicability across various domains, ensuring LLMs can be safely used in\nhigh-risk situations by eliminating the occurrence of key errors, including the\ngeneration of incorrect pharmacovigilance-related terms, thus adhering to\nstringent regulatory and quality standards in medical safety-critical\nenvironments."
                },
                "authors": [
                    {
                        "name": "Joe B Hakim"
                    },
                    {
                        "name": "Jeffery L Painter"
                    },
                    {
                        "name": "Darmendra Ramcharran"
                    },
                    {
                        "name": "Vijay Kara"
                    },
                    {
                        "name": "Greg Powell"
                    },
                    {
                        "name": "Paulina Sobczak"
                    },
                    {
                        "name": "Chiho Sato"
                    },
                    {
                        "name": "Andrew Bate"
                    },
                    {
                        "name": "Andrew Beam"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Beam"
                },
                "author": "Andrew Beam",
                "arxiv_comment": "27 pages, 6 figures, 4 tables and supplementary material provided",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1; I.2.7; I.7.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02877v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02877v1",
                "updated": "2024-09-04T17:01:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    1,
                    2,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T17:01:02Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    1,
                    2,
                    2,
                    248,
                    0
                ],
                "title": "Configurable Foundation Models: Building LLMs from a Modular Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Configurable Foundation Models: Building LLMs from a Modular Perspective"
                },
                "summary": "Advancements in LLMs have recently unveiled challenges tied to computational\nefficiency and continual scalability due to their requirements of huge\nparameters, making the applications and evolution of these models on devices\nwith limited computation resources and scenarios requiring various abilities\nincreasingly cumbersome. Inspired by modularity within the human brain, there\nis a growing tendency to decompose LLMs into numerous functional modules,\nallowing for inference with part of modules and dynamic assembly of modules to\ntackle complex tasks, such as mixture-of-experts. To highlight the inherent\nefficiency and composability of the modular approach, we coin the term brick to\nrepresent each functional module, designating the modularized structure as\nconfigurable foundation models. In this paper, we offer a comprehensive\noverview and investigation of the construction, utilization, and limitation of\nconfigurable foundation models. We first formalize modules into emergent bricks\n- functional neuron partitions that emerge during the pre-training phase, and\ncustomized bricks - bricks constructed via additional post-training to improve\nthe capabilities and knowledge of LLMs. Based on diverse functional bricks, we\nfurther present four brick-oriented operations: retrieval and routing, merging,\nupdating, and growing. These operations allow for dynamic configuration of LLMs\nbased on instructions to handle complex tasks. To verify our perspective, we\nconduct an empirical analysis on widely-used LLMs. We find that the FFN layers\nfollow modular patterns with functional specialization of neurons and\nfunctional neuron partitions. Finally, we highlight several open issues and\ndirections for future research. Overall, this paper aims to offer a fresh\nmodular perspective on existing LLM research and inspire the future creation of\nmore efficient and scalable foundational models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in LLMs have recently unveiled challenges tied to computational\nefficiency and continual scalability due to their requirements of huge\nparameters, making the applications and evolution of these models on devices\nwith limited computation resources and scenarios requiring various abilities\nincreasingly cumbersome. Inspired by modularity within the human brain, there\nis a growing tendency to decompose LLMs into numerous functional modules,\nallowing for inference with part of modules and dynamic assembly of modules to\ntackle complex tasks, such as mixture-of-experts. To highlight the inherent\nefficiency and composability of the modular approach, we coin the term brick to\nrepresent each functional module, designating the modularized structure as\nconfigurable foundation models. In this paper, we offer a comprehensive\noverview and investigation of the construction, utilization, and limitation of\nconfigurable foundation models. We first formalize modules into emergent bricks\n- functional neuron partitions that emerge during the pre-training phase, and\ncustomized bricks - bricks constructed via additional post-training to improve\nthe capabilities and knowledge of LLMs. Based on diverse functional bricks, we\nfurther present four brick-oriented operations: retrieval and routing, merging,\nupdating, and growing. These operations allow for dynamic configuration of LLMs\nbased on instructions to handle complex tasks. To verify our perspective, we\nconduct an empirical analysis on widely-used LLMs. We find that the FFN layers\nfollow modular patterns with functional specialization of neurons and\nfunctional neuron partitions. Finally, we highlight several open issues and\ndirections for future research. Overall, this paper aims to offer a fresh\nmodular perspective on existing LLM research and inspire the future creation of\nmore efficient and scalable foundational models."
                },
                "authors": [
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Zhengyan Zhang"
                    },
                    {
                        "name": "Chenyang Song"
                    },
                    {
                        "name": "Dazhi Jiang"
                    },
                    {
                        "name": "Feng Yao"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Xiaozhi Wang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Guanyu Lin"
                    },
                    {
                        "name": "Yingfa Chen"
                    },
                    {
                        "name": "Weilin Zhao"
                    },
                    {
                        "name": "Yuge Tu"
                    },
                    {
                        "name": "Zexuan Zhong"
                    },
                    {
                        "name": "Ao Zhang"
                    },
                    {
                        "name": "Chenglei Si"
                    },
                    {
                        "name": "Khai Hao Moo"
                    },
                    {
                        "name": "Chenyang Zhao"
                    },
                    {
                        "name": "Huimin Chen"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Jingbo Shang"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02877v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02877v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10690v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10690v3",
                "updated": "2024-09-04T16:58:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    58,
                    25,
                    2,
                    248,
                    0
                ],
                "published": "2024-06-15T17:07:31Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    17,
                    7,
                    31,
                    5,
                    167,
                    0
                ],
                "title": "Automating Pharmacovigilance Evidence Generation: Using Large Language\n  Models to Produce Context-Aware SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Pharmacovigilance Evidence Generation: Using Large Language\n  Models to Produce Context-Aware SQL"
                },
                "summary": "Objective: To enhance the efficiency and accuracy of information retrieval\nfrom pharmacovigilance (PV) databases by employing Large Language Models (LLMs)\nto convert natural language queries (NLQs) into Structured Query Language (SQL)\nqueries, leveraging a business context document.\n  Materials and Methods: We utilized OpenAI's GPT-4 model within a\nretrieval-augmented generation (RAG) framework, enriched with a business\ncontext document, to transform NLQs into syntactically precise SQL queries.\nEach NLQ was presented to the LLM randomly and independently to prevent\nmemorization. The study was conducted in three phases, varying query\ncomplexity, and assessing the LLM's performance both with and without the\nbusiness context document.\n  Results: Our approach significantly improved NLQ-to-SQL accuracy, increasing\nfrom 8.3\\% with the database schema alone to 78.3\\% with the business context\ndocument. This enhancement was consistent across low, medium, and high\ncomplexity queries, indicating the critical role of contextual knowledge in\nquery generation.\n  Discussion: The integration of a business context document markedly improved\nthe LLM's ability to generate accurate and contextually relevant SQL queries.\nPerformance achieved a maximum of 85\\% when high complexity queries are\nexcluded, suggesting promise for routine deployment.\n  Conclusion: This study presents a novel approach to employing LLMs for safety\ndata retrieval and analysis, demonstrating significant advancements in query\ngeneration accuracy. The methodology offers a framework applicable to various\ndata-intensive domains, enhancing the accessibility and efficiency of\ninformation retrieval for non-technical users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objective: To enhance the efficiency and accuracy of information retrieval\nfrom pharmacovigilance (PV) databases by employing Large Language Models (LLMs)\nto convert natural language queries (NLQs) into Structured Query Language (SQL)\nqueries, leveraging a business context document.\n  Materials and Methods: We utilized OpenAI's GPT-4 model within a\nretrieval-augmented generation (RAG) framework, enriched with a business\ncontext document, to transform NLQs into syntactically precise SQL queries.\nEach NLQ was presented to the LLM randomly and independently to prevent\nmemorization. The study was conducted in three phases, varying query\ncomplexity, and assessing the LLM's performance both with and without the\nbusiness context document.\n  Results: Our approach significantly improved NLQ-to-SQL accuracy, increasing\nfrom 8.3\\% with the database schema alone to 78.3\\% with the business context\ndocument. This enhancement was consistent across low, medium, and high\ncomplexity queries, indicating the critical role of contextual knowledge in\nquery generation.\n  Discussion: The integration of a business context document markedly improved\nthe LLM's ability to generate accurate and contextually relevant SQL queries.\nPerformance achieved a maximum of 85\\% when high complexity queries are\nexcluded, suggesting promise for routine deployment.\n  Conclusion: This study presents a novel approach to employing LLMs for safety\ndata retrieval and analysis, demonstrating significant advancements in query\ngeneration accuracy. The methodology offers a framework applicable to various\ndata-intensive domains, enhancing the accessibility and efficiency of\ninformation retrieval for non-technical users."
                },
                "authors": [
                    {
                        "name": "Jeffery L. Painter"
                    },
                    {
                        "name": "Venkateswara Rao Chalamalasetti"
                    },
                    {
                        "name": "Raymond Kassekert"
                    },
                    {
                        "name": "Andrew Bate"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Bate"
                },
                "author": "Andrew Bate",
                "arxiv_comment": "15 pages, 3 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10690v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10690v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00847v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00847v2",
                "updated": "2024-09-04T16:39:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    39,
                    22,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-01T21:30:14Z",
                "published_parsed": [
                    2024,
                    9,
                    1,
                    21,
                    30,
                    14,
                    6,
                    245,
                    0
                ],
                "title": "The Design of an LLM-powered Unstructured Analytics System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Design of an LLM-powered Unstructured Analytics System"
                },
                "summary": "LLMs demonstrate an uncanny ability to process unstructured data, and as\nsuch, have the potential to go beyond search and run complex, semantic analyses\nat scale. We describe the design of an unstructured analytics system, Aryn, and\nthe tenets and use cases that motivate its design. With Aryn, users can specify\nqueries in natural language and the system automatically determines a semantic\nplan and executes it to compute an answer from a large collection of\nunstructured documents using LLMs. At the core of Aryn is Sycamore, a\ndeclarative document processing engine, built using Ray, that provides a\nreliable distributed abstraction called DocSets. Sycamore allows users to\nanalyze, enrich, and transform complex documents at scale. Aryn also comprises\nLuna, a query planner that translates natural language queries to Sycamore\nscripts, and the Aryn Partitioner, which takes raw PDFs and document images,\nand converts them to DocSets for downstream processing. Using Aryn, we\ndemonstrate a real world use case for analyzing accident reports from the\nNational Transportation Safety Board (NTSB), and discuss some of the major\nchallenges we encountered in deploying Aryn in the wild.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs demonstrate an uncanny ability to process unstructured data, and as\nsuch, have the potential to go beyond search and run complex, semantic analyses\nat scale. We describe the design of an unstructured analytics system, Aryn, and\nthe tenets and use cases that motivate its design. With Aryn, users can specify\nqueries in natural language and the system automatically determines a semantic\nplan and executes it to compute an answer from a large collection of\nunstructured documents using LLMs. At the core of Aryn is Sycamore, a\ndeclarative document processing engine, built using Ray, that provides a\nreliable distributed abstraction called DocSets. Sycamore allows users to\nanalyze, enrich, and transform complex documents at scale. Aryn also comprises\nLuna, a query planner that translates natural language queries to Sycamore\nscripts, and the Aryn Partitioner, which takes raw PDFs and document images,\nand converts them to DocSets for downstream processing. Using Aryn, we\ndemonstrate a real world use case for analyzing accident reports from the\nNational Transportation Safety Board (NTSB), and discuss some of the major\nchallenges we encountered in deploying Aryn in the wild."
                },
                "authors": [
                    {
                        "name": "Eric Anderson"
                    },
                    {
                        "name": "Jonathan Fritz"
                    },
                    {
                        "name": "Austin Lee"
                    },
                    {
                        "name": "Bohou Li"
                    },
                    {
                        "name": "Mark Lindblad"
                    },
                    {
                        "name": "Henry Lindeman"
                    },
                    {
                        "name": "Alex Meyer"
                    },
                    {
                        "name": "Parth Parmar"
                    },
                    {
                        "name": "Tanvi Ranade"
                    },
                    {
                        "name": "Mehul A. Shah"
                    },
                    {
                        "name": "Benjamin Sowell"
                    },
                    {
                        "name": "Dan Tecuci"
                    },
                    {
                        "name": "Vinayak Thapliyal"
                    },
                    {
                        "name": "Matt Welsh"
                    }
                ],
                "author_detail": {
                    "name": "Matt Welsh"
                },
                "author": "Matt Welsh",
                "arxiv_comment": "6 pages, 3 figures, fixed typos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00847v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00847v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02849v1",
                "updated": "2024-09-04T16:19:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    19,
                    55,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T16:19:55Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    19,
                    55,
                    2,
                    248,
                    0
                ],
                "title": "Anomaly Detection in Offshore Open Radio Access Network Using Long\n  Short-Term Memory Models on a Novel Artificial Intelligence-Driven\n  Cloud-Native Data Platform",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anomaly Detection in Offshore Open Radio Access Network Using Long\n  Short-Term Memory Models on a Novel Artificial Intelligence-Driven\n  Cloud-Native Data Platform"
                },
                "summary": "The radio access network (RAN) is a critical component of modern telecom\ninfrastructure, currently undergoing significant transformation towards\ndisaggregated and open architectures. These advancements are pivotal for\nintegrating intelligent, data-driven applications aimed at enhancing network\nreliability and operational autonomy through the introduction of cognition\ncapabilities, exemplified by the set of enhancements proposed by the emerging\nOpen radio access network (O-RAN) standards. Despite its potential, the nascent\nnature of O-RAN technology presents challenges, primarily due to the absence of\nmature operational standards. This complicates the management of data and\napplications, particularly in integrating with traditional network management\nand operational support systems. Divergent vendor-specific design approaches\nfurther hinder migration and limit solution reusability. Addressing the skills\ngap in telecom business-oriented engineering is crucial for the effective\ndeployment of O-RAN and the development of robust data-driven applications. To\naddress these challenges, Boldyn Networks, a global Neutral Host provider, has\nimplemented a novel cloud-native data analytics platform. This platform\nunderwent rigorous testing in real-world scenarios of using advanced artificial\nintelligence (AI) techniques, significantly improving operational efficiency,\nand enhancing customer experience. Implementation involved adopting development\noperations (DevOps) practices, leveraging data lakehouse architectures tailored\nfor AI applications, and employing sophisticated data engineering strategies.\nThe platform successfully addresses connectivity challenges inherent in\noffshore windfarm deployments using long short-term memory (LSTM) Models for\nanomaly detection of the connectivity, providing detailed insights into its\nspecialized architecture developed for this purpose.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The radio access network (RAN) is a critical component of modern telecom\ninfrastructure, currently undergoing significant transformation towards\ndisaggregated and open architectures. These advancements are pivotal for\nintegrating intelligent, data-driven applications aimed at enhancing network\nreliability and operational autonomy through the introduction of cognition\ncapabilities, exemplified by the set of enhancements proposed by the emerging\nOpen radio access network (O-RAN) standards. Despite its potential, the nascent\nnature of O-RAN technology presents challenges, primarily due to the absence of\nmature operational standards. This complicates the management of data and\napplications, particularly in integrating with traditional network management\nand operational support systems. Divergent vendor-specific design approaches\nfurther hinder migration and limit solution reusability. Addressing the skills\ngap in telecom business-oriented engineering is crucial for the effective\ndeployment of O-RAN and the development of robust data-driven applications. To\naddress these challenges, Boldyn Networks, a global Neutral Host provider, has\nimplemented a novel cloud-native data analytics platform. This platform\nunderwent rigorous testing in real-world scenarios of using advanced artificial\nintelligence (AI) techniques, significantly improving operational efficiency,\nand enhancing customer experience. Implementation involved adopting development\noperations (DevOps) practices, leveraging data lakehouse architectures tailored\nfor AI applications, and employing sophisticated data engineering strategies.\nThe platform successfully addresses connectivity challenges inherent in\noffshore windfarm deployments using long short-term memory (LSTM) Models for\nanomaly detection of the connectivity, providing detailed insights into its\nspecialized architecture developed for this purpose."
                },
                "authors": [
                    {
                        "name": "Abdelrahim Ahmad"
                    },
                    {
                        "name": "Peizheng Li"
                    },
                    {
                        "name": "Robert Piechocki"
                    },
                    {
                        "name": "Rui Inacio"
                    }
                ],
                "author_detail": {
                    "name": "Rui Inacio"
                },
                "author": "Rui Inacio",
                "arxiv_comment": "16 pages, 12 figures. This manuscript has been submitted to Elsevier\n  for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02842v1",
                "updated": "2024-09-04T16:14:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    14,
                    14,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T16:14:14Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    14,
                    14,
                    2,
                    248,
                    0
                ],
                "title": "SNNAX -- Spiking Neural Networks in JAX",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SNNAX -- Spiking Neural Networks in JAX"
                },
                "summary": "Spiking Neural Networks (SNNs) simulators are essential tools to prototype\nbiologically inspired models and neuromorphic hardware architectures and\npredict their performance. For such a tool, ease of use and flexibility are\ncritical, but so is simulation speed especially given the complexity inherent\nto simulating SNN. Here, we present SNNAX, a JAX-based framework for simulating\nand training such models with PyTorch-like intuitiveness and JAX-like execution\nspeed. SNNAX models are easily extended and customized to fit the desired model\nspecifications and target neuromorphic hardware. Additionally, SNNAX offers key\nfeatures for optimizing the training and deployment of SNNs such as flexible\nautomatic differentiation and just-in-time compilation. We evaluate and compare\nSNNAX to other commonly used machine learning (ML) frameworks used for\nprogramming SNNs. We provide key performance metrics, best practices,\ndocumented examples for simulating SNNs in SNNAX, and implement several\nbenchmarks used in the literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) simulators are essential tools to prototype\nbiologically inspired models and neuromorphic hardware architectures and\npredict their performance. For such a tool, ease of use and flexibility are\ncritical, but so is simulation speed especially given the complexity inherent\nto simulating SNN. Here, we present SNNAX, a JAX-based framework for simulating\nand training such models with PyTorch-like intuitiveness and JAX-like execution\nspeed. SNNAX models are easily extended and customized to fit the desired model\nspecifications and target neuromorphic hardware. Additionally, SNNAX offers key\nfeatures for optimizing the training and deployment of SNNs such as flexible\nautomatic differentiation and just-in-time compilation. We evaluate and compare\nSNNAX to other commonly used machine learning (ML) frameworks used for\nprogramming SNNs. We provide key performance metrics, best practices,\ndocumented examples for simulating SNNs in SNNAX, and implement several\nbenchmarks used in the literature."
                },
                "authors": [
                    {
                        "name": "Jamie Lohoff"
                    },
                    {
                        "name": "Jan Finkbeiner"
                    },
                    {
                        "name": "Emre Neftci"
                    }
                ],
                "author_detail": {
                    "name": "Emre Neftci"
                },
                "author": "Emre Neftci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08763v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08763v4",
                "updated": "2024-09-04T16:13:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    13,
                    18,
                    2,
                    248,
                    0
                ],
                "published": "2024-03-13T17:58:57Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    17,
                    58,
                    57,
                    2,
                    73,
                    0
                ],
                "title": "Simple and Scalable Strategies to Continually Pre-train Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple and Scalable Strategies to Continually Pre-train Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by the final loss\nand the average score on several language model (LM) evaluation benchmarks.\nSpecifically, we show this for a weak but realistic distribution shift between\ntwo commonly used LLM pre-training datasets (English$\\rightarrow$English) and a\nstronger distribution shift (English$\\rightarrow$German) at the $405$M\nparameter model scale with large dataset sizes (hundreds of billions of\ntokens). Selecting the weak but realistic shift for larger-scale experiments,\nwe also find that our continual learning strategies match the re-training\nbaseline for a 10B parameter LLM. Our results demonstrate that LLMs can be\nsuccessfully updated via simple and scalable continual learning strategies,\nmatching the re-training baseline using only a fraction of the compute.\nFinally, inspired by previous work, we propose alternatives to the cosine\nlearning rate schedule that help circumvent forgetting induced by LR re-warming\nand that are not bound to a fixed token budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by the final loss\nand the average score on several language model (LM) evaluation benchmarks.\nSpecifically, we show this for a weak but realistic distribution shift between\ntwo commonly used LLM pre-training datasets (English$\\rightarrow$English) and a\nstronger distribution shift (English$\\rightarrow$German) at the $405$M\nparameter model scale with large dataset sizes (hundreds of billions of\ntokens). Selecting the weak but realistic shift for larger-scale experiments,\nwe also find that our continual learning strategies match the re-training\nbaseline for a 10B parameter LLM. Our results demonstrate that LLMs can be\nsuccessfully updated via simple and scalable continual learning strategies,\nmatching the re-training baseline using only a fraction of the compute.\nFinally, inspired by previous work, we propose alternatives to the cosine\nlearning rate schedule that help circumvent forgetting induced by LR re-warming\nand that are not bound to a fixed token budget."
                },
                "authors": [
                    {
                        "name": "Adam Ibrahim"
                    },
                    {
                        "name": "Benjamin Thrien"
                    },
                    {
                        "name": "Kshitij Gupta"
                    },
                    {
                        "name": "Mats L. Richter"
                    },
                    {
                        "name": "Quentin Anthony"
                    },
                    {
                        "name": "Timothe Lesort"
                    },
                    {
                        "name": "Eugene Belilovsky"
                    },
                    {
                        "name": "Irina Rish"
                    }
                ],
                "author_detail": {
                    "name": "Irina Rish"
                },
                "author": "Irina Rish",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08763v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08763v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02839v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02839v1",
                "updated": "2024-09-04T16:09:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    9,
                    28,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T16:09:28Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    9,
                    28,
                    2,
                    248,
                    0
                ],
                "title": "Jger: Automated Telephone Call Traceback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jger: Automated Telephone Call Traceback"
                },
                "summary": "Unsolicited telephone calls that facilitate fraud or unlawful telemarketing\ncontinue to overwhelm network users and the regulators who prosecute them. The\nfirst step in prosecuting phone abuse is traceback -- identifying the call\noriginator. This fundamental investigative task currently requires hours of\nmanual effort per call. In this paper, we introduce J\\\"ager, a distributed\nsecure call traceback system. J\\\"ager can trace a call in a few seconds, even\nwith partial deployment, while cryptographically preserving the privacy of call\nparties, carrier trade secrets like peers and call volume, and limiting the\nthreat of bulk analysis. We establish definitions and requirements of secure\ntraceback, then develop a suite of protocols that meet these requirements using\nwitness encryption, oblivious pseudorandom functions, and group signatures. We\nprove these protocols secure in the universal composibility framework. We then\ndemonstrate that J\\\"ager has low compute and bandwidth costs per call, and\nthese costs scale linearly with call volume. J\\\"ager provides an efficient,\nsecure, privacy-preserving system to revolutionize telephone abuse\ninvestigation with minimal costs to operators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsolicited telephone calls that facilitate fraud or unlawful telemarketing\ncontinue to overwhelm network users and the regulators who prosecute them. The\nfirst step in prosecuting phone abuse is traceback -- identifying the call\noriginator. This fundamental investigative task currently requires hours of\nmanual effort per call. In this paper, we introduce J\\\"ager, a distributed\nsecure call traceback system. J\\\"ager can trace a call in a few seconds, even\nwith partial deployment, while cryptographically preserving the privacy of call\nparties, carrier trade secrets like peers and call volume, and limiting the\nthreat of bulk analysis. We establish definitions and requirements of secure\ntraceback, then develop a suite of protocols that meet these requirements using\nwitness encryption, oblivious pseudorandom functions, and group signatures. We\nprove these protocols secure in the universal composibility framework. We then\ndemonstrate that J\\\"ager has low compute and bandwidth costs per call, and\nthese costs scale linearly with call volume. J\\\"ager provides an efficient,\nsecure, privacy-preserving system to revolutionize telephone abuse\ninvestigation with minimal costs to operators."
                },
                "authors": [
                    {
                        "name": "David Adei"
                    },
                    {
                        "name": "Varun Madathil"
                    },
                    {
                        "name": "Sathvik Prasad"
                    },
                    {
                        "name": "Bradley Reaves"
                    },
                    {
                        "name": "Alessandra Scafuro"
                    }
                ],
                "author_detail": {
                    "name": "Alessandra Scafuro"
                },
                "author": "Alessandra Scafuro",
                "arxiv_doi": "10.1145/3658644.3690290",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690290",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02839v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02839v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 2024 ACM SIGSAC Conference on Computer and\n  Communications Security (CCS '24), October 14---18, 2024, Salt Lake City, UT,\n  USA. ACM, New York, NY, USA, 24 pages",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02834v1",
                "updated": "2024-09-04T16:00:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    0,
                    21,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T16:00:21Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    0,
                    21,
                    2,
                    248,
                    0
                ],
                "title": "CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the\n  Mathematics Reasoning of Large Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the\n  Mathematics Reasoning of Large Multimodal Models"
                },
                "summary": "Large language models (LLMs) have obtained promising results in mathematical\nreasoning, which is a foundational skill for human intelligence. Most previous\nstudies focus on improving and measuring the performance of LLMs based on\ntextual math reasoning datasets (e.g., MATH, GSM8K). Recently, a few\nresearchers have released English multimodal math datasets (e.g., MATHVISTA and\nMATH-V) to evaluate the effectiveness of large multimodal models (LMMs). In\nthis paper, we release a Chinese multimodal math (CMM-Math) dataset, including\nbenchmark and training parts, to evaluate and enhance the mathematical\nreasoning of LMMs. CMM-Math contains over 28,000 high-quality samples,\nfeaturing a variety of problem types (e.g., multiple-choice, fill-in-the-blank,\nand so on) with detailed solutions across 12 grade levels from elementary to\nhigh school in China. Specifically, the visual context may be present in the\nquestions or opinions, which makes this dataset more challenging. Through\ncomprehensive analysis, we discover that state-of-the-art LMMs on the CMM-Math\ndataset face challenges, emphasizing the necessity for further improvements in\nLMM development. We also propose a Multimodal Mathematical LMM (Math-LMM) to\nhandle the problems with mixed input of multiple images and text segments. We\ntrain our model using three stages, including foundational pre-training,\nfoundational fine-tuning, and mathematical fine-tuning. The extensive\nexperiments indicate that our model effectively improves math reasoning\nperformance by comparing it with the SOTA LMMs over three multimodal\nmathematical datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have obtained promising results in mathematical\nreasoning, which is a foundational skill for human intelligence. Most previous\nstudies focus on improving and measuring the performance of LLMs based on\ntextual math reasoning datasets (e.g., MATH, GSM8K). Recently, a few\nresearchers have released English multimodal math datasets (e.g., MATHVISTA and\nMATH-V) to evaluate the effectiveness of large multimodal models (LMMs). In\nthis paper, we release a Chinese multimodal math (CMM-Math) dataset, including\nbenchmark and training parts, to evaluate and enhance the mathematical\nreasoning of LMMs. CMM-Math contains over 28,000 high-quality samples,\nfeaturing a variety of problem types (e.g., multiple-choice, fill-in-the-blank,\nand so on) with detailed solutions across 12 grade levels from elementary to\nhigh school in China. Specifically, the visual context may be present in the\nquestions or opinions, which makes this dataset more challenging. Through\ncomprehensive analysis, we discover that state-of-the-art LMMs on the CMM-Math\ndataset face challenges, emphasizing the necessity for further improvements in\nLMM development. We also propose a Multimodal Mathematical LMM (Math-LMM) to\nhandle the problems with mixed input of multiple images and text segments. We\ntrain our model using three stages, including foundational pre-training,\nfoundational fine-tuning, and mathematical fine-tuning. The extensive\nexperiments indicate that our model effectively improves math reasoning\nperformance by comparing it with the SOTA LMMs over three multimodal\nmathematical datasets."
                },
                "authors": [
                    {
                        "name": "Wentao Liu"
                    },
                    {
                        "name": "Qianjun Pan"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Zhuo Liu"
                    },
                    {
                        "name": "Ji Wu"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Aimin Zhou"
                    },
                    {
                        "name": "Qin Chen"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Liang He"
                    }
                ],
                "author_detail": {
                    "name": "Liang He"
                },
                "author": "Liang He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00509v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00509v2",
                "updated": "2024-09-04T15:55:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    15,
                    55,
                    22,
                    2,
                    248,
                    0
                ],
                "published": "2024-08-31T17:19:30Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    17,
                    19,
                    30,
                    5,
                    244,
                    0
                ],
                "title": "LongRecipe: Recipe for Efficient Long Context Generalization in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongRecipe: Recipe for Efficient Long Context Generalization in Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) face significant challenges in handling\nlong-context tasks because of their limited effective context window size\nduring pretraining, which restricts their ability to generalize over extended\nsequences. Meanwhile, extending the context window in LLMs through\npost-pretraining is highly resource-intensive. To address this, we introduce\nLongRecipe, an efficient training strategy for extending the context window of\nLLMs, including impactful token analysis, position index transformation, and\ntraining optimization strategies. It simulates long-sequence inputs while\nmaintaining training efficiency and significantly improves the model's\nunderstanding of long-range dependencies. Experiments on three types of LLMs\nshow that LongRecipe can utilize long sequences while requiring only 30% of the\ntarget context window size, and reduces computational training resource over\n85% compared to full sequence training. Furthermore, LongRecipe also preserves\nthe original LLM's capabilities in general tasks. Ultimately, we can extend the\neffective context window of open-source LLMs from 8k to 128k, achieving\nperformance close to GPT-4 with just one day of dedicated training using a\nsingle GPU with 80G memory. Our code is released at\nhttps://github.com/zhiyuanhubj/LongRecipe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face significant challenges in handling\nlong-context tasks because of their limited effective context window size\nduring pretraining, which restricts their ability to generalize over extended\nsequences. Meanwhile, extending the context window in LLMs through\npost-pretraining is highly resource-intensive. To address this, we introduce\nLongRecipe, an efficient training strategy for extending the context window of\nLLMs, including impactful token analysis, position index transformation, and\ntraining optimization strategies. It simulates long-sequence inputs while\nmaintaining training efficiency and significantly improves the model's\nunderstanding of long-range dependencies. Experiments on three types of LLMs\nshow that LongRecipe can utilize long sequences while requiring only 30% of the\ntarget context window size, and reduces computational training resource over\n85% compared to full sequence training. Furthermore, LongRecipe also preserves\nthe original LLM's capabilities in general tasks. Ultimately, we can extend the\neffective context window of open-source LLMs from 8k to 128k, achieving\nperformance close to GPT-4 with just one day of dedicated training using a\nsingle GPU with 80G memory. Our code is released at\nhttps://github.com/zhiyuanhubj/LongRecipe."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Hu"
                    },
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Jinman Zhao"
                    },
                    {
                        "name": "Suyuchen Wang"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Wei Shen"
                    },
                    {
                        "name": "Qing Gu"
                    },
                    {
                        "name": "Anh Tuan Luu"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Zhiwei Jiang"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00509v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00509v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02823v1",
                "updated": "2024-09-04T15:42:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    15,
                    42,
                    59,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T15:42:59Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    15,
                    42,
                    59,
                    2,
                    248,
                    0
                ],
                "title": "Design Contradictions: Help or Hindrance?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design Contradictions: Help or Hindrance?"
                },
                "summary": "The need for innovative ideas in data visualisation drives us to explore new\ncreative approaches. Combining two or more creative words, particularly those\nthat contradict each other, can positively impact the creative process,\nsparking novel ideas and designs. As we move towards AI-driven design, an open\nquestion arises: do these design contradictions work positively with AI tools?\nCurrently, the answer is no. AI systems, like large language models (LLMs),\nrely on algorithms that engender similarity, whereas creativity often requires\ndivergence and novelty. This poster initiates a conversation on how to drive AI\nsystems to be more creative and generate new ideas. This research invites us to\nreconsider traditional design methods and explore new approaches in an\nAI-driven world. Can we apply the same techniques used in traditional design,\nlike the double diamond model, or do we need new methods for design\nengineering? How can we quickly design visualisations and craft new ideas with\ngenerative AI? This paper seeks to start this critical conversation and offers\npractical insights into the potential of AI in driving creativity in data\nvisualisation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The need for innovative ideas in data visualisation drives us to explore new\ncreative approaches. Combining two or more creative words, particularly those\nthat contradict each other, can positively impact the creative process,\nsparking novel ideas and designs. As we move towards AI-driven design, an open\nquestion arises: do these design contradictions work positively with AI tools?\nCurrently, the answer is no. AI systems, like large language models (LLMs),\nrely on algorithms that engender similarity, whereas creativity often requires\ndivergence and novelty. This poster initiates a conversation on how to drive AI\nsystems to be more creative and generate new ideas. This research invites us to\nreconsider traditional design methods and explore new approaches in an\nAI-driven world. Can we apply the same techniques used in traditional design,\nlike the double diamond model, or do we need new methods for design\nengineering? How can we quickly design visualisations and craft new ideas with\ngenerative AI? This paper seeks to start this critical conversation and offers\npractical insights into the potential of AI in driving creativity in data\nvisualisation."
                },
                "authors": [
                    {
                        "name": "Aron E. Owen"
                    },
                    {
                        "name": "Jonathan C. Roberts"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan C. Roberts"
                },
                "author": "Jonathan C. Roberts",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02822v1",
                "updated": "2024-09-04T15:42:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    15,
                    42,
                    29,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T15:42:29Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    15,
                    42,
                    29,
                    2,
                    248,
                    0
                ],
                "title": "Language Understanding as a Constraint on Consensus Size in LLM\n  Societies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Understanding as a Constraint on Consensus Size in LLM\n  Societies"
                },
                "summary": "The applications of Large Language Models (LLMs) are going towards\ncollaborative tasks where several agents interact with each other like in an\nLLM society. In such a setting, large groups of LLMs could reach consensus\nabout arbitrary norms for which there is no information supporting one option\nover another, regulating their own behavior in a self-organized way. In human\nsocieties, the ability to reach consensus without institutions has a limit in\nthe cognitive capacities of humans. To understand if a similar phenomenon\ncharacterizes also LLMs, we apply methods from complexity science and\nprinciples from behavioral sciences in a new approach of AI anthropology. We\nfind that LLMs are able to reach consensus in groups and that the opinion\ndynamics of LLMs can be understood with a function parametrized by a majority\nforce coefficient that determines whether consensus is possible. This majority\nforce is stronger for models with higher language understanding capabilities\nand decreases for larger groups, leading to a critical group size beyond which,\nfor a given LLM, consensus is unfeasible. This critical group size grows\nexponentially with the language understanding capabilities of models and for\nthe most advanced models, it can reach an order of magnitude beyond the typical\nsize of informal human groups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The applications of Large Language Models (LLMs) are going towards\ncollaborative tasks where several agents interact with each other like in an\nLLM society. In such a setting, large groups of LLMs could reach consensus\nabout arbitrary norms for which there is no information supporting one option\nover another, regulating their own behavior in a self-organized way. In human\nsocieties, the ability to reach consensus without institutions has a limit in\nthe cognitive capacities of humans. To understand if a similar phenomenon\ncharacterizes also LLMs, we apply methods from complexity science and\nprinciples from behavioral sciences in a new approach of AI anthropology. We\nfind that LLMs are able to reach consensus in groups and that the opinion\ndynamics of LLMs can be understood with a function parametrized by a majority\nforce coefficient that determines whether consensus is possible. This majority\nforce is stronger for models with higher language understanding capabilities\nand decreases for larger groups, leading to a critical group size beyond which,\nfor a given LLM, consensus is unfeasible. This critical group size grows\nexponentially with the language understanding capabilities of models and for\nthe most advanced models, it can reach an order of magnitude beyond the typical\nsize of informal human groups."
                },
                "authors": [
                    {
                        "name": "Giordano De Marzo"
                    },
                    {
                        "name": "Claudio Castellano"
                    },
                    {
                        "name": "David Garcia"
                    }
                ],
                "author_detail": {
                    "name": "David Garcia"
                },
                "author": "David Garcia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16961v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16961v2",
                "updated": "2024-09-04T15:39:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    15,
                    39,
                    47,
                    2,
                    248,
                    0
                ],
                "published": "2024-08-15T17:59:14Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    17,
                    59,
                    14,
                    3,
                    228,
                    0
                ],
                "title": "The Future of Open Human Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Future of Open Human Feedback"
                },
                "summary": "Human feedback on conversations with language language models (LLMs) is\ncentral to how these systems learn about the world, improve their capabilities,\nand are steered toward desirable and safe behaviors. However, this feedback is\nmostly collected by frontier AI labs and kept behind closed doors. In this\nwork, we bring together interdisciplinary experts to assess the opportunities\nand challenges to realizing an open ecosystem of human feedback for AI. We\nfirst look for successful practices in peer production, open source, and\ncitizen science communities. We then characterize the main challenges for open\nhuman feedback. For each, we survey current approaches and offer\nrecommendations. We end by envisioning the components needed to underpin a\nsustainable and open human feedback ecosystem. In the center of this ecosystem\nare mutually beneficial feedback loops, between users and specialized models,\nincentivizing a diverse stakeholders community of model trainers and feedback\nproviders to support a general open feedback pool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human feedback on conversations with language language models (LLMs) is\ncentral to how these systems learn about the world, improve their capabilities,\nand are steered toward desirable and safe behaviors. However, this feedback is\nmostly collected by frontier AI labs and kept behind closed doors. In this\nwork, we bring together interdisciplinary experts to assess the opportunities\nand challenges to realizing an open ecosystem of human feedback for AI. We\nfirst look for successful practices in peer production, open source, and\ncitizen science communities. We then characterize the main challenges for open\nhuman feedback. For each, we survey current approaches and offer\nrecommendations. We end by envisioning the components needed to underpin a\nsustainable and open human feedback ecosystem. In the center of this ecosystem\nare mutually beneficial feedback loops, between users and specialized models,\nincentivizing a diverse stakeholders community of model trainers and feedback\nproviders to support a general open feedback pool."
                },
                "authors": [
                    {
                        "name": "Shachar Don-Yehiya"
                    },
                    {
                        "name": "Ben Burtenshaw"
                    },
                    {
                        "name": "Ramon Fernandez Astudillo"
                    },
                    {
                        "name": "Cailean Osborne"
                    },
                    {
                        "name": "Mimansa Jaiswal"
                    },
                    {
                        "name": "Tzu-Sheng Kuo"
                    },
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Idan Shenfeld"
                    },
                    {
                        "name": "Andi Peng"
                    },
                    {
                        "name": "Mikhail Yurochkin"
                    },
                    {
                        "name": "Atoosa Kasirzadeh"
                    },
                    {
                        "name": "Yangsibo Huang"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    },
                    {
                        "name": "Yacine Jernite"
                    },
                    {
                        "name": "Daniel Vila-Suero"
                    },
                    {
                        "name": "Omri Abend"
                    },
                    {
                        "name": "Jennifer Ding"
                    },
                    {
                        "name": "Sara Hooker"
                    },
                    {
                        "name": "Hannah Rose Kirk"
                    },
                    {
                        "name": "Leshem Choshen"
                    }
                ],
                "author_detail": {
                    "name": "Leshem Choshen"
                },
                "author": "Leshem Choshen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16961v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16961v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02817v1",
                "updated": "2024-09-04T15:35:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    15,
                    35,
                    18,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T15:35:18Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    15,
                    35,
                    18,
                    2,
                    248,
                    0
                ],
                "title": "Obsidian: Cooperative State-Space Exploration for Performant Inference\n  on Secure ML Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Obsidian: Cooperative State-Space Exploration for Performant Inference\n  on Secure ML Accelerators"
                },
                "summary": "Trusted execution environments (TEEs) for machine learning accelerators are\nindispensable in secure and efficient ML inference. Optimizing workloads\nthrough state-space exploration for the accelerator architectures improves\nperformance and energy consumption. However, such explorations are expensive\nand slow due to the large search space. Current research has to use fast\nanalytical models that forego critical hardware details and cross-layer\nopportunities unique to the hardware security primitives. While cycle-accurate\nmodels can theoretically reach better designs, their high runtime cost\nrestricts them to a smaller state space.\n  We present Obsidian, an optimization framework for finding the optimal\nmapping from ML kernels to a secure ML accelerator. Obsidian addresses the\nabove challenge by exploring the state space using analytical and\ncycle-accurate models cooperatively. The two main exploration components\ninclude: (1) A secure accelerator analytical model, that includes the effect of\nsecure hardware while traversing the large mapping state space and produce the\nbest m model mappings; (2) A compiler profiling step on a cycle-accurate model,\nthat captures runtime bottlenecks to further improve execution runtime, energy\nand resource utilization and find the optimal model mapping.\n  We compare our results to a baseline secure accelerator, comprising of the\nstate-of-the-art security schemes obtained from guardnn [ 33 ] and sesame [11].\nThe analytical model reduces the inference latency by 20.5% for a cloud and\n8.4% for an edge deployment with an energy improvement of 24% and 19%\nrespectively. The cycle-accurate model, further reduces the latency by 9.1% for\na cloud and 12.2% for an edge with an energy improvement of 13.8% and 13.1%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trusted execution environments (TEEs) for machine learning accelerators are\nindispensable in secure and efficient ML inference. Optimizing workloads\nthrough state-space exploration for the accelerator architectures improves\nperformance and energy consumption. However, such explorations are expensive\nand slow due to the large search space. Current research has to use fast\nanalytical models that forego critical hardware details and cross-layer\nopportunities unique to the hardware security primitives. While cycle-accurate\nmodels can theoretically reach better designs, their high runtime cost\nrestricts them to a smaller state space.\n  We present Obsidian, an optimization framework for finding the optimal\nmapping from ML kernels to a secure ML accelerator. Obsidian addresses the\nabove challenge by exploring the state space using analytical and\ncycle-accurate models cooperatively. The two main exploration components\ninclude: (1) A secure accelerator analytical model, that includes the effect of\nsecure hardware while traversing the large mapping state space and produce the\nbest m model mappings; (2) A compiler profiling step on a cycle-accurate model,\nthat captures runtime bottlenecks to further improve execution runtime, energy\nand resource utilization and find the optimal model mapping.\n  We compare our results to a baseline secure accelerator, comprising of the\nstate-of-the-art security schemes obtained from guardnn [ 33 ] and sesame [11].\nThe analytical model reduces the inference latency by 20.5% for a cloud and\n8.4% for an edge deployment with an energy improvement of 24% and 19%\nrespectively. The cycle-accurate model, further reduces the latency by 9.1% for\na cloud and 12.2% for an edge with an energy improvement of 13.8% and 13.1%."
                },
                "authors": [
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Shijia Wei"
                    },
                    {
                        "name": "Prakash Ramrakhyani"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15778v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15778v3",
                "updated": "2024-09-05T10:30:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    10,
                    30,
                    39,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-28T13:16:41Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    16,
                    41,
                    2,
                    241,
                    0
                ],
                "title": "LogicGame: Benchmarking Rule-Based Reasoning Abilities of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogicGame: Benchmarking Rule-Based Reasoning Abilities of Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated notable capabilities across\nvarious tasks, showcasing complex problem-solving abilities. Understanding and\nexecuting complex rules, along with multi-step planning, are fundamental to\nlogical reasoning and critical for practical LLM agents and decision-making\nsystems. However, evaluating LLMs as effective rule-based executors and\nplanners remains underexplored. In this paper, we introduce LogicGame, a novel\nbenchmark designed to evaluate the comprehensive rule understanding, execution,\nand planning capabilities of LLMs. Unlike traditional benchmarks, LogicGame\nprovides diverse games that contain a series of rules with an initial state,\nrequiring models to comprehend and apply predefined regulations to solve\nproblems. We create simulated scenarios in which models execute or plan\noperations to achieve specific outcomes. These game scenarios are specifically\ndesigned to distinguish logical reasoning from mere knowledge by relying\nexclusively on predefined rules. This separation allows for a pure assessment\nof rule-based reasoning capabilities. The evaluation considers not only final\noutcomes but also intermediate steps, providing a comprehensive assessment of\nmodel performance. Moreover, these intermediate steps are deterministic and can\nbe automatically verified. LogicGame defines game scenarios with varying\ndifficulty levels, from simple rule applications to complex reasoning chains,\nin order to offer a precise evaluation of model performance on rule\nunderstanding and multi-step execution. Utilizing LogicGame, we test various\nLLMs and identify notable shortcomings in their rule-based logical reasoning\nabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated notable capabilities across\nvarious tasks, showcasing complex problem-solving abilities. Understanding and\nexecuting complex rules, along with multi-step planning, are fundamental to\nlogical reasoning and critical for practical LLM agents and decision-making\nsystems. However, evaluating LLMs as effective rule-based executors and\nplanners remains underexplored. In this paper, we introduce LogicGame, a novel\nbenchmark designed to evaluate the comprehensive rule understanding, execution,\nand planning capabilities of LLMs. Unlike traditional benchmarks, LogicGame\nprovides diverse games that contain a series of rules with an initial state,\nrequiring models to comprehend and apply predefined regulations to solve\nproblems. We create simulated scenarios in which models execute or plan\noperations to achieve specific outcomes. These game scenarios are specifically\ndesigned to distinguish logical reasoning from mere knowledge by relying\nexclusively on predefined rules. This separation allows for a pure assessment\nof rule-based reasoning capabilities. The evaluation considers not only final\noutcomes but also intermediate steps, providing a comprehensive assessment of\nmodel performance. Moreover, these intermediate steps are deterministic and can\nbe automatically verified. LogicGame defines game scenarios with varying\ndifficulty levels, from simple rule applications to complex reasoning chains,\nin order to offer a precise evaluation of model performance on rule\nunderstanding and multi-step execution. Utilizing LogicGame, we test various\nLLMs and identify notable shortcomings in their rule-based logical reasoning\nabilities."
                },
                "authors": [
                    {
                        "name": "Jiayi Gui"
                    },
                    {
                        "name": "Yiming Liu"
                    },
                    {
                        "name": "Jiale Cheng"
                    },
                    {
                        "name": "Xiaotao Gu"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15778v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15778v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02795v1",
                "updated": "2024-09-04T15:11:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    15,
                    11,
                    55,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T15:11:55Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    15,
                    11,
                    55,
                    2,
                    248,
                    0
                ],
                "title": "Towards a Unified View of Preference Learning for Large Language Models:\n  A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Unified View of Preference Learning for Large Language Models:\n  A Survey"
                },
                "summary": "Large Language Models (LLMs) exhibit remarkably powerful capabilities. One of\nthe crucial factors to achieve success is aligning the LLM's output with human\npreferences. This alignment process often requires only a small amount of data\nto efficiently enhance the LLM's performance. While effective, research in this\narea spans multiple domains, and the methods involved are relatively complex to\nunderstand. The relationships between different methods have been\nunder-explored, limiting the development of the preference alignment. In light\nof this, we break down the existing popular alignment strategies into different\ncomponents and provide a unified framework to study the current alignment\nstrategies, thereby establishing connections among them. In this survey, we\ndecompose all the strategies in preference learning into four components:\nmodel, data, feedback, and algorithm. This unified view offers an in-depth\nunderstanding of existing alignment algorithms and also opens up possibilities\nto synergize the strengths of different strategies. Furthermore, we present\ndetailed working examples of prevalent existing algorithms to facilitate a\ncomprehensive understanding for the readers. Finally, based on our unified\nperspective, we explore the challenges and future research directions for\naligning large language models with human preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit remarkably powerful capabilities. One of\nthe crucial factors to achieve success is aligning the LLM's output with human\npreferences. This alignment process often requires only a small amount of data\nto efficiently enhance the LLM's performance. While effective, research in this\narea spans multiple domains, and the methods involved are relatively complex to\nunderstand. The relationships between different methods have been\nunder-explored, limiting the development of the preference alignment. In light\nof this, we break down the existing popular alignment strategies into different\ncomponents and provide a unified framework to study the current alignment\nstrategies, thereby establishing connections among them. In this survey, we\ndecompose all the strategies in preference learning into four components:\nmodel, data, feedback, and algorithm. This unified view offers an in-depth\nunderstanding of existing alignment algorithms and also opens up possibilities\nto synergize the strengths of different strategies. Furthermore, we present\ndetailed working examples of prevalent existing algorithms to facilitate a\ncomprehensive understanding for the readers. Finally, based on our unified\nperspective, we explore the challenges and future research directions for\naligning large language models with human preferences."
                },
                "authors": [
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Feifan Song"
                    },
                    {
                        "name": "Yibo Miao"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Zhe Yang"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Helan Hu"
                    },
                    {
                        "name": "Runxin Xu"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Ce Zheng"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Daoguang Zan"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Zeyu Cui"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Lei Sha"
                    },
                    {
                        "name": "Houfeng Wang"
                    },
                    {
                        "name": "Zhifang Sui"
                    },
                    {
                        "name": "Peiyi Wang"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Baobao Chang"
                    }
                ],
                "author_detail": {
                    "name": "Baobao Chang"
                },
                "author": "Baobao Chang",
                "arxiv_comment": "Initial Commit, 21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00105v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00105v2",
                "updated": "2024-09-04T14:40:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    14,
                    40,
                    14,
                    2,
                    248,
                    0
                ],
                "published": "2024-08-27T14:40:16Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    40,
                    16,
                    1,
                    240,
                    0
                ],
                "title": "Negation Blindness in Large Language Models: Unveiling the NO Syndrome\n  in Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Negation Blindness in Large Language Models: Unveiling the NO Syndrome\n  in Image Generation"
                },
                "summary": "Foundational Large Language Models (LLMs) have changed the way we perceive\ntechnology. They have been shown to excel in tasks ranging from poem writing\nand coding to essay generation and puzzle solving. With the incorporation of\nimage generation capability, they have become more comprehensive and versatile\nAI tools. At the same time, researchers are striving to identify the\nlimitations of these tools to improve them further. Currently identified flaws\ninclude hallucination, biases, and bypassing restricted commands to generate\nharmful content. In the present work, we have identified a fundamental\nlimitation related to the image generation ability of LLMs, and termed it The\nNO Syndrome. This negation blindness refers to LLMs inability to correctly\ncomprehend NO related natural language prompts to generate the desired images.\nInterestingly, all tested LLMs including GPT-4, Gemini, and Copilot were found\nto be suffering from this syndrome. To demonstrate the generalization of this\nlimitation, we carried out simulation experiments and conducted entropy-based\nand benchmark statistical analysis tests on various LLMs in multiple languages,\nincluding English, Hindi, and French. We conclude that the NO syndrome is a\nsignificant flaw in current LLMs that needs to be addressed. A related finding\nof this study showed a consistent discrepancy between image and textual\nresponses as a result of this NO syndrome. We posit that the introduction of a\nnegation context-aware reinforcement learning based feedback loop between the\nLLMs textual response and generated image could help ensure the generated text\nis based on both the LLMs correct contextual understanding of the negation\nquery and the generated visual output.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundational Large Language Models (LLMs) have changed the way we perceive\ntechnology. They have been shown to excel in tasks ranging from poem writing\nand coding to essay generation and puzzle solving. With the incorporation of\nimage generation capability, they have become more comprehensive and versatile\nAI tools. At the same time, researchers are striving to identify the\nlimitations of these tools to improve them further. Currently identified flaws\ninclude hallucination, biases, and bypassing restricted commands to generate\nharmful content. In the present work, we have identified a fundamental\nlimitation related to the image generation ability of LLMs, and termed it The\nNO Syndrome. This negation blindness refers to LLMs inability to correctly\ncomprehend NO related natural language prompts to generate the desired images.\nInterestingly, all tested LLMs including GPT-4, Gemini, and Copilot were found\nto be suffering from this syndrome. To demonstrate the generalization of this\nlimitation, we carried out simulation experiments and conducted entropy-based\nand benchmark statistical analysis tests on various LLMs in multiple languages,\nincluding English, Hindi, and French. We conclude that the NO syndrome is a\nsignificant flaw in current LLMs that needs to be addressed. A related finding\nof this study showed a consistent discrepancy between image and textual\nresponses as a result of this NO syndrome. We posit that the introduction of a\nnegation context-aware reinforcement learning based feedback loop between the\nLLMs textual response and generated image could help ensure the generated text\nis based on both the LLMs correct contextual understanding of the negation\nquery and the generated visual output."
                },
                "authors": [
                    {
                        "name": "Mohammad Nadeem"
                    },
                    {
                        "name": "Shahab Saquib Sohail"
                    },
                    {
                        "name": "Erik Cambria"
                    },
                    {
                        "name": "Bjrn W. Schuller"
                    },
                    {
                        "name": "Amir Hussain"
                    }
                ],
                "author_detail": {
                    "name": "Amir Hussain"
                },
                "author": "Amir Hussain",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00105v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00105v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04183v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04183v2",
                "updated": "2024-09-04T14:07:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    14,
                    7,
                    7,
                    2,
                    248,
                    0
                ],
                "published": "2024-07-04T23:05:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    23,
                    5,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "Seeing Like an AI: How LLMs Apply (and Misapply) Wikipedia Neutrality\n  Norms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seeing Like an AI: How LLMs Apply (and Misapply) Wikipedia Neutrality\n  Norms"
                },
                "summary": "Large language models (LLMs) are trained on broad corpora and then used in\ncommunities with specialized norms. Is providing LLMs with community rules\nenough for models to follow these norms? We evaluate LLMs' capacity to detect\n(Task 1) and correct (Task 2) biased Wikipedia edits according to Wikipedia's\nNeutral Point of View (NPOV) policy. LLMs struggled with bias detection,\nachieving only 64% accuracy on a balanced dataset. Models exhibited contrasting\nbiases (some under- and others over-predicted bias), suggesting distinct priors\nabout neutrality. LLMs performed better at generation, removing 79% of words\nremoved by Wikipedia editors. However, LLMs made additional changes beyond\nWikipedia editors' simpler neutralizations, resulting in high-recall but\nlow-precision editing. Interestingly, crowdworkers rated AI rewrites as more\nneutral (70%) and fluent (61%) than Wikipedia-editor rewrites. Qualitative\nanalysis found LLMs sometimes applied NPOV more comprehensively than Wikipedia\neditors but often made extraneous non-NPOV-related changes (such as grammar).\nLLMs may apply rules in ways that resonate with the public but diverge from\ncommunity experts. While potentially effective for generation, LLMs may reduce\neditor agency and increase moderation workload (e.g., verifying additions).\nEven when rules are easy to articulate, having LLMs apply them like community\nmembers may still be difficult.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are trained on broad corpora and then used in\ncommunities with specialized norms. Is providing LLMs with community rules\nenough for models to follow these norms? We evaluate LLMs' capacity to detect\n(Task 1) and correct (Task 2) biased Wikipedia edits according to Wikipedia's\nNeutral Point of View (NPOV) policy. LLMs struggled with bias detection,\nachieving only 64% accuracy on a balanced dataset. Models exhibited contrasting\nbiases (some under- and others over-predicted bias), suggesting distinct priors\nabout neutrality. LLMs performed better at generation, removing 79% of words\nremoved by Wikipedia editors. However, LLMs made additional changes beyond\nWikipedia editors' simpler neutralizations, resulting in high-recall but\nlow-precision editing. Interestingly, crowdworkers rated AI rewrites as more\nneutral (70%) and fluent (61%) than Wikipedia-editor rewrites. Qualitative\nanalysis found LLMs sometimes applied NPOV more comprehensively than Wikipedia\neditors but often made extraneous non-NPOV-related changes (such as grammar).\nLLMs may apply rules in ways that resonate with the public but diverge from\ncommunity experts. While potentially effective for generation, LLMs may reduce\neditor agency and increase moderation workload (e.g., verifying additions).\nEven when rules are easy to articulate, having LLMs apply them like community\nmembers may still be difficult."
                },
                "authors": [
                    {
                        "name": "Joshua Ashkinaze"
                    },
                    {
                        "name": "Ruijia Guan"
                    },
                    {
                        "name": "Laura Kurek"
                    },
                    {
                        "name": "Eytan Adar"
                    },
                    {
                        "name": "Ceren Budak"
                    },
                    {
                        "name": "Eric Gilbert"
                    }
                ],
                "author_detail": {
                    "name": "Eric Gilbert"
                },
                "author": "Eric Gilbert",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04183v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04183v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02727v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02727v2",
                "updated": "2024-09-05T07:17:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    7,
                    17,
                    59,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-04T14:01:48Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    14,
                    1,
                    48,
                    2,
                    248,
                    0
                ],
                "title": "Pooling And Attention: What Are Effective Designs For LLM-Based\n  Embedding Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pooling And Attention: What Are Effective Designs For LLM-Based\n  Embedding Models?"
                },
                "summary": "The significant advancements of Large Language Models (LLMs) in generative\ntasks have led to a growing body of work exploring LLM-based embedding models.\nWhile these models, employing different pooling and attention strategies, have\nachieved state-of-the-art performance on public embedding benchmarks, questions\nstill arise about what constitutes an effective design for LLM-based embedding\nmodels. However, these models are often trained on different datasets, using\ndifferent LLM base models or training settings. Moreover, evaluations on public\nembedding benchmarks often fail to report statistical significance, making it\ndifficult to determine which designs truly contribute to final performance.\nThis complicates the process for practitioners seeking optimal training recipes\nfor LLM-based embedding models. In this study, we conduct a large-scale\nexperiment by training a series of LLM-based embedding models using the same\ntraining data and base model but differing in their pooling and attention\nstrategies. The results show that there is no one-size-fits-all solution: while\nbidirectional attention and an additional trainable pooling layer outperform in\ntext similarity and information retrieval tasks, they do not significantly\nsurpass simpler designs like EOS-last token pooling and default causal\nattention in clustering and classification tasks. Furthermore, we propose a new\npooling strategy, Multi-Layers Trainable Pooling, which transforms the outputs\nof all hidden layers, rather than just the last layer, using a cross-attention\nnetwork. This method proves to be statistically superior in text similarity and\nretrieval tasks compared to existing pooling methods. Overall, this paper sheds\nlight on effective training strategies for LLM-based embedding models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The significant advancements of Large Language Models (LLMs) in generative\ntasks have led to a growing body of work exploring LLM-based embedding models.\nWhile these models, employing different pooling and attention strategies, have\nachieved state-of-the-art performance on public embedding benchmarks, questions\nstill arise about what constitutes an effective design for LLM-based embedding\nmodels. However, these models are often trained on different datasets, using\ndifferent LLM base models or training settings. Moreover, evaluations on public\nembedding benchmarks often fail to report statistical significance, making it\ndifficult to determine which designs truly contribute to final performance.\nThis complicates the process for practitioners seeking optimal training recipes\nfor LLM-based embedding models. In this study, we conduct a large-scale\nexperiment by training a series of LLM-based embedding models using the same\ntraining data and base model but differing in their pooling and attention\nstrategies. The results show that there is no one-size-fits-all solution: while\nbidirectional attention and an additional trainable pooling layer outperform in\ntext similarity and information retrieval tasks, they do not significantly\nsurpass simpler designs like EOS-last token pooling and default causal\nattention in clustering and classification tasks. Furthermore, we propose a new\npooling strategy, Multi-Layers Trainable Pooling, which transforms the outputs\nof all hidden layers, rather than just the last layer, using a cross-attention\nnetwork. This method proves to be statistically superior in text similarity and\nretrieval tasks compared to existing pooling methods. Overall, this paper sheds\nlight on effective training strategies for LLM-based embedding models."
                },
                "authors": [
                    {
                        "name": "Yixuan Tang"
                    },
                    {
                        "name": "Yi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Yang"
                },
                "author": "Yi Yang",
                "arxiv_comment": "https://github.com/yixuantt/PoolingAndAttn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02727v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02727v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02718v1",
                "updated": "2024-09-04T13:54:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    54,
                    38,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T13:54:38Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    54,
                    38,
                    2,
                    248,
                    0
                ],
                "title": "Alignment-Aware Model Extraction Attacks on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment-Aware Model Extraction Attacks on Large Language Models"
                },
                "summary": "Model extraction attacks (MEAs) on large language models (LLMs) have received\nincreasing research attention lately. Existing attack methods on LLMs inherit\nthe extraction strategies from those designed for deep neural networks (DNNs)\nyet neglect the inconsistency of training tasks between MEA and LLMs'\nalignments. As such, they result in poor attack performances. To tackle this\nissue, we present Locality Reinforced Distillation (LoRD), a novel model\nextraction attack algorithm specifically for LLMs. In particular, we design a\npolicy-gradient-style training task, which utilizes victim models' responses as\na signal to guide the crafting of preference for the local model. Theoretical\nanalysis has shown that i) LoRD's convergence procedure in MEAs is consistent\nwith the alignments of LLMs, and ii) LoRD can reduce query complexity while\nmitigating watermark protection through exploration-based stealing. Extensive\nexperiments on domain-specific extractions demonstrate the superiority of our\nmethod by examining the extraction of various state-of-the-art commercial LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model extraction attacks (MEAs) on large language models (LLMs) have received\nincreasing research attention lately. Existing attack methods on LLMs inherit\nthe extraction strategies from those designed for deep neural networks (DNNs)\nyet neglect the inconsistency of training tasks between MEA and LLMs'\nalignments. As such, they result in poor attack performances. To tackle this\nissue, we present Locality Reinforced Distillation (LoRD), a novel model\nextraction attack algorithm specifically for LLMs. In particular, we design a\npolicy-gradient-style training task, which utilizes victim models' responses as\na signal to guide the crafting of preference for the local model. Theoretical\nanalysis has shown that i) LoRD's convergence procedure in MEAs is consistent\nwith the alignments of LLMs, and ii) LoRD can reduce query complexity while\nmitigating watermark protection through exploration-based stealing. Extensive\nexperiments on domain-specific extractions demonstrate the superiority of our\nmethod by examining the extraction of various state-of-the-art commercial LLMs."
                },
                "authors": [
                    {
                        "name": "Zi Liang"
                    },
                    {
                        "name": "Qingqing Ye"
                    },
                    {
                        "name": "Yanyun Wang"
                    },
                    {
                        "name": "Sen Zhang"
                    },
                    {
                        "name": "Yaxin Xiao"
                    },
                    {
                        "name": "Ronghua Li"
                    },
                    {
                        "name": "Jianliang Xu"
                    },
                    {
                        "name": "Haibo Hu"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Hu"
                },
                "author": "Haibo Hu",
                "arxiv_comment": "Source code: https://github.com/liangzid/alignmentExtraction",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02711v1",
                "updated": "2024-09-04T13:49:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    49,
                    19,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T13:49:19Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    49,
                    19,
                    2,
                    248,
                    0
                ],
                "title": "Creating a Gen-AI based Track and Trace Assistant MVP (SuperTracy) for\n  PostNL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating a Gen-AI based Track and Trace Assistant MVP (SuperTracy) for\n  PostNL"
                },
                "summary": "The developments in the field of generative AI has brought a lot of\nopportunities for companies, for instance to improve efficiency in customer\nservice and automating tasks. PostNL, the biggest parcel and E-commerce\ncorporation of the Netherlands wants to use generative AI to enhance the\ncommunication around track and trace of parcels. During the internship a\nMinimal Viable Product (MVP) is created to showcase the value of using\ngenerative AI technologies, to enhance parcel tracking, analyzing the parcel's\njourney and being able to communicate about it in an easy to understand manner.\nThe primary goal was to develop an in-house LLM-based system, reducing\ndependency on external platforms and establishing the feasibility of a\ndedicated generative AI team within the company. This multi-agent LLM based\nsystem aimed to construct parcel journey stories and identify logistical\ndisruptions with heightened efficiency and accuracy. The research involved\ndeploying a sophisticated AI-driven communication system, employing\nRetrieval-Augmented Generation (RAG) for enhanced response precision, and\noptimizing large language models (LLMs) tailored to domain specific tasks.\n  The MVP successfully implemented a multi-agent open-source LLM system, called\nSuperTracy. SuperTracy is capable of autonomously managing a broad spectrum of\nuser inquiries and improving internal knowledge handling. Results and\nevaluation demonstrated technological innovation and feasibility, notably in\ncommunication about the track and trace of a parcel, which exceeded initial\nexpectations. These advancements highlight the potential of AI-driven solutions\nin logistics, suggesting many opportunities for further refinement and broader\nimplementation within PostNL operational framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The developments in the field of generative AI has brought a lot of\nopportunities for companies, for instance to improve efficiency in customer\nservice and automating tasks. PostNL, the biggest parcel and E-commerce\ncorporation of the Netherlands wants to use generative AI to enhance the\ncommunication around track and trace of parcels. During the internship a\nMinimal Viable Product (MVP) is created to showcase the value of using\ngenerative AI technologies, to enhance parcel tracking, analyzing the parcel's\njourney and being able to communicate about it in an easy to understand manner.\nThe primary goal was to develop an in-house LLM-based system, reducing\ndependency on external platforms and establishing the feasibility of a\ndedicated generative AI team within the company. This multi-agent LLM based\nsystem aimed to construct parcel journey stories and identify logistical\ndisruptions with heightened efficiency and accuracy. The research involved\ndeploying a sophisticated AI-driven communication system, employing\nRetrieval-Augmented Generation (RAG) for enhanced response precision, and\noptimizing large language models (LLMs) tailored to domain specific tasks.\n  The MVP successfully implemented a multi-agent open-source LLM system, called\nSuperTracy. SuperTracy is capable of autonomously managing a broad spectrum of\nuser inquiries and improving internal knowledge handling. Results and\nevaluation demonstrated technological innovation and feasibility, notably in\ncommunication about the track and trace of a parcel, which exceeded initial\nexpectations. These advancements highlight the potential of AI-driven solutions\nin logistics, suggesting many opportunities for further refinement and broader\nimplementation within PostNL operational framework."
                },
                "authors": [
                    {
                        "name": "Mohammad Reshadati"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Reshadati"
                },
                "author": "Mohammad Reshadati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02699v1",
                "updated": "2024-09-04T13:35:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    35,
                    15,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T13:35:15Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    35,
                    15,
                    2,
                    248,
                    0
                ],
                "title": "CLDA: Collaborative Learning for Enhanced Unsupervised Domain Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLDA: Collaborative Learning for Enhanced Unsupervised Domain Adaptation"
                },
                "summary": "Unsupervised Domain Adaptation (UDA) endeavors to bridge the gap between a\nmodel trained on a labeled source domain and its deployment in an unlabeled\ntarget domain. However, current high-performance models demand significant\nresources, resulting in prohibitive deployment costs and highlighting the need\nfor small yet effective models. For UDA of lightweight models, Knowledge\nDistillation (KD) in a Teacher-Student framework can be a common approach, but\nwe find that domain shift in UDA leads to a significant increase in non-salient\nparameters in the teacher model, degrading model's generalization ability and\ntransferring misleading information to the student model. Interestingly, we\nobserved that this phenomenon occurs considerably less in the student model.\nDriven by this insight, we introduce Collaborative Learning, a method that\nupdates the teacher's non-salient parameters using the student model and at the\nsame time enhance the student's performance using the updated teacher model.\nExperiments across various tasks and datasets show consistent performance\nimprovements for both student and teacher models. For example, in semantic\nsegmentation, CLDA achieves an improvement of +0.7% mIoU for teacher and +1.4%\nmIoU for student compared to the baseline model in the GTA to Cityscapes. In\nthe Synthia to Cityscapes, it achieves an improvement of +0.8% mIoU for teacher\nand +2.0% mIoU for student.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised Domain Adaptation (UDA) endeavors to bridge the gap between a\nmodel trained on a labeled source domain and its deployment in an unlabeled\ntarget domain. However, current high-performance models demand significant\nresources, resulting in prohibitive deployment costs and highlighting the need\nfor small yet effective models. For UDA of lightweight models, Knowledge\nDistillation (KD) in a Teacher-Student framework can be a common approach, but\nwe find that domain shift in UDA leads to a significant increase in non-salient\nparameters in the teacher model, degrading model's generalization ability and\ntransferring misleading information to the student model. Interestingly, we\nobserved that this phenomenon occurs considerably less in the student model.\nDriven by this insight, we introduce Collaborative Learning, a method that\nupdates the teacher's non-salient parameters using the student model and at the\nsame time enhance the student's performance using the updated teacher model.\nExperiments across various tasks and datasets show consistent performance\nimprovements for both student and teacher models. For example, in semantic\nsegmentation, CLDA achieves an improvement of +0.7% mIoU for teacher and +1.4%\nmIoU for student compared to the baseline model in the GTA to Cityscapes. In\nthe Synthia to Cityscapes, it achieves an improvement of +0.8% mIoU for teacher\nand +2.0% mIoU for student."
                },
                "authors": [
                    {
                        "name": "Minhee Cho"
                    },
                    {
                        "name": "Hyesong Choi"
                    },
                    {
                        "name": "Hayeon Jo"
                    },
                    {
                        "name": "Dongbo Min"
                    }
                ],
                "author_detail": {
                    "name": "Dongbo Min"
                },
                "author": "Dongbo Min",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04160v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04160v2",
                "updated": "2024-09-04T13:29:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    29,
                    56,
                    2,
                    248,
                    0
                ],
                "published": "2024-05-07T09:55:05Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    9,
                    55,
                    5,
                    1,
                    128,
                    0
                ],
                "title": "A Causal Explainable Guardrails for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Causal Explainable Guardrails for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have shown impressive performance in natural\nlanguage tasks, but their outputs can exhibit undesirable attributes or biases.\nExisting methods for steering LLMs toward desired attributes often assume\nunbiased representations and rely solely on steering prompts. However, the\nrepresentations learned from pre-training can introduce semantic biases that\ninfluence the steering process, leading to suboptimal results. We propose\nLLMGuardrail, a novel framework that incorporates causal analysis and\nadversarial learning to obtain unbiased steering representations in LLMs.\nLLMGuardrail systematically identifies and blocks the confounding effects of\nbiases, enabling the extraction of unbiased steering representations.\nAdditionally, it includes an explainable component that provides insights into\nthe alignment between the generated output and the desired direction.\nExperiments demonstrate LLMGuardrail's effectiveness in steering LLMs toward\ndesired attributes while mitigating biases. Our work contributes to the\ndevelopment of safe and reliable LLMs that align with desired attributes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive performance in natural\nlanguage tasks, but their outputs can exhibit undesirable attributes or biases.\nExisting methods for steering LLMs toward desired attributes often assume\nunbiased representations and rely solely on steering prompts. However, the\nrepresentations learned from pre-training can introduce semantic biases that\ninfluence the steering process, leading to suboptimal results. We propose\nLLMGuardrail, a novel framework that incorporates causal analysis and\nadversarial learning to obtain unbiased steering representations in LLMs.\nLLMGuardrail systematically identifies and blocks the confounding effects of\nbiases, enabling the extraction of unbiased steering representations.\nAdditionally, it includes an explainable component that provides insights into\nthe alignment between the generated output and the desired direction.\nExperiments demonstrate LLMGuardrail's effectiveness in steering LLMs toward\ndesired attributes while mitigating biases. Our work contributes to the\ndevelopment of safe and reliable LLMs that align with desired attributes."
                },
                "authors": [
                    {
                        "name": "Zhixuan Chu"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Longfei Li"
                    },
                    {
                        "name": "Zhibo Wang"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04160v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04160v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02691v1",
                "updated": "2024-09-04T13:24:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    24,
                    3,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T13:24:03Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    24,
                    3,
                    2,
                    248,
                    0
                ],
                "title": "LLM-Assisted Visual Analytics: Opportunities and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Assisted Visual Analytics: Opportunities and Challenges"
                },
                "summary": "We explore the integration of large language models (LLMs) into visual\nanalytics (VA) systems to transform their capabilities through intuitive\nnatural language interactions. We survey current research directions in this\nemerging field, examining how LLMs are integrated into data management,\nlanguage interaction, visualisation generation, and language generation\nprocesses. We highlight the new possibilities that LLMs bring to VA, especially\nhow they can change VA processes beyond the usual use cases. We especially\nhighlight building new visualisation-language models, allowing access of a\nbreadth of domain knowledge, multimodal interaction, and opportunities with\nguidance. Finally, we carefully consider the prominent challenges of using\ncurrent LLMs in VA tasks. Our discussions in this paper aim to guide future\nresearchers working on LLM-assisted VA systems and help them navigate common\nobstacles when developing these systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the integration of large language models (LLMs) into visual\nanalytics (VA) systems to transform their capabilities through intuitive\nnatural language interactions. We survey current research directions in this\nemerging field, examining how LLMs are integrated into data management,\nlanguage interaction, visualisation generation, and language generation\nprocesses. We highlight the new possibilities that LLMs bring to VA, especially\nhow they can change VA processes beyond the usual use cases. We especially\nhighlight building new visualisation-language models, allowing access of a\nbreadth of domain knowledge, multimodal interaction, and opportunities with\nguidance. Finally, we carefully consider the prominent challenges of using\ncurrent LLMs in VA tasks. Our discussions in this paper aim to guide future\nresearchers working on LLM-assisted VA systems and help them navigate common\nobstacles when developing these systems."
                },
                "authors": [
                    {
                        "name": "Maeve Hutchinson"
                    },
                    {
                        "name": "Radu Jianu"
                    },
                    {
                        "name": "Aidan Slingsby"
                    },
                    {
                        "name": "Pranava Madhyastha"
                    }
                ],
                "author_detail": {
                    "name": "Pranava Madhyastha"
                },
                "author": "Pranava Madhyastha",
                "arxiv_comment": "Accepted at EG UK Computer Graphics & Visual Computing 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02686v1",
                "updated": "2024-09-04T13:17:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    17,
                    9,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T13:17:09Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    17,
                    9,
                    2,
                    248,
                    0
                ],
                "title": "Deconfounded Causality-aware Parameter-Efficient Fine-Tuning for\n  Problem-Solving Improvement of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deconfounded Causality-aware Parameter-Efficient Fine-Tuning for\n  Problem-Solving Improvement of LLMs"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable efficiency in\ntackling various tasks based on human instructions, but recent studies reveal\nthat these models often fail to achieve satisfactory results on questions\ninvolving reasoning, such as mathematics or physics questions. This phenomenon\nis usually attributed to the uncertainty regarding whether these models could\ngenuinely comprehend the knowledge embedded in the text or merely learn to\nreplicate the token distribution without a true understanding of the content.\nIn this paper, we delve into this problem and aim to enhance the reasoning\ncapabilities of LLMs. First, we investigate if the model has genuine reasoning\ncapabilities by visualizing the text generation process at the attention and\nrepresentation level. Then, we formulate the reasoning process of LLMs into a\ncausal framework, which provides a formal explanation of the problems we\nobserve in the visualization. Finally, building upon this causal framework, we\npropose Deconfounded Causal Adaptation (DCA), a novel parameter-efficient\nfine-tuning (PEFT) method to enhance the model's reasoning capabilities by\nencouraging the model to extract the general problem-solving skills and apply\nthese skills to different questions. Experiments show that our method\noutperforms the baseline consistently across multiple benchmarks, and with only\n1.2M tunable parameters, we achieve better or comparable results to other\nfine-tuning methods. This demonstrates the effectiveness and efficiency of our\nmethod in improving the overall accuracy and reliability of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable efficiency in\ntackling various tasks based on human instructions, but recent studies reveal\nthat these models often fail to achieve satisfactory results on questions\ninvolving reasoning, such as mathematics or physics questions. This phenomenon\nis usually attributed to the uncertainty regarding whether these models could\ngenuinely comprehend the knowledge embedded in the text or merely learn to\nreplicate the token distribution without a true understanding of the content.\nIn this paper, we delve into this problem and aim to enhance the reasoning\ncapabilities of LLMs. First, we investigate if the model has genuine reasoning\ncapabilities by visualizing the text generation process at the attention and\nrepresentation level. Then, we formulate the reasoning process of LLMs into a\ncausal framework, which provides a formal explanation of the problems we\nobserve in the visualization. Finally, building upon this causal framework, we\npropose Deconfounded Causal Adaptation (DCA), a novel parameter-efficient\nfine-tuning (PEFT) method to enhance the model's reasoning capabilities by\nencouraging the model to extract the general problem-solving skills and apply\nthese skills to different questions. Experiments show that our method\noutperforms the baseline consistently across multiple benchmarks, and with only\n1.2M tunable parameters, we achieve better or comparable results to other\nfine-tuning methods. This demonstrates the effectiveness and efficiency of our\nmethod in improving the overall accuracy and reliability of LLMs."
                },
                "authors": [
                    {
                        "name": "Ruoyu Wang"
                    },
                    {
                        "name": "Xiaoxuan Li"
                    },
                    {
                        "name": "Lina Yao"
                    }
                ],
                "author_detail": {
                    "name": "Lina Yao"
                },
                "author": "Lina Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11850v2",
                "updated": "2024-09-04T13:14:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    14,
                    57,
                    2,
                    248,
                    0
                ],
                "published": "2024-08-13T08:32:06Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    8,
                    32,
                    6,
                    1,
                    226,
                    0
                ],
                "title": "Parallel Speculative Decoding with Adaptive Draft Length",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Speculative Decoding with Adaptive Draft Length"
                },
                "summary": "Speculative decoding (SD), where an extra draft model is employed to provide\nmultiple \\textit{draft} tokens first and then the original target model\nverifies these tokens in parallel, has shown great power for LLM inference\nacceleration. However, existing SD methods suffer from the mutual waiting\nproblem, i.e., the target model gets stuck when the draft model is\n\\textit{guessing} tokens, and vice versa. This problem is directly incurred by\nthe asynchronous execution of the draft model and the target model, and is\nexacerbated due to the fixed draft length in speculative decoding. To address\nthese challenges, we propose a conceptually simple, flexible, and general\nframework to boost speculative decoding, namely \\textbf{P}arallel\nsp\\textbf{E}culative decoding with \\textbf{A}daptive d\\textbf{R}aft\n\\textbf{L}ength (PEARL). Specifically, PEARL proposes \\textit{pre-verify} to\nverify the first draft token in advance during the drafting phase, and\n\\textit{post-verify} to generate more draft tokens during the verification\nphase. PEARL parallels the drafting phase and the verification phase via\napplying the two strategies, and achieves adaptive draft length for different\nscenarios, which effectively alleviates the mutual waiting problem. Moreover,\nwe theoretically demonstrate that the mean accepted tokens of PEARL is more\nthan existing \\textit{draft-then-verify} works. Experiments on various text\ngeneration benchmarks demonstrate the effectiveness of our \\name, leading to a\nsuperior speedup performance up to \\textbf{3.79$\\times$} and\n\\textbf{1.52$\\times$}, compared to auto-regressive decoding and vanilla\nspeculative decoding, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD), where an extra draft model is employed to provide\nmultiple \\textit{draft} tokens first and then the original target model\nverifies these tokens in parallel, has shown great power for LLM inference\nacceleration. However, existing SD methods suffer from the mutual waiting\nproblem, i.e., the target model gets stuck when the draft model is\n\\textit{guessing} tokens, and vice versa. This problem is directly incurred by\nthe asynchronous execution of the draft model and the target model, and is\nexacerbated due to the fixed draft length in speculative decoding. To address\nthese challenges, we propose a conceptually simple, flexible, and general\nframework to boost speculative decoding, namely \\textbf{P}arallel\nsp\\textbf{E}culative decoding with \\textbf{A}daptive d\\textbf{R}aft\n\\textbf{L}ength (PEARL). Specifically, PEARL proposes \\textit{pre-verify} to\nverify the first draft token in advance during the drafting phase, and\n\\textit{post-verify} to generate more draft tokens during the verification\nphase. PEARL parallels the drafting phase and the verification phase via\napplying the two strategies, and achieves adaptive draft length for different\nscenarios, which effectively alleviates the mutual waiting problem. Moreover,\nwe theoretically demonstrate that the mean accepted tokens of PEARL is more\nthan existing \\textit{draft-then-verify} works. Experiments on various text\ngeneration benchmarks demonstrate the effectiveness of our \\name, leading to a\nsuperior speedup performance up to \\textbf{3.79$\\times$} and\n\\textbf{1.52$\\times$}, compared to auto-regressive decoding and vanilla\nspeculative decoding, respectively."
                },
                "authors": [
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Yun Li"
                    },
                    {
                        "name": "Qitan Lv"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Winston Hu"
                    }
                ],
                "author_detail": {
                    "name": "Winston Hu"
                },
                "author": "Winston Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11614v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11614v2",
                "updated": "2024-09-04T13:02:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    2,
                    15,
                    2,
                    248,
                    0
                ],
                "published": "2024-05-19T17:09:43Z",
                "published_parsed": [
                    2024,
                    5,
                    19,
                    17,
                    9,
                    43,
                    6,
                    140,
                    0
                ],
                "title": "Nickel and Diming Your GAN: A Dual-Method Approach to Enhancing GAN\n  Efficiency via Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nickel and Diming Your GAN: A Dual-Method Approach to Enhancing GAN\n  Efficiency via Knowledge Distillation"
                },
                "summary": "In this paper, we address the challenge of compressing generative adversarial\nnetworks (GANs) for deployment in resource-constrained environments by\nproposing two novel methodologies: Distribution Matching for Efficient\ncompression (DiME) and Network Interactive Compression via Knowledge Exchange\nand Learning (NICKEL). DiME employs foundation models as embedding kernels for\nefficient distribution matching, leveraging maximum mean discrepancy to\nfacilitate effective knowledge distillation. Simultaneously, NICKEL employs an\ninteractive compression method that enhances the communication between the\nstudent generator and discriminator, achieving a balanced and stable\ncompression process. Our comprehensive evaluation on the StyleGAN2 architecture\nwith the FFHQ dataset shows the effectiveness of our approach, with NICKEL &\nDiME achieving FID scores of 10.45 and 15.93 at compression rates of 95.73% and\n98.92%, respectively. Remarkably, our methods sustain generative quality even\nat an extreme compression rate of 99.69%, surpassing the previous\nstate-of-the-art performance by a large margin. These findings not only\ndemonstrate our methodologies' capacity to significantly lower GANs'\ncomputational demands but also pave the way for deploying high-quality GAN\nmodels in settings with limited resources. Our code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we address the challenge of compressing generative adversarial\nnetworks (GANs) for deployment in resource-constrained environments by\nproposing two novel methodologies: Distribution Matching for Efficient\ncompression (DiME) and Network Interactive Compression via Knowledge Exchange\nand Learning (NICKEL). DiME employs foundation models as embedding kernels for\nefficient distribution matching, leveraging maximum mean discrepancy to\nfacilitate effective knowledge distillation. Simultaneously, NICKEL employs an\ninteractive compression method that enhances the communication between the\nstudent generator and discriminator, achieving a balanced and stable\ncompression process. Our comprehensive evaluation on the StyleGAN2 architecture\nwith the FFHQ dataset shows the effectiveness of our approach, with NICKEL &\nDiME achieving FID scores of 10.45 and 15.93 at compression rates of 95.73% and\n98.92%, respectively. Remarkably, our methods sustain generative quality even\nat an extreme compression rate of 99.69%, surpassing the previous\nstate-of-the-art performance by a large margin. These findings not only\ndemonstrate our methodologies' capacity to significantly lower GANs'\ncomputational demands but also pave the way for deploying high-quality GAN\nmodels in settings with limited resources. Our code will be released soon."
                },
                "authors": [
                    {
                        "name": "Sangyeop Yeo"
                    },
                    {
                        "name": "Yoojin Jang"
                    },
                    {
                        "name": "Jaejun Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Jaejun Yoo"
                },
                "author": "Jaejun Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11614v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11614v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09979v2",
                "updated": "2024-09-04T12:33:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    12,
                    33,
                    24,
                    2,
                    248,
                    0
                ],
                "published": "2024-06-14T12:41:07Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    12,
                    41,
                    7,
                    4,
                    166,
                    0
                ],
                "title": "HIRO: Hierarchical Information Retrieval Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HIRO: Hierarchical Information Retrieval Optimization"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has revolutionized natural language\nprocessing by dynamically integrating external knowledge into Large Language\nModels (LLMs), addressing their limitation of static training datasets. Recent\nimplementations of RAG leverage hierarchical data structures, which organize\ndocuments at various levels of summarization and information density. This\ncomplexity, however, can cause LLMs to \"choke\" on information overload,\nnecessitating more sophisticated querying mechanisms. In this context, we\nintroduce Hierarchical Information Retrieval Optimization (HIRO), a novel\nquerying approach that employs a Depth-First Search (DFS)-based recursive\nsimilarity score calculation and branch pruning. This method uniquely minimizes\nthe context delivered to the LLM without informational loss, effectively\nmanaging the challenge of excessive data. HIRO's refined approach is validated\nby a 10.85% improvement in performance on the NarrativeQA dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has revolutionized natural language\nprocessing by dynamically integrating external knowledge into Large Language\nModels (LLMs), addressing their limitation of static training datasets. Recent\nimplementations of RAG leverage hierarchical data structures, which organize\ndocuments at various levels of summarization and information density. This\ncomplexity, however, can cause LLMs to \"choke\" on information overload,\nnecessitating more sophisticated querying mechanisms. In this context, we\nintroduce Hierarchical Information Retrieval Optimization (HIRO), a novel\nquerying approach that employs a Depth-First Search (DFS)-based recursive\nsimilarity score calculation and branch pruning. This method uniquely minimizes\nthe context delivered to the LLM without informational loss, effectively\nmanaging the challenge of excessive data. HIRO's refined approach is validated\nby a 10.85% improvement in performance on the NarrativeQA dataset."
                },
                "authors": [
                    {
                        "name": "Krish Goel"
                    },
                    {
                        "name": "Mahek Chandak"
                    }
                ],
                "author_detail": {
                    "name": "Mahek Chandak"
                },
                "author": "Mahek Chandak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12333v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12333v2",
                "updated": "2024-09-04T12:00:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    12,
                    0,
                    25,
                    2,
                    248,
                    0
                ],
                "published": "2024-08-22T12:21:22Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    21,
                    22,
                    3,
                    235,
                    0
                ],
                "title": "Graph Retrieval Augmented Trustworthiness Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Retrieval Augmented Trustworthiness Reasoning"
                },
                "summary": "Trustworthiness reasoning is crucial in multiplayer games with incomplete\ninformation, enabling agents to identify potential allies and adversaries,\nthereby enhancing reasoning and decision-making processes. Traditional\napproaches relying on pre-trained models necessitate extensive domain-specific\ndata and considerable reward feedback, with their lack of real-time\nadaptability hindering their effectiveness in dynamic environments. In this\npaper, we introduce the Graph Retrieval Augmented Reasoning (GRATR) framework,\nleveraging the Retrieval-Augmented Generation (RAG) technique to bolster\ntrustworthiness reasoning in agents. GRATR constructs a dynamic trustworthiness\ngraph, updating it in real-time with evidential information, and retrieves\nrelevant trust data to augment the reasoning capabilities of Large Language\nModels (LLMs). We validate our approach through experiments on the multiplayer\ngame \"Werewolf,\" comparing GRATR against baseline LLM and LLM enhanced with\nNative RAG and Rerank RAG. Our results demonstrate that GRATR surpasses the\nbaseline methods by over 30\\% in winning rate, with superior reasoning\nperformance. Moreover, GRATR effectively mitigates LLM hallucinations, such as\nidentity and objective amnesia, and crucially, it renders the reasoning process\nmore transparent and traceable through the use of the trustworthiness graph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthiness reasoning is crucial in multiplayer games with incomplete\ninformation, enabling agents to identify potential allies and adversaries,\nthereby enhancing reasoning and decision-making processes. Traditional\napproaches relying on pre-trained models necessitate extensive domain-specific\ndata and considerable reward feedback, with their lack of real-time\nadaptability hindering their effectiveness in dynamic environments. In this\npaper, we introduce the Graph Retrieval Augmented Reasoning (GRATR) framework,\nleveraging the Retrieval-Augmented Generation (RAG) technique to bolster\ntrustworthiness reasoning in agents. GRATR constructs a dynamic trustworthiness\ngraph, updating it in real-time with evidential information, and retrieves\nrelevant trust data to augment the reasoning capabilities of Large Language\nModels (LLMs). We validate our approach through experiments on the multiplayer\ngame \"Werewolf,\" comparing GRATR against baseline LLM and LLM enhanced with\nNative RAG and Rerank RAG. Our results demonstrate that GRATR surpasses the\nbaseline methods by over 30\\% in winning rate, with superior reasoning\nperformance. Moreover, GRATR effectively mitigates LLM hallucinations, such as\nidentity and objective amnesia, and crucially, it renders the reasoning process\nmore transparent and traceable through the use of the trustworthiness graph."
                },
                "authors": [
                    {
                        "name": "Ying Zhu"
                    },
                    {
                        "name": "Shengchang Li"
                    },
                    {
                        "name": "Ziqian Kong"
                    },
                    {
                        "name": "Peilan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Peilan Xu"
                },
                "author": "Peilan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12333v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12333v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02636v1",
                "updated": "2024-09-04T11:59:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    11,
                    59,
                    53,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T11:59:53Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    11,
                    59,
                    53,
                    2,
                    248,
                    0
                ],
                "title": "Mamba as a motion encoder for robotic imitation learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mamba as a motion encoder for robotic imitation learning"
                },
                "summary": "Recent advancements in imitation learning, particularly with the integration\nof LLM techniques, are set to significantly improve robots' dexterity and\nadaptability. In this study, we propose using Mamba, a state-of-the-art\narchitecture with potential applications in LLMs, for robotic imitation\nlearning, highlighting its ability to function as an encoder that effectively\ncaptures contextual information. By reducing the dimensionality of the state\nspace, Mamba operates similarly to an autoencoder. It effectively compresses\nthe sequential information into state variables while preserving the essential\ntemporal dynamics necessary for accurate motion prediction. Experimental\nresults in tasks such as cup placing and case loading demonstrate that despite\nexhibiting higher estimation errors, Mamba achieves superior success rates\ncompared to Transformers in practical task execution. This performance is\nattributed to Mamba's structure, which encompasses the state space model.\nAdditionally, the study investigates Mamba's capacity to serve as a real-time\nmotion generator with a limited amount of training data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in imitation learning, particularly with the integration\nof LLM techniques, are set to significantly improve robots' dexterity and\nadaptability. In this study, we propose using Mamba, a state-of-the-art\narchitecture with potential applications in LLMs, for robotic imitation\nlearning, highlighting its ability to function as an encoder that effectively\ncaptures contextual information. By reducing the dimensionality of the state\nspace, Mamba operates similarly to an autoencoder. It effectively compresses\nthe sequential information into state variables while preserving the essential\ntemporal dynamics necessary for accurate motion prediction. Experimental\nresults in tasks such as cup placing and case loading demonstrate that despite\nexhibiting higher estimation errors, Mamba achieves superior success rates\ncompared to Transformers in practical task execution. This performance is\nattributed to Mamba's structure, which encompasses the state space model.\nAdditionally, the study investigates Mamba's capacity to serve as a real-time\nmotion generator with a limited amount of training data."
                },
                "authors": [
                    {
                        "name": "Toshiaki Tsuji"
                    }
                ],
                "author_detail": {
                    "name": "Toshiaki Tsuji"
                },
                "author": "Toshiaki Tsuji",
                "arxiv_comment": "7 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.07107v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.07107v4",
                "updated": "2024-09-04T11:39:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    11,
                    39,
                    56,
                    2,
                    248,
                    0
                ],
                "published": "2023-08-14T12:47:22Z",
                "published_parsed": [
                    2023,
                    8,
                    14,
                    12,
                    47,
                    22,
                    0,
                    226,
                    0
                ],
                "title": "Large Language Models for Information Retrieval: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Information Retrieval: A Survey"
                },
                "summary": "As a primary means of information acquisition, information retrieval (IR)\nsystems, such as search engines, have integrated themselves into our daily\nlives. These systems also serve as components of dialogue, question-answering,\nand recommender systems. The trajectory of IR has evolved dynamically from its\norigins in term-based methods to its integration with advanced neural models.\nWhile the neural models excel at capturing complex contextual signals and\nsemantic nuances, thereby reshaping the IR landscape, they still face\nchallenges such as data scarcity, interpretability, and the generation of\ncontextually plausible yet potentially inaccurate responses. This evolution\nrequires a combination of both traditional methods (such as term-based sparse\nretrieval methods with rapid response) and modern neural architectures (such as\nlanguage models with powerful language understanding capacity). Meanwhile, the\nemergence of large language models (LLMs), typified by ChatGPT and GPT-4, has\nrevolutionized natural language processing due to their remarkable language\nunderstanding, generation, generalization, and reasoning abilities.\nConsequently, recent research has sought to leverage LLMs to improve IR\nsystems. Given the rapid evolution of this research trajectory, it is necessary\nto consolidate existing methodologies and provide nuanced insights through a\ncomprehensive overview. In this survey, we delve into the confluence of LLMs\nand IR systems, including crucial aspects such as query rewriters, retrievers,\nrerankers, and readers. Additionally, we explore promising directions, such as\nsearch agents, within this expanding field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a primary means of information acquisition, information retrieval (IR)\nsystems, such as search engines, have integrated themselves into our daily\nlives. These systems also serve as components of dialogue, question-answering,\nand recommender systems. The trajectory of IR has evolved dynamically from its\norigins in term-based methods to its integration with advanced neural models.\nWhile the neural models excel at capturing complex contextual signals and\nsemantic nuances, thereby reshaping the IR landscape, they still face\nchallenges such as data scarcity, interpretability, and the generation of\ncontextually plausible yet potentially inaccurate responses. This evolution\nrequires a combination of both traditional methods (such as term-based sparse\nretrieval methods with rapid response) and modern neural architectures (such as\nlanguage models with powerful language understanding capacity). Meanwhile, the\nemergence of large language models (LLMs), typified by ChatGPT and GPT-4, has\nrevolutionized natural language processing due to their remarkable language\nunderstanding, generation, generalization, and reasoning abilities.\nConsequently, recent research has sought to leverage LLMs to improve IR\nsystems. Given the rapid evolution of this research trajectory, it is necessary\nto consolidate existing methodologies and provide nuanced insights through a\ncomprehensive overview. In this survey, we delve into the confluence of LLMs\nand IR systems, including crucial aspects such as query rewriters, retrievers,\nrerankers, and readers. Additionally, we explore promising directions, such as\nsearch agents, within this expanding field."
                },
                "authors": [
                    {
                        "name": "Yutao Zhu"
                    },
                    {
                        "name": "Huaying Yuan"
                    },
                    {
                        "name": "Shuting Wang"
                    },
                    {
                        "name": "Jiongnan Liu"
                    },
                    {
                        "name": "Wenhan Liu"
                    },
                    {
                        "name": "Chenlong Deng"
                    },
                    {
                        "name": "Haonan Chen"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "updated to version 3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.07107v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.07107v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07569v2",
                "updated": "2024-09-04T11:34:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    11,
                    34,
                    33,
                    2,
                    248,
                    0
                ],
                "published": "2024-04-11T08:57:48Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    8,
                    57,
                    48,
                    3,
                    102,
                    0
                ],
                "title": "Can Vehicle Motion Planning Generalize to Realistic Long-tail Scenarios?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Vehicle Motion Planning Generalize to Realistic Long-tail Scenarios?"
                },
                "summary": "Real-world autonomous driving systems must make safe decisions in the face of\nrare and diverse traffic scenarios. Current state-of-the-art planners are\nmostly evaluated on real-world datasets like nuScenes (open-loop) or nuPlan\n(closed-loop). In particular, nuPlan seems to be an expressive evaluation\nmethod since it is based on real-world data and closed-loop, yet it mostly\ncovers basic driving scenarios. This makes it difficult to judge a planner's\ncapabilities to generalize to rarely-seen situations. Therefore, we propose a\nnovel closed-loop benchmark interPlan containing several edge cases and\nchallenging driving scenarios. We assess existing state-of-the-art planners on\nour benchmark and show that neither rule-based nor learning-based planners can\nsafely navigate the interPlan scenarios. A recently evolving direction is the\nusage of foundation models like large language models (LLM) to handle\ngeneralization. We evaluate an LLM-only planner and introduce a novel hybrid\nplanner that combines an LLM-based behavior planner with a rule-based motion\nplanner that achieves state-of-the-art performance on our benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world autonomous driving systems must make safe decisions in the face of\nrare and diverse traffic scenarios. Current state-of-the-art planners are\nmostly evaluated on real-world datasets like nuScenes (open-loop) or nuPlan\n(closed-loop). In particular, nuPlan seems to be an expressive evaluation\nmethod since it is based on real-world data and closed-loop, yet it mostly\ncovers basic driving scenarios. This makes it difficult to judge a planner's\ncapabilities to generalize to rarely-seen situations. Therefore, we propose a\nnovel closed-loop benchmark interPlan containing several edge cases and\nchallenging driving scenarios. We assess existing state-of-the-art planners on\nour benchmark and show that neither rule-based nor learning-based planners can\nsafely navigate the interPlan scenarios. A recently evolving direction is the\nusage of foundation models like large language models (LLM) to handle\ngeneralization. We evaluate an LLM-only planner and introduce a novel hybrid\nplanner that combines an LLM-based behavior planner with a rule-based motion\nplanner that achieves state-of-the-art performance on our benchmark."
                },
                "authors": [
                    {
                        "name": "Marcel Hallgarten"
                    },
                    {
                        "name": "Julian Zapata"
                    },
                    {
                        "name": "Martin Stoll"
                    },
                    {
                        "name": "Katrin Renz"
                    },
                    {
                        "name": "Andreas Zell"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Zell"
                },
                "author": "Andreas Zell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02617v1",
                "updated": "2024-09-04T11:19:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    11,
                    19,
                    17,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T11:19:17Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    11,
                    19,
                    17,
                    2,
                    248,
                    0
                ],
                "title": "PUB: Plot Understanding Benchmark and Dataset for Evaluating Large\n  Language Models on Synthetic Visual Data Interpretation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PUB: Plot Understanding Benchmark and Dataset for Evaluating Large\n  Language Models on Synthetic Visual Data Interpretation"
                },
                "summary": "The ability of large language models (LLMs) to interpret visual\nrepresentations of data is crucial for advancing their application in data\nanalysis and decision-making processes. This paper presents a novel synthetic\ndataset designed to evaluate the proficiency of LLMs in interpreting various\nforms of data visualizations, including plots like time series, histograms,\nviolins, boxplots, and clusters. Our dataset is generated using controlled\nparameters to ensure comprehensive coverage of potential real-world scenarios.\nWe employ multimodal text prompts with questions related to visual data in\nimages to benchmark several state-of-the-art models like ChatGPT or Gemini,\nassessing their understanding and interpretative accuracy.\n  To ensure data integrity, our benchmark dataset is generated automatically,\nmaking it entirely new and free from prior exposure to the models being tested.\nThis strategy allows us to evaluate the models' ability to truly interpret and\nunderstand the data, eliminating possibility of pre-learned responses, and\nallowing for an unbiased evaluation of the models' capabilities. We also\nintroduce quantitative metrics to assess the performance of the models,\nproviding a robust and comprehensive evaluation tool.\n  Benchmarking several state-of-the-art LLMs with this dataset reveals varying\ndegrees of success, highlighting specific strengths and weaknesses in\ninterpreting diverse types of visual data. The results provide valuable\ninsights into the current capabilities of LLMs and identify key areas for\nimprovement. This work establishes a foundational benchmark for future research\nand development aimed at enhancing the visual interpretative abilities of\nlanguage models. In the future, improved LLMs with robust visual interpretation\nskills can significantly aid in automated data analysis, scientific research,\neducational tools, and business intelligence applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability of large language models (LLMs) to interpret visual\nrepresentations of data is crucial for advancing their application in data\nanalysis and decision-making processes. This paper presents a novel synthetic\ndataset designed to evaluate the proficiency of LLMs in interpreting various\nforms of data visualizations, including plots like time series, histograms,\nviolins, boxplots, and clusters. Our dataset is generated using controlled\nparameters to ensure comprehensive coverage of potential real-world scenarios.\nWe employ multimodal text prompts with questions related to visual data in\nimages to benchmark several state-of-the-art models like ChatGPT or Gemini,\nassessing their understanding and interpretative accuracy.\n  To ensure data integrity, our benchmark dataset is generated automatically,\nmaking it entirely new and free from prior exposure to the models being tested.\nThis strategy allows us to evaluate the models' ability to truly interpret and\nunderstand the data, eliminating possibility of pre-learned responses, and\nallowing for an unbiased evaluation of the models' capabilities. We also\nintroduce quantitative metrics to assess the performance of the models,\nproviding a robust and comprehensive evaluation tool.\n  Benchmarking several state-of-the-art LLMs with this dataset reveals varying\ndegrees of success, highlighting specific strengths and weaknesses in\ninterpreting diverse types of visual data. The results provide valuable\ninsights into the current capabilities of LLMs and identify key areas for\nimprovement. This work establishes a foundational benchmark for future research\nand development aimed at enhancing the visual interpretative abilities of\nlanguage models. In the future, improved LLMs with robust visual interpretation\nskills can significantly aid in automated data analysis, scientific research,\neducational tools, and business intelligence applications."
                },
                "authors": [
                    {
                        "name": "Aneta Pawelec"
                    },
                    {
                        "name": "Victoria Sara Wesoowska"
                    },
                    {
                        "name": "Zuzanna Bczek"
                    },
                    {
                        "name": "Piotr Sankowski"
                    }
                ],
                "author_detail": {
                    "name": "Piotr Sankowski"
                },
                "author": "Piotr Sankowski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00960v2",
                "updated": "2024-09-04T10:58:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    58,
                    26,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-02T06:01:20Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    6,
                    1,
                    20,
                    0,
                    246,
                    0
                ],
                "title": "Unveiling the Vulnerability of Private Fine-Tuning in Split-Based\n  Frameworks for Large Language Models: A Bidirectionally Enhanced Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Vulnerability of Private Fine-Tuning in Split-Based\n  Frameworks for Large Language Models: A Bidirectionally Enhanced Attack"
                },
                "summary": "Recent advancements in pre-trained large language models (LLMs) have\nsignificantly influenced various domains. Adapting these models for specific\ntasks often involves fine-tuning (FT) with private, domain-specific data.\nHowever, privacy concerns keep this data undisclosed, and the computational\ndemands for deploying LLMs pose challenges for resource-limited data holders.\nThis has sparked interest in split learning (SL), a Model-as-a-Service (MaaS)\nparadigm that divides LLMs into smaller segments for distributed training and\ndeployment, transmitting only intermediate activations instead of raw data. SL\nhas garnered substantial interest in both industry and academia as it aims to\nbalance user data privacy, model ownership, and resource challenges in the\nprivate fine-tuning of LLMs. Despite its privacy claims, this paper reveals\nsignificant vulnerabilities arising from the combination of SL and LLM-FT: the\nNot-too-far property of fine-tuning and the auto-regressive nature of LLMs.\nExploiting these vulnerabilities, we propose Bidirectional Semi-white-box\nReconstruction (BiSR), the first data reconstruction attack (DRA) designed to\ntarget both the forward and backward propagation processes of SL. BiSR utilizes\npre-trained weights as prior knowledge, combining a learning-based attack with\na bidirectional optimization-based approach for highly effective data\nreconstruction. Additionally, it incorporates a Noise-adaptive Mixture of\nExperts (NaMoE) model to enhance reconstruction performance under perturbation.\nWe conducted systematic experiments on various mainstream LLMs and different\nsetups, empirically demonstrating BiSR's state-of-the-art performance.\nFurthermore, we thoroughly examined three representative defense mechanisms,\nshowcasing our method's capability to reconstruct private data even in the\npresence of these defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in pre-trained large language models (LLMs) have\nsignificantly influenced various domains. Adapting these models for specific\ntasks often involves fine-tuning (FT) with private, domain-specific data.\nHowever, privacy concerns keep this data undisclosed, and the computational\ndemands for deploying LLMs pose challenges for resource-limited data holders.\nThis has sparked interest in split learning (SL), a Model-as-a-Service (MaaS)\nparadigm that divides LLMs into smaller segments for distributed training and\ndeployment, transmitting only intermediate activations instead of raw data. SL\nhas garnered substantial interest in both industry and academia as it aims to\nbalance user data privacy, model ownership, and resource challenges in the\nprivate fine-tuning of LLMs. Despite its privacy claims, this paper reveals\nsignificant vulnerabilities arising from the combination of SL and LLM-FT: the\nNot-too-far property of fine-tuning and the auto-regressive nature of LLMs.\nExploiting these vulnerabilities, we propose Bidirectional Semi-white-box\nReconstruction (BiSR), the first data reconstruction attack (DRA) designed to\ntarget both the forward and backward propagation processes of SL. BiSR utilizes\npre-trained weights as prior knowledge, combining a learning-based attack with\na bidirectional optimization-based approach for highly effective data\nreconstruction. Additionally, it incorporates a Noise-adaptive Mixture of\nExperts (NaMoE) model to enhance reconstruction performance under perturbation.\nWe conducted systematic experiments on various mainstream LLMs and different\nsetups, empirically demonstrating BiSR's state-of-the-art performance.\nFurthermore, we thoroughly examined three representative defense mechanisms,\nshowcasing our method's capability to reconstruct private data even in the\npresence of these defenses."
                },
                "authors": [
                    {
                        "name": "Guanzhong Chen"
                    },
                    {
                        "name": "Zhenghan Qin"
                    },
                    {
                        "name": "Mingxin Yang"
                    },
                    {
                        "name": "Yajie Zhou"
                    },
                    {
                        "name": "Tao Fan"
                    },
                    {
                        "name": "Tianyu Du"
                    },
                    {
                        "name": "Zenglin Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zenglin Xu"
                },
                "author": "Zenglin Xu",
                "arxiv_comment": "ACM Conference on Computer and Communications Security 2024 (CCS 24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02604v1",
                "updated": "2024-09-04T10:37:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    37,
                    44,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T10:37:44Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    37,
                    44,
                    2,
                    248,
                    0
                ],
                "title": "Hypothesizing Missing Causal Variables with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hypothesizing Missing Causal Variables with LLMs"
                },
                "summary": "Scientific discovery is a catalyst for human intellectual advances, driven by\nthe cycle of hypothesis generation, experimental design, data evaluation, and\niterative assumption refinement. This process, while crucial, is expensive and\nheavily dependent on the domain knowledge of scientists to generate hypotheses\nand navigate the scientific cycle. Central to this is causality, the ability to\nestablish the relationship between the cause and the effect. Motivated by the\nscientific discovery process, in this work, we formulate a novel task where the\ninput is a partial causal graph with missing variables, and the output is a\nhypothesis about the missing variables to complete the partial graph. We design\na benchmark with varying difficulty levels and knowledge assumptions about the\ncausal graph. With the growing interest in using Large Language Models (LLMs)\nto assist in scientific discovery, we benchmark open-source and closed models\non our testbed. We show the strong ability of LLMs to hypothesize the mediation\nvariables between a cause and its effect. In contrast, they underperform in\nhypothesizing the cause and effect variables themselves. We also observe\nsurprising results where some of the open-source models outperform the closed\nGPT-4 model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific discovery is a catalyst for human intellectual advances, driven by\nthe cycle of hypothesis generation, experimental design, data evaluation, and\niterative assumption refinement. This process, while crucial, is expensive and\nheavily dependent on the domain knowledge of scientists to generate hypotheses\nand navigate the scientific cycle. Central to this is causality, the ability to\nestablish the relationship between the cause and the effect. Motivated by the\nscientific discovery process, in this work, we formulate a novel task where the\ninput is a partial causal graph with missing variables, and the output is a\nhypothesis about the missing variables to complete the partial graph. We design\na benchmark with varying difficulty levels and knowledge assumptions about the\ncausal graph. With the growing interest in using Large Language Models (LLMs)\nto assist in scientific discovery, we benchmark open-source and closed models\non our testbed. We show the strong ability of LLMs to hypothesize the mediation\nvariables between a cause and its effect. In contrast, they underperform in\nhypothesizing the cause and effect variables themselves. We also observe\nsurprising results where some of the open-source models outperform the closed\nGPT-4 model."
                },
                "authors": [
                    {
                        "name": "Ivaxi Sheth"
                    },
                    {
                        "name": "Sahar Abdelnabi"
                    },
                    {
                        "name": "Mario Fritz"
                    }
                ],
                "author_detail": {
                    "name": "Mario Fritz"
                },
                "author": "Mario Fritz",
                "arxiv_comment": "Code - https://github.com/ivaxi0s/hypothesizing-causal-variable-llm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02601v1",
                "updated": "2024-09-04T10:33:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    33,
                    37,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T10:33:37Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    33,
                    37,
                    2,
                    248,
                    0
                ],
                "title": "ChatGPT vs Social Surveys: Probing the Objective and Subjective Human\n  Society",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatGPT vs Social Surveys: Probing the Objective and Subjective Human\n  Society"
                },
                "summary": "The extent to which Large Language Models (LLMs) can simulate the\ndata-generating process for social surveys remains unclear. Current research\nhas not thoroughly assessed potential biases in the sociodemographic population\nrepresented within the language model's framework. Additionally, the subjective\nworlds of LLMs often show inconsistencies in how closely their responses match\nthose of groups of human respondents. In this paper, we used ChatGPT-3.5 to\nsimulate the sampling process and generated six socioeconomic characteristics\nfrom the 2020 US population. We also analyzed responses to questions about\nincome inequality and gender roles to explore GPT's subjective attitudes. By\nusing repeated random sampling, we created a sampling distribution to identify\nthe parameters of the GPT-generated population and compared these with Census\ndata. Our findings show some alignment in gender and age means with the actual\n2020 US population, but we also found mismatches in the distributions of racial\nand educational groups. Furthermore, there were significant differences between\nthe distribution of GPT's responses and human self-reported attitudes. While\nthe overall point estimates of GPT's income attitudinal responses seem to align\nwith the mean of the population occasionally, their response distributions\nfollow a normal distribution that diverges from human responses. In terms of\ngender relations, GPT's answers tend to cluster in the most frequently answered\ncategory, demonstrating a deterministic pattern. We conclude by emphasizing the\ndistinct design philosophies of LLMs and social surveys: LLMs aim to predict\nthe most suitable answers, while social surveys seek to reveal the\nheterogeneity among social groups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The extent to which Large Language Models (LLMs) can simulate the\ndata-generating process for social surveys remains unclear. Current research\nhas not thoroughly assessed potential biases in the sociodemographic population\nrepresented within the language model's framework. Additionally, the subjective\nworlds of LLMs often show inconsistencies in how closely their responses match\nthose of groups of human respondents. In this paper, we used ChatGPT-3.5 to\nsimulate the sampling process and generated six socioeconomic characteristics\nfrom the 2020 US population. We also analyzed responses to questions about\nincome inequality and gender roles to explore GPT's subjective attitudes. By\nusing repeated random sampling, we created a sampling distribution to identify\nthe parameters of the GPT-generated population and compared these with Census\ndata. Our findings show some alignment in gender and age means with the actual\n2020 US population, but we also found mismatches in the distributions of racial\nand educational groups. Furthermore, there were significant differences between\nthe distribution of GPT's responses and human self-reported attitudes. While\nthe overall point estimates of GPT's income attitudinal responses seem to align\nwith the mean of the population occasionally, their response distributions\nfollow a normal distribution that diverges from human responses. In terms of\ngender relations, GPT's answers tend to cluster in the most frequently answered\ncategory, demonstrating a deterministic pattern. We conclude by emphasizing the\ndistinct design philosophies of LLMs and social surveys: LLMs aim to predict\nthe most suitable answers, while social surveys seek to reveal the\nheterogeneity among social groups."
                },
                "authors": [
                    {
                        "name": "Muzhi Zhou"
                    },
                    {
                        "name": "Lu Yu"
                    },
                    {
                        "name": "Xiaomin Geng"
                    },
                    {
                        "name": "Lan Luo"
                    }
                ],
                "author_detail": {
                    "name": "Lan Luo"
                },
                "author": "Lan Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01227v2",
                "updated": "2024-09-04T10:20:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    20,
                    59,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-02T13:02:51Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    13,
                    2,
                    51,
                    0,
                    246,
                    0
                ],
                "title": "Prompt Compression with Context-Aware Sentence Encoding for Fast and\n  Improved LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Compression with Context-Aware Sentence Encoding for Fast and\n  Improved LLM Inference"
                },
                "summary": "Large language models (LLMs) have triggered a new stream of research focusing\non compressing the context length to reduce the computational cost while\nensuring the retention of helpful information for LLMs to answer the given\nquestion. Token-based removal methods are one of the most prominent approaches\nin this direction, but risk losing the semantics of the context caused by\nintermediate token removal, especially under high compression ratios, while\nalso facing challenges in computational efficiency. In this work, we propose\ncontext-aware prompt compression (CPC), a sentence-level prompt compression\ntechnique where its key innovation is a novel context-aware sentence encoder\nthat provides a relevance score for each sentence for a given question. To\ntrain this encoder, we generate a new dataset consisting of questions,\npositives, and negative pairs where positives are sentences relevant to the\nquestion, while negatives are irrelevant context sentences. We train the\nencoder in a contrastive setup to learn context-aware sentence representations.\nOur method considerably outperforms prior works on prompt compression on\nbenchmark datasets and is up to 10.93x faster at inference compared to the best\ntoken-level compression method. We also find better improvement for shorter\nlength constraints in most benchmarks, showing the effectiveness of our\nproposed solution in the compression of relevant information in a shorter\ncontext. Finally, we release the code and the dataset for quick reproducibility\nand further development: https://github.com/Workday/cpc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have triggered a new stream of research focusing\non compressing the context length to reduce the computational cost while\nensuring the retention of helpful information for LLMs to answer the given\nquestion. Token-based removal methods are one of the most prominent approaches\nin this direction, but risk losing the semantics of the context caused by\nintermediate token removal, especially under high compression ratios, while\nalso facing challenges in computational efficiency. In this work, we propose\ncontext-aware prompt compression (CPC), a sentence-level prompt compression\ntechnique where its key innovation is a novel context-aware sentence encoder\nthat provides a relevance score for each sentence for a given question. To\ntrain this encoder, we generate a new dataset consisting of questions,\npositives, and negative pairs where positives are sentences relevant to the\nquestion, while negatives are irrelevant context sentences. We train the\nencoder in a contrastive setup to learn context-aware sentence representations.\nOur method considerably outperforms prior works on prompt compression on\nbenchmark datasets and is up to 10.93x faster at inference compared to the best\ntoken-level compression method. We also find better improvement for shorter\nlength constraints in most benchmarks, showing the effectiveness of our\nproposed solution in the compression of relevant information in a shorter\ncontext. Finally, we release the code and the dataset for quick reproducibility\nand further development: https://github.com/Workday/cpc."
                },
                "authors": [
                    {
                        "name": "Barys Liskavets"
                    },
                    {
                        "name": "Maxim Ushakov"
                    },
                    {
                        "name": "Shuvendu Roy"
                    },
                    {
                        "name": "Mark Klibanov"
                    },
                    {
                        "name": "Ali Etemad"
                    },
                    {
                        "name": "Shane Luke"
                    }
                ],
                "author_detail": {
                    "name": "Shane Luke"
                },
                "author": "Shane Luke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04985v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04985v6",
                "updated": "2024-09-04T10:04:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    4,
                    52,
                    2,
                    248,
                    0
                ],
                "published": "2023-12-08T11:47:35Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    11,
                    47,
                    35,
                    4,
                    342,
                    0
                ],
                "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQ Attention: Bandwidth-Efficient LLM Inference"
                },
                "summary": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Luka Ribar"
                    },
                    {
                        "name": "Ivan Chelombiev"
                    },
                    {
                        "name": "Luke Hudlass-Galley"
                    },
                    {
                        "name": "Charlie Blake"
                    },
                    {
                        "name": "Carlo Luschi"
                    },
                    {
                        "name": "Douglas Orr"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Orr"
                },
                "author": "Douglas Orr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04985v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04985v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02572v1",
                "updated": "2024-09-04T09:46:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    9,
                    46,
                    33,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T09:46:33Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    9,
                    46,
                    33,
                    2,
                    248,
                    0
                ],
                "title": "Advancing Cyber Incident Timeline Analysis Through Rule Based AI and\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Cyber Incident Timeline Analysis Through Rule Based AI and\n  Large Language Models"
                },
                "summary": "Timeline Analysis (TA) is a key part of Timeline Forensics (TF) in Digital\nForensics (DF), focusing primarily on examining and analysing temporal digital\nartefacts such as timestamps, derived from event logs, file metadata, and other\nrelated data to correlate events resulting from cyber incidents and reconstruct\ntheir chronological timeline. Traditional tools often struggle to efficiently\nprocess the vast volume and variety of data acquired during DF investigations\nand Incident Response (IR) processes. This paper presents a novel framework,\nGenDFIR, that combines Rule-Based Artificial Intelligence (R-BAI) algorithms\nwith Large Language Models (LLMs) to advance and automate the TA process. Our\napproach consists of two main stages (1) We use R-BAI to identify and select\nanomalous digital artefacts based on predefined rules. (2) The selected\nartefacts are then converted into embeddings for processing by an LLM with the\nhelp of a Retrieval-Augmented Generation (RAG) agent. The LLM consequently\nleverages its capabilities to perform automated TA on the artefacts and predict\npotential incident scenarios. To validate our framework, we evaluate GenDFIR\nperformance, efficiency, and reliability using various metrics across synthetic\ncyber incident simulation scenarios. This paper presents a proof of concept,\nwhere the findings demonstrate the significant potential of integrating R-BAI\nand LLMs for TA. This novel approach highlights the power of Generative AI\n(GenAI), specifically LLMs, and opens new avenues for advanced threat detection\nand incident reconstruction, representing a significant step forward in the\nfield.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timeline Analysis (TA) is a key part of Timeline Forensics (TF) in Digital\nForensics (DF), focusing primarily on examining and analysing temporal digital\nartefacts such as timestamps, derived from event logs, file metadata, and other\nrelated data to correlate events resulting from cyber incidents and reconstruct\ntheir chronological timeline. Traditional tools often struggle to efficiently\nprocess the vast volume and variety of data acquired during DF investigations\nand Incident Response (IR) processes. This paper presents a novel framework,\nGenDFIR, that combines Rule-Based Artificial Intelligence (R-BAI) algorithms\nwith Large Language Models (LLMs) to advance and automate the TA process. Our\napproach consists of two main stages (1) We use R-BAI to identify and select\nanomalous digital artefacts based on predefined rules. (2) The selected\nartefacts are then converted into embeddings for processing by an LLM with the\nhelp of a Retrieval-Augmented Generation (RAG) agent. The LLM consequently\nleverages its capabilities to perform automated TA on the artefacts and predict\npotential incident scenarios. To validate our framework, we evaluate GenDFIR\nperformance, efficiency, and reliability using various metrics across synthetic\ncyber incident simulation scenarios. This paper presents a proof of concept,\nwhere the findings demonstrate the significant potential of integrating R-BAI\nand LLMs for TA. This novel approach highlights the power of Generative AI\n(GenAI), specifically LLMs, and opens new avenues for advanced threat detection\nand incident reconstruction, representing a significant step forward in the\nfield."
                },
                "authors": [
                    {
                        "name": "Fatma Yasmine Loumachi"
                    },
                    {
                        "name": "Mohamed Chahine Ghanem"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Chahine Ghanem"
                },
                "author": "Mohamed Chahine Ghanem",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02569v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02569v1",
                "updated": "2024-09-04T09:39:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    9,
                    39,
                    7,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T09:39:07Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    9,
                    39,
                    7,
                    2,
                    248,
                    0
                ],
                "title": "More is More: Addition Bias in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More is More: Addition Bias in Large Language Models"
                },
                "summary": "In this paper, we investigate the presence of additive bias in Large Language\nModels (LLMs), drawing a parallel to the cognitive bias observed in humans\nwhere individuals tend to favor additive over subtractive changes. Using a\nseries of controlled experiments, we tested various LLMs, including GPT-3.5\nTurbo, Claude 3.5 Sonnet, Mistral, Math$\\Sigma$tral, and Llama 3.1, on tasks\ndesigned to measure their propensity for additive versus subtractive\nmodifications. Our findings demonstrate a significant preference for additive\nchanges across all tested models. For example, in a palindrome creation task,\nLlama 3.1 favored adding letters 97.85% of the time over removing them.\nSimilarly, in a Lego tower balancing task, GPT-3.5 Turbo chose to add a brick\n76.38% of the time rather than remove one. In a text summarization task,\nMistral 7B produced longer summaries in 59.40% to 75.10% of cases when asked to\nimprove its own or others' writing. These results indicate that, similar to\nhumans, LLMs exhibit a marked additive bias, which might have implications when\nLLMs are used on a large scale. Addittive bias might increase resource use and\nenvironmental impact, leading to higher economic costs due to overconsumption\nand waste. This bias should be considered in the development and application of\nLLMs to ensure balanced and efficient problem-solving approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate the presence of additive bias in Large Language\nModels (LLMs), drawing a parallel to the cognitive bias observed in humans\nwhere individuals tend to favor additive over subtractive changes. Using a\nseries of controlled experiments, we tested various LLMs, including GPT-3.5\nTurbo, Claude 3.5 Sonnet, Mistral, Math$\\Sigma$tral, and Llama 3.1, on tasks\ndesigned to measure their propensity for additive versus subtractive\nmodifications. Our findings demonstrate a significant preference for additive\nchanges across all tested models. For example, in a palindrome creation task,\nLlama 3.1 favored adding letters 97.85% of the time over removing them.\nSimilarly, in a Lego tower balancing task, GPT-3.5 Turbo chose to add a brick\n76.38% of the time rather than remove one. In a text summarization task,\nMistral 7B produced longer summaries in 59.40% to 75.10% of cases when asked to\nimprove its own or others' writing. These results indicate that, similar to\nhumans, LLMs exhibit a marked additive bias, which might have implications when\nLLMs are used on a large scale. Addittive bias might increase resource use and\nenvironmental impact, leading to higher economic costs due to overconsumption\nand waste. This bias should be considered in the development and application of\nLLMs to ensure balanced and efficient problem-solving approaches."
                },
                "authors": [
                    {
                        "name": "Luca Santagata"
                    },
                    {
                        "name": "Cristiano De Nobili"
                    }
                ],
                "author_detail": {
                    "name": "Cristiano De Nobili"
                },
                "author": "Cristiano De Nobili",
                "arxiv_comment": "25 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02569v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.00109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.00109v2",
                "updated": "2024-09-04T09:27:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    9,
                    27,
                    5,
                    2,
                    248,
                    0
                ],
                "published": "2023-07-26T18:58:53Z",
                "published_parsed": [
                    2023,
                    7,
                    26,
                    18,
                    58,
                    53,
                    2,
                    207,
                    0
                ],
                "title": "A Sentence is Worth a Thousand Pictures: Can Large Language Models\n  Understand Hum4n L4ngu4ge and the W0rld behind W0rds?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Sentence is Worth a Thousand Pictures: Can Large Language Models\n  Understand Hum4n L4ngu4ge and the W0rld behind W0rds?"
                },
                "summary": "Modern Artificial Intelligence applications show great potential for\nlanguage-related tasks that rely on next-word prediction. The current\ngeneration of Large Language Models (LLMs) have been linked to claims about\nhuman-like linguistic performance and their applications are hailed both as a\nstep towards artificial general intelligence and as a major advance in\nunderstanding the cognitive, and even neural basis of human language. To assess\nthese claims, first we analyze the contribution of LLMs as theoretically\ninformative representations of a target cognitive system vs. atheoretical\nmechanistic tools. Second, we evaluate the models' ability to see the bigger\npicture, through top-down feedback from higher levels of processing, which\nrequires grounding in previous expectations and past world experience. We\nhypothesize that since models lack grounded cognition, they cannot take\nadvantage of these features and instead solely rely on fixed associations\nbetween represented words and word vectors. To assess this, we designed and ran\na novel 'leet task' (l33t t4sk), which requires decoding sentences in which\nletters are systematically replaced by numbers. The results suggest that humans\nexcel in this task whereas models struggle, confirming our hypothesis. We\ninterpret the results by identifying the key abilities that are still missing\nfrom the current state of development of these models, which require solutions\nthat go beyond increased system scaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Artificial Intelligence applications show great potential for\nlanguage-related tasks that rely on next-word prediction. The current\ngeneration of Large Language Models (LLMs) have been linked to claims about\nhuman-like linguistic performance and their applications are hailed both as a\nstep towards artificial general intelligence and as a major advance in\nunderstanding the cognitive, and even neural basis of human language. To assess\nthese claims, first we analyze the contribution of LLMs as theoretically\ninformative representations of a target cognitive system vs. atheoretical\nmechanistic tools. Second, we evaluate the models' ability to see the bigger\npicture, through top-down feedback from higher levels of processing, which\nrequires grounding in previous expectations and past world experience. We\nhypothesize that since models lack grounded cognition, they cannot take\nadvantage of these features and instead solely rely on fixed associations\nbetween represented words and word vectors. To assess this, we designed and ran\na novel 'leet task' (l33t t4sk), which requires decoding sentences in which\nletters are systematically replaced by numbers. The results suggest that humans\nexcel in this task whereas models struggle, confirming our hypothesis. We\ninterpret the results by identifying the key abilities that are still missing\nfrom the current state of development of these models, which require solutions\nthat go beyond increased system scaling."
                },
                "authors": [
                    {
                        "name": "Evelina Leivada"
                    },
                    {
                        "name": "Gary Marcus"
                    },
                    {
                        "name": "Fritz Gnther"
                    },
                    {
                        "name": "Elliot Murphy"
                    }
                ],
                "author_detail": {
                    "name": "Elliot Murphy"
                },
                "author": "Elliot Murphy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.00109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.00109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02530v1",
                "updated": "2024-09-04T08:44:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    8,
                    44,
                    36,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T08:44:36Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    8,
                    44,
                    36,
                    2,
                    248,
                    0
                ],
                "title": "Understanding eGFR Trajectories and Kidney Function Decline via Large\n  Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding eGFR Trajectories and Kidney Function Decline via Large\n  Multimodal Models"
                },
                "summary": "The estimated Glomerular Filtration Rate (eGFR) is an essential indicator of\nkidney function in clinical practice. Although traditional equations and\nMachine Learning (ML) models using clinical and laboratory data can estimate\neGFR, accurately predicting future eGFR levels remains a significant challenge\nfor nephrologists and ML researchers. Recent advances demonstrate that Large\nLanguage Models (LLMs) and Large Multimodal Models (LMMs) can serve as robust\nfoundation models for diverse applications. This study investigates the\npotential of LMMs to predict future eGFR levels with a dataset consisting of\nlaboratory and clinical values from 50 patients. By integrating various\nprompting techniques and ensembles of LMMs, our findings suggest that these\nmodels, when combined with precise prompts and visual representations of eGFR\ntrajectories, offer predictive performance comparable to existing ML models.\nThis research extends the application of foundation models and suggests avenues\nfor future studies to harness these models in addressing complex medical\nforecasting challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The estimated Glomerular Filtration Rate (eGFR) is an essential indicator of\nkidney function in clinical practice. Although traditional equations and\nMachine Learning (ML) models using clinical and laboratory data can estimate\neGFR, accurately predicting future eGFR levels remains a significant challenge\nfor nephrologists and ML researchers. Recent advances demonstrate that Large\nLanguage Models (LLMs) and Large Multimodal Models (LMMs) can serve as robust\nfoundation models for diverse applications. This study investigates the\npotential of LMMs to predict future eGFR levels with a dataset consisting of\nlaboratory and clinical values from 50 patients. By integrating various\nprompting techniques and ensembles of LMMs, our findings suggest that these\nmodels, when combined with precise prompts and visual representations of eGFR\ntrajectories, offer predictive performance comparable to existing ML models.\nThis research extends the application of foundation models and suggests avenues\nfor future studies to harness these models in addressing complex medical\nforecasting challenges."
                },
                "authors": [
                    {
                        "name": "Chih-Yuan Li"
                    },
                    {
                        "name": "Jun-Ting Wu"
                    },
                    {
                        "name": "Chan Hsu"
                    },
                    {
                        "name": "Ming-Yen Lin"
                    },
                    {
                        "name": "Yihuang Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yihuang Kang"
                },
                "author": "Yihuang Kang",
                "arxiv_comment": "This preprint version includes corrections of typographical errors\n  related to numerical values in Table 2, which were present in the version\n  published at the BDH workshop in MIPR 2024. These corrections do not affect\n  the overall conclusions of the study",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02528v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02528v1",
                "updated": "2024-09-04T08:40:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    8,
                    40,
                    27,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T08:40:27Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    8,
                    40,
                    27,
                    2,
                    248,
                    0
                ],
                "title": "A design of magnetic tunnel junctions for the deployment of neuromorphic\n  hardware for edge computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A design of magnetic tunnel junctions for the deployment of neuromorphic\n  hardware for edge computing"
                },
                "summary": "The electrically readable complex dynamics of robust and scalable magnetic\ntunnel junctions (MTJs) offer promising opportunities for advancing\nneuromorphic computing. In this work, we present an MTJ design with a free\nlayer and two polarizers capable of computing the sigmoidal activation function\nand its gradient at the device level. This design enables both feedforward and\nbackpropagation computations within a single device, extending neuromorphic\ncomputing frameworks previously explored in the literature by introducing the\nability to perform backpropagation directly in hardware. Our algorithm\nimplementation reveals two key findings: (i) the small discrepancies between\nthe MTJ-generated curves and the exact software-generated curves have a\nnegligible impact on the performance of the backpropagation algorithm, (ii) the\ndevice implementation is highly robust to inter-device variation and noise, and\n(iii) the proposed method effectively supports transfer learning and knowledge\ndistillation. To demonstrate this, we evaluated the performance of an edge\ncomputing network using weights from a software-trained model implemented with\nour MTJ design. The results show a minimal loss of accuracy of only 0.1% for\nthe Fashion MNIST dataset and 2% for the CIFAR-100 dataset compared to the\noriginal software implementation. These results highlight the potential of our\nMTJ design for compact, hardware-based neural networks in edge computing\napplications, particularly for transfer learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The electrically readable complex dynamics of robust and scalable magnetic\ntunnel junctions (MTJs) offer promising opportunities for advancing\nneuromorphic computing. In this work, we present an MTJ design with a free\nlayer and two polarizers capable of computing the sigmoidal activation function\nand its gradient at the device level. This design enables both feedforward and\nbackpropagation computations within a single device, extending neuromorphic\ncomputing frameworks previously explored in the literature by introducing the\nability to perform backpropagation directly in hardware. Our algorithm\nimplementation reveals two key findings: (i) the small discrepancies between\nthe MTJ-generated curves and the exact software-generated curves have a\nnegligible impact on the performance of the backpropagation algorithm, (ii) the\ndevice implementation is highly robust to inter-device variation and noise, and\n(iii) the proposed method effectively supports transfer learning and knowledge\ndistillation. To demonstrate this, we evaluated the performance of an edge\ncomputing network using weights from a software-trained model implemented with\nour MTJ design. The results show a minimal loss of accuracy of only 0.1% for\nthe Fashion MNIST dataset and 2% for the CIFAR-100 dataset compared to the\noriginal software implementation. These results highlight the potential of our\nMTJ design for compact, hardware-based neural networks in edge computing\napplications, particularly for transfer learning."
                },
                "authors": [
                    {
                        "name": "Davi Rodrigues"
                    },
                    {
                        "name": "Eleonora Raimondo"
                    },
                    {
                        "name": "Riccardo Tomasello"
                    },
                    {
                        "name": "Mario Carpentieri"
                    },
                    {
                        "name": "Giovanni Finocchio"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Finocchio"
                },
                "author": "Giovanni Finocchio",
                "arxiv_comment": "18 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02528v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02522v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02522v1",
                "updated": "2024-09-04T08:30:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    8,
                    30,
                    3,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T08:30:03Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    8,
                    30,
                    3,
                    2,
                    248,
                    0
                ],
                "title": "Cog-GA: A Large Language Models-based Generative Agent for\n  Vision-Language Navigation in Continuous Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cog-GA: A Large Language Models-based Generative Agent for\n  Vision-Language Navigation in Continuous Environments"
                },
                "summary": "Vision Language Navigation in Continuous Environments (VLN-CE) represents a\nfrontier in embodied AI, demanding agents to navigate freely in unbounded 3D\nspaces solely guided by natural language instructions. This task introduces\ndistinct challenges in multimodal comprehension, spatial reasoning, and\ndecision-making. To address these challenges, we introduce Cog-GA, a generative\nagent founded on large language models (LLMs) tailored for VLN-CE tasks. Cog-GA\nemploys a dual-pronged strategy to emulate human-like cognitive processes.\nFirstly, it constructs a cognitive map, integrating temporal, spatial, and\nsemantic elements, thereby facilitating the development of spatial memory\nwithin LLMs. Secondly, Cog-GA employs a predictive mechanism for waypoints,\nstrategically optimizing the exploration trajectory to maximize navigational\nefficiency. Each waypoint is accompanied by a dual-channel scene description,\ncategorizing environmental cues into 'what' and 'where' streams as the brain.\nThis segregation enhances the agent's attentional focus, enabling it to discern\npertinent spatial information for navigation. A reflective mechanism\ncomplements these strategies by capturing feedback from prior navigation\nexperiences, facilitating continual learning and adaptive replanning. Extensive\nevaluations conducted on VLN-CE benchmarks validate Cog-GA's state-of-the-art\nperformance and ability to simulate human-like navigation behaviors. This\nresearch significantly contributes to the development of strategic and\ninterpretable VLN-CE agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Navigation in Continuous Environments (VLN-CE) represents a\nfrontier in embodied AI, demanding agents to navigate freely in unbounded 3D\nspaces solely guided by natural language instructions. This task introduces\ndistinct challenges in multimodal comprehension, spatial reasoning, and\ndecision-making. To address these challenges, we introduce Cog-GA, a generative\nagent founded on large language models (LLMs) tailored for VLN-CE tasks. Cog-GA\nemploys a dual-pronged strategy to emulate human-like cognitive processes.\nFirstly, it constructs a cognitive map, integrating temporal, spatial, and\nsemantic elements, thereby facilitating the development of spatial memory\nwithin LLMs. Secondly, Cog-GA employs a predictive mechanism for waypoints,\nstrategically optimizing the exploration trajectory to maximize navigational\nefficiency. Each waypoint is accompanied by a dual-channel scene description,\ncategorizing environmental cues into 'what' and 'where' streams as the brain.\nThis segregation enhances the agent's attentional focus, enabling it to discern\npertinent spatial information for navigation. A reflective mechanism\ncomplements these strategies by capturing feedback from prior navigation\nexperiences, facilitating continual learning and adaptive replanning. Extensive\nevaluations conducted on VLN-CE benchmarks validate Cog-GA's state-of-the-art\nperformance and ability to simulate human-like navigation behaviors. This\nresearch significantly contributes to the development of strategic and\ninterpretable VLN-CE agents."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Li"
                    },
                    {
                        "name": "Yanfeng Lu"
                    },
                    {
                        "name": "Yao Mu"
                    },
                    {
                        "name": "Hong Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Hong Qiao"
                },
                "author": "Hong Qiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02522v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02522v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02519v1",
                "updated": "2024-09-04T08:27:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    8,
                    27,
                    43,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T08:27:43Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    8,
                    27,
                    43,
                    2,
                    248,
                    0
                ],
                "title": "Language is Scary when Over-Analyzed: Unpacking Implied Misogynistic\n  Reasoning with Argumentation Theory-Driven Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language is Scary when Over-Analyzed: Unpacking Implied Misogynistic\n  Reasoning with Argumentation Theory-Driven Prompts"
                },
                "summary": "We propose misogyny detection as an Argumentative Reasoning task and we\ninvestigate the capacity of large language models (LLMs) to understand the\nimplicit reasoning used to convey misogyny in both Italian and English. The\ncentral aim is to generate the missing reasoning link between a message and the\nimplied meanings encoding the misogyny. Our study uses argumentation theory as\na foundation to form a collection of prompts in both zero-shot and few-shot\nsettings. These prompts integrate different techniques, including\nchain-of-thought reasoning and augmented knowledge. Our findings show that LLMs\nfall short on reasoning capabilities about misogynistic comments and that they\nmostly rely on their implicit knowledge derived from internalized common\nstereotypes about women to generate implied assumptions, rather than on\ninductive reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose misogyny detection as an Argumentative Reasoning task and we\ninvestigate the capacity of large language models (LLMs) to understand the\nimplicit reasoning used to convey misogyny in both Italian and English. The\ncentral aim is to generate the missing reasoning link between a message and the\nimplied meanings encoding the misogyny. Our study uses argumentation theory as\na foundation to form a collection of prompts in both zero-shot and few-shot\nsettings. These prompts integrate different techniques, including\nchain-of-thought reasoning and augmented knowledge. Our findings show that LLMs\nfall short on reasoning capabilities about misogynistic comments and that they\nmostly rely on their implicit knowledge derived from internalized common\nstereotypes about women to generate implied assumptions, rather than on\ninductive reasoning."
                },
                "authors": [
                    {
                        "name": "Arianna Muti"
                    },
                    {
                        "name": "Federico Ruggeri"
                    },
                    {
                        "name": "Khalid Al-Khatib"
                    },
                    {
                        "name": "Alberto Barrn-Cedeo"
                    },
                    {
                        "name": "Tommaso Caselli"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Caselli"
                },
                "author": "Tommaso Caselli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00084v2",
                "updated": "2024-09-04T08:22:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    8,
                    22,
                    28,
                    2,
                    248,
                    0
                ],
                "published": "2024-08-25T14:50:47Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    14,
                    50,
                    47,
                    6,
                    238,
                    0
                ],
                "title": "Vision-Language and Large Language Model Performance in\n  Gastroenterology: GPT, Claude, Llama, Phi, Mistral, Gemma, and Quantized\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language and Large Language Model Performance in\n  Gastroenterology: GPT, Claude, Llama, Phi, Mistral, Gemma, and Quantized\n  Models"
                },
                "summary": "Background and Aims: This study evaluates the medical reasoning performance\nof large language models (LLMs) and vision language models (VLMs) in\ngastroenterology.\n  Methods: We used 300 gastroenterology board exam-style multiple-choice\nquestions, 138 of which contain images to systematically assess the impact of\nmodel configurations and parameters and prompt engineering strategies utilizing\nGPT-3.5. Next, we assessed the performance of proprietary and open-source LLMs\n(versions), including GPT (3.5, 4, 4o, 4omini), Claude (3, 3.5), Gemini (1.0),\nMistral, Llama (2, 3, 3.1), Mixtral, and Phi (3), across different interfaces\n(web and API), computing environments (cloud and local), and model precisions\n(with and without quantization). Finally, we assessed accuracy using a\nsemiautomated pipeline.\n  Results: Among the proprietary models, GPT-4o (73.7%) and Claude3.5-Sonnet\n(74.0%) achieved the highest accuracy, outperforming the top open-source\nmodels: Llama3.1-405b (64%), Llama3.1-70b (58.3%), and Mixtral-8x7b (54.3%).\nAmong the quantized open-source models, the 6-bit quantized Phi3-14b (48.7%)\nperformed best. The scores of the quantized models were comparable to those of\nthe full-precision models Llama2-7b, Llama2--13b, and Gemma2-9b. Notably, VLM\nperformance on image-containing questions did not improve when the images were\nprovided and worsened when LLM-generated captions were provided. In contrast, a\n10% increase in accuracy was observed when images were accompanied by\nhuman-crafted image descriptions.\n  Conclusion: In conclusion, while LLMs exhibit robust zero-shot performance in\nmedical reasoning, the integration of visual data remains a challenge for VLMs.\nEffective deployment involves carefully determining optimal model\nconfigurations, encouraging users to consider either the high performance of\nproprietary models or the flexible adaptability of open-source models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background and Aims: This study evaluates the medical reasoning performance\nof large language models (LLMs) and vision language models (VLMs) in\ngastroenterology.\n  Methods: We used 300 gastroenterology board exam-style multiple-choice\nquestions, 138 of which contain images to systematically assess the impact of\nmodel configurations and parameters and prompt engineering strategies utilizing\nGPT-3.5. Next, we assessed the performance of proprietary and open-source LLMs\n(versions), including GPT (3.5, 4, 4o, 4omini), Claude (3, 3.5), Gemini (1.0),\nMistral, Llama (2, 3, 3.1), Mixtral, and Phi (3), across different interfaces\n(web and API), computing environments (cloud and local), and model precisions\n(with and without quantization). Finally, we assessed accuracy using a\nsemiautomated pipeline.\n  Results: Among the proprietary models, GPT-4o (73.7%) and Claude3.5-Sonnet\n(74.0%) achieved the highest accuracy, outperforming the top open-source\nmodels: Llama3.1-405b (64%), Llama3.1-70b (58.3%), and Mixtral-8x7b (54.3%).\nAmong the quantized open-source models, the 6-bit quantized Phi3-14b (48.7%)\nperformed best. The scores of the quantized models were comparable to those of\nthe full-precision models Llama2-7b, Llama2--13b, and Gemma2-9b. Notably, VLM\nperformance on image-containing questions did not improve when the images were\nprovided and worsened when LLM-generated captions were provided. In contrast, a\n10% increase in accuracy was observed when images were accompanied by\nhuman-crafted image descriptions.\n  Conclusion: In conclusion, while LLMs exhibit robust zero-shot performance in\nmedical reasoning, the integration of visual data remains a challenge for VLMs.\nEffective deployment involves carefully determining optimal model\nconfigurations, encouraging users to consider either the high performance of\nproprietary models or the flexible adaptability of open-source models."
                },
                "authors": [
                    {
                        "name": "Seyed Amir Ahmad Safavi-Naini"
                    },
                    {
                        "name": "Shuhaib Ali"
                    },
                    {
                        "name": "Omer Shahab"
                    },
                    {
                        "name": "Zahra Shahhoseini"
                    },
                    {
                        "name": "Thomas Savage"
                    },
                    {
                        "name": "Sara Rafiee"
                    },
                    {
                        "name": "Jamil S Samaan"
                    },
                    {
                        "name": "Reem Al Shabeeb"
                    },
                    {
                        "name": "Farah Ladak"
                    },
                    {
                        "name": "Jamie O Yang"
                    },
                    {
                        "name": "Juan Echavarria"
                    },
                    {
                        "name": "Sumbal Babar"
                    },
                    {
                        "name": "Aasma Shaukat"
                    },
                    {
                        "name": "Samuel Margolis"
                    },
                    {
                        "name": "Nicholas P Tatonetti"
                    },
                    {
                        "name": "Girish Nadkarni"
                    },
                    {
                        "name": "Bara El Kurdi"
                    },
                    {
                        "name": "Ali Soroush"
                    }
                ],
                "author_detail": {
                    "name": "Ali Soroush"
                },
                "author": "Ali Soroush",
                "arxiv_comment": "Manuscript Pages: 34, Figures: 7, Tables: 2, Supplementary File\n  Pages: 35, Data Transparency Statement: Code is available at:\n  https://github.com/Sdamirsa/LLM-VLM-in-Gastroenterology . Study data from\n  American College of Gastroenterology (ACG) are restricted and available upon\n  request with ACG permission. Correction: updated abstract considering\n  Llama3.1 results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92C50, 68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01836v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01836v2",
                "updated": "2024-09-04T08:01:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    8,
                    1,
                    32,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-03T12:34:33Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    12,
                    34,
                    33,
                    1,
                    247,
                    0
                ],
                "title": "Reuse and Blend: Energy-Efficient Optical Neural Network Enabled by\n  Weight Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reuse and Blend: Energy-Efficient Optical Neural Network Enabled by\n  Weight Sharing"
                },
                "summary": "Optical neural networks (ONN) based on micro-ring resonators (MRR) have\nemerged as a promising alternative to significantly accelerating the massive\nmatrix-vector multiplication (MVM) operations in artificial intelligence (AI)\napplications. However, the limited scale of MRR arrays presents a challenge for\nAI acceleration. The disparity between the small MRR arrays and the large\nweight matrices in AI necessitates extensive MRR writings, including\nreprogramming and calibration, resulting in considerable latency and energy\noverheads. To address this problem, we propose a novel design methodology to\nlessen the need for frequent weight reloading. Specifically, we propose a reuse\nand blend (R&B) architecture to support efficient layer-wise and block-wise\nweight sharing, which allows weights to be reused several times between\nlayers/blocks. Experimental results demonstrate the R&B system can maintain\ncomparable accuracy with 69% energy savings and 57% latency improvement. These\nresults highlight the promise of the R&B to enable the efficient deployment of\nadvanced deep learning models on photonic accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optical neural networks (ONN) based on micro-ring resonators (MRR) have\nemerged as a promising alternative to significantly accelerating the massive\nmatrix-vector multiplication (MVM) operations in artificial intelligence (AI)\napplications. However, the limited scale of MRR arrays presents a challenge for\nAI acceleration. The disparity between the small MRR arrays and the large\nweight matrices in AI necessitates extensive MRR writings, including\nreprogramming and calibration, resulting in considerable latency and energy\noverheads. To address this problem, we propose a novel design methodology to\nlessen the need for frequent weight reloading. Specifically, we propose a reuse\nand blend (R&B) architecture to support efficient layer-wise and block-wise\nweight sharing, which allows weights to be reused several times between\nlayers/blocks. Experimental results demonstrate the R&B system can maintain\ncomparable accuracy with 69% energy savings and 57% latency improvement. These\nresults highlight the promise of the R&B to enable the efficient deployment of\nadvanced deep learning models on photonic accelerators."
                },
                "authors": [
                    {
                        "name": "Bo Xu"
                    },
                    {
                        "name": "Yuetong Fang"
                    },
                    {
                        "name": "Shaoliang Yu"
                    },
                    {
                        "name": "Renjing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Renjing Xu"
                },
                "author": "Renjing Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01836v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01836v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02486v1",
                "updated": "2024-09-04T07:25:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    25,
                    50,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T07:25:50Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    25,
                    50,
                    2,
                    248,
                    0
                ],
                "title": "Boosting Generalizability towards Zero-Shot Cross-Dataset Single-Image\n  Indoor Depth by Meta-Initialization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Generalizability towards Zero-Shot Cross-Dataset Single-Image\n  Indoor Depth by Meta-Initialization"
                },
                "summary": "Indoor robots rely on depth to perform tasks like navigation or obstacle\ndetection, and single-image depth estimation is widely used to assist\nperception. Most indoor single-image depth prediction focuses less on model\ngeneralizability to unseen datasets, concerned with in-the-wild robustness for\nsystem deployment. This work leverages gradient-based meta-learning to gain\nhigher generalizability on zero-shot cross-dataset inference. Unlike the\nmost-studied meta-learning of image classification associated with explicit\nclass labels, no explicit task boundaries exist for continuous depth values\ntied to highly varying indoor environments regarding object arrangement and\nscene composition. We propose fine-grained task that treats each RGB-D\nmini-batch as a task in our meta-learning formulation. We first show that our\nmethod on limited data induces a much better prior (max 27.8% in RMSE). Then,\nfinetuning on meta-learned initialization consistently outperforms baselines\nwithout the meta approach. Aiming at generalization, we propose zero-shot\ncross-dataset protocols and validate higher generalizability induced by our\nmeta-initialization, as a simple and useful plugin to many existing depth\nestimation methods. The work at the intersection of depth and meta-learning\npotentially drives both research to step closer to practical robotic and\nmachine perception usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indoor robots rely on depth to perform tasks like navigation or obstacle\ndetection, and single-image depth estimation is widely used to assist\nperception. Most indoor single-image depth prediction focuses less on model\ngeneralizability to unseen datasets, concerned with in-the-wild robustness for\nsystem deployment. This work leverages gradient-based meta-learning to gain\nhigher generalizability on zero-shot cross-dataset inference. Unlike the\nmost-studied meta-learning of image classification associated with explicit\nclass labels, no explicit task boundaries exist for continuous depth values\ntied to highly varying indoor environments regarding object arrangement and\nscene composition. We propose fine-grained task that treats each RGB-D\nmini-batch as a task in our meta-learning formulation. We first show that our\nmethod on limited data induces a much better prior (max 27.8% in RMSE). Then,\nfinetuning on meta-learned initialization consistently outperforms baselines\nwithout the meta approach. Aiming at generalization, we propose zero-shot\ncross-dataset protocols and validate higher generalizability induced by our\nmeta-initialization, as a simple and useful plugin to many existing depth\nestimation methods. The work at the intersection of depth and meta-learning\npotentially drives both research to step closer to practical robotic and\nmachine perception usage."
                },
                "authors": [
                    {
                        "name": "Cho-Ying Wu"
                    },
                    {
                        "name": "Yiqi Zhong"
                    },
                    {
                        "name": "Junying Wang"
                    },
                    {
                        "name": "Ulrich Neumann"
                    }
                ],
                "author_detail": {
                    "name": "Ulrich Neumann"
                },
                "author": "Ulrich Neumann",
                "arxiv_comment": "IROS 2024. The version supersedes 2305.07269. arXiv admin note: text\n  overlap with arXiv:2305.07269",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02474v1",
                "updated": "2024-09-04T06:46:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    6,
                    46,
                    31,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T06:46:31Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    6,
                    46,
                    31,
                    2,
                    248,
                    0
                ],
                "title": "A Comparative Study on Large Language Models for Log Parsing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparative Study on Large Language Models for Log Parsing"
                },
                "summary": "Background: Log messages provide valuable information about the status of\nsoftware systems. This information is provided in an unstructured fashion and\nautomated approaches are applied to extract relevant parameters. To ease this\nprocess, log parsing can be applied, which transforms log messages into\nstructured log templates. Recent advances in language models have led to\nseveral studies that apply ChatGPT to the task of log parsing with promising\nresults. However, the performance of other state-of-the-art large language\nmodels (LLMs) on the log parsing task remains unclear.\n  Aims: In this study, we investigate the current capability of\nstate-of-the-art LLMs to perform log parsing.\n  Method: We select six recent LLMs, including both paid proprietary (GPT-3.5,\nClaude 2.1) and four free-to-use open models, and compare their performance on\nsystem logs obtained from a selection of mature open-source projects. We design\ntwo different prompting approaches and apply the LLMs on 1, 354 log templates\nacross 16 different projects. We evaluate their effectiveness, in the number of\ncorrectly identified templates, and the syntactic similarity between the\ngenerated templates and the ground truth.\n  Results: We found that free-to-use models are able to compete with paid\nmodels, with CodeLlama extracting 10% more log templates correctly than\nGPT-3.5. Moreover, we provide qualitative insights into the usability of\nlanguage models (e.g., how easy it is to use their responses).\n  Conclusions: Our results reveal that some of the smaller, free-to-use LLMs\ncan considerably assist log parsing compared to their paid proprietary\ncompetitors, especially code-specialized models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Log messages provide valuable information about the status of\nsoftware systems. This information is provided in an unstructured fashion and\nautomated approaches are applied to extract relevant parameters. To ease this\nprocess, log parsing can be applied, which transforms log messages into\nstructured log templates. Recent advances in language models have led to\nseveral studies that apply ChatGPT to the task of log parsing with promising\nresults. However, the performance of other state-of-the-art large language\nmodels (LLMs) on the log parsing task remains unclear.\n  Aims: In this study, we investigate the current capability of\nstate-of-the-art LLMs to perform log parsing.\n  Method: We select six recent LLMs, including both paid proprietary (GPT-3.5,\nClaude 2.1) and four free-to-use open models, and compare their performance on\nsystem logs obtained from a selection of mature open-source projects. We design\ntwo different prompting approaches and apply the LLMs on 1, 354 log templates\nacross 16 different projects. We evaluate their effectiveness, in the number of\ncorrectly identified templates, and the syntactic similarity between the\ngenerated templates and the ground truth.\n  Results: We found that free-to-use models are able to compete with paid\nmodels, with CodeLlama extracting 10% more log templates correctly than\nGPT-3.5. Moreover, we provide qualitative insights into the usability of\nlanguage models (e.g., how easy it is to use their responses).\n  Conclusions: Our results reveal that some of the smaller, free-to-use LLMs\ncan considerably assist log parsing compared to their paid proprietary\ncompetitors, especially code-specialized models."
                },
                "authors": [
                    {
                        "name": "Merve Astekin"
                    },
                    {
                        "name": "Max Hort"
                    },
                    {
                        "name": "Leon Moonen"
                    }
                ],
                "author_detail": {
                    "name": "Leon Moonen"
                },
                "author": "Leon Moonen",
                "arxiv_doi": "10.1145/3674805.3686684",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3674805.3686684",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication in the 18th ACM/IEEE International Symposium\n  on Empirical Software Engineering and Measurement (ESEM '24)",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15796v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15796v2",
                "updated": "2024-09-04T06:36:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    6,
                    36,
                    22,
                    2,
                    248,
                    0
                ],
                "published": "2024-08-28T13:42:28Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    42,
                    28,
                    2,
                    241,
                    0
                ],
                "title": "Evaluating Named Entity Recognition Using Few-Shot Prompting with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Named Entity Recognition Using Few-Shot Prompting with Large\n  Language Models"
                },
                "summary": "This paper evaluates Few-Shot Prompting with Large Language Models for Named\nEntity Recognition (NER). Traditional NER systems rely on extensive labeled\ndatasets, which are costly and time-consuming to obtain. Few-Shot Prompting or\nin-context learning enables models to recognize entities with minimal examples.\nWe assess state-of-the-art models like GPT-4 in NER tasks, comparing their\nfew-shot performance to fully supervised benchmarks. Results show that while\nthere is a performance gap, large models excel in adapting to new entity types\nand domains with very limited data. We also explore the effects of prompt\nengineering, guided output format and context length on performance. This study\nunderscores Few-Shot Learning's potential to reduce the need for large labeled\ndatasets, enhancing NER scalability and accessibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper evaluates Few-Shot Prompting with Large Language Models for Named\nEntity Recognition (NER). Traditional NER systems rely on extensive labeled\ndatasets, which are costly and time-consuming to obtain. Few-Shot Prompting or\nin-context learning enables models to recognize entities with minimal examples.\nWe assess state-of-the-art models like GPT-4 in NER tasks, comparing their\nfew-shot performance to fully supervised benchmarks. Results show that while\nthere is a performance gap, large models excel in adapting to new entity types\nand domains with very limited data. We also explore the effects of prompt\nengineering, guided output format and context length on performance. This study\nunderscores Few-Shot Learning's potential to reduce the need for large labeled\ndatasets, enhancing NER scalability and accessibility."
                },
                "authors": [
                    {
                        "name": "Hdi Zeghidi"
                    },
                    {
                        "name": "Ludovic Moncla"
                    }
                ],
                "author_detail": {
                    "name": "Ludovic Moncla"
                },
                "author": "Ludovic Moncla",
                "arxiv_comment": "Github repo: https://github.com/GEODE-project/ner-llm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15796v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15796v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02465v1",
                "updated": "2024-09-04T06:28:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    6,
                    28,
                    22,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T06:28:22Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    6,
                    28,
                    22,
                    2,
                    248,
                    0
                ],
                "title": "DetectiveQA: Evaluating Long-Context Reasoning on Detective Novels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DetectiveQA: Evaluating Long-Context Reasoning on Detective Novels"
                },
                "summary": "With the rapid advancement of Large Language Models (LLMs), long-context\ninformation understanding and processing have become a hot topic in academia\nand industry. However, benchmarks for evaluating the ability of LLMs to handle\nlong-context information do not seem to have kept pace with the development of\nLLMs. Despite the emergence of various long-context evaluation benchmarks, the\ntypes of capability assessed are still limited, without new capability\ndimensions. In this paper, we introduce DetectiveQA, a narrative reasoning\nbenchmark featured with an average context length of over 100K tokens.\nDetectiveQA focuses on evaluating the long-context reasoning ability of LLMs,\nwhich not only requires a full understanding of context but also requires\nextracting important evidences from the context and reasoning according to\nextracted evidences to answer the given questions. This is a new dimension of\ncapability evaluation, which is more in line with the current intelligence\nlevel of LLMs. We use detective novels as data sources, which naturally have\nvarious reasoning elements. Finally, we manually annotated 600 questions in\nChinese and then also provided an English edition of the context information\nand questions. We evaluate many long-context LLMs on DetectiveQA, including\ncommercial and open-sourced models, and the results indicate that existing\nlong-context LLMs still require significant advancements to effectively process\ntrue long-context dependency questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of Large Language Models (LLMs), long-context\ninformation understanding and processing have become a hot topic in academia\nand industry. However, benchmarks for evaluating the ability of LLMs to handle\nlong-context information do not seem to have kept pace with the development of\nLLMs. Despite the emergence of various long-context evaluation benchmarks, the\ntypes of capability assessed are still limited, without new capability\ndimensions. In this paper, we introduce DetectiveQA, a narrative reasoning\nbenchmark featured with an average context length of over 100K tokens.\nDetectiveQA focuses on evaluating the long-context reasoning ability of LLMs,\nwhich not only requires a full understanding of context but also requires\nextracting important evidences from the context and reasoning according to\nextracted evidences to answer the given questions. This is a new dimension of\ncapability evaluation, which is more in line with the current intelligence\nlevel of LLMs. We use detective novels as data sources, which naturally have\nvarious reasoning elements. Finally, we manually annotated 600 questions in\nChinese and then also provided an English edition of the context information\nand questions. We evaluate many long-context LLMs on DetectiveQA, including\ncommercial and open-sourced models, and the results indicate that existing\nlong-context LLMs still require significant advancements to effectively process\ntrue long-context dependency questions."
                },
                "authors": [
                    {
                        "name": "Zhe Xu"
                    },
                    {
                        "name": "Jiasheng Ye"
                    },
                    {
                        "name": "Xiangyang Liu"
                    },
                    {
                        "name": "Tianxiang Sun"
                    },
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00899v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00899v2",
                "updated": "2024-09-04T06:19:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    6,
                    19,
                    8,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-02T02:24:38Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    24,
                    38,
                    0,
                    246,
                    0
                ],
                "title": "MarsCode Agent: AI-native Automated Bug Fixing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MarsCode Agent: AI-native Automated Bug Fixing"
                },
                "summary": "Recent advances in large language models (LLMs) have shown significant\npotential to automate various software development tasks, including code\ncompletion, test generation, and bug fixing. However, the application of LLMs\nfor automated bug fixing remains challenging due to the complexity and\ndiversity of real-world software systems. In this paper, we introduce MarsCode\nAgent, a novel framework that leverages LLMs to automatically identify and\nrepair bugs in software code. MarsCode Agent combines the power of LLMs with\nadvanced code analysis techniques to accurately localize faults and generate\npatches. Our approach follows a systematic process of planning, bug\nreproduction, fault localization, candidate patch generation, and validation to\nensure high-quality bug fixes. We evaluated MarsCode Agent on SWE-bench, a\ncomprehensive benchmark of real-world software projects, and our results show\nthat MarsCode Agent achieves a high success rate in bug fixing compared to most\nof the existing automated approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have shown significant\npotential to automate various software development tasks, including code\ncompletion, test generation, and bug fixing. However, the application of LLMs\nfor automated bug fixing remains challenging due to the complexity and\ndiversity of real-world software systems. In this paper, we introduce MarsCode\nAgent, a novel framework that leverages LLMs to automatically identify and\nrepair bugs in software code. MarsCode Agent combines the power of LLMs with\nadvanced code analysis techniques to accurately localize faults and generate\npatches. Our approach follows a systematic process of planning, bug\nreproduction, fault localization, candidate patch generation, and validation to\nensure high-quality bug fixes. We evaluated MarsCode Agent on SWE-bench, a\ncomprehensive benchmark of real-world software projects, and our results show\nthat MarsCode Agent achieves a high success rate in bug fixing compared to most\nof the existing automated approaches."
                },
                "authors": [
                    {
                        "name": "Yizhou Liu"
                    },
                    {
                        "name": "Pengfei Gao"
                    },
                    {
                        "name": "Xinchen Wang"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Yexuan Shi"
                    },
                    {
                        "name": "Zhao Zhang"
                    },
                    {
                        "name": "Chao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Chao Peng"
                },
                "author": "Chao Peng",
                "arxiv_comment": "Yizhou Liu and Pengfei Gao contributed equally and the order is\n  determined by rolling the dice. Chao Peng is the corresponding author",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00899v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00899v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10155v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10155v3",
                "updated": "2024-09-04T06:10:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    6,
                    10,
                    47,
                    2,
                    248,
                    0
                ],
                "published": "2024-04-15T22:02:58Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    22,
                    2,
                    58,
                    0,
                    106,
                    0
                ],
                "title": "The Fault in our Stars: Quality Assessment of Code Generation Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Fault in our Stars: Quality Assessment of Code Generation Benchmarks"
                },
                "summary": "Large Language Models (LLMs) are gaining popularity among software engineers.\nA crucial aspect of developing effective code generation LLMs is to evaluate\nthese models using a robust benchmark. Evaluation benchmarks with quality\nissues can provide a false sense of performance. In this work, we conduct the\nfirst-of-its-kind study of the quality of prompts within benchmarks used to\ncompare the performance of different code generation models. To conduct this\nstudy, we analyzed 3,566 prompts from 9 code generation benchmarks to identify\nquality issues in them. We also investigated whether fixing the identified\nquality issues in the benchmarks' prompts affects a model's performance. We\nalso studied memorization issues of the evaluation dataset, which can put into\nquestion a benchmark's trustworthiness. We found that code generation\nevaluation benchmarks mainly focused on Python and coding exercises and had\nvery limited contextual dependencies to challenge the model. These datasets and\nthe developers' prompts suffer from quality issues like spelling and\ngrammatical errors, unclear sentences to express developers' intent, and not\nusing proper documentation style. Fixing all these issues in the benchmarks can\nlead to a better performance for Python code generation, but not a significant\nimprovement was observed for Java code generation. We also found evidence that\nGPT-3.5-Turbo and CodeGen-2.5 models may have data contamination issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are gaining popularity among software engineers.\nA crucial aspect of developing effective code generation LLMs is to evaluate\nthese models using a robust benchmark. Evaluation benchmarks with quality\nissues can provide a false sense of performance. In this work, we conduct the\nfirst-of-its-kind study of the quality of prompts within benchmarks used to\ncompare the performance of different code generation models. To conduct this\nstudy, we analyzed 3,566 prompts from 9 code generation benchmarks to identify\nquality issues in them. We also investigated whether fixing the identified\nquality issues in the benchmarks' prompts affects a model's performance. We\nalso studied memorization issues of the evaluation dataset, which can put into\nquestion a benchmark's trustworthiness. We found that code generation\nevaluation benchmarks mainly focused on Python and coding exercises and had\nvery limited contextual dependencies to challenge the model. These datasets and\nthe developers' prompts suffer from quality issues like spelling and\ngrammatical errors, unclear sentences to express developers' intent, and not\nusing proper documentation style. Fixing all these issues in the benchmarks can\nlead to a better performance for Python code generation, but not a significant\nimprovement was observed for Java code generation. We also found evidence that\nGPT-3.5-Turbo and CodeGen-2.5 models may have data contamination issues."
                },
                "authors": [
                    {
                        "name": "Mohammed Latif Siddiq"
                    },
                    {
                        "name": "Simantika Dristi"
                    },
                    {
                        "name": "Joy Saha"
                    },
                    {
                        "name": "Joanna C. S. Santos"
                    }
                ],
                "author_detail": {
                    "name": "Joanna C. S. Santos"
                },
                "author": "Joanna C. S. Santos",
                "arxiv_comment": "Accepted at the 24th IEEE International Conference on Source Code\n  Analysis and Manipulation(SCAM 2024) Research Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10155v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10155v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14411v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14411v2",
                "updated": "2024-09-04T06:00:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    6,
                    0,
                    56,
                    2,
                    248,
                    0
                ],
                "published": "2024-05-23T10:32:38Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    10,
                    32,
                    38,
                    3,
                    144,
                    0
                ],
                "title": "Large Language Models for Explainable Decisions in Dynamic Digital Twins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Explainable Decisions in Dynamic Digital Twins"
                },
                "summary": "Dynamic data-driven Digital Twins (DDTs) can enable informed decision-making\nand provide an optimisation platform for the underlying system. By leveraging\nprinciples of Dynamic Data-Driven Applications Systems (DDDAS), DDTs can\nformulate computational modalities for feedback loops, model updates and\ndecision-making, including autonomous ones. However, understanding autonomous\ndecision-making often requires technical and domain-specific knowledge. This\npaper explores using large language models (LLMs) to provide an explainability\nplatform for DDTs, generating natural language explanations of the system's\ndecision-making by leveraging domain-specific knowledge bases. A case study\nfrom smart agriculture is presented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic data-driven Digital Twins (DDTs) can enable informed decision-making\nand provide an optimisation platform for the underlying system. By leveraging\nprinciples of Dynamic Data-Driven Applications Systems (DDDAS), DDTs can\nformulate computational modalities for feedback loops, model updates and\ndecision-making, including autonomous ones. However, understanding autonomous\ndecision-making often requires technical and domain-specific knowledge. This\npaper explores using large language models (LLMs) to provide an explainability\nplatform for DDTs, generating natural language explanations of the system's\ndecision-making by leveraging domain-specific knowledge bases. A case study\nfrom smart agriculture is presented."
                },
                "authors": [
                    {
                        "name": "Nan Zhang"
                    },
                    {
                        "name": "Christian Vergara-Marcillo"
                    },
                    {
                        "name": "Georgios Diamantopoulos"
                    },
                    {
                        "name": "Jingran Shen"
                    },
                    {
                        "name": "Nikos Tziritas"
                    },
                    {
                        "name": "Rami Bahsoon"
                    },
                    {
                        "name": "Georgios Theodoropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Theodoropoulos"
                },
                "author": "Georgios Theodoropoulos",
                "arxiv_comment": "9 pages, 3 figures, accepted by DDDAS2024 -- the 5th International\n  Conference on Dynamic Data Driven Applications Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14411v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14411v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15412v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15412v5",
                "updated": "2024-09-04T05:12:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    5,
                    12,
                    54,
                    2,
                    248,
                    0
                ],
                "published": "2024-03-05T08:29:36Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    8,
                    29,
                    36,
                    1,
                    65,
                    0
                ],
                "title": "Towards Measuring and Modeling \"Culture\" in LLMs: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Measuring and Modeling \"Culture\" in LLMs: A Survey"
                },
                "summary": "We present a survey of more than 90 recent papers that aim to study cultural\nrepresentation and inclusion in large language models (LLMs). We observe that\nnone of the studies explicitly define \"culture, which is a complex,\nmultifaceted concept; instead, they probe the models on some specially designed\ndatasets which represent certain aspects of \"culture\". We call these aspects\nthe proxies of culture, and organize them across two dimensions of demographic\nand semantic proxies. We also categorize the probing methods employed. Our\nanalysis indicates that only certain aspects of ``culture,'' such as values and\nobjectives, have been studied, leaving several other interesting and important\nfacets, especially the multitude of semantic domains (Thompson et al., 2020)\nand aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps\nare the lack of robustness of probing techniques and situated studies on the\nimpact of cultural mis- and under-representation in LLM-based applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a survey of more than 90 recent papers that aim to study cultural\nrepresentation and inclusion in large language models (LLMs). We observe that\nnone of the studies explicitly define \"culture, which is a complex,\nmultifaceted concept; instead, they probe the models on some specially designed\ndatasets which represent certain aspects of \"culture\". We call these aspects\nthe proxies of culture, and organize them across two dimensions of demographic\nand semantic proxies. We also categorize the probing methods employed. Our\nanalysis indicates that only certain aspects of ``culture,'' such as values and\nobjectives, have been studied, leaving several other interesting and important\nfacets, especially the multitude of semantic domains (Thompson et al., 2020)\nand aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps\nare the lack of robustness of probing techniques and situated studies on the\nimpact of cultural mis- and under-representation in LLM-based applications."
                },
                "authors": [
                    {
                        "name": "Muhammad Farid Adilazuarda"
                    },
                    {
                        "name": "Sagnik Mukherjee"
                    },
                    {
                        "name": "Pradhyumna Lavania"
                    },
                    {
                        "name": "Siddhant Singh"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    },
                    {
                        "name": "Jacki O'Neill"
                    },
                    {
                        "name": "Ashutosh Modi"
                    },
                    {
                        "name": "Monojit Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Monojit Choudhury"
                },
                "author": "Monojit Choudhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15412v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15412v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02443v1",
                "updated": "2024-09-04T04:41:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    4,
                    41,
                    15,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T04:41:15Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    4,
                    41,
                    15,
                    2,
                    248,
                    0
                ],
                "title": "Exploring the applicability of Large Language Models to citation context\n  analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the applicability of Large Language Models to citation context\n  analysis"
                },
                "summary": "Unlike traditional citation analysis -- which assumes that all citations in a\npaper are equivalent -- citation context analysis considers the contextual\ninformation of individual citations. However, citation context analysis\nrequires creating large amounts of data through annotation, which hinders the\nwidespread use of this methodology. This study explored the applicability of\nLarge Language Models (LLMs) -- particularly ChatGPT -- to citation context\nanalysis by comparing LLMs and human annotation results. The results show that\nthe LLMs annotation is as good as or better than the human annotation in terms\nof consistency but poor in terms of predictive performance. Thus, having LLMs\nimmediately replace human annotators in citation context analysis is\ninappropriate. However, the annotation results obtained by LLMs can be used as\nreference information when narrowing the annotation results obtained by\nmultiple human annotators to one, or LLMs can be used as one of the annotators\nwhen it is difficult to prepare sufficient human annotators. This study\nprovides basic findings important for the future development of citation\ncontext analyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlike traditional citation analysis -- which assumes that all citations in a\npaper are equivalent -- citation context analysis considers the contextual\ninformation of individual citations. However, citation context analysis\nrequires creating large amounts of data through annotation, which hinders the\nwidespread use of this methodology. This study explored the applicability of\nLarge Language Models (LLMs) -- particularly ChatGPT -- to citation context\nanalysis by comparing LLMs and human annotation results. The results show that\nthe LLMs annotation is as good as or better than the human annotation in terms\nof consistency but poor in terms of predictive performance. Thus, having LLMs\nimmediately replace human annotators in citation context analysis is\ninappropriate. However, the annotation results obtained by LLMs can be used as\nreference information when narrowing the annotation results obtained by\nmultiple human annotators to one, or LLMs can be used as one of the annotators\nwhen it is difficult to prepare sufficient human annotators. This study\nprovides basic findings important for the future development of citation\ncontext analyses."
                },
                "authors": [
                    {
                        "name": "Kai Nishikawa"
                    },
                    {
                        "name": "Hitoshi Koshiba"
                    }
                ],
                "author_detail": {
                    "name": "Hitoshi Koshiba"
                },
                "author": "Hitoshi Koshiba",
                "arxiv_doi": "10.1007/s11192-024-05142-9",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s11192-024-05142-9",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02428v1",
                "updated": "2024-09-04T04:15:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    4,
                    15,
                    14,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T04:15:14Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    4,
                    15,
                    14,
                    2,
                    248,
                    0
                ],
                "title": "Large Language Models as Efficient Reward Function Searchers for\n  Custom-Environment Multi-Objective Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Efficient Reward Function Searchers for\n  Custom-Environment Multi-Objective Reinforcement Learning"
                },
                "summary": "Leveraging large language models (LLMs) for designing reward functions\ndemonstrates significant potential. However, achieving effective design and\nimprovement of reward functions in reinforcement learning (RL) tasks with\ncomplex custom environments and multiple requirements presents considerable\nchallenges. In this paper, we enable LLMs to be effective white-box searchers,\nhighlighting their advanced semantic understanding capabilities. Specifically,\nwe generate reward components for each explicit user requirement and employ the\nreward critic to identify the correct code form. Then, LLMs assign weights to\nthe reward components to balance their values and iteratively search and\noptimize these weights based on the context provided by the training log\nanalyzer, while adaptively determining the search step size. We applied the\nframework to an underwater information collection RL task without direct human\nfeedback or reward examples (zero-shot). The reward critic successfully correct\nthe reward code with only one feedback for each requirement, effectively\npreventing irreparable errors that can occur when reward function feedback is\nprovided in aggregate. The effective initialization of weights enables the\nacquisition of different reward functions within the Pareto solution set\nwithout weight search. Even in the case where a weight is 100 times off, fewer\nthan four iterations are needed to obtain solutions that meet user\nrequirements. The framework also works well with most prompts utilizing GPT-3.5\nTurbo, since it does not require advanced numerical understanding or\ncalculation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging large language models (LLMs) for designing reward functions\ndemonstrates significant potential. However, achieving effective design and\nimprovement of reward functions in reinforcement learning (RL) tasks with\ncomplex custom environments and multiple requirements presents considerable\nchallenges. In this paper, we enable LLMs to be effective white-box searchers,\nhighlighting their advanced semantic understanding capabilities. Specifically,\nwe generate reward components for each explicit user requirement and employ the\nreward critic to identify the correct code form. Then, LLMs assign weights to\nthe reward components to balance their values and iteratively search and\noptimize these weights based on the context provided by the training log\nanalyzer, while adaptively determining the search step size. We applied the\nframework to an underwater information collection RL task without direct human\nfeedback or reward examples (zero-shot). The reward critic successfully correct\nthe reward code with only one feedback for each requirement, effectively\npreventing irreparable errors that can occur when reward function feedback is\nprovided in aggregate. The effective initialization of weights enables the\nacquisition of different reward functions within the Pareto solution set\nwithout weight search. Even in the case where a weight is 100 times off, fewer\nthan four iterations are needed to obtain solutions that meet user\nrequirements. The framework also works well with most prompts utilizing GPT-3.5\nTurbo, since it does not require advanced numerical understanding or\ncalculation."
                },
                "authors": [
                    {
                        "name": "Guanwen Xie"
                    },
                    {
                        "name": "Jingzehua Xu"
                    },
                    {
                        "name": "Yiyuan Yang"
                    },
                    {
                        "name": "Shuai Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Zhang"
                },
                "author": "Shuai Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02423v1",
                "updated": "2024-09-04T04:05:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    4,
                    5,
                    30,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T04:05:30Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    4,
                    5,
                    30,
                    2,
                    248,
                    0
                ],
                "title": "Accelerating Large Language Model Training with Hybrid GPU-based\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Large Language Model Training with Hybrid GPU-based\n  Compression"
                },
                "summary": "Data Parallelism (DP), Tensor Parallelism (TP), and Pipeline Parallelism (PP)\nare the three strategies widely adopted to enable fast and efficient Large\nLanguage Model (LLM) training. However, these approaches rely on data-intensive\ncommunication routines to collect, aggregate, and re-distribute gradients,\nactivations, and other important model information, which pose significant\noverhead. Co-designed with GPU-based compression libraries, MPI libraries have\nbeen proven to reduce message size significantly, and leverage interconnect\nbandwidth, thus increasing training efficiency while maintaining acceptable\naccuracy.\n  In this work, we investigate the efficacy of compression-assisted MPI\ncollectives under the context of distributed LLM training using 3D parallelism\nand ZeRO optimizations. We scaled up to 192 V100 GPUs on the Lassen\nsupercomputer. First, we enabled a na\\\"ive compression scheme across all\ncollectives and observed a 22.5\\% increase in TFLOPS per GPU and a 23.6\\%\nincrease in samples per second for GPT-NeoX-20B training. Nonetheless, such a\nstrategy ignores the sparsity discrepancy among messages communicated in each\nparallelism degree, thus introducing more errors and causing degradation in\ntraining loss. Therefore, we incorporated hybrid compression settings toward\neach parallel dimension and adjusted the compression intensity accordingly.\nGiven their low-rank structure (arXiv:2301.02654), we apply aggressive\ncompression on gradients when performing DP All-reduce. We adopt milder\ncompression to preserve precision while communicating activations, optimizer\nstates, and model parameters in TP and PP. Using the adjusted hybrid\ncompression scheme, we demonstrate a 17.3\\% increase in TFLOPS per GPU and a\n12.7\\% increase in samples per second while reaching baseline loss convergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Parallelism (DP), Tensor Parallelism (TP), and Pipeline Parallelism (PP)\nare the three strategies widely adopted to enable fast and efficient Large\nLanguage Model (LLM) training. However, these approaches rely on data-intensive\ncommunication routines to collect, aggregate, and re-distribute gradients,\nactivations, and other important model information, which pose significant\noverhead. Co-designed with GPU-based compression libraries, MPI libraries have\nbeen proven to reduce message size significantly, and leverage interconnect\nbandwidth, thus increasing training efficiency while maintaining acceptable\naccuracy.\n  In this work, we investigate the efficacy of compression-assisted MPI\ncollectives under the context of distributed LLM training using 3D parallelism\nand ZeRO optimizations. We scaled up to 192 V100 GPUs on the Lassen\nsupercomputer. First, we enabled a na\\\"ive compression scheme across all\ncollectives and observed a 22.5\\% increase in TFLOPS per GPU and a 23.6\\%\nincrease in samples per second for GPT-NeoX-20B training. Nonetheless, such a\nstrategy ignores the sparsity discrepancy among messages communicated in each\nparallelism degree, thus introducing more errors and causing degradation in\ntraining loss. Therefore, we incorporated hybrid compression settings toward\neach parallel dimension and adjusted the compression intensity accordingly.\nGiven their low-rank structure (arXiv:2301.02654), we apply aggressive\ncompression on gradients when performing DP All-reduce. We adopt milder\ncompression to preserve precision while communicating activations, optimizer\nstates, and model parameters in TP and PP. Using the adjusted hybrid\ncompression scheme, we demonstrate a 17.3\\% increase in TFLOPS per GPU and a\n12.7\\% increase in samples per second while reaching baseline loss convergence."
                },
                "authors": [
                    {
                        "name": "Lang Xu"
                    },
                    {
                        "name": "Quentin Anthony"
                    },
                    {
                        "name": "Qinghua Zhou"
                    },
                    {
                        "name": "Nawras Alnaasan"
                    },
                    {
                        "name": "Radha R. Gulhane"
                    },
                    {
                        "name": "Aamir Shafi"
                    },
                    {
                        "name": "Hari Subramoni"
                    },
                    {
                        "name": "Dhabaleswar K. Panda"
                    }
                ],
                "author_detail": {
                    "name": "Dhabaleswar K. Panda"
                },
                "author": "Dhabaleswar K. Panda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00369v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00369v2",
                "updated": "2024-09-04T03:50:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    3,
                    50,
                    38,
                    2,
                    248,
                    0
                ],
                "published": "2024-08-31T07:10:16Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    7,
                    10,
                    16,
                    5,
                    244,
                    0
                ],
                "title": "An Empirical Study on Information Extraction using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on Information Extraction using Large Language Models"
                },
                "summary": "Human-like large language models (LLMs), especially the most powerful and\npopular ones in OpenAI's GPT family, have proven to be very helpful for many\nnatural language processing (NLP) related tasks. Therefore, various attempts\nhave been made to apply LLMs to information extraction (IE), which is a\nfundamental NLP task that involves extracting information from unstructured\nplain text. To demonstrate the latest representative progress in LLMs'\ninformation extraction ability, we assess the information extraction ability of\nGPT-4 (the latest version of GPT at the time of writing this paper) from four\nperspectives: Performance, Evaluation Criteria, Robustness, and Error Types.\nOur results suggest a visible performance gap between GPT-4 and\nstate-of-the-art (SOTA) IE methods. To alleviate this problem, considering the\nLLMs' human-like characteristics, we propose and analyze the effects of a\nseries of simple prompt-based methods, which can be generalized to other LLMs\nand NLP tasks. Rich experiments show our methods' effectiveness and some of\ntheir remaining issues in improving GPT-4's information extraction ability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-like large language models (LLMs), especially the most powerful and\npopular ones in OpenAI's GPT family, have proven to be very helpful for many\nnatural language processing (NLP) related tasks. Therefore, various attempts\nhave been made to apply LLMs to information extraction (IE), which is a\nfundamental NLP task that involves extracting information from unstructured\nplain text. To demonstrate the latest representative progress in LLMs'\ninformation extraction ability, we assess the information extraction ability of\nGPT-4 (the latest version of GPT at the time of writing this paper) from four\nperspectives: Performance, Evaluation Criteria, Robustness, and Error Types.\nOur results suggest a visible performance gap between GPT-4 and\nstate-of-the-art (SOTA) IE methods. To alleviate this problem, considering the\nLLMs' human-like characteristics, we propose and analyze the effects of a\nseries of simple prompt-based methods, which can be generalized to other LLMs\nand NLP tasks. Rich experiments show our methods' effectiveness and some of\ntheir remaining issues in improving GPT-4's information extraction ability."
                },
                "authors": [
                    {
                        "name": "Ridong Han"
                    },
                    {
                        "name": "Chaohao Yang"
                    },
                    {
                        "name": "Tao Peng"
                    },
                    {
                        "name": "Prayag Tiwari"
                    },
                    {
                        "name": "Xiang Wan"
                    },
                    {
                        "name": "Lu Liu"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "arxiv_comment": "This article has an original arxiv version entitled \"Is Information\n  Extraction Solved by ChatGPT? An Analysis of Performance, Evaluation\n  Criteria, Robustness and Errors\", whose url link is arXiv/2305.14450",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00369v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00369v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00128v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00128v2",
                "updated": "2024-09-04T03:21:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    3,
                    21,
                    7,
                    2,
                    248,
                    0
                ],
                "published": "2024-08-29T05:18:50Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    5,
                    18,
                    50,
                    3,
                    242,
                    0
                ],
                "title": "Can AI Replace Human Subjects? A Large-Scale Replication of\n  Psychological Experiments with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can AI Replace Human Subjects? A Large-Scale Replication of\n  Psychological Experiments with LLMs"
                },
                "summary": "Artificial Intelligence (AI) is increasingly being integrated into scientific\nresearch, particularly in the social sciences, where understanding human\nbehavior is critical. Large Language Models (LLMs) like GPT-4 have shown\npromise in replicating human-like responses in various psychological\nexperiments. However, the extent to which LLMs can effectively replace human\nsubjects across diverse experimental contexts remains unclear. Here, we conduct\na large-scale study replicating 154 psychological experiments from top social\nscience journals with 618 main effects and 138 interaction effects using GPT-4\nas a simulated participant. We find that GPT-4 successfully replicates 76.0\npercent of main effects and 47.0 percent of interaction effects observed in the\noriginal studies, closely mirroring human responses in both direction and\nsignificance. However, only 19.44 percent of GPT-4's replicated confidence\nintervals contain the original effect sizes, with the majority of replicated\neffect sizes exceeding the 95 percent confidence interval of the original\nstudies. Additionally, there is a 71.6 percent rate of unexpected significant\nresults where the original studies reported null findings, suggesting potential\noverestimation or false positives. Our results demonstrate the potential of\nLLMs as powerful tools in psychological research but also emphasize the need\nfor caution in interpreting AI-driven findings. While LLMs can complement human\nstudies, they cannot yet fully replace the nuanced insights provided by human\nsubjects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) is increasingly being integrated into scientific\nresearch, particularly in the social sciences, where understanding human\nbehavior is critical. Large Language Models (LLMs) like GPT-4 have shown\npromise in replicating human-like responses in various psychological\nexperiments. However, the extent to which LLMs can effectively replace human\nsubjects across diverse experimental contexts remains unclear. Here, we conduct\na large-scale study replicating 154 psychological experiments from top social\nscience journals with 618 main effects and 138 interaction effects using GPT-4\nas a simulated participant. We find that GPT-4 successfully replicates 76.0\npercent of main effects and 47.0 percent of interaction effects observed in the\noriginal studies, closely mirroring human responses in both direction and\nsignificance. However, only 19.44 percent of GPT-4's replicated confidence\nintervals contain the original effect sizes, with the majority of replicated\neffect sizes exceeding the 95 percent confidence interval of the original\nstudies. Additionally, there is a 71.6 percent rate of unexpected significant\nresults where the original studies reported null findings, suggesting potential\noverestimation or false positives. Our results demonstrate the potential of\nLLMs as powerful tools in psychological research but also emphasize the need\nfor caution in interpreting AI-driven findings. While LLMs can complement human\nstudies, they cannot yet fully replace the nuanced insights provided by human\nsubjects."
                },
                "authors": [
                    {
                        "name": "Ziyan Cui"
                    },
                    {
                        "name": "Ning Li"
                    },
                    {
                        "name": "Huaikang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Huaikang Zhou"
                },
                "author": "Huaikang Zhou",
                "arxiv_comment": "5 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00128v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00128v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02404v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02404v1",
                "updated": "2024-09-04T03:06:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    3,
                    6,
                    13,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T03:06:13Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    3,
                    6,
                    13,
                    2,
                    248,
                    0
                ],
                "title": "Learning Privacy-Preserving Student Networks via\n  Discriminative-Generative Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Privacy-Preserving Student Networks via\n  Discriminative-Generative Distillation"
                },
                "summary": "While deep models have proved successful in learning rich knowledge from\nmassive well-annotated data, they may pose a privacy leakage risk in practical\ndeployment. It is necessary to find an effective trade-off between high utility\nand strong privacy. In this work, we propose a discriminative-generative\ndistillation approach to learn privacy-preserving deep models. Our key idea is\ntaking models as bridge to distill knowledge from private data and then\ntransfer it to learn a student network via two streams. First, discriminative\nstream trains a baseline classifier on private data and an ensemble of teachers\non multiple disjoint private subsets, respectively. Then, generative stream\ntakes the classifier as a fixed discriminator and trains a generator in a\ndata-free manner. After that, the generator is used to generate massive\nsynthetic data which are further applied to train a variational autoencoder\n(VAE). Among these synthetic data, a few of them are fed into the teacher\nensemble to query labels via differentially private aggregation, while most of\nthem are embedded to the trained VAE for reconstructing synthetic data.\nFinally, a semi-supervised student learning is performed to simultaneously\nhandle two tasks: knowledge transfer from the teachers with distillation on few\nprivately labeled synthetic data, and knowledge enhancement with tangent-normal\nadversarial regularization on many triples of reconstructed synthetic data. In\nthis way, our approach can control query cost over private data and mitigate\naccuracy degradation in a unified manner, leading to a privacy-preserving\nstudent model. Extensive experiments and analysis clearly show the\neffectiveness of the proposed approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While deep models have proved successful in learning rich knowledge from\nmassive well-annotated data, they may pose a privacy leakage risk in practical\ndeployment. It is necessary to find an effective trade-off between high utility\nand strong privacy. In this work, we propose a discriminative-generative\ndistillation approach to learn privacy-preserving deep models. Our key idea is\ntaking models as bridge to distill knowledge from private data and then\ntransfer it to learn a student network via two streams. First, discriminative\nstream trains a baseline classifier on private data and an ensemble of teachers\non multiple disjoint private subsets, respectively. Then, generative stream\ntakes the classifier as a fixed discriminator and trains a generator in a\ndata-free manner. After that, the generator is used to generate massive\nsynthetic data which are further applied to train a variational autoencoder\n(VAE). Among these synthetic data, a few of them are fed into the teacher\nensemble to query labels via differentially private aggregation, while most of\nthem are embedded to the trained VAE for reconstructing synthetic data.\nFinally, a semi-supervised student learning is performed to simultaneously\nhandle two tasks: knowledge transfer from the teachers with distillation on few\nprivately labeled synthetic data, and knowledge enhancement with tangent-normal\nadversarial regularization on many triples of reconstructed synthetic data. In\nthis way, our approach can control query cost over private data and mitigate\naccuracy degradation in a unified manner, leading to a privacy-preserving\nstudent model. Extensive experiments and analysis clearly show the\neffectiveness of the proposed approach."
                },
                "authors": [
                    {
                        "name": "Shiming Ge"
                    },
                    {
                        "name": "Bochao Liu"
                    },
                    {
                        "name": "Pengju Wang"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Dan Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Dan Zeng"
                },
                "author": "Dan Zeng",
                "arxiv_doi": "10.1109/TIP.2022.3226416",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TIP.2022.3226416",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02404v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02404v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper is accepted by IEEE Transactions on Image Processing (TIP)",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02392v1",
                "updated": "2024-09-04T02:41:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    2,
                    41,
                    4,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T02:41:04Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    2,
                    41,
                    4,
                    2,
                    248,
                    0
                ],
                "title": "Building Math Agents with Multi-Turn Iterative Preference Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building Math Agents with Multi-Turn Iterative Preference Learning"
                },
                "summary": "Recent studies have shown that large language models' (LLMs) mathematical\nproblem-solving capabilities can be enhanced by integrating external tools,\nsuch as code interpreters, and employing multi-turn Chain-of-Thought (CoT)\nreasoning. While current methods focus on synthetic data generation and\nSupervised Fine-Tuning (SFT), this paper studies the complementary direct\npreference learning approach to further improve model performance. However,\nexisting direct preference learning algorithms are originally designed for the\nsingle-turn chat task, and do not fully address the complexities of multi-turn\nreasoning and external tool integration required for tool-integrated\nmathematical reasoning tasks. To fill in this gap, we introduce a multi-turn\ndirect preference learning framework, tailored for this context, that leverages\nfeedback from code interpreters and optimizes trajectory-level preferences.\nThis framework includes multi-turn DPO and multi-turn KTO as specific\nimplementations. The effectiveness of our framework is validated through\ntraining of various language models using an augmented prompt set from the\nGSM8K and MATH datasets. Our results demonstrate substantial improvements: a\nsupervised fine-tuned Gemma-1.1-it-7B model's performance increased from 77.5%\nto 83.9% on GSM8K and from 46.1% to 51.2% on MATH. Similarly, a Gemma-2-it-9B\nmodel improved from 84.1% to 86.3% on GSM8K and from 51.0% to 54.5% on MATH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have shown that large language models' (LLMs) mathematical\nproblem-solving capabilities can be enhanced by integrating external tools,\nsuch as code interpreters, and employing multi-turn Chain-of-Thought (CoT)\nreasoning. While current methods focus on synthetic data generation and\nSupervised Fine-Tuning (SFT), this paper studies the complementary direct\npreference learning approach to further improve model performance. However,\nexisting direct preference learning algorithms are originally designed for the\nsingle-turn chat task, and do not fully address the complexities of multi-turn\nreasoning and external tool integration required for tool-integrated\nmathematical reasoning tasks. To fill in this gap, we introduce a multi-turn\ndirect preference learning framework, tailored for this context, that leverages\nfeedback from code interpreters and optimizes trajectory-level preferences.\nThis framework includes multi-turn DPO and multi-turn KTO as specific\nimplementations. The effectiveness of our framework is validated through\ntraining of various language models using an augmented prompt set from the\nGSM8K and MATH datasets. Our results demonstrate substantial improvements: a\nsupervised fine-tuned Gemma-1.1-it-7B model's performance increased from 77.5%\nto 83.9% on GSM8K and from 46.1% to 51.2% on MATH. Similarly, a Gemma-2-it-9B\nmodel improved from 84.1% to 86.3% on GSM8K and from 51.0% to 54.5% on MATH."
                },
                "authors": [
                    {
                        "name": "Wei Xiong"
                    },
                    {
                        "name": "Chengshuai Shi"
                    },
                    {
                        "name": "Jiaming Shen"
                    },
                    {
                        "name": "Aviv Rosenberg"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Daniele Calandriello"
                    },
                    {
                        "name": "Misha Khalman"
                    },
                    {
                        "name": "Rishabh Joshi"
                    },
                    {
                        "name": "Bilal Piot"
                    },
                    {
                        "name": "Mohammad Saleh"
                    },
                    {
                        "name": "Chi Jin"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Tianqi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tianqi Liu"
                },
                "author": "Tianqi Liu",
                "arxiv_comment": "A multi-turn direct preference learning framework for tool-integrated\n  reasoning tasks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02391v1",
                "updated": "2024-09-04T02:39:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    2,
                    39,
                    31,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T02:39:31Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    2,
                    39,
                    31,
                    2,
                    248,
                    0
                ],
                "title": "Scaling Laws for Economic Productivity: Experimental Evidence in\n  LLM-Assisted Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Laws for Economic Productivity: Experimental Evidence in\n  LLM-Assisted Translation"
                },
                "summary": "This paper derives 'scaling laws' -- empirical relationships between the\namount of training compute used for a Large Language Model (LLM) and its\nperformance -- for economic outcomes. In a preregistered experiment, 300\nprofessional translators completed 1800 tasks with access to one of thirteen\nLLMs with differing model training compute sizes (or a control). Our results\nshow that model scaling substantially raises productivity: for every 10x\nincrease in model compute, translators completed tasks 12.3% quicker, received\n0.18 s.d. higher grades, and earned 16.1% more per minute (including bonus\npayments). Further, the gains from model scaling are much higher for\nlower-skilled workers who gain a 4x larger improvement in task completion\nspeed. These results imply further frontier model scaling -- which is currently\nestimated at 4x increase per year -- may have significant economic\nimplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper derives 'scaling laws' -- empirical relationships between the\namount of training compute used for a Large Language Model (LLM) and its\nperformance -- for economic outcomes. In a preregistered experiment, 300\nprofessional translators completed 1800 tasks with access to one of thirteen\nLLMs with differing model training compute sizes (or a control). Our results\nshow that model scaling substantially raises productivity: for every 10x\nincrease in model compute, translators completed tasks 12.3% quicker, received\n0.18 s.d. higher grades, and earned 16.1% more per minute (including bonus\npayments). Further, the gains from model scaling are much higher for\nlower-skilled workers who gain a 4x larger improvement in task completion\nspeed. These results imply further frontier model scaling -- which is currently\nestimated at 4x increase per year -- may have significant economic\nimplications."
                },
                "authors": [
                    {
                        "name": "Ali Merali"
                    }
                ],
                "author_detail": {
                    "name": "Ali Merali"
                },
                "author": "Ali Merali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02387v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02387v2",
                "updated": "2024-09-05T05:36:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    5,
                    36,
                    10,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-04T02:30:12Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    2,
                    30,
                    12,
                    2,
                    248,
                    0
                ],
                "title": "Large Language Models and Cognitive Science: A Comprehensive Review of\n  Similarities, Differences, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models and Cognitive Science: A Comprehensive Review of\n  Similarities, Differences, and Challenges"
                },
                "summary": "This comprehensive review explores the intersection of Large Language Models\n(LLMs) and cognitive science, examining similarities and differences between\nLLMs and human cognitive processes. We analyze methods for evaluating LLMs\ncognitive abilities and discuss their potential as cognitive models. The review\ncovers applications of LLMs in various cognitive fields, highlighting insights\ngained for cognitive science research. We assess cognitive biases and\nlimitations of LLMs, along with proposed methods for improving their\nperformance. The integration of LLMs with cognitive architectures is examined,\nrevealing promising avenues for enhancing artificial intelligence (AI)\ncapabilities. Key challenges and future research directions are identified,\nemphasizing the need for continued refinement of LLMs to better align with\nhuman cognition. This review provides a balanced perspective on the current\nstate and future potential of LLMs in advancing our understanding of both\nartificial and human intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This comprehensive review explores the intersection of Large Language Models\n(LLMs) and cognitive science, examining similarities and differences between\nLLMs and human cognitive processes. We analyze methods for evaluating LLMs\ncognitive abilities and discuss their potential as cognitive models. The review\ncovers applications of LLMs in various cognitive fields, highlighting insights\ngained for cognitive science research. We assess cognitive biases and\nlimitations of LLMs, along with proposed methods for improving their\nperformance. The integration of LLMs with cognitive architectures is examined,\nrevealing promising avenues for enhancing artificial intelligence (AI)\ncapabilities. Key challenges and future research directions are identified,\nemphasizing the need for continued refinement of LLMs to better align with\nhuman cognition. This review provides a balanced perspective on the current\nstate and future potential of LLMs in advancing our understanding of both\nartificial and human intelligence."
                },
                "authors": [
                    {
                        "name": "Qian Niu"
                    },
                    {
                        "name": "Junyu Liu"
                    },
                    {
                        "name": "Ziqian Bi"
                    },
                    {
                        "name": "Pohsun Feng"
                    },
                    {
                        "name": "Benji Peng"
                    },
                    {
                        "name": "Keyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Keyu Chen"
                },
                "author": "Keyu Chen",
                "arxiv_comment": "10 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02387v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02387v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16586v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16586v2",
                "updated": "2024-09-04T02:24:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    2,
                    24,
                    8,
                    2,
                    248,
                    0
                ],
                "published": "2024-08-29T14:49:13Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    49,
                    13,
                    3,
                    242,
                    0
                ],
                "title": "Enhancing Dialogue Generation in Werewolf Game Through Situation\n  Analysis and Persuasion Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Dialogue Generation in Werewolf Game Through Situation\n  Analysis and Persuasion Strategies"
                },
                "summary": "Recent advancements in natural language processing, particularly with large\nlanguage models (LLMs) like GPT-4, have significantly enhanced dialogue\nsystems, enabling them to generate more natural and fluent conversations.\nDespite these improvements, challenges persist, such as managing continuous\ndialogues, memory retention, and minimizing hallucinations. The AIWolfDial2024\naddresses these challenges by employing the Werewolf Game, an incomplete\ninformation game, to test the capabilities of LLMs in complex interactive\nenvironments. This paper introduces a LLM-based Werewolf Game AI, where each\nrole is supported by situation analysis to aid response generation.\nAdditionally, for the werewolf role, various persuasion strategies, including\nlogical appeal, credibility appeal, and emotional appeal, are employed to\neffectively persuade other players to align with its actions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in natural language processing, particularly with large\nlanguage models (LLMs) like GPT-4, have significantly enhanced dialogue\nsystems, enabling them to generate more natural and fluent conversations.\nDespite these improvements, challenges persist, such as managing continuous\ndialogues, memory retention, and minimizing hallucinations. The AIWolfDial2024\naddresses these challenges by employing the Werewolf Game, an incomplete\ninformation game, to test the capabilities of LLMs in complex interactive\nenvironments. This paper introduces a LLM-based Werewolf Game AI, where each\nrole is supported by situation analysis to aid response generation.\nAdditionally, for the werewolf role, various persuasion strategies, including\nlogical appeal, credibility appeal, and emotional appeal, are employed to\neffectively persuade other players to align with its actions."
                },
                "authors": [
                    {
                        "name": "Zhiyang Qi"
                    },
                    {
                        "name": "Michimasa Inaba"
                    }
                ],
                "author_detail": {
                    "name": "Michimasa Inaba"
                },
                "author": "Michimasa Inaba",
                "arxiv_comment": "Accepted to the AIWolfDial2024 workshop at INLG 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16586v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16586v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02384v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02384v1",
                "updated": "2024-09-04T02:20:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    2,
                    20,
                    59,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T02:20:59Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    2,
                    20,
                    59,
                    2,
                    248,
                    0
                ],
                "title": "STAB: Speech Tokenizer Assessment Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAB: Speech Tokenizer Assessment Benchmark"
                },
                "summary": "Representing speech as discrete tokens provides a framework for transforming\nspeech into a format that closely resembles text, thus enabling the use of\nspeech as an input to the widely successful large language models (LLMs).\nCurrently, while several speech tokenizers have been proposed, there is\nambiguity regarding the properties that are desired from a tokenizer for\nspecific downstream tasks and its overall generalizability. Evaluating the\nperformance of tokenizers across different downstream tasks is a\ncomputationally intensive effort that poses challenges for scalability. To\ncircumvent this requirement, we present STAB (Speech Tokenizer Assessment\nBenchmark), a systematic evaluation framework designed to assess speech\ntokenizers comprehensively and shed light on their inherent characteristics.\nThis framework provides a deeper understanding of the underlying mechanisms of\nspeech tokenization, thereby offering a valuable resource for expediting the\nadvancement of future tokenizer models and enabling comparative analysis using\na standardized benchmark. We evaluate the STAB metrics and correlate this with\ndownstream task performance across a range of speech tasks and tokenizer\nchoices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representing speech as discrete tokens provides a framework for transforming\nspeech into a format that closely resembles text, thus enabling the use of\nspeech as an input to the widely successful large language models (LLMs).\nCurrently, while several speech tokenizers have been proposed, there is\nambiguity regarding the properties that are desired from a tokenizer for\nspecific downstream tasks and its overall generalizability. Evaluating the\nperformance of tokenizers across different downstream tasks is a\ncomputationally intensive effort that poses challenges for scalability. To\ncircumvent this requirement, we present STAB (Speech Tokenizer Assessment\nBenchmark), a systematic evaluation framework designed to assess speech\ntokenizers comprehensively and shed light on their inherent characteristics.\nThis framework provides a deeper understanding of the underlying mechanisms of\nspeech tokenization, thereby offering a valuable resource for expediting the\nadvancement of future tokenizer models and enabling comparative analysis using\na standardized benchmark. We evaluate the STAB metrics and correlate this with\ndownstream task performance across a range of speech tasks and tokenizer\nchoices."
                },
                "authors": [
                    {
                        "name": "Shikhar Vashishth"
                    },
                    {
                        "name": "Harman Singh"
                    },
                    {
                        "name": "Shikhar Bharadwaj"
                    },
                    {
                        "name": "Sriram Ganapathy"
                    },
                    {
                        "name": "Chulayuth Asawaroengchai"
                    },
                    {
                        "name": "Kartik Audhkhasi"
                    },
                    {
                        "name": "Andrew Rosenberg"
                    },
                    {
                        "name": "Ankur Bapna"
                    },
                    {
                        "name": "Bhuvana Ramabhadran"
                    }
                ],
                "author_detail": {
                    "name": "Bhuvana Ramabhadran"
                },
                "author": "Bhuvana Ramabhadran",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02384v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04298v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04298v2",
                "updated": "2024-09-04T02:00:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    2,
                    0,
                    58,
                    2,
                    248,
                    0
                ],
                "published": "2024-04-04T20:27:37Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    20,
                    27,
                    37,
                    3,
                    95,
                    0
                ],
                "title": "SELF-[IN]CORRECT: LLMs Struggle with Discriminating Self-Generated\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SELF-[IN]CORRECT: LLMs Struggle with Discriminating Self-Generated\n  Responses"
                },
                "summary": "Can LLMs consistently improve their previous outputs for better results? For\nthis to be true, LLMs would need to be better at discriminating among\npreviously-generated alternatives, than generating initial responses. We\nexplore the validity of this hypothesis in practice. We first formulate a\nunified framework that allows us to compare the generative and discriminative\ncapability of any model on any task. In our resulting experimental analysis of\nseveral open-source and industrial LLMs, we observe that models are not\nreliably better at discriminating among previously-generated alternatives than\ngenerating initial responses. This finding challenges the notion that LLMs may\nbe able to enhance their performance only through their own judgment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs consistently improve their previous outputs for better results? For\nthis to be true, LLMs would need to be better at discriminating among\npreviously-generated alternatives, than generating initial responses. We\nexplore the validity of this hypothesis in practice. We first formulate a\nunified framework that allows us to compare the generative and discriminative\ncapability of any model on any task. In our resulting experimental analysis of\nseveral open-source and industrial LLMs, we observe that models are not\nreliably better at discriminating among previously-generated alternatives than\ngenerating initial responses. This finding challenges the notion that LLMs may\nbe able to enhance their performance only through their own judgment."
                },
                "authors": [
                    {
                        "name": "Dongwei Jiang"
                    },
                    {
                        "name": "Jingyu Zhang"
                    },
                    {
                        "name": "Orion Weller"
                    },
                    {
                        "name": "Nathaniel Weir"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    },
                    {
                        "name": "Daniel Khashabi"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Khashabi"
                },
                "author": "Daniel Khashabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04298v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04298v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02375v1",
                "updated": "2024-09-04T01:51:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    1,
                    51,
                    37,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T01:51:37Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    1,
                    51,
                    37,
                    2,
                    248,
                    0
                ],
                "title": "How Privacy-Savvy Are Large Language Models? A Case Study on Compliance\n  and Privacy Technical Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Privacy-Savvy Are Large Language Models? A Case Study on Compliance\n  and Privacy Technical Review"
                },
                "summary": "The recent advances in large language models (LLMs) have significantly\nexpanded their applications across various fields such as language generation,\nsummarization, and complex question answering. However, their application to\nprivacy compliance and technical privacy reviews remains under-explored,\nraising critical concerns about their ability to adhere to global privacy\nstandards and protect sensitive user data. This paper seeks to address this gap\nby providing a comprehensive case study evaluating LLMs' performance in\nprivacy-related tasks such as privacy information extraction (PIE), legal and\nregulatory key point detection (KPD), and question answering (QA) with respect\nto privacy policies and data protection regulations. We introduce a Privacy\nTechnical Review (PTR) framework, highlighting its role in mitigating privacy\nrisks during the software development life-cycle. Through an empirical\nassessment, we investigate the capacity of several prominent LLMs, including\nBERT, GPT-3.5, GPT-4, and custom models, in executing privacy compliance checks\nand technical privacy reviews. Our experiments benchmark the models across\nmultiple dimensions, focusing on their precision, recall, and F1-scores in\nextracting privacy-sensitive information and detecting key regulatory\ncompliance points. While LLMs show promise in automating privacy reviews and\nidentifying regulatory discrepancies, significant gaps persist in their ability\nto fully comply with evolving legal standards. We provide actionable\nrecommendations for enhancing LLMs' capabilities in privacy compliance,\nemphasizing the need for robust model improvements and better integration with\nlegal and regulatory requirements. This study underscores the growing\nimportance of developing privacy-aware LLMs that can both support businesses in\ncompliance efforts and safeguard user privacy rights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advances in large language models (LLMs) have significantly\nexpanded their applications across various fields such as language generation,\nsummarization, and complex question answering. However, their application to\nprivacy compliance and technical privacy reviews remains under-explored,\nraising critical concerns about their ability to adhere to global privacy\nstandards and protect sensitive user data. This paper seeks to address this gap\nby providing a comprehensive case study evaluating LLMs' performance in\nprivacy-related tasks such as privacy information extraction (PIE), legal and\nregulatory key point detection (KPD), and question answering (QA) with respect\nto privacy policies and data protection regulations. We introduce a Privacy\nTechnical Review (PTR) framework, highlighting its role in mitigating privacy\nrisks during the software development life-cycle. Through an empirical\nassessment, we investigate the capacity of several prominent LLMs, including\nBERT, GPT-3.5, GPT-4, and custom models, in executing privacy compliance checks\nand technical privacy reviews. Our experiments benchmark the models across\nmultiple dimensions, focusing on their precision, recall, and F1-scores in\nextracting privacy-sensitive information and detecting key regulatory\ncompliance points. While LLMs show promise in automating privacy reviews and\nidentifying regulatory discrepancies, significant gaps persist in their ability\nto fully comply with evolving legal standards. We provide actionable\nrecommendations for enhancing LLMs' capabilities in privacy compliance,\nemphasizing the need for robust model improvements and better integration with\nlegal and regulatory requirements. This study underscores the growing\nimportance of developing privacy-aware LLMs that can both support businesses in\ncompliance efforts and safeguard user privacy rights."
                },
                "authors": [
                    {
                        "name": "Xichou Zhu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zhou Shen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Min Li"
                    },
                    {
                        "name": "Yujun Chen"
                    },
                    {
                        "name": "Benzi John"
                    },
                    {
                        "name": "Zhenzhen Ma"
                    },
                    {
                        "name": "Tao Hu"
                    },
                    {
                        "name": "Bolong Yang"
                    },
                    {
                        "name": "Manman Wang"
                    },
                    {
                        "name": "Zongxing Xie"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Dan Cai"
                    },
                    {
                        "name": "Junhui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Junhui Wang"
                },
                "author": "Junhui Wang",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02370v1",
                "updated": "2024-09-04T01:40:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    1,
                    40,
                    20,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T01:40:20Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    1,
                    40,
                    20,
                    2,
                    248,
                    0
                ],
                "title": "Do Large Language Models Possess Sensitive to Sentiment?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Possess Sensitive to Sentiment?"
                },
                "summary": "Large Language Models (LLMs) have recently displayed their extraordinary\ncapabilities in language understanding. However, how to comprehensively assess\nthe sentiment capabilities of LLMs continues to be a challenge. This paper\ninvestigates the ability of LLMs to detect and react to sentiment in text\nmodal. As the integration of LLMs into diverse applications is on the rise, it\nbecomes highly critical to comprehend their sensitivity to emotional tone, as\nit can influence the user experience and the efficacy of sentiment-driven\ntasks. We conduct a series of experiments to evaluate the performance of\nseveral prominent LLMs in identifying and responding appropriately to\nsentiments like positive, negative, and neutral emotions. The models' outputs\nare analyzed across various sentiment benchmarks, and their responses are\ncompared with human evaluations. Our discoveries indicate that although LLMs\nshow a basic sensitivity to sentiment, there are substantial variations in\ntheir accuracy and consistency, emphasizing the requirement for further\nenhancements in their training processes to better capture subtle emotional\ncues. Take an example in our findings, in some cases, the models might wrongly\nclassify a strongly positive sentiment as neutral, or fail to recognize sarcasm\nor irony in the text. Such misclassifications highlight the complexity of\nsentiment analysis and the areas where the models need to be refined. Another\naspect is that different LLMs might perform differently on the same set of\ndata, depending on their architecture and training datasets. This variance\ncalls for a more in-depth study of the factors that contribute to the\nperformance differences and how they can be optimized.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently displayed their extraordinary\ncapabilities in language understanding. However, how to comprehensively assess\nthe sentiment capabilities of LLMs continues to be a challenge. This paper\ninvestigates the ability of LLMs to detect and react to sentiment in text\nmodal. As the integration of LLMs into diverse applications is on the rise, it\nbecomes highly critical to comprehend their sensitivity to emotional tone, as\nit can influence the user experience and the efficacy of sentiment-driven\ntasks. We conduct a series of experiments to evaluate the performance of\nseveral prominent LLMs in identifying and responding appropriately to\nsentiments like positive, negative, and neutral emotions. The models' outputs\nare analyzed across various sentiment benchmarks, and their responses are\ncompared with human evaluations. Our discoveries indicate that although LLMs\nshow a basic sensitivity to sentiment, there are substantial variations in\ntheir accuracy and consistency, emphasizing the requirement for further\nenhancements in their training processes to better capture subtle emotional\ncues. Take an example in our findings, in some cases, the models might wrongly\nclassify a strongly positive sentiment as neutral, or fail to recognize sarcasm\nor irony in the text. Such misclassifications highlight the complexity of\nsentiment analysis and the areas where the models need to be refined. Another\naspect is that different LLMs might perform differently on the same set of\ndata, depending on their architecture and training datasets. This variance\ncalls for a more in-depth study of the factors that contribute to the\nperformance differences and how they can be optimized."
                },
                "authors": [
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xichou Zhu"
                    },
                    {
                        "name": "Zhou Shen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Min Li"
                    },
                    {
                        "name": "Yujun Chen"
                    },
                    {
                        "name": "Benzi John"
                    },
                    {
                        "name": "Zhenzhen Ma"
                    },
                    {
                        "name": "Tao Hu"
                    },
                    {
                        "name": "Zhiyang Xu"
                    },
                    {
                        "name": "Wei Luo"
                    },
                    {
                        "name": "Junhui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Junhui Wang"
                },
                "author": "Junhui Wang",
                "arxiv_comment": "10 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02366v1",
                "updated": "2024-09-04T01:33:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    1,
                    33,
                    16,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T01:33:16Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    1,
                    33,
                    16,
                    2,
                    248,
                    0
                ],
                "title": "The Hidden Costs of Automation: An Empirical Study on GitHub Actions\n  Workflow Maintenance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hidden Costs of Automation: An Empirical Study on GitHub Actions\n  Workflow Maintenance"
                },
                "summary": "GitHub Actions (GA) is an orchestration platform that streamlines the\nautomatic execution of software engineering tasks such as building, testing,\nand deployment. Although GA workflows are the primary means for automation,\naccording to our experience and observations, human intervention is necessary\nto correct defects, update dependencies, or refactor existing workflow files.\nIn fact, previous research has shown that software artifacts similar to\nworkflows, such as build files and bots, can introduce additional maintenance\ntasks in software projects. This suggests that workflow files, which are also\nused to automate repetitive tasks in professional software production, may\ngenerate extra workload for developers. However, the nature of such effort has\nnot been well studied. This paper presents a large-scale empirical\ninvestigation towards characterizing the maintenance of GA workflows by\nstudying the evolution of workflow files in almost 200 mature GitHub projects\nacross ten programming languages. Our findings largely confirm the results of\nprevious studies on the maintenance of similar artifacts, while also revealing\nGA-specific insights such as bug fixing and CI/CD improvement being among the\nmajor drivers of GA maintenance. A direct implication is that practitioners\nshould be aware of proper resource planning and allocation for maintaining GA\nworkflows, thus exposing the ``hidden costs of automation.'' Our findings also\ncall for identifying and documenting best practices for such maintenance, and\nfor enhanced tool features supporting dependency tracking and better error\nreporting of workflow specifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GitHub Actions (GA) is an orchestration platform that streamlines the\nautomatic execution of software engineering tasks such as building, testing,\nand deployment. Although GA workflows are the primary means for automation,\naccording to our experience and observations, human intervention is necessary\nto correct defects, update dependencies, or refactor existing workflow files.\nIn fact, previous research has shown that software artifacts similar to\nworkflows, such as build files and bots, can introduce additional maintenance\ntasks in software projects. This suggests that workflow files, which are also\nused to automate repetitive tasks in professional software production, may\ngenerate extra workload for developers. However, the nature of such effort has\nnot been well studied. This paper presents a large-scale empirical\ninvestigation towards characterizing the maintenance of GA workflows by\nstudying the evolution of workflow files in almost 200 mature GitHub projects\nacross ten programming languages. Our findings largely confirm the results of\nprevious studies on the maintenance of similar artifacts, while also revealing\nGA-specific insights such as bug fixing and CI/CD improvement being among the\nmajor drivers of GA maintenance. A direct implication is that practitioners\nshould be aware of proper resource planning and allocation for maintaining GA\nworkflows, thus exposing the ``hidden costs of automation.'' Our findings also\ncall for identifying and documenting best practices for such maintenance, and\nfor enhanced tool features supporting dependency tracking and better error\nreporting of workflow specifications."
                },
                "authors": [
                    {
                        "name": "Pablo Valenzuela-Toledo"
                    },
                    {
                        "name": "Alexandre Bergel"
                    },
                    {
                        "name": "Timo Kehrer"
                    },
                    {
                        "name": "Oscar Nierstrasz"
                    }
                ],
                "author_detail": {
                    "name": "Oscar Nierstrasz"
                },
                "author": "Oscar Nierstrasz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08464v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08464v2",
                "updated": "2024-09-04T01:30:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    1,
                    30,
                    23,
                    2,
                    248,
                    0
                ],
                "published": "2024-08-16T00:18:23Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    0,
                    18,
                    23,
                    4,
                    229,
                    0
                ],
                "title": "$\\textit{MMJ-Bench}$: A Comprehensive Study on Jailbreak Attacks and\n  Defenses for Vision Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$\\textit{MMJ-Bench}$: A Comprehensive Study on Jailbreak Attacks and\n  Defenses for Vision Language Models"
                },
                "summary": "As deep learning advances, Large Language Models (LLMs) and their multimodal\ncounterparts, Vision-Language Models (VLMs), have shown exceptional performance\nin many real-world tasks. However, VLMs face significant security challenges,\nsuch as jailbreak attacks, where attackers attempt to bypass the model's safety\nalignment to elicit harmful responses. The threat of jailbreak attacks on VLMs\narises from both the inherent vulnerabilities of LLMs and the multiple\ninformation channels that VLMs process. While various attacks and defenses have\nbeen proposed, there is a notable gap in unified and comprehensive evaluations,\nas each method is evaluated on different dataset and metrics, making it\nimpossible to compare the effectiveness of each method. To address this gap, we\nintroduce \\textit{MMJ-Bench}, a unified pipeline for evaluating jailbreak\nattacks and defense techniques for VLMs. Through extensive experiments, we\nassess the effectiveness of various attack methods against SoTA VLMs and\nevaluate the impact of defense mechanisms on both defense effectiveness and\nmodel utility for normal tasks. Our comprehensive evaluation contribute to the\nfield by offering a unified and systematic evaluation framework and the first\npublic-available benchmark for VLM jailbreak research. We also demonstrate\nseveral insightful findings that highlights directions for future studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As deep learning advances, Large Language Models (LLMs) and their multimodal\ncounterparts, Vision-Language Models (VLMs), have shown exceptional performance\nin many real-world tasks. However, VLMs face significant security challenges,\nsuch as jailbreak attacks, where attackers attempt to bypass the model's safety\nalignment to elicit harmful responses. The threat of jailbreak attacks on VLMs\narises from both the inherent vulnerabilities of LLMs and the multiple\ninformation channels that VLMs process. While various attacks and defenses have\nbeen proposed, there is a notable gap in unified and comprehensive evaluations,\nas each method is evaluated on different dataset and metrics, making it\nimpossible to compare the effectiveness of each method. To address this gap, we\nintroduce \\textit{MMJ-Bench}, a unified pipeline for evaluating jailbreak\nattacks and defense techniques for VLMs. Through extensive experiments, we\nassess the effectiveness of various attack methods against SoTA VLMs and\nevaluate the impact of defense mechanisms on both defense effectiveness and\nmodel utility for normal tasks. Our comprehensive evaluation contribute to the\nfield by offering a unified and systematic evaluation framework and the first\npublic-available benchmark for VLM jailbreak research. We also demonstrate\nseveral insightful findings that highlights directions for future studies."
                },
                "authors": [
                    {
                        "name": "Fenghua Weng"
                    },
                    {
                        "name": "Yue Xu"
                    },
                    {
                        "name": "Chengyan Fu"
                    },
                    {
                        "name": "Wenjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Wang"
                },
                "author": "Wenjie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08464v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08464v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15221v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15221v2",
                "updated": "2024-09-04T00:58:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    0,
                    58,
                    59,
                    2,
                    248,
                    0
                ],
                "published": "2024-08-27T17:33:30Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    33,
                    30,
                    1,
                    240,
                    0
                ],
                "title": "LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet"
                },
                "summary": "Recent large language model (LLM) defenses have greatly improved models'\nability to refuse harmful queries, even when adversarially attacked. However,\nLLM defenses are primarily evaluated against automated adversarial attacks in a\nsingle turn of conversation, an insufficient threat model for real-world\nmalicious use. We demonstrate that multi-turn human jailbreaks uncover\nsignificant vulnerabilities, exceeding 70% attack success rate (ASR) on\nHarmBench against defenses that report single-digit ASRs with automated\nsingle-turn attacks. Human jailbreaks also reveal vulnerabilities in machine\nunlearning defenses, successfully recovering dual-use biosecurity knowledge\nfrom unlearned models. We compile these results into Multi-Turn Human\nJailbreaks (MHJ), a dataset of 2,912 prompts across 537 multi-turn jailbreaks.\nWe publicly release MHJ alongside a compendium of jailbreak tactics developed\nacross dozens of commercial red teaming engagements, supporting research\ntowards stronger LLM defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model (LLM) defenses have greatly improved models'\nability to refuse harmful queries, even when adversarially attacked. However,\nLLM defenses are primarily evaluated against automated adversarial attacks in a\nsingle turn of conversation, an insufficient threat model for real-world\nmalicious use. We demonstrate that multi-turn human jailbreaks uncover\nsignificant vulnerabilities, exceeding 70% attack success rate (ASR) on\nHarmBench against defenses that report single-digit ASRs with automated\nsingle-turn attacks. Human jailbreaks also reveal vulnerabilities in machine\nunlearning defenses, successfully recovering dual-use biosecurity knowledge\nfrom unlearned models. We compile these results into Multi-Turn Human\nJailbreaks (MHJ), a dataset of 2,912 prompts across 537 multi-turn jailbreaks.\nWe publicly release MHJ alongside a compendium of jailbreak tactics developed\nacross dozens of commercial red teaming engagements, supporting research\ntowards stronger LLM defenses."
                },
                "authors": [
                    {
                        "name": "Nathaniel Li"
                    },
                    {
                        "name": "Ziwen Han"
                    },
                    {
                        "name": "Ian Steneker"
                    },
                    {
                        "name": "Willow Primack"
                    },
                    {
                        "name": "Riley Goodside"
                    },
                    {
                        "name": "Hugh Zhang"
                    },
                    {
                        "name": "Zifan Wang"
                    },
                    {
                        "name": "Cristina Menghini"
                    },
                    {
                        "name": "Summer Yue"
                    }
                ],
                "author_detail": {
                    "name": "Summer Yue"
                },
                "author": "Summer Yue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15221v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15221v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06266v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06266v4",
                "updated": "2024-09-04T00:22:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    0,
                    22,
                    45,
                    2,
                    248,
                    0
                ],
                "published": "2024-08-12T16:24:51Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    24,
                    51,
                    0,
                    225,
                    0
                ],
                "title": "Anchored Preference Optimization and Contrastive Revisions: Addressing\n  Underspecification in Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anchored Preference Optimization and Contrastive Revisions: Addressing\n  Underspecification in Alignment"
                },
                "summary": "Large Language Models (LLMs) are often aligned using contrastive alignment\nobjectives and preference pair datasets. The interaction between model, paired\ndata, and objective makes alignment a complicated procedure, sometimes\nproducing subpar results. We study this and find that (i) preference data gives\na better learning signal when the underlying responses are contrastive, and\n(ii) alignment objectives lead to better performance when they specify more\ncontrol over the model during training. Based on these insights, we introduce\nContrastive Learning from AI Revisions (CLAIR), a data-creation method which\nleads to more contrastive preference pairs, and Anchored Preference\nOptimization (APO), a controllable and more stable alignment objective. We\nalign Llama-3-8B-Instruct using various comparable datasets and alignment\nobjectives and measure MixEval-Hard scores, which correlate highly with human\njudgments. The CLAIR preferences lead to the strongest performance out of all\ndatasets, and APO consistently outperforms less controllable objectives. Our\nbest model, trained on 32K CLAIR preferences with APO, improves\nLlama-3-8B-Instruct by 7.65%, closing the gap with GPT4-turbo by 45%. Our code\nis available at https://github.com/ContextualAI/CLAIR_and_APO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are often aligned using contrastive alignment\nobjectives and preference pair datasets. The interaction between model, paired\ndata, and objective makes alignment a complicated procedure, sometimes\nproducing subpar results. We study this and find that (i) preference data gives\na better learning signal when the underlying responses are contrastive, and\n(ii) alignment objectives lead to better performance when they specify more\ncontrol over the model during training. Based on these insights, we introduce\nContrastive Learning from AI Revisions (CLAIR), a data-creation method which\nleads to more contrastive preference pairs, and Anchored Preference\nOptimization (APO), a controllable and more stable alignment objective. We\nalign Llama-3-8B-Instruct using various comparable datasets and alignment\nobjectives and measure MixEval-Hard scores, which correlate highly with human\njudgments. The CLAIR preferences lead to the strongest performance out of all\ndatasets, and APO consistently outperforms less controllable objectives. Our\nbest model, trained on 32K CLAIR preferences with APO, improves\nLlama-3-8B-Instruct by 7.65%, closing the gap with GPT4-turbo by 45%. Our code\nis available at https://github.com/ContextualAI/CLAIR_and_APO."
                },
                "authors": [
                    {
                        "name": "Karel D'Oosterlinck"
                    },
                    {
                        "name": "Winnie Xu"
                    },
                    {
                        "name": "Chris Develder"
                    },
                    {
                        "name": "Thomas Demeester"
                    },
                    {
                        "name": "Amanpreet Singh"
                    },
                    {
                        "name": "Christopher Potts"
                    },
                    {
                        "name": "Douwe Kiela"
                    },
                    {
                        "name": "Shikib Mehri"
                    }
                ],
                "author_detail": {
                    "name": "Shikib Mehri"
                },
                "author": "Shikib Mehri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06266v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06266v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.02078v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.02078v2",
                "updated": "2024-09-04T00:06:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    0,
                    6,
                    20,
                    2,
                    248,
                    0
                ],
                "published": "2023-12-04T17:41:52Z",
                "published_parsed": [
                    2023,
                    12,
                    4,
                    17,
                    41,
                    52,
                    0,
                    338,
                    0
                ],
                "title": "From Lab to Field: Real-World Evaluation of an AI-Driven Smart Video\n  Solution to Enhance Community Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Lab to Field: Real-World Evaluation of an AI-Driven Smart Video\n  Solution to Enhance Community Safety"
                },
                "summary": "This article adopts and evaluates an AI-enabled Smart Video Solution (SVS)\ndesigned to enhance safety in the real world. The system integrates with\nexisting infrastructure camera networks, leveraging recent advancements in AI\nfor easy adoption. Prioritizing privacy and ethical standards, pose based data\nis used for downstream AI tasks such as anomaly detection. Cloud-based\ninfrastructure and mobile app are deployed, enabling real-time alerts within\ncommunities. The SVS employs innovative data representation and visualization\ntechniques, such as the Occupancy Indicator, Statistical Anomaly Detection,\nBird's Eye View, and Heatmaps, to understand pedestrian behaviors and enhance\npublic safety. Evaluation of the SVS demonstrates its capacity to convert\ncomplex computer vision outputs into actionable insights for stakeholders,\ncommunity partners, law enforcement, urban planners, and social scientists.\nThis article presents a comprehensive real-world deployment and evaluation of\nthe SVS, implemented in a community college environment across 16 cameras. The\nsystem integrates AI-driven visual processing, supported by statistical\nanalysis, database management, cloud communication, and user notifications.\nAdditionally, the article evaluates the end-to-end latency from the moment an\nAI algorithm detects anomalous behavior in real-time at the camera level to the\ntime stakeholders receive a notification. The results demonstrate the system's\nrobustness, effectively managing 16 CCTV cameras with a consistent throughput\nof 16.5 frames per second (FPS) over a 21-hour period and an average end-to-end\nlatency of 26.76 seconds between anomaly detection and alert issuance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article adopts and evaluates an AI-enabled Smart Video Solution (SVS)\ndesigned to enhance safety in the real world. The system integrates with\nexisting infrastructure camera networks, leveraging recent advancements in AI\nfor easy adoption. Prioritizing privacy and ethical standards, pose based data\nis used for downstream AI tasks such as anomaly detection. Cloud-based\ninfrastructure and mobile app are deployed, enabling real-time alerts within\ncommunities. The SVS employs innovative data representation and visualization\ntechniques, such as the Occupancy Indicator, Statistical Anomaly Detection,\nBird's Eye View, and Heatmaps, to understand pedestrian behaviors and enhance\npublic safety. Evaluation of the SVS demonstrates its capacity to convert\ncomplex computer vision outputs into actionable insights for stakeholders,\ncommunity partners, law enforcement, urban planners, and social scientists.\nThis article presents a comprehensive real-world deployment and evaluation of\nthe SVS, implemented in a community college environment across 16 cameras. The\nsystem integrates AI-driven visual processing, supported by statistical\nanalysis, database management, cloud communication, and user notifications.\nAdditionally, the article evaluates the end-to-end latency from the moment an\nAI algorithm detects anomalous behavior in real-time at the camera level to the\ntime stakeholders receive a notification. The results demonstrate the system's\nrobustness, effectively managing 16 CCTV cameras with a consistent throughput\nof 16.5 frames per second (FPS) over a 21-hour period and an average end-to-end\nlatency of 26.76 seconds between anomaly detection and alert issuance."
                },
                "authors": [
                    {
                        "name": "Shanle Yao"
                    },
                    {
                        "name": "Babak Rahimi Ardabili"
                    },
                    {
                        "name": "Armin Danesh Pazho"
                    },
                    {
                        "name": "Ghazal Alinezhad Noghre"
                    },
                    {
                        "name": "Christopher Neff"
                    },
                    {
                        "name": "Lauren Bourque"
                    },
                    {
                        "name": "Hamed Tabkhi"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Tabkhi"
                },
                "author": "Hamed Tabkhi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.02078v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.02078v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16746v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16746v3",
                "updated": "2024-09-03T23:03:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    23,
                    3,
                    41,
                    1,
                    247,
                    0
                ],
                "published": "2024-06-24T15:55:49Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    15,
                    55,
                    49,
                    0,
                    176,
                    0
                ],
                "title": "The Responsible Foundation Model Development Cheatsheet: A Review of\n  Tools & Resources",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Responsible Foundation Model Development Cheatsheet: A Review of\n  Tools & Resources"
                },
                "summary": "Foundation model development attracts a rapidly expanding body of\ncontributors, scientists, and applications. To help shape responsible\ndevelopment practices, we introduce the Foundation Model Development\nCheatsheet: a growing collection of 250+ tools and resources spanning text,\nvision, and speech modalities. We draw on a large body of prior work to survey\nresources (e.g. software, documentation, frameworks, guides, and practical\ntools) that support informed data selection, processing, and understanding,\nprecise and limitation-aware artifact documentation, efficient model training,\nadvance awareness of the environmental impact from training, careful model\nevaluation of capabilities, risks, and claims, as well as responsible model\nrelease, licensing and deployment practices. We hope this curated collection of\nresources helps guide more responsible development. The process of curating\nthis list, enabled us to review the AI development ecosystem, revealing what\ntools are critically missing, misused, or over-used in existing practices. We\nfind that (i) tools for data sourcing, model evaluation, and monitoring are\ncritically under-serving ethical and real-world needs, (ii) evaluations for\nmodel safety, capabilities, and environmental impact all lack reproducibility\nand transparency, (iii) text and particularly English-centric analyses continue\nto dominate over multilingual and multi-modal analyses, and (iv) evaluation of\nsystems, rather than just models, is needed so that capabilities and impact are\nassessed in context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation model development attracts a rapidly expanding body of\ncontributors, scientists, and applications. To help shape responsible\ndevelopment practices, we introduce the Foundation Model Development\nCheatsheet: a growing collection of 250+ tools and resources spanning text,\nvision, and speech modalities. We draw on a large body of prior work to survey\nresources (e.g. software, documentation, frameworks, guides, and practical\ntools) that support informed data selection, processing, and understanding,\nprecise and limitation-aware artifact documentation, efficient model training,\nadvance awareness of the environmental impact from training, careful model\nevaluation of capabilities, risks, and claims, as well as responsible model\nrelease, licensing and deployment practices. We hope this curated collection of\nresources helps guide more responsible development. The process of curating\nthis list, enabled us to review the AI development ecosystem, revealing what\ntools are critically missing, misused, or over-used in existing practices. We\nfind that (i) tools for data sourcing, model evaluation, and monitoring are\ncritically under-serving ethical and real-world needs, (ii) evaluations for\nmodel safety, capabilities, and environmental impact all lack reproducibility\nand transparency, (iii) text and particularly English-centric analyses continue\nto dominate over multilingual and multi-modal analyses, and (iv) evaluation of\nsystems, rather than just models, is needed so that capabilities and impact are\nassessed in context."
                },
                "authors": [
                    {
                        "name": "Shayne Longpre"
                    },
                    {
                        "name": "Stella Biderman"
                    },
                    {
                        "name": "Alon Albalak"
                    },
                    {
                        "name": "Hailey Schoelkopf"
                    },
                    {
                        "name": "Daniel McDuff"
                    },
                    {
                        "name": "Sayash Kapoor"
                    },
                    {
                        "name": "Kevin Klyman"
                    },
                    {
                        "name": "Kyle Lo"
                    },
                    {
                        "name": "Gabriel Ilharco"
                    },
                    {
                        "name": "Nay San"
                    },
                    {
                        "name": "Maribeth Rauh"
                    },
                    {
                        "name": "Aviya Skowron"
                    },
                    {
                        "name": "Bertie Vidgen"
                    },
                    {
                        "name": "Laura Weidinger"
                    },
                    {
                        "name": "Arvind Narayanan"
                    },
                    {
                        "name": "Victor Sanh"
                    },
                    {
                        "name": "David Adelani"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Rishi Bommasani"
                    },
                    {
                        "name": "Peter Henderson"
                    },
                    {
                        "name": "Sasha Luccioni"
                    },
                    {
                        "name": "Yacine Jernite"
                    },
                    {
                        "name": "Luca Soldaini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Soldaini"
                },
                "author": "Luca Soldaini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16746v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16746v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02329v1",
                "updated": "2024-09-03T22:39:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    22,
                    39,
                    27,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T22:39:27Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    22,
                    39,
                    27,
                    1,
                    247,
                    0
                ],
                "title": "Site Selection for the Second Flyeye Telescope: A Simulation Study for\n  Optimizing Near-Earth Object Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Site Selection for the Second Flyeye Telescope: A Simulation Study for\n  Optimizing Near-Earth Object Discovery"
                },
                "summary": "The European Space Agency (ESA) is developing a network of wide-field survey\ntelescopes, named Flyeye, to improve the discovery of Near-Earth Objects\n(NEOs). The first telescope in the network will be located in the Northern\nHemisphere on Mount Mufara (Italy), and a second Flyeye telescope, featuring\nincreased detection capabilities, has just started the critical design phase.\nThe potential location for the second Flyeye telescope is investigated by\nperforming simulations of NEOs on impacting trajectories. Approximately 3000\nimpacting asteroids of two absolute magnitudes (H=25 and H=28) were propagated\nand tested for detectability by major existing surveys (Catalina, Pan-STARRS,\nATLAS), the upcoming Vera Rubin Observatory (LSST), and possible Flyeye\nlocations. Chile, South Africa, and a second facility in the Northern\nHemisphere were considered. For each observatory, their past or planned\npointing strategies were taken into account in the simulation. Before LSST\ndeployment, a single Flyeye in the Southern Hemisphere performs similarly to a\ntelescope in the Northern Hemisphere. When combined, having one telescope in\nthe north and one in the south maximizes detections and number of unique\nobjects detected. After LSST, southern and northern Flyeye telescopes remain\ncomplementary. Overall, simulations show that a second Flyeye in the south\ncomplements a Flyeye telescope in the north both before and after LSST. A\nFlyeye located at La Silla would take advantage of the excellent atmospheric\nconditions, while allowing a balance of assets across hemispheres.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The European Space Agency (ESA) is developing a network of wide-field survey\ntelescopes, named Flyeye, to improve the discovery of Near-Earth Objects\n(NEOs). The first telescope in the network will be located in the Northern\nHemisphere on Mount Mufara (Italy), and a second Flyeye telescope, featuring\nincreased detection capabilities, has just started the critical design phase.\nThe potential location for the second Flyeye telescope is investigated by\nperforming simulations of NEOs on impacting trajectories. Approximately 3000\nimpacting asteroids of two absolute magnitudes (H=25 and H=28) were propagated\nand tested for detectability by major existing surveys (Catalina, Pan-STARRS,\nATLAS), the upcoming Vera Rubin Observatory (LSST), and possible Flyeye\nlocations. Chile, South Africa, and a second facility in the Northern\nHemisphere were considered. For each observatory, their past or planned\npointing strategies were taken into account in the simulation. Before LSST\ndeployment, a single Flyeye in the Southern Hemisphere performs similarly to a\ntelescope in the Northern Hemisphere. When combined, having one telescope in\nthe north and one in the south maximizes detections and number of unique\nobjects detected. After LSST, southern and northern Flyeye telescopes remain\ncomplementary. Overall, simulations show that a second Flyeye in the south\ncomplements a Flyeye telescope in the north both before and after LSST. A\nFlyeye located at La Silla would take advantage of the excellent atmospheric\nconditions, while allowing a balance of assets across hemispheres."
                },
                "authors": [
                    {
                        "name": "D. Fhring"
                    },
                    {
                        "name": "L. Conversi"
                    },
                    {
                        "name": "M. Micheli"
                    },
                    {
                        "name": "E. Dlling"
                    },
                    {
                        "name": "P. Ramirez Moreta"
                    }
                ],
                "author_detail": {
                    "name": "P. Ramirez Moreta"
                },
                "author": "P. Ramirez Moreta",
                "arxiv_doi": "10.1016/j.icarus.2024.116281",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.icarus.2024.116281",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages 7 figures",
                "arxiv_journal_ref": "Icarus, 116281 (2024)",
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02291v1",
                "updated": "2024-09-03T21:04:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    21,
                    4,
                    7,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T21:04:07Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    21,
                    4,
                    7,
                    1,
                    247,
                    0
                ],
                "title": "Initial Development and Evaluation of the Creative Artificial\n  Intelligence through Recurring Developments and Determinations (CAIRDD)\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Initial Development and Evaluation of the Creative Artificial\n  Intelligence through Recurring Developments and Determinations (CAIRDD)\n  System"
                },
                "summary": "Computer system creativity is a key step on the pathway to artificial general\nintelligence (AGI). It is elusive, however, due to the fact that human\ncreativity is not fully understood and, thus, it is difficult to develop this\ncapability in software. Large language models (LLMs) provide a facsimile of\ncreativity and the appearance of sentience, while not actually being either\ncreative or sentient. While LLMs have created bona fide new content, in some\ncases - such as with harmful hallucinations - inadvertently, their deliberate\ncreativity is seen by some to not match that of humans. In response to this\nchallenge, this paper proposes a technique for enhancing LLM output creativity\nvia an iterative process of concept injection and refinement. Initial work on\nthe development of the Creative Artificial Intelligence through Recurring\nDevelopments and Determinations (CAIRDD) system is presented and the efficacy\nof key system components is evaluated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer system creativity is a key step on the pathway to artificial general\nintelligence (AGI). It is elusive, however, due to the fact that human\ncreativity is not fully understood and, thus, it is difficult to develop this\ncapability in software. Large language models (LLMs) provide a facsimile of\ncreativity and the appearance of sentience, while not actually being either\ncreative or sentient. While LLMs have created bona fide new content, in some\ncases - such as with harmful hallucinations - inadvertently, their deliberate\ncreativity is seen by some to not match that of humans. In response to this\nchallenge, this paper proposes a technique for enhancing LLM output creativity\nvia an iterative process of concept injection and refinement. Initial work on\nthe development of the Creative Artificial Intelligence through Recurring\nDevelopments and Determinations (CAIRDD) system is presented and the efficacy\nof key system components is evaluated."
                },
                "authors": [
                    {
                        "name": "Jeremy Straub"
                    },
                    {
                        "name": "Zach Johnson"
                    }
                ],
                "author_detail": {
                    "name": "Zach Johnson"
                },
                "author": "Zach Johnson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02273v1",
                "updated": "2024-09-03T20:09:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    20,
                    9,
                    7,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T20:09:07Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    20,
                    9,
                    7,
                    1,
                    247,
                    0
                ],
                "title": "SlipNet: Slip Cost Map for Autonomous Navigation on Heterogeneous\n  Deformable Terrains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlipNet: Slip Cost Map for Autonomous Navigation on Heterogeneous\n  Deformable Terrains"
                },
                "summary": "Autonomous space rovers face significant challenges when navigating\ndeformable and heterogeneous terrains during space exploration. The variability\nin terrain types, influenced by different soil properties, often results in\nsevere wheel slip, compromising navigation efficiency and potentially leading\nto entrapment. This paper proposes SlipNet, an approach for predicting slip in\nsegmented regions of heterogeneous deformable terrain surfaces to enhance\nnavigation algorithms. Unlike previous methods, SlipNet does not depend on\nprior terrain classification, reducing prediction errors and misclassifications\nthrough dynamic terrain segmentation and slip assignment during deployment\nwhile maintaining a history of terrain classes. This adaptive reclassification\nmechanism has improved prediction performance. Extensive simulation results\ndemonstrate that our model (DeepLab v3+ + SlipNet) achieves better slip\nprediction performance than the TerrainNet, with a lower mean absolute error\n(MAE) in five terrain sample tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous space rovers face significant challenges when navigating\ndeformable and heterogeneous terrains during space exploration. The variability\nin terrain types, influenced by different soil properties, often results in\nsevere wheel slip, compromising navigation efficiency and potentially leading\nto entrapment. This paper proposes SlipNet, an approach for predicting slip in\nsegmented regions of heterogeneous deformable terrain surfaces to enhance\nnavigation algorithms. Unlike previous methods, SlipNet does not depend on\nprior terrain classification, reducing prediction errors and misclassifications\nthrough dynamic terrain segmentation and slip assignment during deployment\nwhile maintaining a history of terrain classes. This adaptive reclassification\nmechanism has improved prediction performance. Extensive simulation results\ndemonstrate that our model (DeepLab v3+ + SlipNet) achieves better slip\nprediction performance than the TerrainNet, with a lower mean absolute error\n(MAE) in five terrain sample tests."
                },
                "authors": [
                    {
                        "name": "Mubarak Yakubu"
                    },
                    {
                        "name": "Yahya Zweiri"
                    },
                    {
                        "name": "Ahmad Abubakar"
                    },
                    {
                        "name": "Rana Azzam"
                    },
                    {
                        "name": "Ruqayya Alhammadi"
                    },
                    {
                        "name": "Lakmal Seneviratne"
                    }
                ],
                "author_detail": {
                    "name": "Lakmal Seneviratne"
                },
                "author": "Lakmal Seneviratne",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.00832v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.00832v2",
                "updated": "2024-09-03T19:43:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    19,
                    43,
                    53,
                    1,
                    247,
                    0
                ],
                "published": "2024-01-01T18:11:43Z",
                "published_parsed": [
                    2024,
                    1,
                    1,
                    18,
                    11,
                    43,
                    0,
                    1,
                    0
                ],
                "title": "Taking the Next Step with Generative Artificial Intelligence: The\n  Transformative Role of Multimodal Large Language Models in Science Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taking the Next Step with Generative Artificial Intelligence: The\n  Transformative Role of Multimodal Large Language Models in Science Education"
                },
                "summary": "The integration of Artificial Intelligence (AI), particularly Large Language\nModel (LLM)-based systems, in education has shown promise in enhancing teaching\nand learning experiences. However, the advent of Multimodal Large Language\nModels (MLLMs) like GPT-4 with vision (GPT-4V), capable of processing\nmultimodal data including text, sound, and visual inputs, opens a new era of\nenriched, personalized, and interactive learning landscapes in education.\nGrounded in theory of multimedia learning, this paper explores the\ntransformative role of MLLMs in central aspects of science education by\npresenting exemplary innovative learning scenarios. Possible applications for\nMLLMs could range from content creation to tailored support for learning,\nfostering competencies in scientific practices, and providing assessment and\nfeedback. These scenarios are not limited to text-based and uni-modal formats\nbut can be multimodal, increasing thus personalization, accessibility, and\npotential learning effectiveness. Besides many opportunities, challenges such\nas data protection and ethical considerations become more salient, calling for\nrobust frameworks to ensure responsible integration. This paper underscores the\nnecessity for a balanced approach in implementing MLLMs, where the technology\ncomplements rather than supplants the educator's role, ensuring thus an\neffective and ethical use of AI in science education. It calls for further\nresearch to explore the nuanced implications of MLLMs on the evolving role of\neducators and to extend the discourse beyond science education to other\ndisciplines. Through the exploration of potentials, challenges, and future\nimplications, we aim to contribute to a preliminary understanding of the\ntransformative trajectory of MLLMs in science education and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Artificial Intelligence (AI), particularly Large Language\nModel (LLM)-based systems, in education has shown promise in enhancing teaching\nand learning experiences. However, the advent of Multimodal Large Language\nModels (MLLMs) like GPT-4 with vision (GPT-4V), capable of processing\nmultimodal data including text, sound, and visual inputs, opens a new era of\nenriched, personalized, and interactive learning landscapes in education.\nGrounded in theory of multimedia learning, this paper explores the\ntransformative role of MLLMs in central aspects of science education by\npresenting exemplary innovative learning scenarios. Possible applications for\nMLLMs could range from content creation to tailored support for learning,\nfostering competencies in scientific practices, and providing assessment and\nfeedback. These scenarios are not limited to text-based and uni-modal formats\nbut can be multimodal, increasing thus personalization, accessibility, and\npotential learning effectiveness. Besides many opportunities, challenges such\nas data protection and ethical considerations become more salient, calling for\nrobust frameworks to ensure responsible integration. This paper underscores the\nnecessity for a balanced approach in implementing MLLMs, where the technology\ncomplements rather than supplants the educator's role, ensuring thus an\neffective and ethical use of AI in science education. It calls for further\nresearch to explore the nuanced implications of MLLMs on the evolving role of\neducators and to extend the discourse beyond science education to other\ndisciplines. Through the exploration of potentials, challenges, and future\nimplications, we aim to contribute to a preliminary understanding of the\ntransformative trajectory of MLLMs in science education and beyond."
                },
                "authors": [
                    {
                        "name": "Arne Bewersdorff"
                    },
                    {
                        "name": "Christian Hartmann"
                    },
                    {
                        "name": "Marie Hornberger"
                    },
                    {
                        "name": "Kathrin Seler"
                    },
                    {
                        "name": "Maria Bannert"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    },
                    {
                        "name": "Xiaoming Zhai"
                    },
                    {
                        "name": "Claudia Nerdel"
                    }
                ],
                "author_detail": {
                    "name": "Claudia Nerdel"
                },
                "author": "Claudia Nerdel",
                "arxiv_comment": "revised version 2. September 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.00832v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.00832v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19534v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19534v2",
                "updated": "2024-09-03T19:37:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    19,
                    37,
                    27,
                    1,
                    247,
                    0
                ],
                "published": "2024-05-29T21:29:44Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    21,
                    29,
                    44,
                    2,
                    150,
                    0
                ],
                "title": "Preference Learning Algorithms Do Not Learn Preference Rankings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference Learning Algorithms Do Not Learn Preference Rankings"
                },
                "summary": "Preference learning algorithms (e.g., RLHF and DPO) are frequently used to\nsteer LLMs to produce generations that are more preferred by humans, but our\nunderstanding of their inner workings is still limited. In this work, we study\nthe conventional wisdom that preference learning trains models to assign higher\nlikelihoods to more preferred outputs than less preferred outputs, measured via\n$\\textit{ranking accuracy}$. Surprisingly, we find that most state-of-the-art\npreference-tuned models achieve a ranking accuracy of less than 60% on common\npreference datasets. We furthermore derive the $\\textit{idealized ranking\naccuracy}$ that a preference-tuned LLM would achieve if it optimized the DPO or\nRLHF objective perfectly. We demonstrate that existing models exhibit a\nsignificant $\\textit{alignment gap}$ -- $\\textit{i.e.}$, a gap between the\nobserved and idealized ranking accuracies. We attribute this discrepancy to the\nDPO objective, which is empirically and theoretically ill-suited to fix even\nmild ranking errors in the reference model, and derive a simple and efficient\nformula for quantifying the difficulty of learning a given preference\ndatapoint. Finally, we demonstrate that ranking accuracy strongly correlates\nwith the empirically popular win rate metric when the model is close to the\nreference model used in the objective, shedding further light on the\ndifferences between on-policy (e.g., RLHF) and off-policy (e.g., DPO)\npreference learning algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference learning algorithms (e.g., RLHF and DPO) are frequently used to\nsteer LLMs to produce generations that are more preferred by humans, but our\nunderstanding of their inner workings is still limited. In this work, we study\nthe conventional wisdom that preference learning trains models to assign higher\nlikelihoods to more preferred outputs than less preferred outputs, measured via\n$\\textit{ranking accuracy}$. Surprisingly, we find that most state-of-the-art\npreference-tuned models achieve a ranking accuracy of less than 60% on common\npreference datasets. We furthermore derive the $\\textit{idealized ranking\naccuracy}$ that a preference-tuned LLM would achieve if it optimized the DPO or\nRLHF objective perfectly. We demonstrate that existing models exhibit a\nsignificant $\\textit{alignment gap}$ -- $\\textit{i.e.}$, a gap between the\nobserved and idealized ranking accuracies. We attribute this discrepancy to the\nDPO objective, which is empirically and theoretically ill-suited to fix even\nmild ranking errors in the reference model, and derive a simple and efficient\nformula for quantifying the difficulty of learning a given preference\ndatapoint. Finally, we demonstrate that ranking accuracy strongly correlates\nwith the empirically popular win rate metric when the model is close to the\nreference model used in the objective, shedding further light on the\ndifferences between on-policy (e.g., RLHF) and off-policy (e.g., DPO)\npreference learning algorithms."
                },
                "authors": [
                    {
                        "name": "Angelica Chen"
                    },
                    {
                        "name": "Sadhika Malladi"
                    },
                    {
                        "name": "Lily H. Zhang"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Qiuyi Zhang"
                    },
                    {
                        "name": "Rajesh Ranganath"
                    },
                    {
                        "name": "Kyunghyun Cho"
                    }
                ],
                "author_detail": {
                    "name": "Kyunghyun Cho"
                },
                "author": "Kyunghyun Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19534v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19534v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02257v1",
                "updated": "2024-09-03T19:31:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    19,
                    31,
                    3,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T19:31:03Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    19,
                    31,
                    3,
                    1,
                    247,
                    0
                ],
                "title": "MMLU-Pro+: Evaluating Higher-Order Reasoning and Shortcut Learning in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMLU-Pro+: Evaluating Higher-Order Reasoning and Shortcut Learning in\n  LLMs"
                },
                "summary": "Existing benchmarks for large language models (LLMs) increasingly struggle to\ndifferentiate between top-performing models, underscoring the need for more\nchallenging evaluation frameworks. We introduce MMLU-Pro+, an enhanced\nbenchmark building upon MMLU-Pro to assess shortcut learning and higher-order\nreasoning in LLMs. By incorporating questions with multiple correct answers\nacross diverse domains, MMLU-Pro+ tests LLMs' ability to engage in complex\nreasoning and resist simplistic problem-solving strategies. Our results show\nthat MMLU-Pro+ maintains MMLU-Pro's difficulty while providing a more rigorous\ntest of model discrimination, particularly in multi-correct answer scenarios.\nWe introduce novel metrics like shortcut selection ratio and correct pair\nidentification ratio, offering deeper insights into model behavior and\nanchoring bias. Evaluations of five state-of-the-art LLMs reveal significant\nperformance gaps, highlighting variations in reasoning abilities and bias\nsusceptibility. We release the dataset and evaluation codes at\n\\url{https://github.com/asgsaeid/mmlu-pro-plus}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing benchmarks for large language models (LLMs) increasingly struggle to\ndifferentiate between top-performing models, underscoring the need for more\nchallenging evaluation frameworks. We introduce MMLU-Pro+, an enhanced\nbenchmark building upon MMLU-Pro to assess shortcut learning and higher-order\nreasoning in LLMs. By incorporating questions with multiple correct answers\nacross diverse domains, MMLU-Pro+ tests LLMs' ability to engage in complex\nreasoning and resist simplistic problem-solving strategies. Our results show\nthat MMLU-Pro+ maintains MMLU-Pro's difficulty while providing a more rigorous\ntest of model discrimination, particularly in multi-correct answer scenarios.\nWe introduce novel metrics like shortcut selection ratio and correct pair\nidentification ratio, offering deeper insights into model behavior and\nanchoring bias. Evaluations of five state-of-the-art LLMs reveal significant\nperformance gaps, highlighting variations in reasoning abilities and bias\nsusceptibility. We release the dataset and evaluation codes at\n\\url{https://github.com/asgsaeid/mmlu-pro-plus}."
                },
                "authors": [
                    {
                        "name": "Saeid Asgari Taghanaki"
                    },
                    {
                        "name": "Aliasgahr Khani"
                    },
                    {
                        "name": "Amir Khasahmadi"
                    }
                ],
                "author_detail": {
                    "name": "Amir Khasahmadi"
                },
                "author": "Amir Khasahmadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16994v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16994v2",
                "updated": "2024-09-03T19:28:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    19,
                    28,
                    39,
                    1,
                    247,
                    0
                ],
                "published": "2024-07-24T04:27:55Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    4,
                    27,
                    55,
                    2,
                    206,
                    0
                ],
                "title": "A Voter-Based Stochastic Rejection-Method Framework for Asymptotically\n  Safe Language Model Outputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Voter-Based Stochastic Rejection-Method Framework for Asymptotically\n  Safe Language Model Outputs"
                },
                "summary": "This paper proposes a new method for preventing unsafe or otherwise low\nquality large language model (LLM) outputs, by leveraging the stochasticity of\nLLMs. We propose a system whereby LLM checkers vote on the acceptability of a\ngenerated output, regenerating it if a threshold of disapproval is reached,\nuntil sufficient checkers approve. We further propose estimators for cost and\nfailure rate, and based on those estimators and experimental data tailored to\nthe application, we propose an algorithm that achieves a desired failure rate\nat the least possible cost. We demonstrate that, under these models, failure\nrate decreases exponentially as a function of cost when voter count and\nthreshold are chosen according to the algorithm, and that the models reasonably\nestimate the actual performance of such a system in action, even with limited\ndata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a new method for preventing unsafe or otherwise low\nquality large language model (LLM) outputs, by leveraging the stochasticity of\nLLMs. We propose a system whereby LLM checkers vote on the acceptability of a\ngenerated output, regenerating it if a threshold of disapproval is reached,\nuntil sufficient checkers approve. We further propose estimators for cost and\nfailure rate, and based on those estimators and experimental data tailored to\nthe application, we propose an algorithm that achieves a desired failure rate\nat the least possible cost. We demonstrate that, under these models, failure\nrate decreases exponentially as a function of cost when voter count and\nthreshold are chosen according to the algorithm, and that the models reasonably\nestimate the actual performance of such a system in action, even with limited\ndata."
                },
                "authors": [
                    {
                        "name": "Jake R. Watts"
                    },
                    {
                        "name": "Joel Sokol"
                    }
                ],
                "author_detail": {
                    "name": "Joel Sokol"
                },
                "author": "Joel Sokol",
                "arxiv_comment": "7 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16994v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16994v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.00996v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.00996v4",
                "updated": "2024-09-03T19:27:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    19,
                    27,
                    19,
                    1,
                    247,
                    0
                ],
                "published": "2023-10-02T08:58:29Z",
                "published_parsed": [
                    2023,
                    10,
                    2,
                    8,
                    58,
                    29,
                    0,
                    275,
                    0
                ],
                "title": "ARN: Analogical Reasoning on Narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARN: Analogical Reasoning on Narratives"
                },
                "summary": "As a core cognitive skill that enables the transferability of information\nacross domains, analogical reasoning has been extensively studied for both\nhumans and computational models. However, while cognitive theories of analogy\noften focus on narratives and study the distinction between surface,\nrelational, and system similarities, existing work in natural language\nprocessing has a narrower focus as far as relational analogies between word\npairs. This gap brings a natural question: can state-of-the-art large language\nmodels (LLMs) detect system analogies between narratives? To gain insight into\nthis question and extend word-based relational analogies to relational system\nanalogies, we devise a comprehensive computational framework that\noperationalizes dominant theories of analogy, using narrative elements to\ncreate surface and system mappings. Leveraging the interplay between these\nmappings, we create a binary task and benchmark for Analogical Reasoning on\nNarratives (ARN), covering four categories of far (cross-domain)/near\n(within-domain) analogies and disanalogies. We show that while all LLMs can\nlargely recognize near analogies, even the largest ones struggle with far\nanalogies in a zero-shot setting, with GPT4.0 scoring below random. Guiding the\nmodels through solved examples and chain-of-thought reasoning enhances their\nanalogical reasoning ability. Yet, since even in the few-shot setting, the best\nmodel only performs halfway between random and humans, ARN opens exciting\ndirections for computational analogical reasoners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core cognitive skill that enables the transferability of information\nacross domains, analogical reasoning has been extensively studied for both\nhumans and computational models. However, while cognitive theories of analogy\noften focus on narratives and study the distinction between surface,\nrelational, and system similarities, existing work in natural language\nprocessing has a narrower focus as far as relational analogies between word\npairs. This gap brings a natural question: can state-of-the-art large language\nmodels (LLMs) detect system analogies between narratives? To gain insight into\nthis question and extend word-based relational analogies to relational system\nanalogies, we devise a comprehensive computational framework that\noperationalizes dominant theories of analogy, using narrative elements to\ncreate surface and system mappings. Leveraging the interplay between these\nmappings, we create a binary task and benchmark for Analogical Reasoning on\nNarratives (ARN), covering four categories of far (cross-domain)/near\n(within-domain) analogies and disanalogies. We show that while all LLMs can\nlargely recognize near analogies, even the largest ones struggle with far\nanalogies in a zero-shot setting, with GPT4.0 scoring below random. Guiding the\nmodels through solved examples and chain-of-thought reasoning enhances their\nanalogical reasoning ability. Yet, since even in the few-shot setting, the best\nmodel only performs halfway between random and humans, ARN opens exciting\ndirections for computational analogical reasoners."
                },
                "authors": [
                    {
                        "name": "Zhivar Sourati"
                    },
                    {
                        "name": "Filip Ilievski"
                    },
                    {
                        "name": "Pia Sommerauer"
                    },
                    {
                        "name": "Yifan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Jiang"
                },
                "author": "Yifan Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.00996v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.00996v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02244v1",
                "updated": "2024-09-03T19:19:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    19,
                    19,
                    13,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T19:19:13Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    19,
                    19,
                    13,
                    1,
                    247,
                    0
                ],
                "title": "Therapy as an NLP Task: Psychologists' Comparison of LLMs and Human\n  Peers in CBT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Therapy as an NLP Task: Psychologists' Comparison of LLMs and Human\n  Peers in CBT"
                },
                "summary": "Wider access to therapeutic care is one of the biggest challenges in mental\nhealth treatment. Due to institutional barriers, some people seeking mental\nhealth support have turned to large language models (LLMs) for personalized\ntherapy, even though these models are largely unsanctioned and untested. We\ninvestigate the potential and limitations of using LLMs as providers of\nevidence-based therapy by using mixed methods clinical metrics. Using HELPERT,\na prompt run on a large language model using the same process and training as a\ncomparative group of peer counselors, we replicated publicly accessible mental\nhealth conversations rooted in Cognitive Behavioral Therapy (CBT) to compare\nsession dynamics and counselor's CBT-based behaviors between original peer\nsupport sessions and their reconstructed HELPERT sessions. Two licensed,\nCBT-trained clinical psychologists evaluated the sessions using the Cognitive\nTherapy Rating Scale and provided qualitative feedback. Our findings show that\nthe peer sessions are characterized by empathy, small talk, therapeutic\nalliance, and shared experiences but often exhibit therapist drift. Conversely,\nHELPERT reconstructed sessions exhibit minimal therapist drift and higher\nadherence to CBT methods but display a lack of collaboration, empathy, and\ncultural understanding. Through CTRS ratings and psychologists' feedback, we\nhighlight the importance of human-AI collaboration for scalable mental health.\nOur work outlines the ethical implication of imparting human-like subjective\nqualities to LLMs in therapeutic settings, particularly the risk of deceptive\nempathy, which may lead to unrealistic patient expectations and potential harm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wider access to therapeutic care is one of the biggest challenges in mental\nhealth treatment. Due to institutional barriers, some people seeking mental\nhealth support have turned to large language models (LLMs) for personalized\ntherapy, even though these models are largely unsanctioned and untested. We\ninvestigate the potential and limitations of using LLMs as providers of\nevidence-based therapy by using mixed methods clinical metrics. Using HELPERT,\na prompt run on a large language model using the same process and training as a\ncomparative group of peer counselors, we replicated publicly accessible mental\nhealth conversations rooted in Cognitive Behavioral Therapy (CBT) to compare\nsession dynamics and counselor's CBT-based behaviors between original peer\nsupport sessions and their reconstructed HELPERT sessions. Two licensed,\nCBT-trained clinical psychologists evaluated the sessions using the Cognitive\nTherapy Rating Scale and provided qualitative feedback. Our findings show that\nthe peer sessions are characterized by empathy, small talk, therapeutic\nalliance, and shared experiences but often exhibit therapist drift. Conversely,\nHELPERT reconstructed sessions exhibit minimal therapist drift and higher\nadherence to CBT methods but display a lack of collaboration, empathy, and\ncultural understanding. Through CTRS ratings and psychologists' feedback, we\nhighlight the importance of human-AI collaboration for scalable mental health.\nOur work outlines the ethical implication of imparting human-like subjective\nqualities to LLMs in therapeutic settings, particularly the risk of deceptive\nempathy, which may lead to unrealistic patient expectations and potential harm."
                },
                "authors": [
                    {
                        "name": "Zainab Iftikhar"
                    },
                    {
                        "name": "Sean Ransom"
                    },
                    {
                        "name": "Amy Xiao"
                    },
                    {
                        "name": "Jeff Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Huang"
                },
                "author": "Jeff Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01981v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01981v2",
                "updated": "2024-09-03T19:11:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    19,
                    11,
                    11,
                    1,
                    247,
                    0
                ],
                "published": "2024-06-04T05:47:17Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    5,
                    47,
                    17,
                    1,
                    156,
                    0
                ],
                "title": "Zyda: A 1.3T Dataset for Open Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zyda: A 1.3T Dataset for Open Language Modeling"
                },
                "summary": "The size of large language models (LLMs) has scaled dramatically in recent\nyears and their computational and data requirements have surged\ncorrespondingly. State-of-the-art language models, even at relatively smaller\nsizes, typically require training on at least a trillion tokens. This rapid\nadvancement has eclipsed the growth of open-source datasets available for\nlarge-scale LLM pretraining. In this paper, we introduce Zyda (Zyphra Dataset),\na dataset under a permissive license comprising 1.3 trillion tokens, assembled\nby integrating several major respected open-source datasets into a single,\nhigh-quality corpus. We apply rigorous filtering and deduplication processes,\nboth within and across datasets, to maintain and enhance the quality derived\nfrom the original datasets. Our evaluations show that Zyda not only competes\nfavorably with other open datasets like Dolma, FineWeb, and RefinedWeb, but\nalso substantially improves the performance of comparable models from the\nPythia suite. Our rigorous data processing methods significantly enhance Zyda's\neffectiveness, outperforming even the best of its constituent datasets when\nused independently.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The size of large language models (LLMs) has scaled dramatically in recent\nyears and their computational and data requirements have surged\ncorrespondingly. State-of-the-art language models, even at relatively smaller\nsizes, typically require training on at least a trillion tokens. This rapid\nadvancement has eclipsed the growth of open-source datasets available for\nlarge-scale LLM pretraining. In this paper, we introduce Zyda (Zyphra Dataset),\na dataset under a permissive license comprising 1.3 trillion tokens, assembled\nby integrating several major respected open-source datasets into a single,\nhigh-quality corpus. We apply rigorous filtering and deduplication processes,\nboth within and across datasets, to maintain and enhance the quality derived\nfrom the original datasets. Our evaluations show that Zyda not only competes\nfavorably with other open datasets like Dolma, FineWeb, and RefinedWeb, but\nalso substantially improves the performance of comparable models from the\nPythia suite. Our rigorous data processing methods significantly enhance Zyda's\neffectiveness, outperforming even the best of its constituent datasets when\nused independently."
                },
                "authors": [
                    {
                        "name": "Yury Tokpanov"
                    },
                    {
                        "name": "Beren Millidge"
                    },
                    {
                        "name": "Paolo Glorioso"
                    },
                    {
                        "name": "Jonathan Pilault"
                    },
                    {
                        "name": "Adam Ibrahim"
                    },
                    {
                        "name": "James Whittington"
                    },
                    {
                        "name": "Quentin Anthony"
                    }
                ],
                "author_detail": {
                    "name": "Quentin Anthony"
                },
                "author": "Quentin Anthony",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01981v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01981v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02237v1",
                "updated": "2024-09-03T19:07:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    19,
                    7,
                    53,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T19:07:53Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    19,
                    7,
                    53,
                    1,
                    247,
                    0
                ],
                "title": "Open6G OTIC: A Blueprint for Programmable O-RAN and 3GPP Testing\n  Infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open6G OTIC: A Blueprint for Programmable O-RAN and 3GPP Testing\n  Infrastructure"
                },
                "summary": "Softwarized and programmable Radio Access Networks (RANs) come with\nvirtualized and disaggregated components, increasing the supply chain\nrobustness and the flexibility and dynamism of the network deployments. This is\na key tenet of Open RAN, with open interfaces across disaggregated components\nspecified by the O-RAN ALLIANCE. It is mandatory, however, to validate that all\ncomponents are compliant with the specifications and can successfully\ninteroperate, without performance gaps with traditional, monolithic appliances.\nOpen Testing & Integration Centers (OTICs) are entities that can verify such\ninteroperability and adherence to the standard through rigorous testing.\nHowever, how to design, instrument, and deploy an OTIC which can offer testing\nfor multiple tenants, heterogeneous devices, and is ready to support automated\ntesting is still an open challenge. In this paper, we introduce a blueprint for\na programmable OTIC testing infrastructure, based on the design and deployment\nof the Open6G OTIC at Northeastern University, Boston, and provide insights on\ntechnical challenges and solutions for O-RAN testing at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Softwarized and programmable Radio Access Networks (RANs) come with\nvirtualized and disaggregated components, increasing the supply chain\nrobustness and the flexibility and dynamism of the network deployments. This is\na key tenet of Open RAN, with open interfaces across disaggregated components\nspecified by the O-RAN ALLIANCE. It is mandatory, however, to validate that all\ncomponents are compliant with the specifications and can successfully\ninteroperate, without performance gaps with traditional, monolithic appliances.\nOpen Testing & Integration Centers (OTICs) are entities that can verify such\ninteroperability and adherence to the standard through rigorous testing.\nHowever, how to design, instrument, and deploy an OTIC which can offer testing\nfor multiple tenants, heterogeneous devices, and is ready to support automated\ntesting is still an open challenge. In this paper, we introduce a blueprint for\na programmable OTIC testing infrastructure, based on the design and deployment\nof the Open6G OTIC at Northeastern University, Boston, and provide insights on\ntechnical challenges and solutions for O-RAN testing at scale."
                },
                "authors": [
                    {
                        "name": "Gabriele Gemmi"
                    },
                    {
                        "name": "Michele Polese"
                    },
                    {
                        "name": "Pedram Johari"
                    },
                    {
                        "name": "Stefano Maxenti"
                    },
                    {
                        "name": "Michael Seltser"
                    },
                    {
                        "name": "Tommaso Melodia"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Melodia"
                },
                "author": "Tommaso Melodia",
                "arxiv_comment": "Presented at IEEE VTC Fall RitiRAN Workshop, 5 pages, 3 figures, 3\n  tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02231v1",
                "updated": "2024-09-03T18:59:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    18,
                    59,
                    20,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T18:59:20Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    18,
                    59,
                    20,
                    1,
                    247,
                    0
                ],
                "title": "SmileyLlama: Modifying Large Language Models for Directed Chemical Space\n  Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmileyLlama: Modifying Large Language Models for Directed Chemical Space\n  Exploration"
                },
                "summary": "Here we show that a Large Language Model (LLM) can serve as a foundation\nmodel for a Chemical Language Model (CLM) which performs at or above the level\nof CLMs trained solely on chemical SMILES string data. Using supervised\nfine-tuning (SFT) and direct preference optimization (DPO) on the open-source\nLlama LLM, we demonstrate that we can train an LLM to respond to prompts such\nas generating molecules with properties of interest to drug development. This\noverall framework allows an LLM to not just be a chatbot client for chemistry\nand materials tasks, but can be adapted to speak more directly as a CLM which\ncan generate molecules with user-specified properties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Here we show that a Large Language Model (LLM) can serve as a foundation\nmodel for a Chemical Language Model (CLM) which performs at or above the level\nof CLMs trained solely on chemical SMILES string data. Using supervised\nfine-tuning (SFT) and direct preference optimization (DPO) on the open-source\nLlama LLM, we demonstrate that we can train an LLM to respond to prompts such\nas generating molecules with properties of interest to drug development. This\noverall framework allows an LLM to not just be a chatbot client for chemistry\nand materials tasks, but can be adapted to speak more directly as a CLM which\ncan generate molecules with user-specified properties."
                },
                "authors": [
                    {
                        "name": "Joseph M. Cavanagh"
                    },
                    {
                        "name": "Kunyang Sun"
                    },
                    {
                        "name": "Andrew Gritsevskiy"
                    },
                    {
                        "name": "Dorian Bagni"
                    },
                    {
                        "name": "Thomas D. Bannister"
                    },
                    {
                        "name": "Teresa Head-Gordon"
                    }
                ],
                "author_detail": {
                    "name": "Teresa Head-Gordon"
                },
                "author": "Teresa Head-Gordon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02098v1",
                "updated": "2024-09-03T17:54:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    54,
                    40,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T17:54:40Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    54,
                    40,
                    1,
                    247,
                    0
                ],
                "title": "CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through\n  Corpus Retrieval and Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through\n  Corpus Retrieval and Augmentation"
                },
                "summary": "Building high-quality datasets for specialized tasks is a time-consuming and\nresource-intensive process that often requires specialized domain knowledge. We\npropose Corpus Retrieval and Augmentation for Fine-Tuning (CRAFT), a method for\ngenerating synthetic datasets, given a small number of user-written few-shots\nthat demonstrate the task to be performed. Given the few-shot examples, we use\nlarge-scale public web-crawled corpora and similarity-based document retrieval\nto find other relevant human-written documents. Lastly, instruction-tuned large\nlanguage models (LLMs) augment the retrieved documents into custom-formatted\ntask samples, which then can be used for fine-tuning. We demonstrate that CRAFT\ncan efficiently generate large-scale task-specific training datasets for four\ndiverse tasks: biology question-answering (QA), medicine QA and commonsense QA\nas well as summarization. Our experiments show that CRAFT-based models\noutperform or achieve comparable performance to general LLMs for QA tasks,\nwhile CRAFT-based summarization models outperform models trained on\nhuman-curated data by 46 preference points.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building high-quality datasets for specialized tasks is a time-consuming and\nresource-intensive process that often requires specialized domain knowledge. We\npropose Corpus Retrieval and Augmentation for Fine-Tuning (CRAFT), a method for\ngenerating synthetic datasets, given a small number of user-written few-shots\nthat demonstrate the task to be performed. Given the few-shot examples, we use\nlarge-scale public web-crawled corpora and similarity-based document retrieval\nto find other relevant human-written documents. Lastly, instruction-tuned large\nlanguage models (LLMs) augment the retrieved documents into custom-formatted\ntask samples, which then can be used for fine-tuning. We demonstrate that CRAFT\ncan efficiently generate large-scale task-specific training datasets for four\ndiverse tasks: biology question-answering (QA), medicine QA and commonsense QA\nas well as summarization. Our experiments show that CRAFT-based models\noutperform or achieve comparable performance to general LLMs for QA tasks,\nwhile CRAFT-based summarization models outperform models trained on\nhuman-curated data by 46 preference points."
                },
                "authors": [
                    {
                        "name": "Ingo Ziegler"
                    },
                    {
                        "name": "Abdullatif Kksal"
                    },
                    {
                        "name": "Desmond Elliott"
                    },
                    {
                        "name": "Hinrich Schtze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schtze"
                },
                "author": "Hinrich Schtze",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15444v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15444v2",
                "updated": "2024-09-03T17:48:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    48,
                    55,
                    1,
                    247,
                    0
                ],
                "published": "2024-05-30T18:07:13Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    18,
                    7,
                    13,
                    3,
                    151,
                    0
                ],
                "title": "Investigating the Robustness of LLMs on Math Word Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating the Robustness of LLMs on Math Word Problems"
                },
                "summary": "Large Language Models (LLMs) excel at various tasks, including solving math\nword problems (MWPs), but struggle with real-world problems containing\nirrelevant information. To address this, we propose a prompting framework that\ngenerates adversarial variants of MWPs by adding irrelevant variables. We\nintroduce a dataset, ProbleMATHIC, containing both adversarial and\nnon-adversarial MWPs. Our experiments reveal that LLMs are susceptible to\ndistraction by numerical noise, resulting in an average relative performance\ndrop of ~26% on adversarial MWPs. To mitigate this, we fine-tune LLMs (Llama-2,\nMistral) on the adversarial samples from our dataset. Fine-tuning on\nadversarial training instances improves performance on adversarial MWPs by ~8%,\nindicating increased robustness to noise and better ability to identify\nrelevant data for reasoning. Finally, to assess the generalizability of our\nprompting framework, we introduce GSM-8K-Adv, an adversarial variant of the\nGSM-8K benchmark. LLMs continue to struggle when faced with adversarial\ninformation, reducing performance by up to ~6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at various tasks, including solving math\nword problems (MWPs), but struggle with real-world problems containing\nirrelevant information. To address this, we propose a prompting framework that\ngenerates adversarial variants of MWPs by adding irrelevant variables. We\nintroduce a dataset, ProbleMATHIC, containing both adversarial and\nnon-adversarial MWPs. Our experiments reveal that LLMs are susceptible to\ndistraction by numerical noise, resulting in an average relative performance\ndrop of ~26% on adversarial MWPs. To mitigate this, we fine-tune LLMs (Llama-2,\nMistral) on the adversarial samples from our dataset. Fine-tuning on\nadversarial training instances improves performance on adversarial MWPs by ~8%,\nindicating increased robustness to noise and better ability to identify\nrelevant data for reasoning. Finally, to assess the generalizability of our\nprompting framework, we introduce GSM-8K-Adv, an adversarial variant of the\nGSM-8K benchmark. LLMs continue to struggle when faced with adversarial\ninformation, reducing performance by up to ~6%."
                },
                "authors": [
                    {
                        "name": "Ujjwala Anantheswaran"
                    },
                    {
                        "name": "Himanshu Gupta"
                    },
                    {
                        "name": "Kevin Scaria"
                    },
                    {
                        "name": "Shreyas Verma"
                    },
                    {
                        "name": "Chitta Baral"
                    },
                    {
                        "name": "Swaroop Mishra"
                    }
                ],
                "author_detail": {
                    "name": "Swaroop Mishra"
                },
                "author": "Swaroop Mishra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15444v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15444v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02081v1",
                "updated": "2024-09-03T17:32:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    32,
                    35,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T17:32:35Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    32,
                    35,
                    1,
                    247,
                    0
                ],
                "title": "Physical Rule-Guided Convolutional Neural Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Rule-Guided Convolutional Neural Network"
                },
                "summary": "The black-box nature of Convolutional Neural Networks (CNNs) and their\nreliance on large datasets limit their use in complex domains with limited\nlabeled data. Physics-Guided Neural Networks (PGNNs) have emerged to address\nthese limitations by integrating scientific principles and real-world\nknowledge, enhancing model interpretability and efficiency. This paper proposes\na novel Physics-Guided CNN (PGCNN) architecture that incorporates dynamic,\ntrainable, and automated LLM-generated, widely recognized rules integrated into\nthe model as custom layers to address challenges like limited data and low\nconfidence scores. The PGCNN is evaluated on multiple datasets, demonstrating\nsuperior performance compared to a baseline CNN model. Key improvements include\na significant reduction in false positives and enhanced confidence scores for\ntrue detection. The results highlight the potential of PGCNNs to improve CNN\nperformance for broader application areas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The black-box nature of Convolutional Neural Networks (CNNs) and their\nreliance on large datasets limit their use in complex domains with limited\nlabeled data. Physics-Guided Neural Networks (PGNNs) have emerged to address\nthese limitations by integrating scientific principles and real-world\nknowledge, enhancing model interpretability and efficiency. This paper proposes\na novel Physics-Guided CNN (PGCNN) architecture that incorporates dynamic,\ntrainable, and automated LLM-generated, widely recognized rules integrated into\nthe model as custom layers to address challenges like limited data and low\nconfidence scores. The PGCNN is evaluated on multiple datasets, demonstrating\nsuperior performance compared to a baseline CNN model. Key improvements include\na significant reduction in false positives and enhanced confidence scores for\ntrue detection. The results highlight the potential of PGCNNs to improve CNN\nperformance for broader application areas."
                },
                "authors": [
                    {
                        "name": "Kishor Datta Gupta"
                    },
                    {
                        "name": "Marufa Kamal"
                    },
                    {
                        "name": "Rakib Hossain Rifat"
                    },
                    {
                        "name": "Mohd Ariful Haque"
                    },
                    {
                        "name": "Roy George"
                    }
                ],
                "author_detail": {
                    "name": "Roy George"
                },
                "author": "Roy George",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02079v1",
                "updated": "2024-09-03T17:26:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    26,
                    50,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T17:26:50Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    26,
                    50,
                    1,
                    247,
                    0
                ],
                "title": "Synthetic Data Generation and Automated Multidimensional Data Labeling\n  for AI/ML in General and Circular Coordinates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Data Generation and Automated Multidimensional Data Labeling\n  for AI/ML in General and Circular Coordinates"
                },
                "summary": "Insufficient amounts of available training data is a critical challenge for\nboth development and deployment of artificial intelligence and machine learning\n(AI/ML) models. This paper proposes a unified approach to both synthetic data\ngeneration (SDG) and automated data labeling (ADL) with a unified SDG-ADL\nalgorithm. SDG-ADL uses multidimensional (n-D) representations of data\nvisualized losslessly with General Line Coordinates (GLCs), relying on\nreversible GLC properties to visualize n-D data in multiple GLCs. This paper\ndemonstrates use of the new Circular Coordinates in Static and Dynamic forms,\nused with Parallel Coordinates and Shifted Paired Coordinates, since each GLC\nexemplifies unique data properties, such as interattribute n-D distributions\nand outlier detection. The approach is interactively implemented in computer\nsoftware with the Dynamic Coordinates Visualization system (DCVis). Results\nwith real data are demonstrated in case studies, evaluating impact on\nclassifiers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insufficient amounts of available training data is a critical challenge for\nboth development and deployment of artificial intelligence and machine learning\n(AI/ML) models. This paper proposes a unified approach to both synthetic data\ngeneration (SDG) and automated data labeling (ADL) with a unified SDG-ADL\nalgorithm. SDG-ADL uses multidimensional (n-D) representations of data\nvisualized losslessly with General Line Coordinates (GLCs), relying on\nreversible GLC properties to visualize n-D data in multiple GLCs. This paper\ndemonstrates use of the new Circular Coordinates in Static and Dynamic forms,\nused with Parallel Coordinates and Shifted Paired Coordinates, since each GLC\nexemplifies unique data properties, such as interattribute n-D distributions\nand outlier detection. The approach is interactively implemented in computer\nsoftware with the Dynamic Coordinates Visualization system (DCVis). Results\nwith real data are demonstrated in case studies, evaluating impact on\nclassifiers."
                },
                "authors": [
                    {
                        "name": "Alice Williams"
                    },
                    {
                        "name": "Boris Kovalerchuk"
                    }
                ],
                "author_detail": {
                    "name": "Boris Kovalerchuk"
                },
                "author": "Boris Kovalerchuk",
                "arxiv_comment": "8 pages, 17 figures, 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02074v1",
                "updated": "2024-09-03T17:22:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    22,
                    0,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T17:22:00Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    22,
                    0,
                    1,
                    247,
                    0
                ],
                "title": "RACONTEUR: A Knowledgeable, Insightful, and Portable LLM-Powered Shell\n  Command Explainer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RACONTEUR: A Knowledgeable, Insightful, and Portable LLM-Powered Shell\n  Command Explainer"
                },
                "summary": "Malicious shell commands are linchpins to many cyber-attacks, but may not be\neasy to understand by security analysts due to complicated and often disguised\ncode structures. Advances in large language models (LLMs) have unlocked the\npossibility of generating understandable explanations for shell commands.\nHowever, existing general-purpose LLMs suffer from a lack of expert knowledge\nand a tendency to hallucinate in the task of shell command explanation. In this\npaper, we present Raconteur, a knowledgeable, expressive and portable shell\ncommand explainer powered by LLM. Raconteur is infused with professional\nknowledge to provide comprehensive explanations on shell commands, including\nnot only what the command does (i.e., behavior) but also why the command does\nit (i.e., purpose). To shed light on the high-level intent of the command, we\nalso translate the natural-language-based explanation into standard technique &\ntactic defined by MITRE ATT&CK, the worldwide knowledge base of cybersecurity.\nTo enable Raconteur to explain unseen private commands, we further develop a\ndocumentation retriever to obtain relevant information from complementary\ndocumentations to assist the explanation process. We have created a large-scale\ndataset for training and conducted extensive experiments to evaluate the\ncapability of Raconteur in shell command explanation. The experiments verify\nthat Raconteur is able to provide high-quality explanations and in-depth\ninsight of the intent of the command.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Malicious shell commands are linchpins to many cyber-attacks, but may not be\neasy to understand by security analysts due to complicated and often disguised\ncode structures. Advances in large language models (LLMs) have unlocked the\npossibility of generating understandable explanations for shell commands.\nHowever, existing general-purpose LLMs suffer from a lack of expert knowledge\nand a tendency to hallucinate in the task of shell command explanation. In this\npaper, we present Raconteur, a knowledgeable, expressive and portable shell\ncommand explainer powered by LLM. Raconteur is infused with professional\nknowledge to provide comprehensive explanations on shell commands, including\nnot only what the command does (i.e., behavior) but also why the command does\nit (i.e., purpose). To shed light on the high-level intent of the command, we\nalso translate the natural-language-based explanation into standard technique &\ntactic defined by MITRE ATT&CK, the worldwide knowledge base of cybersecurity.\nTo enable Raconteur to explain unseen private commands, we further develop a\ndocumentation retriever to obtain relevant information from complementary\ndocumentations to assist the explanation process. We have created a large-scale\ndataset for training and conducted extensive experiments to evaluate the\ncapability of Raconteur in shell command explanation. The experiments verify\nthat Raconteur is able to provide high-quality explanations and in-depth\ninsight of the intent of the command."
                },
                "authors": [
                    {
                        "name": "Jiangyi Deng"
                    },
                    {
                        "name": "Xinfeng Li"
                    },
                    {
                        "name": "Yanjiao Chen"
                    },
                    {
                        "name": "Yijie Bai"
                    },
                    {
                        "name": "Haiqin Weng"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Tao Wei"
                    },
                    {
                        "name": "Wenyuan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wenyuan Xu"
                },
                "arxiv_affiliation": "Zhejiang University",
                "author": "Wenyuan Xu",
                "arxiv_comment": "Accepted by NDSS Symposium 2025. Please cite this paper as \"Jiangyi\n  Deng, Xinfeng Li, Yanjiao Chen, Yijie Bai, Haiqin Weng, Yan Liu, Tao Wei,\n  Wenyuan Xu. RACONTEUR: A Knowledgeable, Insightful, and Portable LLM-Powered\n  Shell Command Explainer. In the 32nd Annual Network and Distributed System\n  Security Symposium (NDSS 2025).\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02069v1",
                "updated": "2024-09-03T17:16:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    16,
                    1,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T17:16:01Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    16,
                    1,
                    1,
                    247,
                    0
                ],
                "title": "A Deployed Online Reinforcement Learning Algorithm In An Oral Health\n  Clinical Trial",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Deployed Online Reinforcement Learning Algorithm In An Oral Health\n  Clinical Trial"
                },
                "summary": "Dental disease is a prevalent chronic condition associated with substantial\nfinancial burden, personal suffering, and increased risk of systemic diseases.\nDespite widespread recommendations for twice-daily tooth brushing, adherence to\nrecommended oral self-care behaviors remains sub-optimal due to factors such as\nforgetfulness and disengagement. To address this, we developed Oralytics, a\nmHealth intervention system designed to complement clinician-delivered\npreventative care for marginalized individuals at risk for dental disease.\nOralytics incorporates an online reinforcement learning algorithm to determine\noptimal times to deliver intervention prompts that encourage oral self-care\nbehaviors. We have deployed Oralytics in a registered clinical trial. The\ndeployment required careful design to manage challenges specific to the\nclinical trials setting in the U.S. In this paper, we (1) highlight key design\ndecisions of the RL algorithm that address these challenges and (2) conduct a\nre-sampling analysis to evaluate algorithm design decisions. A second phase\n(randomized control trial) of Oralytics is planned to start in spring 2025.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dental disease is a prevalent chronic condition associated with substantial\nfinancial burden, personal suffering, and increased risk of systemic diseases.\nDespite widespread recommendations for twice-daily tooth brushing, adherence to\nrecommended oral self-care behaviors remains sub-optimal due to factors such as\nforgetfulness and disengagement. To address this, we developed Oralytics, a\nmHealth intervention system designed to complement clinician-delivered\npreventative care for marginalized individuals at risk for dental disease.\nOralytics incorporates an online reinforcement learning algorithm to determine\noptimal times to deliver intervention prompts that encourage oral self-care\nbehaviors. We have deployed Oralytics in a registered clinical trial. The\ndeployment required careful design to manage challenges specific to the\nclinical trials setting in the U.S. In this paper, we (1) highlight key design\ndecisions of the RL algorithm that address these challenges and (2) conduct a\nre-sampling analysis to evaluate algorithm design decisions. A second phase\n(randomized control trial) of Oralytics is planned to start in spring 2025."
                },
                "authors": [
                    {
                        "name": "Anna L. Trella"
                    },
                    {
                        "name": "Kelly W. Zhang"
                    },
                    {
                        "name": "Hinal Jajal"
                    },
                    {
                        "name": "Inbal Nahum-Shani"
                    },
                    {
                        "name": "Vivek Shetty"
                    },
                    {
                        "name": "Finale Doshi-Velez"
                    },
                    {
                        "name": "Susan A. Murphy"
                    }
                ],
                "author_detail": {
                    "name": "Susan A. Murphy"
                },
                "author": "Susan A. Murphy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15460v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15460v2",
                "updated": "2024-09-03T16:56:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    16,
                    56,
                    16,
                    1,
                    247,
                    0
                ],
                "published": "2024-08-28T00:52:39Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    0,
                    52,
                    39,
                    2,
                    241,
                    0
                ],
                "title": "Lagrangian approach to origami vertex analysis: Kinematics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lagrangian approach to origami vertex analysis: Kinematics"
                },
                "summary": "The use of origami in engineering has significantly expanded in recent years,\nspanning deployable structures across scales, folding robotics, and mechanical\nmetamaterials. However, finding foldable paths can be a formidable task as the\nkinematics are determined by a nonlinear system of equations, often with\nseveral degrees of freedom. In this work, we leverage a Lagrangian approach to\nderive reduced-order compatibility conditions for rigid-facet origami vertices\nwith reflection and rotational symmetries. Then, using the reduced-order\nconditions, we derive exact, multi-degree of freedom solutions for degree 6 and\ndegree 8 vertices with prescribed symmetries. The exact kinematic solutions\nallow us to efficiently investigate the topology of allowable kinematics,\nincluding the consideration of a self-contact constraint, and then visually\ninterpret the role of geometric design parameters on these admissible fold\npaths by monitoring the change in the kinematic topology. We then introduce a\nprocedure to construct lower symmetry kinematic solutions by breaking symmetry\nof higher order kinematic solutions in a systematic way that preserves\ncompatibility. The multi-degree of freedom solutions discovered here should\nassist with building intuition of the kinematic feasibility of higher degree\norigami vertices and also facilitate the development of new algorithmic\nprocedures for origami-engineering design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of origami in engineering has significantly expanded in recent years,\nspanning deployable structures across scales, folding robotics, and mechanical\nmetamaterials. However, finding foldable paths can be a formidable task as the\nkinematics are determined by a nonlinear system of equations, often with\nseveral degrees of freedom. In this work, we leverage a Lagrangian approach to\nderive reduced-order compatibility conditions for rigid-facet origami vertices\nwith reflection and rotational symmetries. Then, using the reduced-order\nconditions, we derive exact, multi-degree of freedom solutions for degree 6 and\ndegree 8 vertices with prescribed symmetries. The exact kinematic solutions\nallow us to efficiently investigate the topology of allowable kinematics,\nincluding the consideration of a self-contact constraint, and then visually\ninterpret the role of geometric design parameters on these admissible fold\npaths by monitoring the change in the kinematic topology. We then introduce a\nprocedure to construct lower symmetry kinematic solutions by breaking symmetry\nof higher order kinematic solutions in a systematic way that preserves\ncompatibility. The multi-degree of freedom solutions discovered here should\nassist with building intuition of the kinematic feasibility of higher degree\norigami vertices and also facilitate the development of new algorithmic\nprocedures for origami-engineering design."
                },
                "authors": [
                    {
                        "name": "Matthew Grasinger"
                    },
                    {
                        "name": "Andrew Gillman"
                    },
                    {
                        "name": "Philip Buskohl"
                    }
                ],
                "author_detail": {
                    "name": "Philip Buskohl"
                },
                "author": "Philip Buskohl",
                "arxiv_doi": "10.1098/rsta.2024.0203",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1098/rsta.2024.0203",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.15460v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15460v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Electronic supplementary information can be found at the published\n  article, https://doi.org/10.1098/rsta.2024.0203",
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02038v1",
                "updated": "2024-09-03T16:37:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    16,
                    37,
                    45,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T16:37:45Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    16,
                    37,
                    45,
                    1,
                    247,
                    0
                ],
                "title": "BEAVER: An Enterprise Benchmark for Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEAVER: An Enterprise Benchmark for Text-to-SQL"
                },
                "summary": "Existing text-to-SQL benchmarks have largely been constructed using publicly\navailable tables from the web with human-generated tests containing question\nand SQL statement pairs. They typically show very good results and lead people\nto think that LLMs are effective at text-to-SQL tasks. In this paper, we apply\noff-the-shelf LLMs to a benchmark containing enterprise data warehouse data. In\nthis environment, LLMs perform poorly, even when standard prompt engineering\nand RAG techniques are utilized. As we will show, the reasons for poor\nperformance are largely due to three characteristics: (1) public LLMs cannot\ntrain on enterprise data warehouses because they are largely in the \"dark web\",\n(2) schemas of enterprise tables are more complex than the schemas in public\ndata, which leads the SQL-generation task innately harder, and (3)\nbusiness-oriented questions are often more complex, requiring joins over\nmultiple tables and aggregations. As a result, we propose a new dataset BEAVER,\nsourced from real enterprise data warehouses together with natural language\nqueries and their correct SQL statements which we collected from actual user\nhistory. We evaluated this dataset using recent LLMs and demonstrated their\npoor performance on this task. We hope this dataset will facilitate future\nresearchers building more sophisticated text-to-SQL systems which can do better\non this important class of data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing text-to-SQL benchmarks have largely been constructed using publicly\navailable tables from the web with human-generated tests containing question\nand SQL statement pairs. They typically show very good results and lead people\nto think that LLMs are effective at text-to-SQL tasks. In this paper, we apply\noff-the-shelf LLMs to a benchmark containing enterprise data warehouse data. In\nthis environment, LLMs perform poorly, even when standard prompt engineering\nand RAG techniques are utilized. As we will show, the reasons for poor\nperformance are largely due to three characteristics: (1) public LLMs cannot\ntrain on enterprise data warehouses because they are largely in the \"dark web\",\n(2) schemas of enterprise tables are more complex than the schemas in public\ndata, which leads the SQL-generation task innately harder, and (3)\nbusiness-oriented questions are often more complex, requiring joins over\nmultiple tables and aggregations. As a result, we propose a new dataset BEAVER,\nsourced from real enterprise data warehouses together with natural language\nqueries and their correct SQL statements which we collected from actual user\nhistory. We evaluated this dataset using recent LLMs and demonstrated their\npoor performance on this task. We hope this dataset will facilitate future\nresearchers building more sophisticated text-to-SQL systems which can do better\non this important class of data."
                },
                "authors": [
                    {
                        "name": "Peter Baile Chen"
                    },
                    {
                        "name": "Fabian Wenz"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Moe Kayali"
                    },
                    {
                        "name": "Nesime Tatbul"
                    },
                    {
                        "name": "Michael Cafarella"
                    },
                    {
                        "name": "aatay Demiralp"
                    },
                    {
                        "name": "Michael Stonebraker"
                    }
                ],
                "author_detail": {
                    "name": "Michael Stonebraker"
                },
                "author": "Michael Stonebraker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]