[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.05221v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05221v3",
                "updated": "2025-01-17T16:16:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    16,
                    16,
                    54,
                    4,
                    17,
                    0
                ],
                "published": "2024-09-08T20:47:44Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    20,
                    47,
                    44,
                    6,
                    252,
                    0
                ],
                "title": "Geometric rigidity of simple modules for algebraic groups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric rigidity of simple modules for algebraic groups"
                },
                "summary": "Let k be a field, let G be an affine algebraic k-group and V a\nfinite-dimensional G-module. We say V is rigid if the socle series and radical\nseries coincide for the action of G on each indecomposable summand of V; say V\nis geometrically rigid (resp. absolutely rigid) if V is rigid after base change\nof G and V to k (resp. any field extension of k). We show that all simple\nG-modules are geometrically rigid, though not in general absolutely rigid. More\nprecisely, we show that if V is a simple G-module, then there is a finite\npurely inseparable extension kV /k naturally attached to V such that V is\nabsolutely rigid as a G-module after base change to kV. The proof turns on an\ninvestigation of algebras of the form K otimes E where K and E are field\nextensions of k; we give an example of such an algebra which is not rigid as a\nmodule over itself. We establish the existence of the purely inseparable field\nextension kV /k through an analogous version for artinian algebras.\n  In the second half of the paper we apply recent results on the structure and\nrepresentation theory of pseudo-reductive groups to give a concrete description\nof kV when G is smooth and connected. Namely, we combine the main structure\ntheorem of the Conrad-Prasad classification of pseudo-reductive G together with\nour previous high weight theory. For V a simple G-module, we calculate the\nminimal field of definition of the geometric Jacobson radical of EndG(V) in\nterms of the high weight of V and the Conrad-Prasad classification data; this\ngives a concrete construction of the field kV as a subextension of the minimal\nfield of definition of the geometric unipotent radical of G. We also observe\nthat the Conrad-Prasad classification can be used to hone the dimension formula\nfor V we had previously established; we also use it to give a description of\nEndG(V) which includes a dimension formula.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let k be a field, let G be an affine algebraic k-group and V a\nfinite-dimensional G-module. We say V is rigid if the socle series and radical\nseries coincide for the action of G on each indecomposable summand of V; say V\nis geometrically rigid (resp. absolutely rigid) if V is rigid after base change\nof G and V to k (resp. any field extension of k). We show that all simple\nG-modules are geometrically rigid, though not in general absolutely rigid. More\nprecisely, we show that if V is a simple G-module, then there is a finite\npurely inseparable extension kV /k naturally attached to V such that V is\nabsolutely rigid as a G-module after base change to kV. The proof turns on an\ninvestigation of algebras of the form K otimes E where K and E are field\nextensions of k; we give an example of such an algebra which is not rigid as a\nmodule over itself. We establish the existence of the purely inseparable field\nextension kV /k through an analogous version for artinian algebras.\n  In the second half of the paper we apply recent results on the structure and\nrepresentation theory of pseudo-reductive groups to give a concrete description\nof kV when G is smooth and connected. Namely, we combine the main structure\ntheorem of the Conrad-Prasad classification of pseudo-reductive G together with\nour previous high weight theory. For V a simple G-module, we calculate the\nminimal field of definition of the geometric Jacobson radical of EndG(V) in\nterms of the high weight of V and the Conrad-Prasad classification data; this\ngives a concrete construction of the field kV as a subextension of the minimal\nfield of definition of the geometric unipotent radical of G. We also observe\nthat the Conrad-Prasad classification can be used to hone the dimension formula\nfor V we had previously established; we also use it to give a description of\nEndG(V) which includes a dimension formula."
                },
                "authors": [
                    {
                        "name": "Michael Bate"
                    },
                    {
                        "name": "David I. Stewart"
                    }
                ],
                "author_detail": {
                    "name": "David I. Stewart"
                },
                "author": "David I. Stewart",
                "arxiv_comment": "v3; 30 pages; Theorem 1 now holds for arbitrary affine algebraic\n  groups over fields",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05221v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05221v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.RT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.RT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.RA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "20G05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10138v1",
                "updated": "2025-01-17T12:01:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T12:01:28Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "title": "The NIC should be part of the OS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NIC should be part of the OS"
                },
                "summary": "The network interface adapter (NIC) is a critical component of a modern cloud\nserver which occupies a unique position. Not only is network performance vital\nto the efficient operation of the machine, but unlike application-oriented\ncompute accelerators like GPUs, the network subsystem must react to\nunpredictable events like the arrival of a network packet and communicate with\nthe appropriate application end point with minimal latency. Current approaches\nto server stacks navigate a trade-off between flexibility, efficiency, and\nperformance: the fastest kernel-bypass approaches dedicate cores to\napplications, busy-wait on receive queues, etc. while more flexible approaches\nappropriate to more dynamic workload mixes incur much greater software overhead\non the data path. However, we reject this trade-off, which we ascribe to an\narbitrary (and sub-optimal) split in system state between the OS and the NIC.\nInstead, by exploiting the properties of cache-coherent interconnects and\nintegrating the NIC closely with the OS kernel, we can achieve something\nsurprising: performance for RPC workloads better than the fastest kernel-bypass\napproaches without sacrificing the robustness and dynamic adaptation of\nkernel-based network subsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The network interface adapter (NIC) is a critical component of a modern cloud\nserver which occupies a unique position. Not only is network performance vital\nto the efficient operation of the machine, but unlike application-oriented\ncompute accelerators like GPUs, the network subsystem must react to\nunpredictable events like the arrival of a network packet and communicate with\nthe appropriate application end point with minimal latency. Current approaches\nto server stacks navigate a trade-off between flexibility, efficiency, and\nperformance: the fastest kernel-bypass approaches dedicate cores to\napplications, busy-wait on receive queues, etc. while more flexible approaches\nappropriate to more dynamic workload mixes incur much greater software overhead\non the data path. However, we reject this trade-off, which we ascribe to an\narbitrary (and sub-optimal) split in system state between the OS and the NIC.\nInstead, by exploiting the properties of cache-coherent interconnects and\nintegrating the NIC closely with the OS kernel, we can achieve something\nsurprising: performance for RPC workloads better than the fastest kernel-bypass\napproaches without sacrificing the robustness and dynamic adaptation of\nkernel-based network subsystems."
                },
                "authors": [
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03594v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03594v2",
                "updated": "2025-01-17T09:37:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    37,
                    36,
                    4,
                    17,
                    0
                ],
                "published": "2024-11-29T05:57:37Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    57,
                    37,
                    4,
                    334,
                    0
                ],
                "title": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching"
                },
                "summary": "Large language models (LLMs) increasingly play an important role in a wide\nrange of information processing and management tasks. Many of these tasks are\nperformed in large batches or even offline, and the performance indictor for\nwhich is throughput. These tasks usually show the characteristic of prefix\nsharing, where different prompt input can partially show the common prefix.\nHowever, the existing LLM inference engines tend to optimize the streaming\nrequests and show limitations of supporting the large batched tasks with the\nprefix sharing characteristic. The existing solutions use the LRU-based cache\nto reuse the KV context of common prefix between requests. The KV context that\nare about to be reused may prematurely evicted with the implicit cache\nmanagement. Besides, the streaming oriented systems do not leverage the\nrequest-batch information and can not mix the decoding tokens with the prefill\nchunks to the best for the batched scenarios, and thus fails to saturate the\nGPU. We propose BatchLLM to address the above problems. BatchLLM explicitly\nidentifies the common prefixes globally. The requests sharing the same prefix\nwill be scheduled together to reuse the KV context the best. BatchLLM reorders\nthe requests and schedules the requests with larger ratio of decoding first to\nbetter mix the decoding tokens with the latter prefill chunks, and applies\nmemory-centric token batching to enlarge the token-batch sizes, which helps to\nincrease the GPU utilization. Finally, BatchLLM optimizes the prefix-shared\nAttention kernel with horizontal fusion to reduce tail effect and kernel launch\noverhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang\nby 1.3$\\times$ to 10.8$\\times$ on a set of microbenchmarks and a typical\nindustry workload under different hardware environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly play an important role in a wide\nrange of information processing and management tasks. Many of these tasks are\nperformed in large batches or even offline, and the performance indictor for\nwhich is throughput. These tasks usually show the characteristic of prefix\nsharing, where different prompt input can partially show the common prefix.\nHowever, the existing LLM inference engines tend to optimize the streaming\nrequests and show limitations of supporting the large batched tasks with the\nprefix sharing characteristic. The existing solutions use the LRU-based cache\nto reuse the KV context of common prefix between requests. The KV context that\nare about to be reused may prematurely evicted with the implicit cache\nmanagement. Besides, the streaming oriented systems do not leverage the\nrequest-batch information and can not mix the decoding tokens with the prefill\nchunks to the best for the batched scenarios, and thus fails to saturate the\nGPU. We propose BatchLLM to address the above problems. BatchLLM explicitly\nidentifies the common prefixes globally. The requests sharing the same prefix\nwill be scheduled together to reuse the KV context the best. BatchLLM reorders\nthe requests and schedules the requests with larger ratio of decoding first to\nbetter mix the decoding tokens with the latter prefill chunks, and applies\nmemory-centric token batching to enlarge the token-batch sizes, which helps to\nincrease the GPU utilization. Finally, BatchLLM optimizes the prefix-shared\nAttention kernel with horizontal fusion to reduce tail effect and kernel launch\noverhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang\nby 1.3$\\times$ to 10.8$\\times$ on a set of microbenchmarks and a typical\nindustry workload under different hardware environments."
                },
                "authors": [
                    {
                        "name": "Zhen Zheng"
                    },
                    {
                        "name": "Xin Ji"
                    },
                    {
                        "name": "Taosong Fang"
                    },
                    {
                        "name": "Fanghao Zhou"
                    },
                    {
                        "name": "Chuanjie Liu"
                    },
                    {
                        "name": "Gang Peng"
                    }
                ],
                "author_detail": {
                    "name": "Gang Peng"
                },
                "author": "Gang Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03594v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09902v1",
                "updated": "2025-01-17T01:24:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    24,
                    12,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T01:24:12Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    24,
                    12,
                    4,
                    17,
                    0
                ],
                "title": "Multi-Dimensional Vector ISA Extension for Mobile In-Cache Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Dimensional Vector ISA Extension for Mobile In-Cache Computing"
                },
                "summary": "In-cache computing technology transforms existing caches into long-vector\ncompute units and offers low-cost alternatives to building expensive vector\nengines for mobile CPUs. Unfortunately, existing long-vector Instruction Set\nArchitecture (ISA) extensions, such as RISC-V Vector Extension (RVV) and Arm\nScalable Vector Extension (SVE), provide only one-dimensional strided and\nrandom memory accesses. While this is sufficient for typical vector engines, it\nfails to effectively utilize the large Single Instruction, Multiple Data (SIMD)\nwidths of in-cache vector engines. This is because mobile data-parallel kernels\nexpose limited parallelism across a single dimension.\n  Based on our analysis of mobile vector kernels, we introduce a long-vector\nMulti-dimensional Vector ISA Extension (MVE) for mobile in-cache computing. MVE\nachieves high SIMD resource utilization and enables flexible programming by\nabstracting cache geometry and data layout. The proposed ISA features\nmulti-dimensional strided and random memory accesses and efficient\ndimension-level masked execution to encode parallelism across multiple\ndimensions. Using a wide range of data-parallel mobile workloads, we\ndemonstrate that MVE offers significant performance and energy reduction\nbenefits of 2.9x and 8.8x, on average, compared to the SIMD units of a\ncommercial mobile processor, at an area overhead of 3.6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-cache computing technology transforms existing caches into long-vector\ncompute units and offers low-cost alternatives to building expensive vector\nengines for mobile CPUs. Unfortunately, existing long-vector Instruction Set\nArchitecture (ISA) extensions, such as RISC-V Vector Extension (RVV) and Arm\nScalable Vector Extension (SVE), provide only one-dimensional strided and\nrandom memory accesses. While this is sufficient for typical vector engines, it\nfails to effectively utilize the large Single Instruction, Multiple Data (SIMD)\nwidths of in-cache vector engines. This is because mobile data-parallel kernels\nexpose limited parallelism across a single dimension.\n  Based on our analysis of mobile vector kernels, we introduce a long-vector\nMulti-dimensional Vector ISA Extension (MVE) for mobile in-cache computing. MVE\nachieves high SIMD resource utilization and enables flexible programming by\nabstracting cache geometry and data layout. The proposed ISA features\nmulti-dimensional strided and random memory accesses and efficient\ndimension-level masked execution to encode parallelism across multiple\ndimensions. Using a wide range of data-parallel mobile workloads, we\ndemonstrate that MVE offers significant performance and energy reduction\nbenefits of 2.9x and 8.8x, on average, compared to the SIMD units of a\ncommercial mobile processor, at an area overhead of 3.6%."
                },
                "authors": [
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Daichi Fujiki"
                    },
                    {
                        "name": "Hilbert Chen"
                    },
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Nishil Talati"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_comment": "2025 IEEE International Symposium on High-Performance Computer\n  Architecture (HPCA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04501v2",
                "updated": "2025-01-16T15:11:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    15,
                    11,
                    42,
                    3,
                    16,
                    0
                ],
                "published": "2024-07-05T13:42:30Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    13,
                    42,
                    30,
                    4,
                    187,
                    0
                ],
                "title": "Cryogenic Behavior of High-Permittivity Gate Dielectrics: The Impact of\n  the Atomic Layer Deposition Temperature and the Lithographic Patterning\n  Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cryogenic Behavior of High-Permittivity Gate Dielectrics: The Impact of\n  the Atomic Layer Deposition Temperature and the Lithographic Patterning\n  Method"
                },
                "summary": "Dielectrics featuring a high relative permittivity, i.e., high-k dielectrics,\nhave become the standard insulators in gate architectures, enhancing the\nelectrical performance of both room temperature and cryogenic electronics. This\nstudy delves into the cryogenic (3 K) performance of high-k dielectrics\ncommonly used as gate insulators. We fabricated Al2O3 and HfO2 layers via\nAtomic Layer Deposition (ALD) and we extrapolated relative permittivity (k) and\ndielectric strength (E_BD) from AC (100 Hz to 100 kHz) and DC measurements on\nmetal-insulator-metal capacitors. Our findings reveal a strong dependence of\nHfO2 cryogenic performance on the ALD growth temperature, while the latter\nshows a negligible impact on Al2O3. We estimated a ~9 % and ~14 % reduction of\nthe relative permittivity of HfO2 and Al2O3, respectively, from 300 K to 3 K.\nAdditionally, we designed and fabricated Al2O3/HfO2 bilayers and we checked\ntheir properties at cryogenic temperatures. The study also investigates the\nimpact of the patterning method, namely, UV or electron-beam lithography\n(acceleration voltage of 10, 20, or 30 kV), on the high-k dielectric\nproperties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dielectrics featuring a high relative permittivity, i.e., high-k dielectrics,\nhave become the standard insulators in gate architectures, enhancing the\nelectrical performance of both room temperature and cryogenic electronics. This\nstudy delves into the cryogenic (3 K) performance of high-k dielectrics\ncommonly used as gate insulators. We fabricated Al2O3 and HfO2 layers via\nAtomic Layer Deposition (ALD) and we extrapolated relative permittivity (k) and\ndielectric strength (E_BD) from AC (100 Hz to 100 kHz) and DC measurements on\nmetal-insulator-metal capacitors. Our findings reveal a strong dependence of\nHfO2 cryogenic performance on the ALD growth temperature, while the latter\nshows a negligible impact on Al2O3. We estimated a ~9 % and ~14 % reduction of\nthe relative permittivity of HfO2 and Al2O3, respectively, from 300 K to 3 K.\nAdditionally, we designed and fabricated Al2O3/HfO2 bilayers and we checked\ntheir properties at cryogenic temperatures. The study also investigates the\nimpact of the patterning method, namely, UV or electron-beam lithography\n(acceleration voltage of 10, 20, or 30 kV), on the high-k dielectric\nproperties."
                },
                "authors": [
                    {
                        "name": "Alessandro Paghi"
                    },
                    {
                        "name": "Sebastiano Battisti"
                    },
                    {
                        "name": "Simone Tortorella"
                    },
                    {
                        "name": "Giorgio De Simoni"
                    },
                    {
                        "name": "Francesco Giazotto"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Giazotto"
                },
                "author": "Francesco Giazotto",
                "arxiv_comment": "17 pages, 4 figures, supporting information at the end of the paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11501v2",
                "updated": "2025-01-16T10:35:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    35,
                    59,
                    3,
                    16,
                    0
                ],
                "published": "2023-12-08T15:11:26Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    15,
                    11,
                    26,
                    4,
                    342,
                    0
                ],
                "title": "Write+Sync: Software Cache Write Covert Channels Exploiting Memory-disk\n  Synchronization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Write+Sync: Software Cache Write Covert Channels Exploiting Memory-disk\n  Synchronization"
                },
                "summary": "Memory-disk synchronization is a critical technology for ensuring data\ncorrectness, integrity, and security, especially in systems that handle\nsensitive information like financial transactions and medical records. We\npropose SYNC+SYNC, a group of attacks that exploit the memory-disk\nsynchronization primitives. SYNC+SYNC works by subtly varying the timing of\nsynchronization on the write buffer, offering several advantages: 1)\nimplemented purely in software, enabling deployment on any hardware devices; 2)\nresilient against existing cache partitioning and randomization techniques; 3)\nunaffected by prefetching techniques and cache replacement strategies. We\npresent the principles of SYNC+SYNC through the implementation of two write\ncovert channel protocols, using either a single file or page, and introduce\nthree enhanced strategies that utilize multiple files and pages. The\nfeasibility of these channels is demonstrated in both cross-process and\ncross-sandbox scenarios across diverse operating systems (OSes). Experimental\nresults show that, the average rate can reach 2.036 Kb/s (with a peak rate of\n14.762 Kb/s) and the error rate is 0% on Linux; when running on macOS, the\naverage rate achieves 10.211 Kb/s (with a peak rate of 253.022 Kb/s) and the\nerror rate is 0.004%. To the best of our knowledge, SYNC+SYNC is the first\nhigh-speed write covert channel for software cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-disk synchronization is a critical technology for ensuring data\ncorrectness, integrity, and security, especially in systems that handle\nsensitive information like financial transactions and medical records. We\npropose SYNC+SYNC, a group of attacks that exploit the memory-disk\nsynchronization primitives. SYNC+SYNC works by subtly varying the timing of\nsynchronization on the write buffer, offering several advantages: 1)\nimplemented purely in software, enabling deployment on any hardware devices; 2)\nresilient against existing cache partitioning and randomization techniques; 3)\nunaffected by prefetching techniques and cache replacement strategies. We\npresent the principles of SYNC+SYNC through the implementation of two write\ncovert channel protocols, using either a single file or page, and introduce\nthree enhanced strategies that utilize multiple files and pages. The\nfeasibility of these channels is demonstrated in both cross-process and\ncross-sandbox scenarios across diverse operating systems (OSes). Experimental\nresults show that, the average rate can reach 2.036 Kb/s (with a peak rate of\n14.762 Kb/s) and the error rate is 0% on Linux; when running on macOS, the\naverage rate achieves 10.211 Kb/s (with a peak rate of 253.022 Kb/s) and the\nerror rate is 0.004%. To the best of our knowledge, SYNC+SYNC is the first\nhigh-speed write covert channel for software cache."
                },
                "authors": [
                    {
                        "name": "Congcong Chen"
                    },
                    {
                        "name": "Jinhua Cui"
                    },
                    {
                        "name": "Gang Qu"
                    },
                    {
                        "name": "Jiliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiliang Zhang"
                },
                "author": "Jiliang Zhang",
                "arxiv_comment": "This manuscript was published in IEEE Transactions on Information\n  Forensics and Security, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09383v1",
                "updated": "2025-01-16T08:52:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    52,
                    38,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T08:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    52,
                    38,
                    3,
                    16,
                    0
                ],
                "title": "Adaptive Contextual Caching for Mobile Edge Large Language Model Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Contextual Caching for Mobile Edge Large Language Model Service"
                },
                "summary": "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments."
                },
                "authors": [
                    {
                        "name": "Guangyuan Liu"
                    },
                    {
                        "name": "Yinqiu Liu"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Jiawen Kang"
                    },
                    {
                        "name": "Zehui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Zehui Xiong"
                },
                "author": "Zehui Xiong",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09290v1",
                "updated": "2025-01-16T04:50:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    50,
                    15,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T04:50:15Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    50,
                    15,
                    3,
                    16,
                    0
                ],
                "title": "Interoceptive Robots for Convergent Shared Control in Collaborative\n  Construction Work",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interoceptive Robots for Convergent Shared Control in Collaborative\n  Construction Work"
                },
                "summary": "Building autonomous mobile robots (AMRs) with optimized efficiency and\nadaptive capabilities-able to respond to changing task demands and dynamic\nenvironments-is a strongly desired goal for advancing construction robotics.\nSuch robots can play a critical role in enabling automation, reducing\noperational carbon footprints, and supporting modular construction processes.\nInspired by the adaptive autonomy of living organisms, we introduce\ninteroception, which centers on the robot's internal state representation, as a\nfoundation for developing self-reflection and conscious learning to enable\ncontinual learning and adaptability in robotic agents. In this paper, we\nfactorize internal state variables and mathematical properties as \"cognitive\ndissonance\" in shared control paradigms, where human interventions occasionally\noccur. We offer a new perspective on how interoception can help build adaptive\nmotion planning in AMRs by integrating the legacy of heuristic costs from\ngrid/graph-based algorithms with recent advances in neuroscience and\nreinforcement learning. Declarative and procedural knowledge extracted from\nhuman semantic inputs is encoded into a hypergraph model that overlaps with the\nspatial configuration of onsite layout for path planning. In addition, we\ndesign a velocity-replay module using an encoder-decoder architecture with\nfew-shot learning to enable robots to replicate velocity profiles in\ncontextualized scenarios for multi-robot synchronization and handover\ncollaboration. These \"cached\" knowledge representations are demonstrated in\nsimulated environments for multi-robot motion planning and stacking tasks. The\ninsights from this study pave the way toward artificial general intelligence in\nAMRs, fostering their progression from complexity to competence in construction\nautomation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building autonomous mobile robots (AMRs) with optimized efficiency and\nadaptive capabilities-able to respond to changing task demands and dynamic\nenvironments-is a strongly desired goal for advancing construction robotics.\nSuch robots can play a critical role in enabling automation, reducing\noperational carbon footprints, and supporting modular construction processes.\nInspired by the adaptive autonomy of living organisms, we introduce\ninteroception, which centers on the robot's internal state representation, as a\nfoundation for developing self-reflection and conscious learning to enable\ncontinual learning and adaptability in robotic agents. In this paper, we\nfactorize internal state variables and mathematical properties as \"cognitive\ndissonance\" in shared control paradigms, where human interventions occasionally\noccur. We offer a new perspective on how interoception can help build adaptive\nmotion planning in AMRs by integrating the legacy of heuristic costs from\ngrid/graph-based algorithms with recent advances in neuroscience and\nreinforcement learning. Declarative and procedural knowledge extracted from\nhuman semantic inputs is encoded into a hypergraph model that overlaps with the\nspatial configuration of onsite layout for path planning. In addition, we\ndesign a velocity-replay module using an encoder-decoder architecture with\nfew-shot learning to enable robots to replicate velocity profiles in\ncontextualized scenarios for multi-robot synchronization and handover\ncollaboration. These \"cached\" knowledge representations are demonstrated in\nsimulated environments for multi-robot motion planning and stacking tasks. The\ninsights from this study pave the way toward artificial general intelligence in\nAMRs, fostering their progression from complexity to competence in construction\nautomation."
                },
                "authors": [
                    {
                        "name": "Xiaoshan Zhou"
                    },
                    {
                        "name": "Carol C. Menassa"
                    },
                    {
                        "name": "Vineet R. Kamat"
                    }
                ],
                "author_detail": {
                    "name": "Vineet R. Kamat"
                },
                "author": "Vineet R. Kamat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09253v1",
                "updated": "2025-01-16T02:40:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    40,
                    7,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T02:40:07Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    40,
                    7,
                    3,
                    16,
                    0
                ],
                "title": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving"
                },
                "summary": "The Text-to-Image (T2I) diffusion model is one of the most popular models in\nthe world. However, serving diffusion models at the entire image level faces\nseveral problems, especially when there are multiple candidate resolutions.\nFirst, image based serving system prevents requests with different resolutions\nfrom batching together. On the other hand, requests with hybrid resolutions\nalso indicate diverse locality features, which makes it hard to apply the same\ncache policy to all of them. To this end, we propose PATCHEDSERVE, A Patch\nManagement Framework for SLO-Optimized Hybrid Resolution Diffusion Serving that\nprovides a patch-level management strategy to gather hybrid resolution requests\ninto batches. Specifically, PATCHEDSERVE incorporates a novel patch-based\nprocessing workflow, significantly enhancing throughput for hybrid resolution\ninputs. Furthermore, PATCHEDSERVE designs a patch-level cache reuse policy to\nfully exploit the redundancy in diffusion. In addition, PATCHEDSERVE features\nan SLO-aware scheduling algorithm with lightweight online latency prediction,\nachieving higher SLO satisfaction rates. We show that PATCHEDSERVE can achieve\n30.1 % higher SLO satisfaction compared to SOTA diffusion serving system while\nnot hurt the image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Text-to-Image (T2I) diffusion model is one of the most popular models in\nthe world. However, serving diffusion models at the entire image level faces\nseveral problems, especially when there are multiple candidate resolutions.\nFirst, image based serving system prevents requests with different resolutions\nfrom batching together. On the other hand, requests with hybrid resolutions\nalso indicate diverse locality features, which makes it hard to apply the same\ncache policy to all of them. To this end, we propose PATCHEDSERVE, A Patch\nManagement Framework for SLO-Optimized Hybrid Resolution Diffusion Serving that\nprovides a patch-level management strategy to gather hybrid resolution requests\ninto batches. Specifically, PATCHEDSERVE incorporates a novel patch-based\nprocessing workflow, significantly enhancing throughput for hybrid resolution\ninputs. Furthermore, PATCHEDSERVE designs a patch-level cache reuse policy to\nfully exploit the redundancy in diffusion. In addition, PATCHEDSERVE features\nan SLO-aware scheduling algorithm with lightweight online latency prediction,\nachieving higher SLO satisfaction rates. We show that PATCHEDSERVE can achieve\n30.1 % higher SLO satisfaction compared to SOTA diffusion serving system while\nnot hurt the image quality."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Zepeng Zhao"
                    },
                    {
                        "name": "Yuke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Wang"
                },
                "author": "Yuke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10845v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10845v2",
                "updated": "2025-01-15T21:09:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    21,
                    9,
                    22,
                    2,
                    15,
                    0
                ],
                "published": "2024-04-16T18:47:07Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    18,
                    47,
                    7,
                    1,
                    107,
                    0
                ],
                "title": "Top-k Multi-Armed Bandit Learning for Content Dissemination in Swarms of\n  Micro-UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Top-k Multi-Armed Bandit Learning for Content Dissemination in Swarms of\n  Micro-UAVs"
                },
                "summary": "This paper presents a Micro-Unmanned Aerial Vehicle (UAV)-enhanced content\nmanagement system for disaster scenarios where communication infrastructure is\ngenerally compromised. Utilizing a hybrid network of stationary and mobile\nMicro-UAVs, this system aims to provide crucial content access to isolated\ncommunities. In the developed architecture, stationary anchor UAVs, equipped\nwith vertical and lateral links, serve users in individual disaster-affected\ncommunities. and mobile micro-ferrying UAVs, with enhanced mobility, extend\ncoverage across multiple such communities. The primary goal is to devise a\ncontent dissemination system that dynamically learns caching policies to\nmaximize content accessibility to users left without communication\ninfrastructure. The core contribution is an adaptive content dissemination\nframework that employs a decentralized Top-k Multi-Armed Bandit learning\napproach for efficient UAV caching decisions. This approach accounts for\ngeo-temporal variations in content popularity and diverse user demands.\nAdditionally, a Selective Caching Algorithm is proposed to minimize redundant\ncontent copies by leveraging inter-UAV information sharing. Through functional\nverification and performance evaluation, the proposed framework demonstrates\nimproved system performance and adaptability across varying network sizes,\nmicro-UAV swarms, and content popularity distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a Micro-Unmanned Aerial Vehicle (UAV)-enhanced content\nmanagement system for disaster scenarios where communication infrastructure is\ngenerally compromised. Utilizing a hybrid network of stationary and mobile\nMicro-UAVs, this system aims to provide crucial content access to isolated\ncommunities. In the developed architecture, stationary anchor UAVs, equipped\nwith vertical and lateral links, serve users in individual disaster-affected\ncommunities. and mobile micro-ferrying UAVs, with enhanced mobility, extend\ncoverage across multiple such communities. The primary goal is to devise a\ncontent dissemination system that dynamically learns caching policies to\nmaximize content accessibility to users left without communication\ninfrastructure. The core contribution is an adaptive content dissemination\nframework that employs a decentralized Top-k Multi-Armed Bandit learning\napproach for efficient UAV caching decisions. This approach accounts for\ngeo-temporal variations in content popularity and diverse user demands.\nAdditionally, a Selective Caching Algorithm is proposed to minimize redundant\ncontent copies by leveraging inter-UAV information sharing. Through functional\nverification and performance evaluation, the proposed framework demonstrates\nimproved system performance and adaptability across varying network sizes,\nmicro-UAV swarms, and content popularity distributions."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "16 pages, 8 figures, 2 algorithms, 2 tables, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10845v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10845v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09146v1",
                "updated": "2025-01-15T20:55:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T20:55:13Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "title": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs"
                },
                "summary": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "25 pages, 11 figures, 1 table, 4 algorithms, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20166v2",
                "updated": "2025-01-15T01:34:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    1,
                    34,
                    46,
                    2,
                    15,
                    0
                ],
                "published": "2024-12-28T14:38:16Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "title": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System"
                },
                "summary": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications."
                },
                "authors": [
                    {
                        "name": "Hyucksung Kwon"
                    },
                    {
                        "name": "Kyungmo Koo"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "Woongkyu Lee"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyungdeok Lee"
                    },
                    {
                        "name": "Yousub Jung"
                    },
                    {
                        "name": "Jaehan Park"
                    },
                    {
                        "name": "Yosub Song"
                    },
                    {
                        "name": "Byeongsu Yang"
                    },
                    {
                        "name": "Haerang Choi"
                    },
                    {
                        "name": "Guhyun Kim"
                    },
                    {
                        "name": "Jongsoon Won"
                    },
                    {
                        "name": "Woojae Shin"
                    },
                    {
                        "name": "Changhyun Kim"
                    },
                    {
                        "name": "Gyeongcheol Shin"
                    },
                    {
                        "name": "Yongkee Kwon"
                    },
                    {
                        "name": "Ilkon Kim"
                    },
                    {
                        "name": "Euicheol Lim"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08484v1",
                "updated": "2025-01-14T23:13:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    13,
                    14,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T23:13:14Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    13,
                    14,
                    1,
                    14,
                    0
                ],
                "title": "CORD: Co-design of Resource Allocation and Deadline Decomposition with\n  Generative Profiling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CORD: Co-design of Resource Allocation and Deadline Decomposition with\n  Generative Profiling"
                },
                "summary": "As multicore hardware is becoming increasingly common in real-time systems,\ntraditional scheduling techniques that assume a single worst-case execution\ntime for a task are no longer adequate, since they ignore the impact of shared\nresources on execution time. When tasks execute concurrently on different\ncores, their execution times often vary substantially with their allocated\nbudgets of shared resources, such as cache and memory bandwidth. Even under a\nspecific resource allocation, the resource use pattern of a task also changes\nwith time during a job execution. It is therefore important to consider the\nrelationship between multicore resources and execution time in task modeling\nand scheduling algorithm design.\n  In this paper, we propose a much more precise execution model for DAG-based\nreal-time tasks that captures the time-varying resource use characteristics of\na task under different budgets of shared resources. We present a generative\nresource profiling algorithm that efficiently predicts, from limited\nmeasurement data, the resource profile of a task at any time during its\nexecution under a given resource budget. The generative profiles can then be\nused to construct the execution models for tasks, using which one can make\ninformed resource allocation decisions. We further introduce a multicore\nresource allocation and deadline decomposition co-design technique for\nDAG-based tasks that leverages the generated execution models to jointly\nallocate resources and deadlines to subtasks, to maximize resource efficiency\nand schedulability. Our evaluation results show that our generative profiling\nalgorithm achieves high accuracy while being efficient, and that our\nco-allocation technique substantially improves schedulability compared to a\nstate-of-the-art deadline decomposition method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multicore hardware is becoming increasingly common in real-time systems,\ntraditional scheduling techniques that assume a single worst-case execution\ntime for a task are no longer adequate, since they ignore the impact of shared\nresources on execution time. When tasks execute concurrently on different\ncores, their execution times often vary substantially with their allocated\nbudgets of shared resources, such as cache and memory bandwidth. Even under a\nspecific resource allocation, the resource use pattern of a task also changes\nwith time during a job execution. It is therefore important to consider the\nrelationship between multicore resources and execution time in task modeling\nand scheduling algorithm design.\n  In this paper, we propose a much more precise execution model for DAG-based\nreal-time tasks that captures the time-varying resource use characteristics of\na task under different budgets of shared resources. We present a generative\nresource profiling algorithm that efficiently predicts, from limited\nmeasurement data, the resource profile of a task at any time during its\nexecution under a given resource budget. The generative profiles can then be\nused to construct the execution models for tasks, using which one can make\ninformed resource allocation decisions. We further introduce a multicore\nresource allocation and deadline decomposition co-design technique for\nDAG-based tasks that leverages the generated execution models to jointly\nallocate resources and deadlines to subtasks, to maximize resource efficiency\nand schedulability. Our evaluation results show that our generative profiling\nalgorithm achieves high accuracy while being efficient, and that our\nco-allocation technique substantially improves schedulability compared to a\nstate-of-the-art deadline decomposition method."
                },
                "authors": [
                    {
                        "name": "Robert Gifford"
                    },
                    {
                        "name": "Abby Eisenklam"
                    },
                    {
                        "name": "Georgiy A. Bondar"
                    },
                    {
                        "name": "Yifan Cai"
                    },
                    {
                        "name": "Tushar Sial"
                    },
                    {
                        "name": "Linh Thi Xuan Phan"
                    },
                    {
                        "name": "Abhishek Halder"
                    }
                ],
                "author_detail": {
                    "name": "Abhishek Halder"
                },
                "author": "Abhishek Halder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v3",
                "updated": "2025-01-14T20:04:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    20,
                    4,
                    15,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08192v1",
                "updated": "2025-01-14T15:14:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T15:14:10Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "title": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving"
                },
                "summary": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems."
                },
                "authors": [
                    {
                        "name": "Ahmet Caner Yzgler"
                    },
                    {
                        "name": "Jiawei Zhuang"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v2",
                "updated": "2025-01-14T14:07:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    7,
                    55,
                    1,
                    14,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04987v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04987v2",
                "updated": "2025-01-14T12:06:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    6,
                    33,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-09T06:00:27Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    27,
                    3,
                    9,
                    0
                ],
                "title": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures"
                },
                "summary": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency."
                },
                "authors": [
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04987v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04987v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15896v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15896v2",
                "updated": "2025-01-14T11:41:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    11,
                    41,
                    14,
                    1,
                    14,
                    0
                ],
                "published": "2024-03-23T17:38:57Z",
                "published_parsed": [
                    2024,
                    3,
                    23,
                    17,
                    38,
                    57,
                    5,
                    83,
                    0
                ],
                "title": "Cell-level modelling of homeostasis in confined epithelial monolayers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-level modelling of homeostasis in confined epithelial monolayers"
                },
                "summary": "Tissue homeostasis, the biological process of maintaining a steady state in\ntissue via control of cell proliferation, death, and metabolic function, is\nessential for the development, growth, maintenance, and proper function of\nliving organisms. Disruptions to this process can lead to serious diseases and\neven death. In this study, we use the vertex model for the cell-level\ndescription of tissue mechanics to investigate the impact of the tissue\nmicroenvironment and local mechanical properties of cells on homeostasis in\nconfined epithelial tissues. We find a dynamic steady state, where the balance\nbetween cell divisions and removals sustains homeostasis. By characterising\nhomeostasis in terms of cell count, tissue area, and the cells' neighbour count\ndistribution, we identify the factors that govern regulated and ordered tissue\ngrowth. This work, therefore, sheds light on the mechanisms underlying tissue\nhomeostasis and highlights the importance of mechanics in the control of\nbiological processes such as tissue development and disease pathology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tissue homeostasis, the biological process of maintaining a steady state in\ntissue via control of cell proliferation, death, and metabolic function, is\nessential for the development, growth, maintenance, and proper function of\nliving organisms. Disruptions to this process can lead to serious diseases and\neven death. In this study, we use the vertex model for the cell-level\ndescription of tissue mechanics to investigate the impact of the tissue\nmicroenvironment and local mechanical properties of cells on homeostasis in\nconfined epithelial tissues. We find a dynamic steady state, where the balance\nbetween cell divisions and removals sustains homeostasis. By characterising\nhomeostasis in terms of cell count, tissue area, and the cells' neighbour count\ndistribution, we identify the factors that govern regulated and ordered tissue\ngrowth. This work, therefore, sheds light on the mechanisms underlying tissue\nhomeostasis and highlights the importance of mechanics in the control of\nbiological processes such as tissue development and disease pathology."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Jan Rozman"
                    },
                    {
                        "name": "Andrej Komrlj"
                    },
                    {
                        "name": "Rastko Sknepnek"
                    }
                ],
                "author_detail": {
                    "name": "Rastko Sknepnek"
                },
                "author": "Rastko Sknepnek",
                "arxiv_comment": "18 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15896v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15896v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19255v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19255v2",
                "updated": "2025-01-14T05:48:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    48,
                    7,
                    1,
                    14,
                    0
                ],
                "published": "2024-12-26T15:45:45Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "title": "Multi-matrix Factorization Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-matrix Factorization Attention"
                },
                "summary": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19255v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10480v2",
                "updated": "2025-01-14T05:00:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    0,
                    34,
                    1,
                    14,
                    0
                ],
                "published": "2024-05-17T00:52:39Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    0,
                    52,
                    39,
                    4,
                    138,
                    0
                ],
                "title": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the\n  Decode-Phase of Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the\n  Decode-Phase of Transformers"
                },
                "summary": "Transformer-based models have emerged as one of the most widely used\narchitectures for natural language processing, natural language generation, and\nimage generation. The size of the state-of-the-art models has increased\nsteadily reaching billions of parameters. These huge models are memory hungry\nand incur significant inference latency even on cutting edge AI-accelerators,\nsuch as GPUs. Specifically, the time and memory complexity of the attention\noperation is quadratic in terms of the total context length, i.e., prompt and\noutput tokens. Thus, several optimizations such as key-value tensor caching and\nFlashAttention computation have been proposed to deliver the low latency\ndemands of applications relying on such large models. However, these techniques\ndo not cater to the computationally distinct nature of different phases during\ninference.\n  To that end, we propose LeanAttention, a scalable technique of computing\nself-attention for the token-generation phase (decode-phase) of decoder-only\ntransformer models. LeanAttention enables scaling the attention mechanism\nimplementation for the challenging case of long context lengths by re-designing\nthe execution flow for the decode-phase. We identify that the associative\nproperty of online softmax can be treated as a reduction operation thus\nallowing us to parallelize the attention computation over these large context\nlengths. We extend the \"stream-K\" style reduction of tiled calculation to\nself-attention to enable parallel computation resulting in an average of 2.6x\nattention execution speedup over FlashAttention-2 and up to 8.33x speedup for\n512k context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have emerged as one of the most widely used\narchitectures for natural language processing, natural language generation, and\nimage generation. The size of the state-of-the-art models has increased\nsteadily reaching billions of parameters. These huge models are memory hungry\nand incur significant inference latency even on cutting edge AI-accelerators,\nsuch as GPUs. Specifically, the time and memory complexity of the attention\noperation is quadratic in terms of the total context length, i.e., prompt and\noutput tokens. Thus, several optimizations such as key-value tensor caching and\nFlashAttention computation have been proposed to deliver the low latency\ndemands of applications relying on such large models. However, these techniques\ndo not cater to the computationally distinct nature of different phases during\ninference.\n  To that end, we propose LeanAttention, a scalable technique of computing\nself-attention for the token-generation phase (decode-phase) of decoder-only\ntransformer models. LeanAttention enables scaling the attention mechanism\nimplementation for the challenging case of long context lengths by re-designing\nthe execution flow for the decode-phase. We identify that the associative\nproperty of online softmax can be treated as a reduction operation thus\nallowing us to parallelize the attention computation over these large context\nlengths. We extend the \"stream-K\" style reduction of tiled calculation to\nself-attention to enable parallel computation resulting in an average of 2.6x\nattention execution speedup over FlashAttention-2 and up to 8.33x speedup for\n512k context lengths."
                },
                "authors": [
                    {
                        "name": "Rya Sanovar"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "Renee St. Amant"
                    },
                    {
                        "name": "Victor Rhle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "arxiv_comment": "13 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05262v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05262v2",
                "updated": "2025-01-14T02:02:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    2,
                    2,
                    1,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-09T14:16:43Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    16,
                    43,
                    3,
                    9,
                    0
                ],
                "title": "QMDB: Quick Merkle Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QMDB: Quick Merkle Database"
                },
                "summary": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases."
                },
                "authors": [
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05262v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05262v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07523v1",
                "updated": "2025-01-13T17:50:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    50,
                    30,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T17:50:30Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    50,
                    30,
                    0,
                    13,
                    0
                ],
                "title": "Parallel Key-Value Cache Fusion for Position Invariant RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Key-Value Cache Fusion for Position Invariant RAG"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines."
                },
                "authors": [
                    {
                        "name": "Philhoon Oh"
                    },
                    {
                        "name": "Jinwoo Shin"
                    },
                    {
                        "name": "James Thorne"
                    }
                ],
                "author_detail": {
                    "name": "James Thorne"
                },
                "author": "James Thorne",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v2",
                "updated": "2025-01-13T17:34:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    34,
                    22,
                    0,
                    13,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}"
                },
                "authors": [
                    {
                        "name": "Korbinian Pppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07533v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07533v4",
                "updated": "2025-01-13T09:33:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    33,
                    25,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-13T08:03:32Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    8,
                    3,
                    32,
                    0,
                    134,
                    0
                ],
                "title": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials"
                },
                "summary": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities."
                },
                "authors": [
                    {
                        "name": "Sandro Rodriguez Garzon"
                    },
                    {
                        "name": "Dennis Natusch"
                    },
                    {
                        "name": "Artur Philipp"
                    },
                    {
                        "name": "Axel Kpper"
                    },
                    {
                        "name": "Hans Joachim Einsiedler"
                    },
                    {
                        "name": "Daniela Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Schneider"
                },
                "author": "Daniela Schneider",
                "arxiv_doi": "10.1109/PST62714.2024.10788053",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/PST62714.2024.10788053",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.07533v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07533v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by and presented at 21st Annual International Conference on\n  Privacy, Security, and Trust (PST2024)",
                "arxiv_journal_ref": "2024 21st Annual International Conference on Privacy, Security and\n  Trust (PST), 2024, pp. 1-11",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07056v1",
                "updated": "2025-01-13T04:31:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T04:31:04Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "title": "Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs"
                },
                "summary": "Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines for a variety of different matrices on three Intel x86 architectures.\nFor matrices from the SuiteSparse collection, MAGNUS is faster than all the\nbaselines in most cases and is orders of magnitude faster than Intel MKL for\nseveral matrices. For massive random matrices that model social network graphs,\nMAGNUS scales to the largest matrix sizes, while the baselines fail to do so.\nFurthermore, MAGNUS is close to the optimal bound for these matrices,\nregardless of the matrix size, structure, and density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines for a variety of different matrices on three Intel x86 architectures.\nFor matrices from the SuiteSparse collection, MAGNUS is faster than all the\nbaselines in most cases and is orders of magnitude faster than Intel MKL for\nseveral matrices. For massive random matrices that model social network graphs,\nMAGNUS scales to the largest matrix sizes, while the baselines fail to do so.\nFurthermore, MAGNUS is close to the optimal bound for these matrices,\nregardless of the matrix size, structure, and density."
                },
                "authors": [
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Petrini"
                },
                "author": "Fabrizio Petrini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v5",
                "updated": "2025-01-13T03:11:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    3,
                    11,
                    28,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stphane Pouget"
                    },
                    {
                        "name": "Louis-Nol Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03058v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06872v1",
                "updated": "2025-01-12T17:01:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    17,
                    1,
                    40,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T17:01:40Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    17,
                    1,
                    40,
                    6,
                    12,
                    0
                ],
                "title": "On Optimizing Locality of Graph Transposition on Modern Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Optimizing Locality of Graph Transposition on Modern Architectures"
                },
                "summary": "This paper investigates the shared-memory Graph Transposition (GT) problem, a\nfundamental graph algorithm that is widely used in graph analytics and\nscientific computing.\n  Previous GT algorithms have significant memory requirements that are\nproportional to the number of vertices and threads which obstructs their use on\nlarge graphs. Moreover, atomic memory operations have become comparably fast on\nrecent CPU architectures, which creates new opportunities for improving the\nperformance of concurrent atomic accesses in GT.\n  We design PoTra, a GT algorithm which leverages graph structure and processor\nand memory architecture to optimize locality and performance. PoTra limits the\nsize of additional data structures close to CPU cache sizes and utilizes the\nskewed degree distribution of graph datasets to optimize locality and\nperformance. We present the performance model of PoTra to explain the\nconnection between cache and memory response times and graph locality.\n  Our evaluation of PoTra on three CPU architectures and 20 real-world and\nsynthetic graph datasets with up to 128 billion edges demonstrates that PoTra\nachieves up to 8.7 times speedup compared to previous works and if there is a\nperformance loss it remains limited to 15.7%, on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the shared-memory Graph Transposition (GT) problem, a\nfundamental graph algorithm that is widely used in graph analytics and\nscientific computing.\n  Previous GT algorithms have significant memory requirements that are\nproportional to the number of vertices and threads which obstructs their use on\nlarge graphs. Moreover, atomic memory operations have become comparably fast on\nrecent CPU architectures, which creates new opportunities for improving the\nperformance of concurrent atomic accesses in GT.\n  We design PoTra, a GT algorithm which leverages graph structure and processor\nand memory architecture to optimize locality and performance. PoTra limits the\nsize of additional data structures close to CPU cache sizes and utilizes the\nskewed degree distribution of graph datasets to optimize locality and\nperformance. We present the performance model of PoTra to explain the\nconnection between cache and memory response times and graph locality.\n  Our evaluation of PoTra on three CPU architectures and 20 real-world and\nsynthetic graph datasets with up to 128 billion edges demonstrates that PoTra\nachieves up to 8.7 times speedup compared to previous works and if there is a\nperformance loss it remains limited to 15.7%, on average."
                },
                "authors": [
                    {
                        "name": "Mohsen Koohi Esfahani"
                    },
                    {
                        "name": "Hans Vandierendonck"
                    }
                ],
                "author_detail": {
                    "name": "Hans Vandierendonck"
                },
                "author": "Hans Vandierendonck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06807v1",
                "updated": "2025-01-12T13:18:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T13:18:04Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "title": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large\n  Language Model Inference"
                },
                "summary": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) offers cryptographically-secure protection for both user\nprompt and proprietary model weights. However, it suffers from large latency\noverhead especially for long input sequences. While key-value (KV) cache\neviction algorithms have been proposed to reduce the computation and memory\ncost for plaintext inference, they are not designed for MPC and cannot benefit\nprivate inference easily. In this paper, we propose an accurate and\nMPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on\nthe observation that historical tokens in a long sequence may have different\neffects on the downstream decoding. Hence, MPCache combines a look-once static\neviction algorithm to discard unimportant tokens and a query-aware dynamic\nselection algorithm to further select a small subset of tokens for attention\ncomputation. As existing dynamic selection algorithms incur too much latency,\nwe propose a series of optimizations to drastically reduce the KV cache\nselection overhead, including MPC-friendly similarity approximation,\nhierarchical KV cache clustering, and cross-layer index sharing strategy. With\nextensive experiments, we demonstrate that MPCache consistently outperforms\nprior-art KV cache eviction baselines across different LLM generation tasks and\nachieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction\non different sequence lengths, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) offers cryptographically-secure protection for both user\nprompt and proprietary model weights. However, it suffers from large latency\noverhead especially for long input sequences. While key-value (KV) cache\neviction algorithms have been proposed to reduce the computation and memory\ncost for plaintext inference, they are not designed for MPC and cannot benefit\nprivate inference easily. In this paper, we propose an accurate and\nMPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on\nthe observation that historical tokens in a long sequence may have different\neffects on the downstream decoding. Hence, MPCache combines a look-once static\neviction algorithm to discard unimportant tokens and a query-aware dynamic\nselection algorithm to further select a small subset of tokens for attention\ncomputation. As existing dynamic selection algorithms incur too much latency,\nwe propose a series of optimizations to drastically reduce the KV cache\nselection overhead, including MPC-friendly similarity approximation,\nhierarchical KV cache clustering, and cross-layer index sharing strategy. With\nextensive experiments, we demonstrate that MPCache consistently outperforms\nprior-art KV cache eviction baselines across different LLM generation tasks and\nachieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction\non different sequence lengths, respectively."
                },
                "authors": [
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Ye Dong"
                    },
                    {
                        "name": "Jinjin Zhou"
                    },
                    {
                        "name": "Junming Ma"
                    },
                    {
                        "name": "Jin Tan"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02882v2",
                "updated": "2025-01-12T12:01:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    12,
                    1,
                    47,
                    6,
                    12,
                    0
                ],
                "published": "2024-04-03T17:33:21Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    17,
                    33,
                    21,
                    2,
                    94,
                    0
                ],
                "title": "Linear Attention Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Attention Sequence Parallelism"
                },
                "summary": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. The code is available at\nhttps://github.com/OpenNLPLab/LASP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. The code is available at\nhttps://github.com/OpenNLPLab/LASP."
                },
                "authors": [
                    {
                        "name": "Weigao Sun"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Xuyang Shen"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Yiran Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Zhong"
                },
                "author": "Yiran Zhong",
                "arxiv_comment": "Technical report, 20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v3",
                "updated": "2025-01-12T11:15:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    11,
                    15,
                    41,
                    6,
                    12,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "Gaspard Beaufort"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v2",
                "updated": "2025-01-12T05:25:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    5,
                    25,
                    6,
                    6,
                    12,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    }
                ],
                "author_detail": {
                    "name": "Peiran Dong"
                },
                "author": "Peiran Dong",
                "arxiv_comment": "Project page: https://nevsnev.github.io/FloED/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06709v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06709v1",
                "updated": "2025-01-12T04:29:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    4,
                    29,
                    39,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T04:29:39Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    4,
                    29,
                    39,
                    6,
                    12,
                    0
                ],
                "title": "Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV\n  Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV\n  Cache Management"
                },
                "summary": "Serving large language models (LLMs) for massive users is challenged by the\nsignificant memory footprint of the transient state, known as the key-value\n(KV) cache, which scales with sequence length and number of requests. Instead\nof renting or buying more expensive GPUs, the load imbalance of the KV cache\nacross GPUs, coupled with recent advances in inter-GPU communication, provides\nan opportunity to serve more requests via request migration. However, high\nmigration overhead and unpredictable request patterns make it challenging.\nTherefore, this paper proposes MELL, a memory-efficient LLM serving system via\nmulti-GPU KV cache management. It saves the number of GPUs needed in the system\nby considering the dynamic KV cache load and the costly request migration.\nSpecifically, we first develop an adaptive request migration mechanism to\nbalance the computational and communication overheads and adapt to diverse\nresource conditions. Then, we design an online algorithm tailored to a\nmulti-LLM request and multi-GPU scheduling problem with migration enabled. It\naims to minimise the required GPUs while limiting the number of migrations.\nFinally, we implement a prototype of MELL and demonstrate that it reduces the\nnumber of GPUs by 31% and increases the GPU utilization by 43% at most compared\nto existing LLM serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) for massive users is challenged by the\nsignificant memory footprint of the transient state, known as the key-value\n(KV) cache, which scales with sequence length and number of requests. Instead\nof renting or buying more expensive GPUs, the load imbalance of the KV cache\nacross GPUs, coupled with recent advances in inter-GPU communication, provides\nan opportunity to serve more requests via request migration. However, high\nmigration overhead and unpredictable request patterns make it challenging.\nTherefore, this paper proposes MELL, a memory-efficient LLM serving system via\nmulti-GPU KV cache management. It saves the number of GPUs needed in the system\nby considering the dynamic KV cache load and the costly request migration.\nSpecifically, we first develop an adaptive request migration mechanism to\nbalance the computational and communication overheads and adapt to diverse\nresource conditions. Then, we design an online algorithm tailored to a\nmulti-LLM request and multi-GPU scheduling problem with migration enabled. It\naims to minimise the required GPUs while limiting the number of migrations.\nFinally, we implement a prototype of MELL and demonstrate that it reduces the\nnumber of GPUs by 31% and increases the GPU utilization by 43% at most compared\nto existing LLM serving systems."
                },
                "authors": [
                    {
                        "name": "Liu Qianli"
                    },
                    {
                        "name": "Hong Zicong"
                    },
                    {
                        "name": "Chen Fahao"
                    },
                    {
                        "name": "Li Peng"
                    },
                    {
                        "name": "Guo Song"
                    }
                ],
                "author_detail": {
                    "name": "Guo Song"
                },
                "author": "Guo Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06709v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06709v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17918v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17918v4",
                "updated": "2025-01-11T15:26:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    15,
                    26,
                    48,
                    5,
                    11,
                    0
                ],
                "published": "2024-06-25T20:00:32Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    20,
                    0,
                    32,
                    1,
                    177,
                    0
                ],
                "title": "GraphSnapShot: Caching Local Structure for Fast Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSnapShot: Caching Local Structure for Fast Graph Learning"
                },
                "summary": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17918v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17918v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06428v1",
                "updated": "2025-01-11T03:47:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    47,
                    4,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T03:47:04Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    47,
                    4,
                    5,
                    11,
                    0
                ],
                "title": "Optimizing digital experiences with content delivery networks:\n  Architectures, performance strategies, and future trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing digital experiences with content delivery networks:\n  Architectures, performance strategies, and future trends"
                },
                "summary": "This research investigates how CDNs (Content Delivery Networks) can improve\nthe digital experience, as consumers increasingly expect fast, efficient, and\neffortless access to online resources. CDNs play a crucial role in reducing\nlatency, enhancing scalability, and optimizing delivery mechanisms, which is\nevident across various platforms and regions. The study focuses on key CDN\nconcerns, such as foundational and modern CDN architectures, edge computing,\nhybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing\ntopics, including caching, load balancing, and the novel features of HTTP/3 and\nQUIC.\n  Current trends, such as integrating CDNs with 5G networks, serverless\narchitectures, and AI-driven traffic management, are examined to demonstrate\nhow CDN technology is likely to evolve. The study also addresses challenges\nrelated to security, cost, and global regulations. Practical examples from the\ne-commerce, streaming, and gaming industries highlight how enhanced CDNs are\ntransforming these sectors.\n  The conclusions emphasize the need to evolve CDN strategies to meet growing\nuser expectations and adapt to the rapidly changing digital landscape.\nAdditionally, the research identifies future research opportunities,\nparticularly in exploring the impact of QC, the enhancement of AI services, and\nthe sustainability of CDN solutions. Overall, the study situates architectural\ndesign, performance strategies, and emerging trends to address gaps and create\na more efficient and secure approach for improving digital experiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research investigates how CDNs (Content Delivery Networks) can improve\nthe digital experience, as consumers increasingly expect fast, efficient, and\neffortless access to online resources. CDNs play a crucial role in reducing\nlatency, enhancing scalability, and optimizing delivery mechanisms, which is\nevident across various platforms and regions. The study focuses on key CDN\nconcerns, such as foundational and modern CDN architectures, edge computing,\nhybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing\ntopics, including caching, load balancing, and the novel features of HTTP/3 and\nQUIC.\n  Current trends, such as integrating CDNs with 5G networks, serverless\narchitectures, and AI-driven traffic management, are examined to demonstrate\nhow CDN technology is likely to evolve. The study also addresses challenges\nrelated to security, cost, and global regulations. Practical examples from the\ne-commerce, streaming, and gaming industries highlight how enhanced CDNs are\ntransforming these sectors.\n  The conclusions emphasize the need to evolve CDN strategies to meet growing\nuser expectations and adapt to the rapidly changing digital landscape.\nAdditionally, the research identifies future research opportunities,\nparticularly in exploring the impact of QC, the enhancement of AI services, and\nthe sustainability of CDN solutions. Overall, the study situates architectural\ndesign, performance strategies, and emerging trends to address gaps and create\na more efficient and secure approach for improving digital experiences."
                },
                "authors": [
                    {
                        "name": "Anuj Tyagi"
                    }
                ],
                "author_detail": {
                    "name": "Anuj Tyagi"
                },
                "author": "Anuj Tyagi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v1",
                "updated": "2025-01-11T03:37:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPAs memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPAs memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "23 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06394v1",
                "updated": "2025-01-11T00:47:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    0,
                    47,
                    29,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T00:47:29Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    0,
                    47,
                    29,
                    5,
                    11,
                    0
                ],
                "title": "Unispeaker: A Unified Approach for Multimodality-driven Speaker\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unispeaker: A Unified Approach for Multimodality-driven Speaker\n  Generation"
                },
                "summary": "Recent advancements in personalized speech generation have brought synthetic\nspeech increasingly close to the realism of target speakers' recordings, yet\nmultimodal speaker generation remains on the rise. This paper introduces\nUniSpeaker, a unified approach for multimodality-driven speaker generation.\nSpecifically, we propose a unified voice aggregator based on KV-Former,\napplying soft contrastive loss to map diverse voice description modalities into\na shared voice space, ensuring that the generated voice aligns more closely\nwith the input descriptions. To evaluate multimodality-driven voice control, we\nbuild the first multimodality-based voice control (MVC) benchmark, focusing on\nvoice suitability, voice diversity, and speech quality. UniSpeaker is evaluated\nacross five tasks using the MVC benchmark, and the experimental results\ndemonstrate that UniSpeaker outperforms previous modality-specific models.\nSpeech samples are available at \\url{https://UniSpeaker.github.io}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in personalized speech generation have brought synthetic\nspeech increasingly close to the realism of target speakers' recordings, yet\nmultimodal speaker generation remains on the rise. This paper introduces\nUniSpeaker, a unified approach for multimodality-driven speaker generation.\nSpecifically, we propose a unified voice aggregator based on KV-Former,\napplying soft contrastive loss to map diverse voice description modalities into\na shared voice space, ensuring that the generated voice aligns more closely\nwith the input descriptions. To evaluate multimodality-driven voice control, we\nbuild the first multimodality-based voice control (MVC) benchmark, focusing on\nvoice suitability, voice diversity, and speech quality. UniSpeaker is evaluated\nacross five tasks using the MVC benchmark, and the experimental results\ndemonstrate that UniSpeaker outperforms previous modality-specific models.\nSpeech samples are available at \\url{https://UniSpeaker.github.io}."
                },
                "authors": [
                    {
                        "name": "Zhengyan Sheng"
                    },
                    {
                        "name": "Zhihao Du"
                    },
                    {
                        "name": "Heng Lu"
                    },
                    {
                        "name": "Shiliang Zhang"
                    },
                    {
                        "name": "Zhen-Hua Ling"
                    }
                ],
                "author_detail": {
                    "name": "Zhen-Hua Ling"
                },
                "author": "Zhen-Hua Ling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01030v2",
                "updated": "2025-01-10T10:11:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    11,
                    45,
                    4,
                    10,
                    0
                ],
                "published": "2024-07-01T07:25:08Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    7,
                    25,
                    8,
                    0,
                    183,
                    0
                ],
                "title": "Tame fields, Graded Rings and Finite Complete Sequences of Key\n  Polynomials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tame fields, Graded Rings and Finite Complete Sequences of Key\n  Polynomials"
                },
                "summary": "In this paper, we present a criterion for $(K,v)$ to be henselian and\ndefectless in terms of finite complete sequences of key polynomials. For this,\nwe use the theory of Mac Lane-Vaqui\\'e chains and abstract key polynomials. We\nthen prove that a valued field $(K,v)$ is tame if and only if $vK$ is\n$p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$\nadmits a finite complete sequence of key polynomials. The properties $vK$\n$p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on\nthe associated graded ring. We also make considerations on simply defectless\nand algebraically maximal valued fields and purely inertial and purely ramified\nextensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a criterion for $(K,v)$ to be henselian and\ndefectless in terms of finite complete sequences of key polynomials. For this,\nwe use the theory of Mac Lane-Vaqui\\'e chains and abstract key polynomials. We\nthen prove that a valued field $(K,v)$ is tame if and only if $vK$ is\n$p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$\nadmits a finite complete sequence of key polynomials. The properties $vK$\n$p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on\nthe associated graded ring. We also make considerations on simply defectless\nand algebraically maximal valued fields and purely inertial and purely ramified\nextensions."
                },
                "authors": [
                    {
                        "name": "Caio Henrique Silva de Souza"
                    }
                ],
                "author_detail": {
                    "name": "Caio Henrique Silva de Souza"
                },
                "author": "Caio Henrique Silva de Souza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "13A18",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20433v2",
                "updated": "2025-01-09T15:14:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    14,
                    5,
                    3,
                    9,
                    0
                ],
                "published": "2024-09-30T15:53:36Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "title": "Handover_Management_in_UAV_Networks_with_Blockages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handover_Management_in_UAV_Networks_with_Blockages"
                },
                "summary": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities."
                },
                "authors": [
                    {
                        "name": "Neetu R R"
                    },
                    {
                        "name": "Gourab Ghatak"
                    },
                    {
                        "name": "Vivek Ashok Bohara"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Ashok Bohara"
                },
                "author": "Vivek Ashok Bohara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04993v1",
                "updated": "2025-01-09T06:18:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    18,
                    39,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T06:18:39Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    18,
                    39,
                    3,
                    9,
                    0
                ],
                "title": "ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State\n  Drives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State\n  Drives"
                },
                "summary": "Unlike non-volatile memory that resides on the processor memory bus,\nmemory-semantic solid-state drives (SSDs) support both byte and block access\ngranularity via PCIe or CXL interconnects. They provide scalable memory\ncapacity using NAND flash at a much lower cost. In addition, they have\ndifferent performance characteristics for their dual byte/block interface\nrespectively, while offering essential memory semantics for upper-level\nsoftware. Such a byte-accessible storage device provides new implications on\nthe software system design.\n  In this paper, we develop a new file system, named ByteFS, by rethinking the\ndesign primitives of file systems and SSD firmware to exploit the advantages of\nboth byte and block-granular data accesses. ByteFS supports byte-granular data\npersistence to retain the persistence nature of SSDs. It extends the core data\nstructure of file systems by enabling dual byte/block-granular data accesses.\nTo facilitate the support for byte-granular writes, \\pname{} manages the\ninternal DRAM of SSD firmware in a log-structured manner and enables data\ncoalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also\nenables coordinated data caching between the host page cache and SSD cache for\nbest utilizing the precious memory resource. We implement ByteFS on both a real\nprogrammable SSD and an emulated memory-semantic SSD for sensitivity study.\nCompared to state-of-the-art file systems for non-volatile memory and\nconventional SSDs, ByteFS outperforms them by up to 2.7$\\times$, while\npreserving the essential properties of a file system. ByteFS also reduces the\nwrite traffic to SSDs by up to 5.1$\\times$ by alleviating unnecessary writes\ncaused by both metadata and data updates in file systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlike non-volatile memory that resides on the processor memory bus,\nmemory-semantic solid-state drives (SSDs) support both byte and block access\ngranularity via PCIe or CXL interconnects. They provide scalable memory\ncapacity using NAND flash at a much lower cost. In addition, they have\ndifferent performance characteristics for their dual byte/block interface\nrespectively, while offering essential memory semantics for upper-level\nsoftware. Such a byte-accessible storage device provides new implications on\nthe software system design.\n  In this paper, we develop a new file system, named ByteFS, by rethinking the\ndesign primitives of file systems and SSD firmware to exploit the advantages of\nboth byte and block-granular data accesses. ByteFS supports byte-granular data\npersistence to retain the persistence nature of SSDs. It extends the core data\nstructure of file systems by enabling dual byte/block-granular data accesses.\nTo facilitate the support for byte-granular writes, \\pname{} manages the\ninternal DRAM of SSD firmware in a log-structured manner and enables data\ncoalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also\nenables coordinated data caching between the host page cache and SSD cache for\nbest utilizing the precious memory resource. We implement ByteFS on both a real\nprogrammable SSD and an emulated memory-semantic SSD for sensitivity study.\nCompared to state-of-the-art file systems for non-volatile memory and\nconventional SSDs, ByteFS outperforms them by up to 2.7$\\times$, while\npreserving the essential properties of a file system. ByteFS also reduces the\nwrite traffic to SSDs by up to 5.1$\\times$ by alleviating unnecessary writes\ncaused by both metadata and data updates in file systems."
                },
                "authors": [
                    {
                        "name": "Shaobo Li"
                    },
                    {
                        "name": "Yirui Eric Zhou"
                    },
                    {
                        "name": "Hao Ren"
                    },
                    {
                        "name": "Jian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Huang"
                },
                "author": "Jian Huang",
                "arxiv_comment": "This paper is accepted at the 30th Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04216v2",
                "updated": "2025-01-09T03:02:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    3,
                    2,
                    31,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-08T01:23:29Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    1,
                    23,
                    29,
                    2,
                    8,
                    0
                ],
                "title": "Optimal Oblivious Algorithms for Multi-way Joins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Oblivious Algorithms for Multi-way Joins"
                },
                "summary": "In cloud databases, cloud computation over sensitive data uploaded by clients\ninevitably causes concern about data security and privacy. Even when encryption\nprimitives and trusted computing environments are integrated into query\nprocessing to safeguard the actual contents of the data, access patterns of\nalgorithms can still leak private information about the data. Oblivious Random\nAccess Memory (ORAM) and circuits are two generic approaches to address this\nissue, ensuring that access patterns of algorithms remain oblivious to the\ndata. However, deploying these methods on insecure algorithms, particularly for\nmulti-way join processing, is computationally expensive and inherently\nchallenging.\n  In this paper, we propose a novel sorting-based algorithm for multi-way join\nprocessing that operates without relying on ORAM simulations or other security\nassumptions. Our algorithm is a non-trivial, provably oblivious composition of\nbasic primitives, with time complexity matching the insecure worst-case optimal\njoin algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic,\nwith cache complexity matching the insecure lower bound, also up to a\nlogarithmic factor. This clean and straightforward approach has the potential\nto be extended to other security settings and implemented in practical database\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In cloud databases, cloud computation over sensitive data uploaded by clients\ninevitably causes concern about data security and privacy. Even when encryption\nprimitives and trusted computing environments are integrated into query\nprocessing to safeguard the actual contents of the data, access patterns of\nalgorithms can still leak private information about the data. Oblivious Random\nAccess Memory (ORAM) and circuits are two generic approaches to address this\nissue, ensuring that access patterns of algorithms remain oblivious to the\ndata. However, deploying these methods on insecure algorithms, particularly for\nmulti-way join processing, is computationally expensive and inherently\nchallenging.\n  In this paper, we propose a novel sorting-based algorithm for multi-way join\nprocessing that operates without relying on ORAM simulations or other security\nassumptions. Our algorithm is a non-trivial, provably oblivious composition of\nbasic primitives, with time complexity matching the insecure worst-case optimal\njoin algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic,\nwith cache complexity matching the insecure lower bound, also up to a\nlogarithmic factor. This clean and straightforward approach has the potential\nto be extended to other security settings and implemented in practical database\nsystems."
                },
                "authors": [
                    {
                        "name": "Xiao Hu"
                    },
                    {
                        "name": "Zhiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiang Wu"
                },
                "author": "Zhiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04394v1",
                "updated": "2025-01-08T10:14:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    14,
                    19,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T10:14:19Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    14,
                    19,
                    2,
                    8,
                    0
                ],
                "title": "Modern Hardware Security: A Review of Attacks and Countermeasures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Hardware Security: A Review of Attacks and Countermeasures"
                },
                "summary": "With the exponential rise in the use of cloud services, smart devices, and\nIoT devices, advanced cyber attacks have become increasingly sophisticated and\nubiquitous. Furthermore, the rapid evolution of computing architectures and\nmemory technologies has created an urgent need to understand and address\nhardware security vulnerabilities. In this paper, we review the current state\nof vulnerabilities and mitigation strategies in contemporary computing systems.\nWe discuss cache side-channel attacks (including Spectre and Meltdown), power\nside-channel attacks (such as Simple Power Analysis, Differential Power\nAnalysis, Correlation Power Analysis, and Template Attacks), and advanced\ntechniques like Voltage Glitching and Electromagnetic Analysis to help\nunderstand and build robust cybersecurity defense systems and guide further\nresearch. We also examine memory encryption, focusing on confidentiality,\ngranularity, key management, masking, and re-keying strategies. Additionally,\nwe cover Cryptographic Instruction Set Architectures, Secure Boot, Root of\nTrust mechanisms, Physical Unclonable Functions, and hardware fault injection\ntechniques. The paper concludes with an analysis of the RISC-V architecture's\nunique security challenges. The comprehensive analysis presented in this paper\nis essential for building resilient hardware security solutions that can\nprotect against both current and emerging threats in an increasingly\nchallenging security landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the exponential rise in the use of cloud services, smart devices, and\nIoT devices, advanced cyber attacks have become increasingly sophisticated and\nubiquitous. Furthermore, the rapid evolution of computing architectures and\nmemory technologies has created an urgent need to understand and address\nhardware security vulnerabilities. In this paper, we review the current state\nof vulnerabilities and mitigation strategies in contemporary computing systems.\nWe discuss cache side-channel attacks (including Spectre and Meltdown), power\nside-channel attacks (such as Simple Power Analysis, Differential Power\nAnalysis, Correlation Power Analysis, and Template Attacks), and advanced\ntechniques like Voltage Glitching and Electromagnetic Analysis to help\nunderstand and build robust cybersecurity defense systems and guide further\nresearch. We also examine memory encryption, focusing on confidentiality,\ngranularity, key management, masking, and re-keying strategies. Additionally,\nwe cover Cryptographic Instruction Set Architectures, Secure Boot, Root of\nTrust mechanisms, Physical Unclonable Functions, and hardware fault injection\ntechniques. The paper concludes with an analysis of the RISC-V architecture's\nunique security challenges. The comprehensive analysis presented in this paper\nis essential for building resilient hardware security solutions that can\nprotect against both current and emerging threats in an increasingly\nchallenging security landscape."
                },
                "authors": [
                    {
                        "name": "Jyotiprakash Mishra"
                    },
                    {
                        "name": "Sanjay K. Sahay"
                    }
                ],
                "author_detail": {
                    "name": "Sanjay K. Sahay"
                },
                "author": "Sanjay K. Sahay",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00799v2",
                "updated": "2025-01-07T17:32:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    32,
                    19,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-01T10:50:35Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    10,
                    50,
                    35,
                    2,
                    1,
                    0
                ],
                "title": "Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear\n  Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear\n  Approximation"
                },
                "summary": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy."
                },
                "authors": [
                    {
                        "name": "Samrat Mukhopadhyay"
                    },
                    {
                        "name": "Debasmita Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Debasmita Mukherjee"
                },
                "author": "Debasmita Mukherjee",
                "arxiv_comment": "12 pages, 5 figures, corrected title, added proof of a lemma in\n  appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v1",
                "updated": "2025-01-07T17:00:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-Gonzlez"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martn"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09275v2",
                "updated": "2025-01-06T23:16:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    23,
                    16,
                    22,
                    0,
                    6,
                    0
                ],
                "published": "2024-11-14T08:25:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Parallel $k$d-tree with Batch Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel $k$d-tree with Batch Updates"
                },
                "summary": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code."
                },
                "authors": [
                    {
                        "name": "Ziyang Men"
                    },
                    {
                        "name": "Zheqi Shen"
                    },
                    {
                        "name": "Yan Gu"
                    },
                    {
                        "name": "Yihan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yihan Sun"
                },
                "author": "Yihan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04052v1",
                "updated": "2025-01-06T22:40:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    22,
                    40,
                    40,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T22:40:40Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    22,
                    40,
                    40,
                    0,
                    6,
                    0
                ],
                "title": "The Power of Negative Zero: Datatype Customization for Quantized Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Power of Negative Zero: Datatype Customization for Quantized Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks, quickly becoming one of the most prevalent AI\nworkloads. Yet the substantial memory requirement of LLMs significantly hinders\ntheir deployment for end users. Post-training quantization (PTQ) serves as one\nof the most hardware-efficient methods to mitigate the memory and computational\ndemands of LLMs. Although the traditional integer (INT) datatype has received\nwidespread adoption in PTQ methods, floating-point (FP) quantization has\nemerged as a viable alternative thanks to its effectiveness in fitting LLM\nnumerical distributions. However, the FP datatype in sign-magnitude binary\nrepresentation contains both positive and negative zero, which constrains its\nrepresentation capability, particularly under low precision (3 and 4 bits). In\nthis paper, we extend the basic FP datatype to perform Redundant Zero Remapping\n(RaZeR), which remaps the negative zero FP encoding to a set of pre-defined\nspecial values to maximally utilize FP quantization encodings and to better fit\nLLM numerical distributions. Through careful selection of special values, RaZeR\noutperforms conventional asymmetric INT quantization while achieving high\ncomputational efficiency. We demonstrate that RaZeR can be seamlessly\nintegrated with quantization algorithms for both weights and KV-cache,\nincluding advanced methods with clipping and transformations, and consistently\nachieve better model accuracy. Additionally, we implement a fast GEMV kernel\nwith fused dequantization that efficiently converts the 4-bit RaZeR value to\nFP16 through novel bit-level manipulation. On modern GPUs, our evaluation shows\nthat RaZeR improves the GEMV speed by up to 7.56$\\times$ compared to the FP16\nimplementation, while achieving up to 2.72$\\times$ speedup in the LLM decoding\nthroughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks, quickly becoming one of the most prevalent AI\nworkloads. Yet the substantial memory requirement of LLMs significantly hinders\ntheir deployment for end users. Post-training quantization (PTQ) serves as one\nof the most hardware-efficient methods to mitigate the memory and computational\ndemands of LLMs. Although the traditional integer (INT) datatype has received\nwidespread adoption in PTQ methods, floating-point (FP) quantization has\nemerged as a viable alternative thanks to its effectiveness in fitting LLM\nnumerical distributions. However, the FP datatype in sign-magnitude binary\nrepresentation contains both positive and negative zero, which constrains its\nrepresentation capability, particularly under low precision (3 and 4 bits). In\nthis paper, we extend the basic FP datatype to perform Redundant Zero Remapping\n(RaZeR), which remaps the negative zero FP encoding to a set of pre-defined\nspecial values to maximally utilize FP quantization encodings and to better fit\nLLM numerical distributions. Through careful selection of special values, RaZeR\noutperforms conventional asymmetric INT quantization while achieving high\ncomputational efficiency. We demonstrate that RaZeR can be seamlessly\nintegrated with quantization algorithms for both weights and KV-cache,\nincluding advanced methods with clipping and transformations, and consistently\nachieve better model accuracy. Additionally, we implement a fast GEMV kernel\nwith fused dequantization that efficiently converts the 4-bit RaZeR value to\nFP16 through novel bit-level manipulation. On modern GPUs, our evaluation shows\nthat RaZeR improves the GEMV speed by up to 7.56$\\times$ compared to the FP16\nimplementation, while achieving up to 2.72$\\times$ speedup in the LLM decoding\nthroughput."
                },
                "authors": [
                    {
                        "name": "Yuzong Chen"
                    },
                    {
                        "name": "Xilai Dai"
                    },
                    {
                        "name": "Chi-chih Chang"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "arxiv_comment": "under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03322v1",
                "updated": "2025-01-06T19:00:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    19,
                    0,
                    3,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T19:00:03Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    19,
                    0,
                    3,
                    0,
                    6,
                    0
                ],
                "title": "Twinkle: A GPU-based binary-lens microlensing code with contour\n  integration method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twinkle: A GPU-based binary-lens microlensing code with contour\n  integration method"
                },
                "summary": "With the rapidly increasing rate of microlensing planet detections,\nmicrolensing modeling software faces significant challenges in computation\nefficiency. Here, we develop the Twinkle code, an efficient and robust\nbinary-lens modeling software suite optimized for heterogeneous computing\ndevices, especially GPUs. Existing microlensing codes have the issue of\ncatastrophic cancellation that undermines the numerical stability and\nprecision, and Twinkle resolves them by refining the coefficients of the\nbinary-lens equation. We also devise an improved method for robustly\nidentifying ghost images, thereby enhancing computational reliability. We have\nadvanced the state of the art by optimizing Twinkle specifically for\nheterogeneous computing devices by taking into account the unique task and\ncache memory dispatching patterns of GPUs, while the compatibility with the\ntraditional computing architectures of CPUs is still maintained. Twinkle has\ndemonstrated an acceleration of approximately 2 orders of magnitude (>~100\ntimes) on contemporary GPUs. The enhancement in computational speed of Twinkle\nwill translate to the delivery of accurate and highly efficient data analysis\nfor ongoing and upcoming microlensing projects. Both GPU and CPU versions of\nTwinkle are open-source and publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapidly increasing rate of microlensing planet detections,\nmicrolensing modeling software faces significant challenges in computation\nefficiency. Here, we develop the Twinkle code, an efficient and robust\nbinary-lens modeling software suite optimized for heterogeneous computing\ndevices, especially GPUs. Existing microlensing codes have the issue of\ncatastrophic cancellation that undermines the numerical stability and\nprecision, and Twinkle resolves them by refining the coefficients of the\nbinary-lens equation. We also devise an improved method for robustly\nidentifying ghost images, thereby enhancing computational reliability. We have\nadvanced the state of the art by optimizing Twinkle specifically for\nheterogeneous computing devices by taking into account the unique task and\ncache memory dispatching patterns of GPUs, while the compatibility with the\ntraditional computing architectures of CPUs is still maintained. Twinkle has\ndemonstrated an acceleration of approximately 2 orders of magnitude (>~100\ntimes) on contemporary GPUs. The enhancement in computational speed of Twinkle\nwill translate to the delivery of accurate and highly efficient data analysis\nfor ongoing and upcoming microlensing projects. Both GPU and CPU versions of\nTwinkle are open-source and publicly available."
                },
                "authors": [
                    {
                        "name": "Suwei Wang"
                    },
                    {
                        "name": "Lile Wang"
                    },
                    {
                        "name": "Subo Dong"
                    }
                ],
                "author_detail": {
                    "name": "Subo Dong"
                },
                "author": "Subo Dong",
                "arxiv_doi": "10.3847/1538-4365/ad9b8d",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4365/ad9b8d",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.03322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ApJS, GitHub link:\n  https://github.com/AsterLight0626/Twinkle",
                "arxiv_journal_ref": "Astrophys. j., suppl. ser. 276 (2025) 40",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19919v2",
                "updated": "2025-01-06T15:59:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    15,
                    59,
                    23,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-27T20:47:23Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    20,
                    47,
                    23,
                    4,
                    362,
                    0
                ],
                "title": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)"
                },
                "summary": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family."
                },
                "authors": [
                    {
                        "name": "Austin Kaczmarek"
                    },
                    {
                        "name": "Andrea Capa Salinas"
                    },
                    {
                        "name": "Stephen D. Wilson"
                    },
                    {
                        "name": "Katja C. Nowack"
                    }
                ],
                "author_detail": {
                    "name": "Katja C. Nowack"
                },
                "author": "Katja C. Nowack",
                "arxiv_comment": "5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02803v1",
                "updated": "2025-01-06T06:44:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    44,
                    13,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T06:44:13Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    44,
                    13,
                    0,
                    6,
                    0
                ],
                "title": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism"
                },
                "summary": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial in autonomous warehouse operations.\nLifelong MAPF (L-MAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic approximation of\nreal-world warehouse scenarios. While cache storage systems can enhance\nefficiency and reduce operational costs, existing approaches primarily rely on\nexpectations and mathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper, we introduce a\nnovel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which\nintegrates high-level cache storage with low-level path planning. We have\ninvolved a new type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking mechanism to\nbridge the gap between the new cache grid and L-MAPF algorithm. The TA\ndynamically allocates target locations to agents based on their status in\nvarious scenarios. We evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated performance\nimprovements particularly with high cache hit rates and smooth traffic\nconditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial in autonomous warehouse operations.\nLifelong MAPF (L-MAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic approximation of\nreal-world warehouse scenarios. While cache storage systems can enhance\nefficiency and reduce operational costs, existing approaches primarily rely on\nexpectations and mathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper, we introduce a\nnovel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which\nintegrates high-level cache storage with low-level path planning. We have\ninvolved a new type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking mechanism to\nbridge the gap between the new cache grid and L-MAPF algorithm. The TA\ndynamically allocates target locations to agents based on their status in\nvarious scenarios. We evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated performance\nimprovements particularly with high cache hit rates and smooth traffic\nconditions."
                },
                "authors": [
                    {
                        "name": "Yimin Tang"
                    },
                    {
                        "name": "Zhenghong Yu"
                    },
                    {
                        "name": "Yi Zheng"
                    },
                    {
                        "name": "T. K. Satish Kumar"
                    },
                    {
                        "name": "Jiaoyang Li"
                    },
                    {
                        "name": "Sven Koenig"
                    }
                ],
                "author_detail": {
                    "name": "Sven Koenig"
                },
                "author": "Sven Koenig",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2403.13421",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v2",
                "updated": "2025-01-06T01:26:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    26,
                    42,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner. We will release the code based on an\nopen-source model in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner. We will release the code based on an\nopen-source model in the future."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v2",
                "updated": "2025-01-05T14:11:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    14,
                    11,
                    48,
                    6,
                    5,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe"
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Update performance in MLVU-dev and LVBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02524v1",
                "updated": "2025-01-05T12:51:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    12,
                    51,
                    8,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T12:51:08Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    12,
                    51,
                    8,
                    6,
                    5,
                    0
                ],
                "title": "A Full-System Simulation Framework for CXL-Based SSD Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Full-System Simulation Framework for CXL-Based SSD Memory System"
                },
                "summary": "Compute eXpress Link (CXL) is a promising technology for memory\ndisaggregation and expansion. Especially, CXL makes it more effectively for\nlarge-capacity storage devices such as Solid State Drive (SSD) to be deployed\nin the memory pool. However, CXL-based SSDs are still in early stages,\nnecessitating the development of reliable simulation tools. In this paper, we\npropose CXL-SSD-Sim, the first open-source full-system simulator designed to\nsimulate CXL-based SSD memory system. Constructed on the foundation of gem5 and\nSimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along\nwith the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM\nlayer as a caching mechanism for the SSD, meticulously engineered to counteract\nlatency issues inherent to CXL-based SSD memory access. Experiments are\nperformed among five different memory devices with CXL-SSD-Sim in aspect of\nlatency, bandwidth and real-world benchmark performance. These experiments\nserve to underscore the efficacy of our simulation tool in providing a\ncomprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim\nsimulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute eXpress Link (CXL) is a promising technology for memory\ndisaggregation and expansion. Especially, CXL makes it more effectively for\nlarge-capacity storage devices such as Solid State Drive (SSD) to be deployed\nin the memory pool. However, CXL-based SSDs are still in early stages,\nnecessitating the development of reliable simulation tools. In this paper, we\npropose CXL-SSD-Sim, the first open-source full-system simulator designed to\nsimulate CXL-based SSD memory system. Constructed on the foundation of gem5 and\nSimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along\nwith the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM\nlayer as a caching mechanism for the SSD, meticulously engineered to counteract\nlatency issues inherent to CXL-based SSD memory access. Experiments are\nperformed among five different memory devices with CXL-SSD-Sim in aspect of\nlatency, bandwidth and real-world benchmark performance. These experiments\nserve to underscore the efficacy of our simulation tool in providing a\ncomprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim\nsimulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim."
                },
                "authors": [
                    {
                        "name": "Yaohui Wang"
                    },
                    {
                        "name": "Zicong Wang"
                    },
                    {
                        "name": "Fanfeng Meng"
                    },
                    {
                        "name": "Yanjing Wang"
                    },
                    {
                        "name": "Yang Ou"
                    },
                    {
                        "name": "Lizhou Wu"
                    },
                    {
                        "name": "Wentao Hong"
                    },
                    {
                        "name": "Xuran Ge"
                    },
                    {
                        "name": "Jijun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jijun Cao"
                },
                "author": "Jijun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02469v1",
                "updated": "2025-01-05T07:41:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T07:41:53Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "title": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks"
                },
                "summary": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page."
                },
                "authors": [
                    {
                        "name": "Atonu Ghosh"
                    },
                    {
                        "name": "Sudip Misra"
                    }
                ],
                "author_detail": {
                    "name": "Sudip Misra"
                },
                "author": "Sudip Misra",
                "arxiv_comment": "11 pages, 15 figures, and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01805v1",
                "updated": "2025-01-03T13:32:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T13:32:57Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "title": "End-to-End Long Document Summarization using Gradient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Long Document Summarization using Gradient Caching"
                },
                "summary": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters."
                },
                "authors": [
                    {
                        "name": "Rohit Saxena"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Frank Keller"
                    }
                ],
                "author_detail": {
                    "name": "Frank Keller"
                },
                "author": "Frank Keller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01792v1",
                "updated": "2025-01-03T12:51:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T12:51:37Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "title": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching"
                },
                "summary": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache."
                },
                "authors": [
                    {
                        "name": "Sanghyeon Lee"
                    },
                    {
                        "name": "Hongbeen Kim"
                    },
                    {
                        "name": "Soojin Hwang"
                    },
                    {
                        "name": "Guseul Heo"
                    },
                    {
                        "name": "Minwoo Noh"
                    },
                    {
                        "name": "Jaehyuk Huh"
                    }
                ],
                "author_detail": {
                    "name": "Jaehyuk Huh"
                },
                "author": "Jaehyuk Huh",
                "arxiv_comment": "14 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01424v1",
                "updated": "2025-01-02T18:59:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T18:59:44Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "title": "Object-level Visual Prompts for Compositional Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-level Visual Prompts for Compositional Image Generation"
                },
                "summary": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation."
                },
                "authors": [
                    {
                        "name": "Gaurav Parmar"
                    },
                    {
                        "name": "Or Patashnik"
                    },
                    {
                        "name": "Kuan-Chieh Wang"
                    },
                    {
                        "name": "Daniil Ostashev"
                    },
                    {
                        "name": "Srinivasa Narasimhan"
                    },
                    {
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "name": "Daniel Cohen-Or"
                    },
                    {
                        "name": "Kfir Aberman"
                    }
                ],
                "author_detail": {
                    "name": "Kfir Aberman"
                },
                "author": "Kfir Aberman",
                "arxiv_comment": "Project: https://snap-research.github.io/visual-composer/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01039v1",
                "updated": "2025-01-02T03:41:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T03:41:32Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "title": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention"
                },
                "summary": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shivank Nag"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Lu Tian"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v2",
                "updated": "2025-01-02T03:40:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    40,
                    15,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01005v1",
                "updated": "2025-01-02T02:02:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T02:02:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving"
                },
                "summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."
                },
                "authors": [
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Lequn Chen"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Wuwei Lin"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Vinod Grover"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    }
                ],
                "author_detail": {
                    "name": "Luis Ceze"
                },
                "author": "Luis Ceze",
                "arxiv_comment": "code available at http://github.com/flashinfer-ai/flashinfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00946v1",
                "updated": "2025-01-01T20:16:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T20:16:27Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "title": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model"
                },
                "summary": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches."
                },
                "authors": [
                    {
                        "name": "Omid Saghatchian"
                    },
                    {
                        "name": "Atiyeh Gh. Moghadam"
                    },
                    {
                        "name": "Ahmad Nickabadi"
                    }
                ],
                "author_detail": {
                    "name": "Ahmad Nickabadi"
                },
                "author": "Ahmad Nickabadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21023v2",
                "updated": "2024-12-31T20:40:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    20,
                    40,
                    43,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-30T15:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    46,
                    53,
                    0,
                    365,
                    0
                ],
                "title": "EdgeRAG: Online-Indexed RAG for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeRAG: Online-Indexed RAG for Edge Devices"
                },
                "summary": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory."
                },
                "authors": [
                    {
                        "name": "Korakit Seemakhupt"
                    },
                    {
                        "name": "Sihang Liu"
                    },
                    {
                        "name": "Samira Khan"
                    }
                ],
                "author_detail": {
                    "name": "Samira Khan"
                },
                "author": "Samira Khan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00375v1",
                "updated": "2024-12-31T09:56:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    56,
                    40,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T09:56:40Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    56,
                    40,
                    1,
                    366,
                    0
                ],
                "title": "Token Pruning for Caching Better: 9 Times Acceleration on Stable\n  Diffusion for Free",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Pruning for Caching Better: 9 Times Acceleration on Stable\n  Diffusion for Free"
                },
                "summary": "Stable Diffusion has achieved remarkable success in the field of\ntext-to-image generation, with its powerful generative capabilities and diverse\ngeneration results making a lasting impact. However, its iterative denoising\nintroduces high computational costs and slows generation speed, limiting\nbroader adoption. The community has made numerous efforts to reduce this\ncomputational burden, with methods like feature caching attracting attention\ndue to their effectiveness and simplicity. Nonetheless, simply reusing features\ncomputed at previous timesteps causes the features across adjacent timesteps to\nbecome similar, reducing the dynamics of features over time and ultimately\ncompromising the quality of generated images. In this paper, we introduce a\ndynamics-aware token pruning (DaTo) approach that addresses the limitations of\nfeature caching. DaTo selectively prunes tokens with lower dynamics, allowing\nonly high-dynamic tokens to participate in self-attention layers, thereby\nextending feature dynamics across timesteps. DaTo combines feature caching with\ntoken pruning in a training-free manner, achieving both temporal and token-wise\ninformation reuse. Applied to Stable Diffusion on the ImageNet, our approach\ndelivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced\nimage quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled\nwith a notable FID reduction of 2.17.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable Diffusion has achieved remarkable success in the field of\ntext-to-image generation, with its powerful generative capabilities and diverse\ngeneration results making a lasting impact. However, its iterative denoising\nintroduces high computational costs and slows generation speed, limiting\nbroader adoption. The community has made numerous efforts to reduce this\ncomputational burden, with methods like feature caching attracting attention\ndue to their effectiveness and simplicity. Nonetheless, simply reusing features\ncomputed at previous timesteps causes the features across adjacent timesteps to\nbecome similar, reducing the dynamics of features over time and ultimately\ncompromising the quality of generated images. In this paper, we introduce a\ndynamics-aware token pruning (DaTo) approach that addresses the limitations of\nfeature caching. DaTo selectively prunes tokens with lower dynamics, allowing\nonly high-dynamic tokens to participate in self-attention layers, thereby\nextending feature dynamics across timesteps. DaTo combines feature caching with\ntoken pruning in a training-free manner, achieving both temporal and token-wise\ninformation reuse. Applied to Stable Diffusion on the ImageNet, our approach\ndelivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced\nimage quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled\nwith a notable FID reduction of 2.17."
                },
                "authors": [
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Bang Xiao"
                    },
                    {
                        "name": "Jiayi Tang"
                    },
                    {
                        "name": "Qianli Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v3",
                "updated": "2024-12-31T07:11:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    7,
                    11,
                    0,
                    1,
                    366,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v1",
                "updated": "2024-12-31T05:24:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00243v1",
                "updated": "2024-12-31T03:19:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    19,
                    38,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T03:19:38Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    19,
                    38,
                    1,
                    366,
                    0
                ],
                "title": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained\n  Image Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained\n  Image Recognition"
                },
                "summary": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that\ninvolves classifying images within a macro-category. While traditional FGIR\ndeals with classifying different species, UFGIR goes beyond by classifying\nsub-categories within a species such as cultivars of a plant. In recent times\nthe usage of Vision Transformer-based backbones has allowed methods to obtain\noutstanding recognition performances in this task but this comes at a\nsignificant cost in terms of computation specially since this task\nsignificantly benefits from incorporating higher resolution images. Therefore,\ntechniques such as token reduction have emerged to reduce the computational\ncost. However, dropping tokens leads to loss of essential information for\nfine-grained categories, specially as the token keep rate is reduced.\nTherefore, to counteract the loss of information brought by the usage of token\nreduction we propose a novel Cross-Layer Aggregation Classification Head and a\nCross-Layer Cache mechanism to recover and access information from previous\nlayers in later locations. Extensive experiments covering more than 2000 runs\nacross diverse settings including 5 datasets, 9 backbones, 7 token reduction\nmethods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the\nproposed plug-and-play modules and allow us to push the boundaries of accuracy\nvs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to\n10\\% while maintaining a competitive accuracy to state-of-the-art models. Code\nis available at: \\url{https://github.com/arkel23/CLCA}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that\ninvolves classifying images within a macro-category. While traditional FGIR\ndeals with classifying different species, UFGIR goes beyond by classifying\nsub-categories within a species such as cultivars of a plant. In recent times\nthe usage of Vision Transformer-based backbones has allowed methods to obtain\noutstanding recognition performances in this task but this comes at a\nsignificant cost in terms of computation specially since this task\nsignificantly benefits from incorporating higher resolution images. Therefore,\ntechniques such as token reduction have emerged to reduce the computational\ncost. However, dropping tokens leads to loss of essential information for\nfine-grained categories, specially as the token keep rate is reduced.\nTherefore, to counteract the loss of information brought by the usage of token\nreduction we propose a novel Cross-Layer Aggregation Classification Head and a\nCross-Layer Cache mechanism to recover and access information from previous\nlayers in later locations. Extensive experiments covering more than 2000 runs\nacross diverse settings including 5 datasets, 9 backbones, 7 token reduction\nmethods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the\nproposed plug-and-play modules and allow us to push the boundaries of accuracy\nvs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to\n10\\% while maintaining a competitive accuracy to state-of-the-art models. Code\nis available at: \\url{https://github.com/arkel23/CLCA}"
                },
                "authors": [
                    {
                        "name": "Edwin Arkel Rios"
                    },
                    {
                        "name": "Jansen Christopher Yuanda"
                    },
                    {
                        "name": "Vincent Leon Ghanz"
                    },
                    {
                        "name": "Cheng-Wei Yu"
                    },
                    {
                        "name": "Bo-Cheng Lai"
                    },
                    {
                        "name": "Min-Chun Hu"
                    }
                ],
                "author_detail": {
                    "name": "Min-Chun Hu"
                },
                "author": "Min-Chun Hu",
                "arxiv_comment": "Accepted to ICASSP 2025. Main: 5 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21015v1",
                "updated": "2024-12-30T15:33:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:33:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "MapQaTor: A System for Efficient Annotation of Map Query Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MapQaTor: A System for Efficient Annotation of Map Query Datasets"
                },
                "summary": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q."
                },
                "authors": [
                    {
                        "name": "Mahir Labib Dihan"
                    },
                    {
                        "name": "Mohammed Eunus Ali"
                    },
                    {
                        "name": "Md Rizwan Parvez"
                    }
                ],
                "author_detail": {
                    "name": "Md Rizwan Parvez"
                },
                "author": "Md Rizwan Parvez",
                "arxiv_comment": "13 pages, 35 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v3",
                "updated": "2024-12-30T14:54:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    54,
                    29,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20887v1",
                "updated": "2024-12-30T11:54:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T11:54:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field"
                },
                "summary": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime."
                },
                "authors": [
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Hanbyul Kim"
                    },
                    {
                        "name": "Xinbo Wang"
                    },
                    {
                        "name": "Jianlin Luo"
                    },
                    {
                        "name": "Simone Latini"
                    },
                    {
                        "name": "Dongbin Shin"
                    },
                    {
                        "name": "Jun-Ming Liu"
                    },
                    {
                        "name": "Jing-Feng Li"
                    },
                    {
                        "name": "Angel Rubio"
                    },
                    {
                        "name": "Ce-Wen Nan"
                    },
                    {
                        "name": "Qian Li"
                    }
                ],
                "author_detail": {
                    "name": "Qian Li"
                },
                "author": "Qian Li",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v2",
                "updated": "2024-12-30T05:01:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    1,
                    44,
                    0,
                    365,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20677v1",
                "updated": "2024-12-30T03:05:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T03:05:45Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "title": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA"
                },
                "summary": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning."
                },
                "authors": [
                    {
                        "name": "Qingyun Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Zengchang Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zengchang Qin"
                },
                "author": "Zengchang Qin",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00068v1",
                "updated": "2024-12-29T17:41:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T17:41:40Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques"
                },
                "summary": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1]. The proposed framework operates within the storage\nkernel, ensuring minimal latency and low computational overhead. Through an\nadaptive feedback mechanism, RL-Storage dynamically adjusts critical\nparameters, achieving efficient resource utilization across a wide range of\nworkloads. Experimental evaluations conducted on a range of benchmarks,\nincluding RocksDB and PostgreSQL, demonstrate significant improvements, with\nthroughput gains of up to 2.6x and latency reductions of 43% compared to\nbaseline heuristics. Additionally, RL-Storage achieves these performance\nenhancements with a negligible CPU overhead of 0.11% and a memory footprint of\nonly 5 KB, making it suitable for seamless deployment in production\nenvironments. This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1]. The proposed framework operates within the storage\nkernel, ensuring minimal latency and low computational overhead. Through an\nadaptive feedback mechanism, RL-Storage dynamically adjusts critical\nparameters, achieving efficient resource utilization across a wide range of\nworkloads. Experimental evaluations conducted on a range of benchmarks,\nincluding RocksDB and PostgreSQL, demonstrate significant improvements, with\nthroughput gains of up to 2.6x and latency reductions of 43% compared to\nbaseline heuristics. Additionally, RL-Storage achieves these performance\nenhancements with a negligible CPU overhead of 0.11% and a memory footprint of\nonly 5 KB, making it suitable for seamless deployment in production\nenvironments. This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20524v1",
                "updated": "2024-12-29T17:18:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T17:18:21Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "title": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation"
                },
                "summary": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes."
                },
                "authors": [
                    {
                        "name": "Anatolij Zubow"
                    },
                    {
                        "name": "Yannik Pilz"
                    },
                    {
                        "name": "Sascha Rsler"
                    },
                    {
                        "name": "Falko Dressler"
                    }
                ],
                "author_detail": {
                    "name": "Falko Dressler"
                },
                "author": "Falko Dressler",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20221v1",
                "updated": "2024-12-28T17:17:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T17:17:03Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "title": "Revisiting Cache Freshness for Emerging Real-Time Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Cache Freshness for Emerging Real-Time Applications"
                },
                "summary": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness."
                },
                "authors": [
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Rishabh Iyer"
                    },
                    {
                        "name": "Scott Shenker"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "arxiv_doi": "10.1145/3696348.3696858",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696348.3696858",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.20221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "HotNets '24",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20105v1",
                "updated": "2024-12-28T10:17:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T10:17:29Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "title": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming"
                },
                "summary": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference."
                },
                "authors": [
                    {
                        "name": "Jiedong Zhuang"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Ming Dai"
                    },
                    {
                        "name": "Rui Hu"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Haoji Hu"
                    }
                ],
                "author_detail": {
                    "name": "Haoji Hu"
                },
                "author": "Haoji Hu",
                "arxiv_comment": "Accepted to AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19991v1",
                "updated": "2024-12-28T03:28:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T03:28:52Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "title": "A Robust Federated Learning Framework for Undependable Devices at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Robust Federated Learning Framework for Undependable Devices at Scale"
                },
                "summary": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shilong Wang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Chunming Qiao"
                    },
                    {
                        "name": "Huarong Deng"
                    },
                    {
                        "name": "Qiuye Zheng"
                    },
                    {
                        "name": "Jiantao Gong"
                    }
                ],
                "author_detail": {
                    "name": "Jiantao Gong"
                },
                "author": "Jiantao Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19051v1",
                "updated": "2024-12-26T04:13:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T04:13:52Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "title": "Performance Characterization and Optimizations of Traditional ML\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Characterization and Optimizations of Traditional ML\n  Applications"
                },
                "summary": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement."
                },
                "authors": [
                    {
                        "name": "Harsh Kumar"
                    },
                    {
                        "name": "R. Govindarajan"
                    }
                ],
                "author_detail": {
                    "name": "R. Govindarajan"
                },
                "author": "R. Govindarajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18960v1",
                "updated": "2024-12-25T18:36:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T18:36:21Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "title": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems"
                },
                "summary": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming."
                },
                "authors": [
                    {
                        "name": "Nader Alfares"
                    },
                    {
                        "name": "George Kesidis"
                    }
                ],
                "author_detail": {
                    "name": "George Kesidis"
                },
                "author": "George Kesidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v1",
                "updated": "2024-12-25T14:14:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories"
                },
                "summary": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "23 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18911v1",
                "updated": "2024-12-25T14:00:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:00:14Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Dual Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Dual Feature Caching"
                },
                "summary": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}"
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Runlin Guo"
                    },
                    {
                        "name": "Haohang Xu"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18885v1",
                "updated": "2024-12-25T11:59:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T11:59:17Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "title": "Aspect-oriented Programming with Julia",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-oriented Programming with Julia"
                },
                "summary": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems."
                },
                "authors": [
                    {
                        "name": "Osamu Ishimura"
                    },
                    {
                        "name": "Yoshihide Yoshimoto"
                    }
                ],
                "author_detail": {
                    "name": "Yoshihide Yoshimoto"
                },
                "author": "Yoshihide Yoshimoto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05460v1",
                "updated": "2024-12-25T10:11:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T10:11:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Efficiently serving large multimedia models using EPD Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently serving large multimedia models using EPD Disaggregation"
                },
                "summary": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step helps convert raw inputs into tokenized representations\nthat inflate the token sequence for the prefill phase, negatively impacting key\nService Level Objectives (SLOs) like time to first token (TTFT) and end-to-end\nthroughput. We introduce Encode-Prefill-Decode (EPD) Disaggregation, a novel\nframework that separates the encoding, prefill, and decode stages onto\ndedicated resources. Unlike current systems, which bundle encoding and prefill\ntogether, our disaggregation approach alleviates memory bottlenecks, mitigates\nsynchronization delays, and supports flexible batching. Specifically, we employ\na new caching mechanism for multimodal tokens, enabling asynchronous transfer\nof multimodal tokens and introduce an integrated module to find optimal config\nfor EPD system and minimize resource usage while maximizing SLO-based\nperformance metric. Experimental evaluations with popular LMMs show substantial\ngains in memory efficiency (up to 15$\\times$ lesser for encoding-stage GPUs),\nthat supports upto 22$\\times$ higher batch sizes, 10$\\times$ more number of\nimages/ request, 2.2$\\times$ higher kv cache size. Further, it leads to\nsignificant improvements in end-to-end throughput (up to 57\\% better), and\nlatency metrics (TTFT up to 71\\% lower), compared to systems that do not\ndisaggregate. Our findings underscore the potential of EPD disaggregation to\nenable resource-efficient and high-performance multimodal inference at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step helps convert raw inputs into tokenized representations\nthat inflate the token sequence for the prefill phase, negatively impacting key\nService Level Objectives (SLOs) like time to first token (TTFT) and end-to-end\nthroughput. We introduce Encode-Prefill-Decode (EPD) Disaggregation, a novel\nframework that separates the encoding, prefill, and decode stages onto\ndedicated resources. Unlike current systems, which bundle encoding and prefill\ntogether, our disaggregation approach alleviates memory bottlenecks, mitigates\nsynchronization delays, and supports flexible batching. Specifically, we employ\na new caching mechanism for multimodal tokens, enabling asynchronous transfer\nof multimodal tokens and introduce an integrated module to find optimal config\nfor EPD system and minimize resource usage while maximizing SLO-based\nperformance metric. Experimental evaluations with popular LMMs show substantial\ngains in memory efficiency (up to 15$\\times$ lesser for encoding-stage GPUs),\nthat supports upto 22$\\times$ higher batch sizes, 10$\\times$ more number of\nimages/ request, 2.2$\\times$ higher kv cache size. Further, it leads to\nsignificant improvements in end-to-end throughput (up to 57\\% better), and\nlatency metrics (TTFT up to 71\\% lower), compared to systems that do not\ndisaggregate. Our findings underscore the potential of EPD disaggregation to\nenable resource-efficient and high-performance multimodal inference at scale."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Ivan Hu"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Linzi Xing"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "arxiv_comment": "13 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16187v2",
                "updated": "2024-12-24T13:04:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    4,
                    45,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-13T06:00:27Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    0,
                    27,
                    4,
                    348,
                    0
                ],
                "title": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing"
                },
                "summary": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks."
                },
                "authors": [
                    {
                        "name": "Minghui Liu"
                    },
                    {
                        "name": "Tahseen Rabbani"
                    },
                    {
                        "name": "Tony O'Halloran"
                    },
                    {
                        "name": "Ananth Sankaralingam"
                    },
                    {
                        "name": "Mary-Anne Hartley"
                    },
                    {
                        "name": "Brian Gravelle"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Cornelia Fermller"
                    },
                    {
                        "name": "Yiannis Aloimonos"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Aloimonos"
                },
                "author": "Yiannis Aloimonos",
                "arxiv_comment": "10 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01959v2",
                "updated": "2024-12-24T00:46:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    0,
                    46,
                    0,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-02T20:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Development and Application of a Decentralized Domain Name Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Application of a Decentralized Domain Name Service"
                },
                "summary": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17747v1",
                "updated": "2024-12-23T18:02:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T18:02:25Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "title": "Deliberation in Latent Space via Differentiable Cache Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deliberation in Latent Space via Differentiable Cache Augmentation"
                },
                "summary": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks."
                },
                "authors": [
                    {
                        "name": "Luyang Liu"
                    },
                    {
                        "name": "Jonas Pfeiffer"
                    },
                    {
                        "name": "Jiaxing Wu"
                    },
                    {
                        "name": "Jun Xie"
                    },
                    {
                        "name": "Arthur Szlam"
                    }
                ],
                "author_detail": {
                    "name": "Arthur Szlam"
                },
                "author": "Arthur Szlam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17685v1",
                "updated": "2024-12-23T16:11:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T16:11:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment"
                },
                "summary": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application."
                },
                "authors": [
                    {
                        "name": "Edward J. Oughton"
                    },
                    {
                        "name": "Evan Alexander Peters"
                    },
                    {
                        "name": "Dennies Bor"
                    },
                    {
                        "name": "Noah Rivera"
                    },
                    {
                        "name": "C. Trevor Gaunt"
                    },
                    {
                        "name": "Robert Weigel"
                    }
                ],
                "author_detail": {
                    "name": "Robert Weigel"
                },
                "author": "Robert Weigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18919v2",
                "updated": "2024-12-23T14:40:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    40,
                    26,
                    0,
                    358,
                    0
                ],
                "published": "2024-05-29T09:22:25Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    9,
                    22,
                    25,
                    2,
                    150,
                    0
                ],
                "title": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN"
                },
                "summary": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Chenyu Wu"
                    },
                    {
                        "name": "Shuai Han"
                    },
                    {
                        "name": "Weixiao Meng"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "14 pages, 13 figures. This work has been accepted by IEEE Internet of\n  Things Journal. It is expanded on our previous research presented at the IEEE\n  Globecom 2024: Q. Chen, C. Wu, S. Han, W. Meng, and T. Q. Quek, \"Exploiting\n  Inter-Satellite Links for In-Flight Connectivity Scheme in Space-Air-Ground\n  Integrated Networks,\" in Proc. GLOBECOM 2024, Cape Town, South Africa, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03408v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03408v3",
                "updated": "2024-12-23T12:55:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    12,
                    55,
                    21,
                    0,
                    358,
                    0
                ],
                "published": "2024-02-05T15:10:42Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    15,
                    10,
                    42,
                    0,
                    36,
                    0
                ],
                "title": "A Framework for Effective Invocation Methods of Various LLM Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Effective Invocation Methods of Various LLM Services"
                },
                "summary": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research."
                },
                "authors": [
                    {
                        "name": "Can Wang"
                    },
                    {
                        "name": "Dianbo Sui"
                    },
                    {
                        "name": "Bolin Zhang"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Jiabao Kang"
                    },
                    {
                        "name": "Zhidong Qiao"
                    },
                    {
                        "name": "Zhiying Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiying Tu"
                },
                "author": "Zhiying Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03408v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03408v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17464v1",
                "updated": "2024-12-23T10:41:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T10:41:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "CALLIC: Content Adaptive Learning for Lossless Image Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALLIC: Content Adaptive Learning for Lossless Image Compression"
                },
                "summary": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression."
                },
                "authors": [
                    {
                        "name": "Daxin Li"
                    },
                    {
                        "name": "Yuanchao Bai"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Junjun Jiang"
                    },
                    {
                        "name": "Xianming Liu"
                    },
                    {
                        "name": "Wen Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Gao"
                },
                "author": "Wen Gao",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17246v1",
                "updated": "2024-12-23T03:38:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T03:38:46Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "title": "Fast and Live Model Auto Scaling with O(1) Host Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Live Model Auto Scaling with O(1) Host Caching"
                },
                "summary": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling."
                },
                "authors": [
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05831v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05831v2",
                "updated": "2024-12-23T02:52:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    2,
                    52,
                    36,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-08T06:37:27Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "title": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval"
                },
                "summary": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval."
                },
                "authors": [
                    {
                        "name": "Shanti Stewart"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Lie Lu"
                    },
                    {
                        "name": "Andrea Fanelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Fanelli"
                },
                "author": "Andrea Fanelli",
                "arxiv_comment": "Accepted at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05831v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05831v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17203v1",
                "updated": "2024-12-23T00:46:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T00:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "title": "Agile TLB Prefetching and Prediction Replacement Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agile TLB Prefetching and Prediction Replacement Policy"
                },
                "summary": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management."
                },
                "authors": [
                    {
                        "name": "Melkamu Mersha"
                    },
                    {
                        "name": "Tsion Abay"
                    },
                    {
                        "name": "Mingziem Bitewa"
                    },
                    {
                        "name": "Gedare Bloom"
                    }
                ],
                "author_detail": {
                    "name": "Gedare Bloom"
                },
                "author": "Gedare Bloom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16897v1",
                "updated": "2024-12-22T07:14:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "published": "2024-12-22T07:14:45Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "title": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context"
                },
                "summary": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC"
                },
                "authors": [
                    {
                        "name": "Shuai Lyu"
                    },
                    {
                        "name": "Fangjian Liao"
                    },
                    {
                        "name": "Zeqi Ma"
                    },
                    {
                        "name": "Rongchen Zhang"
                    },
                    {
                        "name": "Dongmei Mo"
                    },
                    {
                        "name": "Waikeung Wong"
                    }
                ],
                "author_detail": {
                    "name": "Waikeung Wong"
                },
                "author": "Waikeung Wong",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17565v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17565v3",
                "updated": "2024-12-21T13:55:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    13,
                    55,
                    49,
                    5,
                    356,
                    0
                ],
                "published": "2024-06-25T14:02:08Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    14,
                    2,
                    8,
                    1,
                    177,
                    0
                ],
                "title": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool"
                },
                "summary": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time."
                },
                "authors": [
                    {
                        "name": "Cunchen Hu"
                    },
                    {
                        "name": "Heyang Huang"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Jiang Xu"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Sa Wang"
                    },
                    {
                        "name": "Yungang Bao"
                    },
                    {
                        "name": "Ninghui Sun"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17565v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17565v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16585v1",
                "updated": "2024-12-21T11:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T11:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "title": "Parameterized Complexity of Caching in Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameterized Complexity of Caching in Networks"
                },
                "summary": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable."
                },
                "authors": [
                    {
                        "name": "Robert Ganian"
                    },
                    {
                        "name": "Fionn Mc Inerney"
                    },
                    {
                        "name": "Dimitra Tsigkari"
                    }
                ],
                "author_detail": {
                    "name": "Dimitra Tsigkari"
                },
                "author": "Dimitra Tsigkari",
                "arxiv_comment": "A shorter version of this paper will appear in the proceedings of\n  AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v4",
                "updated": "2024-12-21T02:36:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    2,
                    36,
                    3,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiming Yang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    },
                    {
                        "name": "Zonghong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zonghong Dai"
                },
                "author": "Zonghong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16434v1",
                "updated": "2024-12-21T01:48:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T01:48:52Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "title": "SYMPHONY: Improving Memory Management for LLM Inference Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SYMPHONY: Improving Memory Management for LLM Inference Workloads"
                },
                "summary": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile."
                },
                "authors": [
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Anyong Mao"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16001v1",
                "updated": "2024-12-20T15:51:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T15:51:42Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "title": "Multi-Strided Access Patterns to Boost Hardware Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Strided Access Patterns to Boost Hardware Prefetching"
                },
                "summary": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future."
                },
                "authors": [
                    {
                        "name": "Miguel O. Blom"
                    },
                    {
                        "name": "Kristian F. D. Rietveld"
                    },
                    {
                        "name": "Rob V. van Nieuwpoort"
                    }
                ],
                "author_detail": {
                    "name": "Rob V. van Nieuwpoort"
                },
                "author": "Rob V. van Nieuwpoort",
                "arxiv_comment": "12 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14485v2",
                "updated": "2024-12-20T15:18:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    18,
                    44,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-19T03:11:33Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    3,
                    11,
                    33,
                    3,
                    354,
                    0
                ],
                "title": "Towards Projected and Incremental Pseudo-Boolean Model Counting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Projected and Incremental Pseudo-Boolean Model Counting"
                },
                "summary": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting."
                },
                "authors": [
                    {
                        "name": "Suwei Yang"
                    },
                    {
                        "name": "Kuldeep S. Meel"
                    }
                ],
                "author_detail": {
                    "name": "Kuldeep S. Meel"
                },
                "author": "Kuldeep S. Meel",
                "arxiv_comment": "To appear in AAAI25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15605v1",
                "updated": "2024-12-20T06:58:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T06:58:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks"
                },
                "summary": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v3",
                "updated": "2024-12-19T23:52:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    23,
                    52,
                    16,
                    3,
                    354,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v2",
                "updated": "2024-12-19T22:34:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    22,
                    34,
                    37,
                    3,
                    354,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jimnez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.10360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10360v1",
                "updated": "2025-01-17T18:59:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    18,
                    59,
                    55,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T18:59:55Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    18,
                    59,
                    55,
                    4,
                    17,
                    0
                ],
                "title": "FaceXBench: Evaluating Multimodal LLMs on Face Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaceXBench: Evaluating Multimodal LLMs on Face Understanding"
                },
                "summary": "Multimodal Large Language Models (MLLMs) demonstrate impressive\nproblem-solving abilities across a wide range of tasks and domains. However,\ntheir capacity for face understanding has not been systematically studied. To\naddress this gap, we introduce FaceXBench, a comprehensive benchmark designed\nto evaluate MLLMs on complex face understanding tasks. FaceXBench includes\n5,000 multimodal multiple-choice questions derived from 25 public datasets and\na newly created dataset, FaceXAPI. These questions cover 14 tasks across 6\nbroad categories, assessing MLLMs' face understanding abilities in bias and\nfairness, face authentication, recognition, analysis, localization and tool\nretrieval. Using FaceXBench, we conduct an extensive evaluation of 26\nopen-source MLLMs alongside 2 proprietary models, revealing the unique\nchallenges in complex face understanding tasks. We analyze the models across\nthree evaluation settings: zero-shot, in-context task description, and\nchain-of-thought prompting. Our detailed analysis reveals that current MLLMs,\nincluding advanced models like GPT-4o, and GeminiPro 1.5, show significant room\nfor improvement. We believe FaceXBench will be a crucial resource for\ndeveloping MLLMs equipped to perform sophisticated face understanding. Code:\nhttps://github.com/Kartik-3004/facexbench",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) demonstrate impressive\nproblem-solving abilities across a wide range of tasks and domains. However,\ntheir capacity for face understanding has not been systematically studied. To\naddress this gap, we introduce FaceXBench, a comprehensive benchmark designed\nto evaluate MLLMs on complex face understanding tasks. FaceXBench includes\n5,000 multimodal multiple-choice questions derived from 25 public datasets and\na newly created dataset, FaceXAPI. These questions cover 14 tasks across 6\nbroad categories, assessing MLLMs' face understanding abilities in bias and\nfairness, face authentication, recognition, analysis, localization and tool\nretrieval. Using FaceXBench, we conduct an extensive evaluation of 26\nopen-source MLLMs alongside 2 proprietary models, revealing the unique\nchallenges in complex face understanding tasks. We analyze the models across\nthree evaluation settings: zero-shot, in-context task description, and\nchain-of-thought prompting. Our detailed analysis reveals that current MLLMs,\nincluding advanced models like GPT-4o, and GeminiPro 1.5, show significant room\nfor improvement. We believe FaceXBench will be a crucial resource for\ndeveloping MLLMs equipped to perform sophisticated face understanding. Code:\nhttps://github.com/Kartik-3004/facexbench"
                },
                "authors": [
                    {
                        "name": "Kartik Narayan"
                    },
                    {
                        "name": "Vibashan VS"
                    },
                    {
                        "name": "Vishal M. Patel"
                    }
                ],
                "author_detail": {
                    "name": "Vishal M. Patel"
                },
                "author": "Vishal M. Patel",
                "arxiv_comment": "Project Page: https://kartik-3004.github.io/facexbench/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10339v1",
                "updated": "2025-01-17T18:23:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    18,
                    23,
                    16,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T18:23:16Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    18,
                    23,
                    16,
                    4,
                    17,
                    0
                ],
                "title": "Principled model selection for stochastic dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Principled model selection for stochastic dynamics"
                },
                "summary": "Complex dynamical systems, from macromolecules to ecosystems, are often\nmodeled by stochastic differential equations (SDEs). To learn such models from\ndata, a common approach involves decomposing the SDE into a linear combination\nof basis functions. However, this can induce overfitting due to the\nproliferation of parameters. To address this, we introduce Parsimonious\nStochastic Inference (PASTIS), a principled method that removes superfluous\nparameters from SDE models by combining likelihood-estimation statistics with\nextreme value theory. We benchmark it against existing methods and show that it\nreliably selects the exact minimal models from large libraries of functions,\neven with a low sampling rate or measurement error. We show that it extends to\nstochastic partial differential equations and demonstrate applications to the\ninference of ecological networks and reaction-diffusion dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex dynamical systems, from macromolecules to ecosystems, are often\nmodeled by stochastic differential equations (SDEs). To learn such models from\ndata, a common approach involves decomposing the SDE into a linear combination\nof basis functions. However, this can induce overfitting due to the\nproliferation of parameters. To address this, we introduce Parsimonious\nStochastic Inference (PASTIS), a principled method that removes superfluous\nparameters from SDE models by combining likelihood-estimation statistics with\nextreme value theory. We benchmark it against existing methods and show that it\nreliably selects the exact minimal models from large libraries of functions,\neven with a low sampling rate or measurement error. We show that it extends to\nstochastic partial differential equations and demonstrate applications to the\ninference of ecological networks and reaction-diffusion dynamics."
                },
                "authors": [
                    {
                        "name": "Andonis Gerardos"
                    },
                    {
                        "name": "Pierre Ronceray"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Ronceray"
                },
                "author": "Pierre Ronceray",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07772v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07772v2",
                "updated": "2025-01-17T18:11:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    18,
                    11,
                    7,
                    4,
                    17,
                    0
                ],
                "published": "2024-03-12T15:58:53Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    15,
                    58,
                    53,
                    1,
                    72,
                    0
                ],
                "title": "Privacy Guarantees in Posterior Sampling under Contamination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy Guarantees in Posterior Sampling under Contamination"
                },
                "summary": "In recent years differential privacy has been adopted by tech-companies and\ngovernmental agencies as the standard for measuring privacy in algorithms. In\nthis article, we study differential privacy in Bayesian posterior sampling\nsettings. We begin by considering differential privacy in the most common\nprivatization setting in which Laplace or Gaussian noise is simply injected\ninto the output. In an effort to achieve better differential privacy, we\nconsider adopting {\\em Huber's contamination model} for use within privacy\nsettings, and replace at random data points with samples from a heavy-tailed\ndistribution ({\\em instead} of injecting noise into the output). We derive\nbounds for the differential privacy level $(\\epsilon,\\delta)$ of our approach,\nwithout the need to impose the restriction of having a bounded observation and\nparameter space which is commonly used by existing approaches and literature.\nWe further consider for our approach the effect of sample size on the privacy\nlevel and the convergence rate of $(\\epsilon,\\delta)$ to zero. Asymptotically,\nour contamination approach is fully private at no cost of information loss. We\nalso provide some examples depicting inference models that our setup is\napplicable to with a theoretical estimation of the convergence rate, together\nwith some simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years differential privacy has been adopted by tech-companies and\ngovernmental agencies as the standard for measuring privacy in algorithms. In\nthis article, we study differential privacy in Bayesian posterior sampling\nsettings. We begin by considering differential privacy in the most common\nprivatization setting in which Laplace or Gaussian noise is simply injected\ninto the output. In an effort to achieve better differential privacy, we\nconsider adopting {\\em Huber's contamination model} for use within privacy\nsettings, and replace at random data points with samples from a heavy-tailed\ndistribution ({\\em instead} of injecting noise into the output). We derive\nbounds for the differential privacy level $(\\epsilon,\\delta)$ of our approach,\nwithout the need to impose the restriction of having a bounded observation and\nparameter space which is commonly used by existing approaches and literature.\nWe further consider for our approach the effect of sample size on the privacy\nlevel and the convergence rate of $(\\epsilon,\\delta)$ to zero. Asymptotically,\nour contamination approach is fully private at no cost of information loss. We\nalso provide some examples depicting inference models that our setup is\napplicable to with a theoretical estimation of the convergence rate, together\nwith some simulations."
                },
                "authors": [
                    {
                        "name": "Shenggang Hu"
                    },
                    {
                        "name": "Louis Aslett"
                    },
                    {
                        "name": "Hongsheng Dai"
                    },
                    {
                        "name": "Murray Pollock"
                    },
                    {
                        "name": "Gareth O. Roberts"
                    }
                ],
                "author_detail": {
                    "name": "Gareth O. Roberts"
                },
                "author": "Gareth O. Roberts",
                "arxiv_comment": "54 pages, 4 figures. Updated for resubmission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07772v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07772v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F15, 62J12",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10332v1",
                "updated": "2025-01-17T18:05:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    18,
                    5,
                    4,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T18:05:04Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    18,
                    5,
                    4,
                    4,
                    17,
                    0
                ],
                "title": "Agent4Edu: Generating Learner Response Data by Generative Agents for\n  Intelligent Education Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent4Edu: Generating Learner Response Data by Generative Agents for\n  Intelligent Education Systems"
                },
                "summary": "Personalized learning represents a promising educational strategy within\nintelligent educational systems, aiming to enhance learners' practice\nefficiency. However, the discrepancy between offline metrics and online\nperformance significantly impedes their progress. To address this challenge, we\nintroduce Agent4Edu, a novel personalized learning simulator leveraging recent\nadvancements in human intelligence through large language models (LLMs).\nAgent4Edu features LLM-powered generative agents equipped with learner profile,\nmemory, and action modules tailored to personalized learning algorithms. The\nlearner profiles are initialized using real-world response data, capturing\npractice styles and cognitive factors. Inspired by human psychology theory, the\nmemory module records practice facts and high-level summaries, integrating\nreflection mechanisms. The action module supports various behaviors, including\nexercise understanding, analysis, and response generation. Each agent can\ninteract with personalized learning algorithms, such as computerized adaptive\ntesting, enabling a multifaceted evaluation and enhancement of customized\nservices. Through a comprehensive assessment, we explore the strengths and\nweaknesses of Agent4Edu, emphasizing the consistency and discrepancies in\nresponses between agents and human learners. The code, data, and appendix are\npublicly available at https://github.com/bigdata-ustc/Agent4Edu.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized learning represents a promising educational strategy within\nintelligent educational systems, aiming to enhance learners' practice\nefficiency. However, the discrepancy between offline metrics and online\nperformance significantly impedes their progress. To address this challenge, we\nintroduce Agent4Edu, a novel personalized learning simulator leveraging recent\nadvancements in human intelligence through large language models (LLMs).\nAgent4Edu features LLM-powered generative agents equipped with learner profile,\nmemory, and action modules tailored to personalized learning algorithms. The\nlearner profiles are initialized using real-world response data, capturing\npractice styles and cognitive factors. Inspired by human psychology theory, the\nmemory module records practice facts and high-level summaries, integrating\nreflection mechanisms. The action module supports various behaviors, including\nexercise understanding, analysis, and response generation. Each agent can\ninteract with personalized learning algorithms, such as computerized adaptive\ntesting, enabling a multifaceted evaluation and enhancement of customized\nservices. Through a comprehensive assessment, we explore the strengths and\nweaknesses of Agent4Edu, emphasizing the consistency and discrepancies in\nresponses between agents and human learners. The code, data, and appendix are\npublicly available at https://github.com/bigdata-ustc/Agent4Edu."
                },
                "authors": [
                    {
                        "name": "Weibo Gao"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Linan Yue"
                    },
                    {
                        "name": "Fangzhou Yao"
                    },
                    {
                        "name": "Rui Lv"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Zhenya Huang"
                    }
                ],
                "author_detail": {
                    "name": "Zhenya Huang"
                },
                "author": "Zhenya Huang",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10326v1",
                "updated": "2025-01-17T17:56:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    17,
                    56,
                    58,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T17:56:58Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    17,
                    56,
                    58,
                    4,
                    17,
                    0
                ],
                "title": "Large language models for automated scholarly paper review: A survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models for automated scholarly paper review: A survey"
                },
                "summary": "Large language models (LLMs) have significantly impacted human society,\ninfluencing various domains. Among them, academia is not simply a domain\naffected by LLMs, but it is also the pivotal force in the development of LLMs.\nIn academic publications, this phenomenon is represented during the\nincorporation of LLMs into the peer review mechanism for reviewing manuscripts.\nWe proposed the concept of automated scholarly paper review (ASPR) in our\nprevious paper. As the incorporation grows, it now enters the coexistence phase\nof ASPR and peer review, which is described in that paper. LLMs hold\ntransformative potential for the full-scale implementation of ASPR, but they\nalso pose new issues and challenges that need to be addressed. In this survey\npaper, we aim to provide a holistic view of ASPR in the era of LLMs. We begin\nwith a survey to find out which LLMs are used to conduct ASPR. Then, we review\nwhat ASPR-related technological bottlenecks have been solved with the\nincorporation of LLM technology. After that, we move on to explore new methods,\nnew datasets, new source code, and new online systems that come with LLMs for\nASPR. Furthermore, we summarize the performance and issues of LLMs in ASPR, and\ninvestigate the attitudes and reactions of publishers and academia to ASPR.\nLastly, we discuss the challenges associated with the development of LLMs for\nASPR. We hope this survey can serve as an inspirational reference for the\nresearchers and promote the progress of ASPR for its actual implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly impacted human society,\ninfluencing various domains. Among them, academia is not simply a domain\naffected by LLMs, but it is also the pivotal force in the development of LLMs.\nIn academic publications, this phenomenon is represented during the\nincorporation of LLMs into the peer review mechanism for reviewing manuscripts.\nWe proposed the concept of automated scholarly paper review (ASPR) in our\nprevious paper. As the incorporation grows, it now enters the coexistence phase\nof ASPR and peer review, which is described in that paper. LLMs hold\ntransformative potential for the full-scale implementation of ASPR, but they\nalso pose new issues and challenges that need to be addressed. In this survey\npaper, we aim to provide a holistic view of ASPR in the era of LLMs. We begin\nwith a survey to find out which LLMs are used to conduct ASPR. Then, we review\nwhat ASPR-related technological bottlenecks have been solved with the\nincorporation of LLM technology. After that, we move on to explore new methods,\nnew datasets, new source code, and new online systems that come with LLMs for\nASPR. Furthermore, we summarize the performance and issues of LLMs in ASPR, and\ninvestigate the attitudes and reactions of publishers and academia to ASPR.\nLastly, we discuss the challenges associated with the development of LLMs for\nASPR. We hope this survey can serve as an inspirational reference for the\nresearchers and promote the progress of ASPR for its actual implementation."
                },
                "authors": [
                    {
                        "name": "Zhenzhen Zhuang"
                    },
                    {
                        "name": "Jiandong Chen"
                    },
                    {
                        "name": "Hongfeng Xu"
                    },
                    {
                        "name": "Yuwen Jiang"
                    },
                    {
                        "name": "Jialiang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jialiang Lin"
                },
                "author": "Jialiang Lin",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10321v1",
                "updated": "2025-01-17T17:51:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    17,
                    51,
                    22,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T17:51:22Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    17,
                    51,
                    22,
                    4,
                    17,
                    0
                ],
                "title": "Towards Human-Guided, Data-Centric LLM Co-Pilots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Human-Guided, Data-Centric LLM Co-Pilots"
                },
                "summary": "Machine learning (ML) has the potential to revolutionize healthcare, but its\nadoption is often hindered by the disconnect between the needs of domain\nexperts and translating these needs into robust and valid ML tools. Despite\nrecent advances in LLM-based co-pilots to democratize ML for non-technical\ndomain experts, these systems remain predominantly focused on model-centric\naspects while overlooking critical data-centric challenges. This limitation is\nproblematic in complex real-world settings where raw data often contains\ncomplex issues, such as missing values, label noise, and domain-specific\nnuances requiring tailored handling. To address this we introduce CliMB-DC, a\nhuman-guided, data-centric framework for LLM co-pilots that combines advanced\ndata-centric tools with LLM-driven reasoning to enable robust, context-aware\ndata processing. At its core, CliMB-DC introduces a novel, multi-agent\nreasoning system that combines a strategic coordinator for dynamic planning and\nadaptation with a specialized worker agent for precise execution. Domain\nexpertise is then systematically incorporated to guide the reasoning process\nusing a human-in-the-loop approach. To guide development, we formalize a\ntaxonomy of key data-centric challenges that co-pilots must address.\nThereafter, to address the dimensions of the taxonomy, we integrate\nstate-of-the-art data-centric tools into an extensible, open-source\narchitecture, facilitating the addition of new tools from the research\ncommunity. Empirically, using real-world healthcare datasets we demonstrate\nCliMB-DC's ability to transform uncurated datasets into ML-ready formats,\nsignificantly outperforming existing co-pilot baselines for handling\ndata-centric challenges. CliMB-DC promises to empower domain experts from\ndiverse domains -- healthcare, finance, social sciences and more -- to actively\nparticipate in driving real-world impact using ML.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) has the potential to revolutionize healthcare, but its\nadoption is often hindered by the disconnect between the needs of domain\nexperts and translating these needs into robust and valid ML tools. Despite\nrecent advances in LLM-based co-pilots to democratize ML for non-technical\ndomain experts, these systems remain predominantly focused on model-centric\naspects while overlooking critical data-centric challenges. This limitation is\nproblematic in complex real-world settings where raw data often contains\ncomplex issues, such as missing values, label noise, and domain-specific\nnuances requiring tailored handling. To address this we introduce CliMB-DC, a\nhuman-guided, data-centric framework for LLM co-pilots that combines advanced\ndata-centric tools with LLM-driven reasoning to enable robust, context-aware\ndata processing. At its core, CliMB-DC introduces a novel, multi-agent\nreasoning system that combines a strategic coordinator for dynamic planning and\nadaptation with a specialized worker agent for precise execution. Domain\nexpertise is then systematically incorporated to guide the reasoning process\nusing a human-in-the-loop approach. To guide development, we formalize a\ntaxonomy of key data-centric challenges that co-pilots must address.\nThereafter, to address the dimensions of the taxonomy, we integrate\nstate-of-the-art data-centric tools into an extensible, open-source\narchitecture, facilitating the addition of new tools from the research\ncommunity. Empirically, using real-world healthcare datasets we demonstrate\nCliMB-DC's ability to transform uncurated datasets into ML-ready formats,\nsignificantly outperforming existing co-pilot baselines for handling\ndata-centric challenges. CliMB-DC promises to empower domain experts from\ndiverse domains -- healthcare, finance, social sciences and more -- to actively\nparticipate in driving real-world impact using ML."
                },
                "authors": [
                    {
                        "name": "Evgeny Saveliev"
                    },
                    {
                        "name": "Jiashuo Liu"
                    },
                    {
                        "name": "Nabeel Seedat"
                    },
                    {
                        "name": "Anders Boyd"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "arxiv_comment": "Saveliev, Liu & Seedat contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03545v2",
                "updated": "2025-01-17T17:47:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    17,
                    47,
                    24,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-07T05:43:23Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    5,
                    43,
                    23,
                    1,
                    7,
                    0
                ],
                "title": "Beyond Factual Accuracy: Evaluating Coverage of Diverse Factual\n  Information in Long-form Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Factual Accuracy: Evaluating Coverage of Diverse Factual\n  Information in Long-form Text Generation"
                },
                "summary": "This paper presents ICAT, an evaluation framework for measuring coverage of\ndiverse factual information in long-form text generation. ICAT breaks down a\nlong output text into a list of atomic claims and not only verifies each claim\nthrough retrieval from a (reliable) knowledge source, but also computes the\nalignment between the atomic factual claims and various aspects expected to be\npresented in the output. We study three implementations of the ICAT framework,\neach with a different assumption on the availability of aspects and alignment\nmethod. By adopting data from the diversification task in the TREC Web Track\nand the ClueWeb corpus, we evaluate the ICAT framework. We demonstrate strong\ncorrelation with human judgments and provide comprehensive evaluation across\nmultiple state-of-the-art LLMs. Our framework further offers interpretable and\nfine-grained analysis of diversity and coverage. Its modular design allows for\neasy adaptation to different domains and datasets, making it a valuable tool\nfor evaluating the qualitative aspects of long-form responses produced by LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents ICAT, an evaluation framework for measuring coverage of\ndiverse factual information in long-form text generation. ICAT breaks down a\nlong output text into a list of atomic claims and not only verifies each claim\nthrough retrieval from a (reliable) knowledge source, but also computes the\nalignment between the atomic factual claims and various aspects expected to be\npresented in the output. We study three implementations of the ICAT framework,\neach with a different assumption on the availability of aspects and alignment\nmethod. By adopting data from the diversification task in the TREC Web Track\nand the ClueWeb corpus, we evaluate the ICAT framework. We demonstrate strong\ncorrelation with human judgments and provide comprehensive evaluation across\nmultiple state-of-the-art LLMs. Our framework further offers interpretable and\nfine-grained analysis of diversity and coverage. Its modular design allows for\neasy adaptation to different domains and datasets, making it a valuable tool\nfor evaluating the qualitative aspects of long-form responses produced by LLMs."
                },
                "authors": [
                    {
                        "name": "Chris Samarinas"
                    },
                    {
                        "name": "Alexander Krubner"
                    },
                    {
                        "name": "Alireza Salemi"
                    },
                    {
                        "name": "Youngwoo Kim"
                    },
                    {
                        "name": "Hamed Zamani"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Zamani"
                },
                "author": "Hamed Zamani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10316v1",
                "updated": "2025-01-17T17:40:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    17,
                    40,
                    12,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T17:40:12Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    17,
                    40,
                    12,
                    4,
                    17,
                    0
                ],
                "title": "Towards Preventing Overreliance on Task-Oriented Conversational AI\n  Through Accountability Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Preventing Overreliance on Task-Oriented Conversational AI\n  Through Accountability Modeling"
                },
                "summary": "Recent LLMs have enabled significant advancements for conversational agents.\nHowever, they are also well-known to hallucinate, i.e., they often produce\nresponses that seem plausible but are not factually correct. On the other hand,\nusers tend to over-rely on LLM-based AI agents; they accept the AI's suggestion\neven when it is wrong. Adding good friction, such as explanations or getting\nuser confirmations, has been proposed as a mitigation in AI-supported\ndecision-making systems. In this paper, we propose an accountability model for\nLLM-based task-oriented dialogue agents to address user overreliance via\nfriction turns in cases of model uncertainty and errors associated with\ndialogue state tracking (DST). The accountability model is an augmented LLM\nwith an additional accountability head, which functions as a binary classifier\nto predict the slots of the dialogue states. We perform our experiments with\nthree backbone LLMs (Llama, Mistral, Gemma) on two established task-oriented\ndatasets (MultiWOZ and Snips). Our empirical findings demonstrate that this\napproach not only enables reliable estimation of AI agent errors but also\nguides the LLM decoder in generating more accurate actions. We observe around\n3% absolute improvement in joint goal accuracy by incorporating accountability\nheads in modern LLMs for the MultiWOZ dataset. We also show that this method\nenables the agent to self-correct its actions, further boosting its performance\nby 3%. Finally, we discuss the application of accountability modeling to\nprevent user overreliance by introducing friction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent LLMs have enabled significant advancements for conversational agents.\nHowever, they are also well-known to hallucinate, i.e., they often produce\nresponses that seem plausible but are not factually correct. On the other hand,\nusers tend to over-rely on LLM-based AI agents; they accept the AI's suggestion\neven when it is wrong. Adding good friction, such as explanations or getting\nuser confirmations, has been proposed as a mitigation in AI-supported\ndecision-making systems. In this paper, we propose an accountability model for\nLLM-based task-oriented dialogue agents to address user overreliance via\nfriction turns in cases of model uncertainty and errors associated with\ndialogue state tracking (DST). The accountability model is an augmented LLM\nwith an additional accountability head, which functions as a binary classifier\nto predict the slots of the dialogue states. We perform our experiments with\nthree backbone LLMs (Llama, Mistral, Gemma) on two established task-oriented\ndatasets (MultiWOZ and Snips). Our empirical findings demonstrate that this\napproach not only enables reliable estimation of AI agent errors but also\nguides the LLM decoder in generating more accurate actions. We observe around\n3% absolute improvement in joint goal accuracy by incorporating accountability\nheads in modern LLMs for the MultiWOZ dataset. We also show that this method\nenables the agent to self-correct its actions, further boosting its performance\nby 3%. Finally, we discuss the application of accountability modeling to\nprevent user overreliance by introducing friction."
                },
                "authors": [
                    {
                        "name": "Suvodip Dey"
                    },
                    {
                        "name": "Yi-Jyun Sun"
                    },
                    {
                        "name": "Gokhan Tur"
                    },
                    {
                        "name": "Dilek Hakkani-Tur"
                    }
                ],
                "author_detail": {
                    "name": "Dilek Hakkani-Tur"
                },
                "author": "Dilek Hakkani-Tur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10313v1",
                "updated": "2025-01-17T17:35:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    17,
                    35,
                    14,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T17:35:14Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    17,
                    35,
                    14,
                    4,
                    17,
                    0
                ],
                "title": "Addressing Popularity Bias in Third-Party Library Recommendations Using\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing Popularity Bias in Third-Party Library Recommendations Using\n  LLMs"
                },
                "summary": "Recommender systems for software engineering (RSSE) play a crucial role in\nautomating development tasks by providing relevant suggestions according to the\ndeveloper's context. However, they suffer from the so-called popularity bias,\ni.e., the phenomenon of recommending popular items that might be irrelevant to\nthe current task. In particular, the long-tail effect can hamper the system's\nperformance in terms of accuracy, thus leading to false positives in the\nprovided recommendations. Foundation models are the most advanced generative\nAI-based models that achieve relevant results in several SE tasks.\n  This paper aims to investigate the capability of large language models (LLMs)\nto address the popularity bias in recommender systems of third-party libraries\n(TPLs). We conduct an ablation study experimenting with state-of-the-art\ntechniques to mitigate the popularity bias, including fine-tuning and\npopularity penalty mechanisms. Our findings reveal that the considered LLMs\ncannot address the popularity bias in TPL recommenders, even though fine-tuning\nand post-processing penalty mechanism contributes to increasing the overall\ndiversity of the provided recommendations. In addition, we discuss the\nlimitations of LLMs in this context and suggest potential improvements to\naddress the popularity bias in TPL recommenders, thus paving the way for\nadditional experiments in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems for software engineering (RSSE) play a crucial role in\nautomating development tasks by providing relevant suggestions according to the\ndeveloper's context. However, they suffer from the so-called popularity bias,\ni.e., the phenomenon of recommending popular items that might be irrelevant to\nthe current task. In particular, the long-tail effect can hamper the system's\nperformance in terms of accuracy, thus leading to false positives in the\nprovided recommendations. Foundation models are the most advanced generative\nAI-based models that achieve relevant results in several SE tasks.\n  This paper aims to investigate the capability of large language models (LLMs)\nto address the popularity bias in recommender systems of third-party libraries\n(TPLs). We conduct an ablation study experimenting with state-of-the-art\ntechniques to mitigate the popularity bias, including fine-tuning and\npopularity penalty mechanisms. Our findings reveal that the considered LLMs\ncannot address the popularity bias in TPL recommenders, even though fine-tuning\nand post-processing penalty mechanism contributes to increasing the overall\ndiversity of the provided recommendations. In addition, we discuss the\nlimitations of LLMs in this context and suggest potential improvements to\naddress the popularity bias in TPL recommenders, thus paving the way for\nadditional experiments in this direction."
                },
                "authors": [
                    {
                        "name": "Claudio Di Sipio"
                    },
                    {
                        "name": "Juri Di Rocco"
                    },
                    {
                        "name": "Davide Di Ruscio"
                    },
                    {
                        "name": "Vladyslav Bulhakov"
                    }
                ],
                "author_detail": {
                    "name": "Vladyslav Bulhakov"
                },
                "author": "Vladyslav Bulhakov",
                "arxiv_comment": "Accepted at the 1st International Workshop on Fairness in Software\n  Systems, co-located with SANER2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09594v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09594v2",
                "updated": "2025-01-17T16:44:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    16,
                    44,
                    35,
                    4,
                    17,
                    0
                ],
                "published": "2024-08-18T20:59:59Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    20,
                    59,
                    59,
                    6,
                    231,
                    0
                ],
                "title": "Moonshine: Distilling Game Content Generators into Steerable Generative\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moonshine: Distilling Game Content Generators into Steerable Generative\n  Models"
                },
                "summary": "Procedural Content Generation via Machine Learning (PCGML) has enhanced game\ncontent creation, yet challenges in controllability and limited training data\npersist. This study addresses these issues by distilling a constructive PCG\nalgorithm into a controllable PCGML model. We first generate a large amount of\ncontent with a constructive algorithm and label it using a Large Language Model\n(LLM). We use these synthetic labels to condition two PCGML models for\ncontent-specific generation, a diffusion model and the five-dollar model. This\nneural network distillation process ensures that the generation aligns with the\noriginal algorithm while introducing controllability through plain text. We\ndefine this text-conditioned PCGML as a Text-to-game-Map (T2M) task, offering\nan alternative to prevalent text-to-image multi-modal tasks. We compare our\ndistilled models with the baseline constructive algorithm. Our analysis of the\nvariety, accuracy, and quality of our generation demonstrates the efficacy of\ndistilling constructive methods into controllable text-conditioned PCGML\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Procedural Content Generation via Machine Learning (PCGML) has enhanced game\ncontent creation, yet challenges in controllability and limited training data\npersist. This study addresses these issues by distilling a constructive PCG\nalgorithm into a controllable PCGML model. We first generate a large amount of\ncontent with a constructive algorithm and label it using a Large Language Model\n(LLM). We use these synthetic labels to condition two PCGML models for\ncontent-specific generation, a diffusion model and the five-dollar model. This\nneural network distillation process ensures that the generation aligns with the\noriginal algorithm while introducing controllability through plain text. We\ndefine this text-conditioned PCGML as a Text-to-game-Map (T2M) task, offering\nan alternative to prevalent text-to-image multi-modal tasks. We compare our\ndistilled models with the baseline constructive algorithm. Our analysis of the\nvariety, accuracy, and quality of our generation demonstrates the efficacy of\ndistilling constructive methods into controllable text-conditioned PCGML\nmodels."
                },
                "authors": [
                    {
                        "name": "Yuhe Nie"
                    },
                    {
                        "name": "Michael Middleton"
                    },
                    {
                        "name": "Tim Merino"
                    },
                    {
                        "name": "Nidhushan Kanagaraja"
                    },
                    {
                        "name": "Ashutosh Kumar"
                    },
                    {
                        "name": "Zhan Zhuang"
                    },
                    {
                        "name": "Julian Togelius"
                    }
                ],
                "author_detail": {
                    "name": "Julian Togelius"
                },
                "author": "Julian Togelius",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09594v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10289v1",
                "updated": "2025-01-17T16:34:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    16,
                    34,
                    38,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T16:34:38Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    16,
                    34,
                    38,
                    4,
                    17,
                    0
                ],
                "title": "Cheap Subsampling bootstrap confidence intervals for fast and robust\n  inference in biostatistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cheap Subsampling bootstrap confidence intervals for fast and robust\n  inference in biostatistics"
                },
                "summary": "Bootstrapping is often applied to get confidence limits for semiparametric\ninference of a target parameter in the presence of nuisance parameters.\nBootstrapping with replacement can be computationally expensive and problematic\nwhen cross-validation is used in the estimation algorithm due to duplicate\nobservations in the bootstrap samples. We provide a valid, fast,\neasy-to-implement subsampling bootstrap method for constructing confidence\nintervals for asymptotically linear estimators and discuss its application to\nsemiparametric causal inference. Our method, inspired by the Cheap Bootstrap\n(Lam, 2022), leverages the quantiles of a t-distribution and has the desired\ncoverage with few bootstrap replications. We show that the method is\nasymptotically valid if the subsample size is chosen appropriately as a\nfunction of the sample size. We illustrate our method with data from the LEADER\ntrial (Marso et al., 2016), obtaining confidence intervals for a longitudinal\ntargeted minimum loss-based estimator (van der Laan and Gruber, 2012). Through\na series of empirical experiments, we also explore the impact of subsample\nsize, sample size, and the number of bootstrap repetitions on the performance\nof the confidence interval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bootstrapping is often applied to get confidence limits for semiparametric\ninference of a target parameter in the presence of nuisance parameters.\nBootstrapping with replacement can be computationally expensive and problematic\nwhen cross-validation is used in the estimation algorithm due to duplicate\nobservations in the bootstrap samples. We provide a valid, fast,\neasy-to-implement subsampling bootstrap method for constructing confidence\nintervals for asymptotically linear estimators and discuss its application to\nsemiparametric causal inference. Our method, inspired by the Cheap Bootstrap\n(Lam, 2022), leverages the quantiles of a t-distribution and has the desired\ncoverage with few bootstrap replications. We show that the method is\nasymptotically valid if the subsample size is chosen appropriately as a\nfunction of the sample size. We illustrate our method with data from the LEADER\ntrial (Marso et al., 2016), obtaining confidence intervals for a longitudinal\ntargeted minimum loss-based estimator (van der Laan and Gruber, 2012). Through\na series of empirical experiments, we also explore the impact of subsample\nsize, sample size, and the number of bootstrap repetitions on the performance\nof the confidence interval."
                },
                "authors": [
                    {
                        "name": "Johan Sebastian Ohlendorff"
                    },
                    {
                        "name": "Anders Munch"
                    },
                    {
                        "name": "Kathrine Kold Srensen"
                    },
                    {
                        "name": "Thomas Alexander Gerds"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Alexander Gerds"
                },
                "author": "Thomas Alexander Gerds",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10282v1",
                "updated": "2025-01-17T16:21:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    16,
                    21,
                    18,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T16:21:18Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    16,
                    21,
                    18,
                    4,
                    17,
                    0
                ],
                "title": "Computational Protein Science in the Era of Large Language Models (LLMs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Protein Science in the Era of Large Language Models (LLMs)"
                },
                "summary": "Considering the significance of proteins, computational protein science has\nalways been a critical scientific field, dedicated to revealing knowledge and\ndeveloping applications within the protein sequence-structure-function\nparadigm. In the last few decades, Artificial Intelligence (AI) has made\nsignificant impacts in computational protein science, leading to notable\nsuccesses in specific protein modeling tasks. However, those previous AI models\nstill meet limitations, such as the difficulty in comprehending the semantics\nof protein sequences, and the inability to generalize across a wide range of\nprotein modeling tasks. Recently, LLMs have emerged as a milestone in AI due to\ntheir unprecedented language processing & generalization capability. They can\npromote comprehensive progress in fields rather than solving individual tasks.\nAs a result, researchers have actively introduced LLM techniques in\ncomputational protein science, developing protein Language Models (pLMs) that\nskillfully grasp the foundational knowledge of proteins and can be effectively\ngeneralized to solve a diversity of sequence-structure-function reasoning\nproblems. While witnessing prosperous developments, it's necessary to present a\nsystematic overview of computational protein science empowered by LLM\ntechniques. First, we summarize existing pLMs into categories based on their\nmastered protein knowledge, i.e., underlying sequence patterns, explicit\nstructural and functional information, and external scientific languages.\nSecond, we introduce the utilization and adaptation of pLMs, highlighting their\nremarkable achievements in promoting protein structure prediction, protein\nfunction prediction, and protein design studies. Then, we describe the\npractical application of pLMs in antibody design, enzyme design, and drug\ndiscovery. Finally, we specifically discuss the promising future directions in\nthis fast-growing field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Considering the significance of proteins, computational protein science has\nalways been a critical scientific field, dedicated to revealing knowledge and\ndeveloping applications within the protein sequence-structure-function\nparadigm. In the last few decades, Artificial Intelligence (AI) has made\nsignificant impacts in computational protein science, leading to notable\nsuccesses in specific protein modeling tasks. However, those previous AI models\nstill meet limitations, such as the difficulty in comprehending the semantics\nof protein sequences, and the inability to generalize across a wide range of\nprotein modeling tasks. Recently, LLMs have emerged as a milestone in AI due to\ntheir unprecedented language processing & generalization capability. They can\npromote comprehensive progress in fields rather than solving individual tasks.\nAs a result, researchers have actively introduced LLM techniques in\ncomputational protein science, developing protein Language Models (pLMs) that\nskillfully grasp the foundational knowledge of proteins and can be effectively\ngeneralized to solve a diversity of sequence-structure-function reasoning\nproblems. While witnessing prosperous developments, it's necessary to present a\nsystematic overview of computational protein science empowered by LLM\ntechniques. First, we summarize existing pLMs into categories based on their\nmastered protein knowledge, i.e., underlying sequence patterns, explicit\nstructural and functional information, and external scientific languages.\nSecond, we introduce the utilization and adaptation of pLMs, highlighting their\nremarkable achievements in promoting protein structure prediction, protein\nfunction prediction, and protein design studies. Then, we describe the\npractical application of pLMs in antibody design, enzyme design, and drug\ndiscovery. Finally, we specifically discuss the promising future directions in\nthis fast-growing field."
                },
                "authors": [
                    {
                        "name": "Wenqi Fan"
                    },
                    {
                        "name": "Yi Zhou"
                    },
                    {
                        "name": "Shijie Wang"
                    },
                    {
                        "name": "Yuyao Yan"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Qian Zhao"
                    },
                    {
                        "name": "Le Song"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14904v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14904v2",
                "updated": "2025-01-17T15:59:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    15,
                    59,
                    34,
                    4,
                    17,
                    0
                ],
                "published": "2024-06-21T06:51:13Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    6,
                    51,
                    13,
                    4,
                    173,
                    0
                ],
                "title": "Enhancing reliability in prediction intervals using point forecasters:\n  Heteroscedastic Quantile Regression and Width-Adaptive Conformal Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing reliability in prediction intervals using point forecasters:\n  Heteroscedastic Quantile Regression and Width-Adaptive Conformal Inference"
                },
                "summary": "Constructing prediction intervals for time series forecasting is challenging,\nparticularly when practitioners rely solely on point forecasts. While previous\nresearch has focused on creating increasingly efficient intervals, we argue\nthat standard measures alone are inadequate. Beyond efficiency, prediction\nintervals must adapt their width based on the difficulty of the prediction\nwhile preserving coverage regardless of complexity. To address these issues, we\npropose combining Heteroscedastic Quantile Regression (HQR) with Width-Adaptive\nConformal Inference (WACI). This integrated procedure guarantees theoretical\ncoverage and enables interval widths to vary with predictive uncertainty. We\nassess its performance using both a synthetic example and a real world\nElectricity Price Forecasting scenario. Our results show that this combined\napproach meets or surpasses typical benchmarks for validity and efficiency,\nwhile also fulfilling important yet often overlooked practical requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing prediction intervals for time series forecasting is challenging,\nparticularly when practitioners rely solely on point forecasts. While previous\nresearch has focused on creating increasingly efficient intervals, we argue\nthat standard measures alone are inadequate. Beyond efficiency, prediction\nintervals must adapt their width based on the difficulty of the prediction\nwhile preserving coverage regardless of complexity. To address these issues, we\npropose combining Heteroscedastic Quantile Regression (HQR) with Width-Adaptive\nConformal Inference (WACI). This integrated procedure guarantees theoretical\ncoverage and enables interval widths to vary with predictive uncertainty. We\nassess its performance using both a synthetic example and a real world\nElectricity Price Forecasting scenario. Our results show that this combined\napproach meets or surpasses typical benchmarks for validity and efficiency,\nwhile also fulfilling important yet often overlooked practical requirements."
                },
                "authors": [
                    {
                        "name": "Carlos Sebastin"
                    },
                    {
                        "name": "Carlos E. Gonzlez-Guilln"
                    },
                    {
                        "name": "Jess Juan"
                    }
                ],
                "author_detail": {
                    "name": "Jess Juan"
                },
                "author": "Jess Juan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14904v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14904v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10272v1",
                "updated": "2025-01-17T15:59:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    15,
                    59,
                    21,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T15:59:21Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    15,
                    59,
                    21,
                    4,
                    17,
                    0
                ],
                "title": "A data-driven approach for extracting tidal information from neutron\n  star binary mergers observed with the Einstein Telescope",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A data-driven approach for extracting tidal information from neutron\n  star binary mergers observed with the Einstein Telescope"
                },
                "summary": "The recent breakthroughs regarding the detection of compact binary mergers\nvia gravitational waves opened up a new window to the Universe.\nGravitational-wave models have been essential to this success since they are\nnecessary to infer the properties of the compact binary system from the\nobservational data. Next-generation detectors, such as the Einstein Telescope,\nwill allow for more observations of binary neutron star mergers with higher\nprecision, making accurate waveform models crucial in describing these systems.\nIn this article, we propose a novel approach for constructing phenomenological\nwaveform models informed by observational data. Using mock data representing a\none-year operation of the Einstein Telescope as our baseline, we demonstrate\nhow the results improve as more events are included in the calibration. This\nmethod offers a new and complementary approach for developing sophisticated\ngravitational-wave models compared to classical techniques that employ\nanalytical computations and numerical-relativity simulations. Improved waveform\nmodels will then yield more accurate parameter estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent breakthroughs regarding the detection of compact binary mergers\nvia gravitational waves opened up a new window to the Universe.\nGravitational-wave models have been essential to this success since they are\nnecessary to infer the properties of the compact binary system from the\nobservational data. Next-generation detectors, such as the Einstein Telescope,\nwill allow for more observations of binary neutron star mergers with higher\nprecision, making accurate waveform models crucial in describing these systems.\nIn this article, we propose a novel approach for constructing phenomenological\nwaveform models informed by observational data. Using mock data representing a\none-year operation of the Einstein Telescope as our baseline, we demonstrate\nhow the results improve as more events are included in the calibration. This\nmethod offers a new and complementary approach for developing sophisticated\ngravitational-wave models compared to classical techniques that employ\nanalytical computations and numerical-relativity simulations. Improved waveform\nmodels will then yield more accurate parameter estimation."
                },
                "authors": [
                    {
                        "name": "Adrian Abac"
                    },
                    {
                        "name": "Anna Puecher"
                    },
                    {
                        "name": "Jonathan Gair"
                    },
                    {
                        "name": "Tim Dietrich"
                    }
                ],
                "author_detail": {
                    "name": "Tim Dietrich"
                },
                "author": "Tim Dietrich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10263v1",
                "updated": "2025-01-17T15:44:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    15,
                    44,
                    57,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T15:44:57Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    15,
                    44,
                    57,
                    4,
                    17,
                    0
                ],
                "title": "Prior distributions for structured semi-orthogonal matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior distributions for structured semi-orthogonal matrices"
                },
                "summary": "Statistical models for multivariate data often include a semi-orthogonal\nmatrix parameter. In many applications, there is reason to expect that the\nsemi-orthogonal matrix parameter satisfies a structural assumption such as\nsparsity or smoothness. From a Bayesian perspective, these structural\nassumptions should be incorporated into an analysis through the prior\ndistribution. In this work, we introduce a general approach to constructing\nprior distributions for structured semi-orthogonal matrices that leads to\ntractable posterior inference via parameter-expanded Markov chain Monte Carlo.\nWe draw upon recent results from random matrix theory to establish a\ntheoretical basis for the proposed approach. We then introduce specific prior\ndistributions for incorporating sparsity or smoothness and illustrate their use\nthrough applications to biological and oceanographic data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical models for multivariate data often include a semi-orthogonal\nmatrix parameter. In many applications, there is reason to expect that the\nsemi-orthogonal matrix parameter satisfies a structural assumption such as\nsparsity or smoothness. From a Bayesian perspective, these structural\nassumptions should be incorporated into an analysis through the prior\ndistribution. In this work, we introduce a general approach to constructing\nprior distributions for structured semi-orthogonal matrices that leads to\ntractable posterior inference via parameter-expanded Markov chain Monte Carlo.\nWe draw upon recent results from random matrix theory to establish a\ntheoretical basis for the proposed approach. We then introduce specific prior\ndistributions for incorporating sparsity or smoothness and illustrate their use\nthrough applications to biological and oceanographic data."
                },
                "authors": [
                    {
                        "name": "Michael Jauch"
                    },
                    {
                        "name": "Marie-Christine Dker"
                    },
                    {
                        "name": "Peter Hoff"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hoff"
                },
                "author": "Peter Hoff",
                "arxiv_comment": "31 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09686v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09686v2",
                "updated": "2025-01-17T15:24:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    15,
                    24,
                    53,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-16T17:37:58Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    17,
                    37,
                    58,
                    3,
                    16,
                    0
                ],
                "title": "Towards Large Reasoning Models: A Survey on Scaling LLM Reasoning\n  Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Large Reasoning Models: A Survey on Scaling LLM Reasoning\n  Capabilities"
                },
                "summary": "Language has long been conceived as an essential tool for human reasoning.\nThe breakthrough of Large Language Models (LLMs) has sparked significant\nresearch interest in leveraging these models to tackle complex reasoning tasks.\nResearchers have moved beyond simple autoregressive token generation by\nintroducing the concept of \"thought\" -- a sequence of tokens representing\nintermediate steps in the reasoning process. This innovative paradigm enables\nLLMs' to mimic complex human reasoning processes, such as tree search and\nreflective thinking. Recently, an emerging trend of learning to reason has\napplied reinforcement learning (RL) to train LLMs to master reasoning\nprocesses. This approach enables the automatic generation of high-quality\nreasoning trajectories through trial-and-error search algorithms, significantly\nexpanding LLMs' reasoning capacity by providing substantially more training\ndata. Furthermore, recent studies demonstrate that encouraging LLMs to \"think\"\nwith more tokens during test-time inference can further significantly boost\nreasoning accuracy. Therefore, the train-time and test-time scaling combined to\nshow a new research frontier -- a path toward Large Reasoning Model. The\nintroduction of OpenAI's o1 series marks a significant milestone in this\nresearch direction. In this survey, we present a comprehensive review of recent\nprogress in LLM reasoning. We begin by introducing the foundational background\nof LLMs and then explore the key technical components driving the development\nof large reasoning models, with a focus on automated data construction,\nlearning-to-reason techniques, and test-time scaling. We also analyze popular\nopen-source projects at building large reasoning models, and conclude with open\nchallenges and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language has long been conceived as an essential tool for human reasoning.\nThe breakthrough of Large Language Models (LLMs) has sparked significant\nresearch interest in leveraging these models to tackle complex reasoning tasks.\nResearchers have moved beyond simple autoregressive token generation by\nintroducing the concept of \"thought\" -- a sequence of tokens representing\nintermediate steps in the reasoning process. This innovative paradigm enables\nLLMs' to mimic complex human reasoning processes, such as tree search and\nreflective thinking. Recently, an emerging trend of learning to reason has\napplied reinforcement learning (RL) to train LLMs to master reasoning\nprocesses. This approach enables the automatic generation of high-quality\nreasoning trajectories through trial-and-error search algorithms, significantly\nexpanding LLMs' reasoning capacity by providing substantially more training\ndata. Furthermore, recent studies demonstrate that encouraging LLMs to \"think\"\nwith more tokens during test-time inference can further significantly boost\nreasoning accuracy. Therefore, the train-time and test-time scaling combined to\nshow a new research frontier -- a path toward Large Reasoning Model. The\nintroduction of OpenAI's o1 series marks a significant milestone in this\nresearch direction. In this survey, we present a comprehensive review of recent\nprogress in LLM reasoning. We begin by introducing the foundational background\nof LLMs and then explore the key technical components driving the development\nof large reasoning models, with a focus on automated data construction,\nlearning-to-reason techniques, and test-time scaling. We also analyze popular\nopen-source projects at building large reasoning models, and conclude with open\nchallenges and future research directions."
                },
                "authors": [
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Qianyue Hao"
                    },
                    {
                        "name": "Zefang Zong"
                    },
                    {
                        "name": "Jingwei Wang"
                    },
                    {
                        "name": "Yunke Zhang"
                    },
                    {
                        "name": "Jingyi Wang"
                    },
                    {
                        "name": "Xiaochong Lan"
                    },
                    {
                        "name": "Jiahui Gong"
                    },
                    {
                        "name": "Tianjian Ouyang"
                    },
                    {
                        "name": "Fanjin Meng"
                    },
                    {
                        "name": "Chenyang Shao"
                    },
                    {
                        "name": "Yuwei Yan"
                    },
                    {
                        "name": "Qinglong Yang"
                    },
                    {
                        "name": "Yiwen Song"
                    },
                    {
                        "name": "Sijian Ren"
                    },
                    {
                        "name": "Xinyuan Hu"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "36 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09686v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09686v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09274v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09274v2",
                "updated": "2025-01-17T15:22:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    15,
                    22,
                    0,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-16T03:44:16Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    3,
                    44,
                    16,
                    3,
                    16,
                    0
                ],
                "title": "Large Language Model is Secretly a Protein Sequence Optimizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model is Secretly a Protein Sequence Optimizer"
                },
                "summary": "We consider the protein sequence engineering problem, which aims to find\nprotein sequences with high fitness levels, starting from a given wild-type\nsequence. Directed evolution has been a dominating paradigm in this field which\nhas an iterative process to generate variants and select via experimental\nfeedback. We demonstrate large language models (LLMs), despite being trained on\nmassive texts, are secretly protein sequence optimizers. With a directed\nevolutionary method, LLM can perform protein engineering through Pareto and\nexperiment-budget constrained optimization, demonstrating success on both\nsynthetic and experimental fitness landscapes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the protein sequence engineering problem, which aims to find\nprotein sequences with high fitness levels, starting from a given wild-type\nsequence. Directed evolution has been a dominating paradigm in this field which\nhas an iterative process to generate variants and select via experimental\nfeedback. We demonstrate large language models (LLMs), despite being trained on\nmassive texts, are secretly protein sequence optimizers. With a directed\nevolutionary method, LLM can perform protein engineering through Pareto and\nexperiment-budget constrained optimization, demonstrating success on both\nsynthetic and experimental fitness landscapes."
                },
                "authors": [
                    {
                        "name": "Yinkai Wang"
                    },
                    {
                        "name": "Jiaxing He"
                    },
                    {
                        "name": "Yuanqi Du"
                    },
                    {
                        "name": "Xiaohui Chen"
                    },
                    {
                        "name": "Jianan Canal Li"
                    },
                    {
                        "name": "Li-Ping Liu"
                    },
                    {
                        "name": "Xiaolin Xu"
                    },
                    {
                        "name": "Soha Hassoun"
                    }
                ],
                "author_detail": {
                    "name": "Soha Hassoun"
                },
                "author": "Soha Hassoun",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09274v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09274v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10245v1",
                "updated": "2025-01-17T15:14:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    15,
                    14,
                    58,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T15:14:58Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    15,
                    14,
                    58,
                    4,
                    17,
                    0
                ],
                "title": "Over-the-Air Multi-Sensor Inference with Neural Networks Using\n  Memristor-Based Analog Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over-the-Air Multi-Sensor Inference with Neural Networks Using\n  Memristor-Based Analog Computing"
                },
                "summary": "Deep neural networks provide reliable solutions for many classification and\nregression tasks; however, their application in real-time wireless systems with\nsimple sensor networks is limited due to high energy consumption and\nsignificant bandwidth needs. This study proposes a multi-sensor wireless\ninference system with memristor-based analog computing. Given the sensors'\nlimited computational capabilities, the features from the network's front end\nare transmitted to a central device where an $L_p$-norm inspired approximation\nof the maximum operation is employed to achieve transformation-invariant\nfeatures, enabling efficient over-the-air transmission. We also introduce a\ntrainable over-the-air sensor fusion method based on $L_p$-norm inspired\ncombining function that customizes sensor fusion to match the network and\nsensor distribution characteristics, enhancing adaptability. To address the\nenergy constraints of sensors, we utilize memristors, known for their\nenergy-efficient in-memory computing, enabling analog-domain computations that\nreduce energy use and computational overhead in edge computing. This dual\napproach of memristors and $L_p$-norm inspired sensor fusion fosters\nenergy-efficient computational and transmission paradigms and serves as a\npractical energy-efficient solution with minimal performance loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks provide reliable solutions for many classification and\nregression tasks; however, their application in real-time wireless systems with\nsimple sensor networks is limited due to high energy consumption and\nsignificant bandwidth needs. This study proposes a multi-sensor wireless\ninference system with memristor-based analog computing. Given the sensors'\nlimited computational capabilities, the features from the network's front end\nare transmitted to a central device where an $L_p$-norm inspired approximation\nof the maximum operation is employed to achieve transformation-invariant\nfeatures, enabling efficient over-the-air transmission. We also introduce a\ntrainable over-the-air sensor fusion method based on $L_p$-norm inspired\ncombining function that customizes sensor fusion to match the network and\nsensor distribution characteristics, enhancing adaptability. To address the\nenergy constraints of sensors, we utilize memristors, known for their\nenergy-efficient in-memory computing, enabling analog-domain computations that\nreduce energy use and computational overhead in edge computing. This dual\napproach of memristors and $L_p$-norm inspired sensor fusion fosters\nenergy-efficient computational and transmission paradigms and serves as a\npractical energy-efficient solution with minimal performance loss."
                },
                "authors": [
                    {
                        "name": "Busra Tegin"
                    },
                    {
                        "name": "Muhammad Atif Ali"
                    },
                    {
                        "name": "Tolga M Duman"
                    }
                ],
                "author_detail": {
                    "name": "Tolga M Duman"
                },
                "author": "Tolga M Duman",
                "arxiv_comment": "34 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10244v1",
                "updated": "2025-01-17T15:14:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    15,
                    14,
                    3,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T15:14:03Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    15,
                    14,
                    3,
                    4,
                    17,
                    0
                ],
                "title": "DeepSSM: an emulator of gravitational wave spectra from sound waves\n  during cosmological first-order phase transitions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSSM: an emulator of gravitational wave spectra from sound waves\n  during cosmological first-order phase transitions"
                },
                "summary": "We present DeepSSM, an open-source code powered by neural networks (NNs) to\nemulate gravitational wave (GW) spectra produced by sound waves during\ncosmological first-order phase transitions in the radiation-dominated era. The\ntraining data is obtained from an enhanced version of the Sound Shell Model\n(SSM), which accounts for the effects of cosmic expansion and yields more\naccurate spectra in the infrared regime. The emulator enables instantaneous\npredictions of GW spectra given the phase transition parameters, while\nachieving agreement with the enhanced SSM model within 10\\% accuracy in the\nworst-case scenarios. The emulator is highly computationally efficient and\nfully differentiable, making it particularly suitable for direct Bayesian\ninference on phase transition parameters without relying on empirical\ntemplates, such as broken power-law models. We demonstrate this capability by\nsuccessfully reconstructing phase transition parameters and their degeneracies\nfrom mock LISA observations using a Hamiltonian Monte Carlo sampler. The code\nis available at: https://github.com/ctian282/DeepSSM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DeepSSM, an open-source code powered by neural networks (NNs) to\nemulate gravitational wave (GW) spectra produced by sound waves during\ncosmological first-order phase transitions in the radiation-dominated era. The\ntraining data is obtained from an enhanced version of the Sound Shell Model\n(SSM), which accounts for the effects of cosmic expansion and yields more\naccurate spectra in the infrared regime. The emulator enables instantaneous\npredictions of GW spectra given the phase transition parameters, while\nachieving agreement with the enhanced SSM model within 10\\% accuracy in the\nworst-case scenarios. The emulator is highly computationally efficient and\nfully differentiable, making it particularly suitable for direct Bayesian\ninference on phase transition parameters without relying on empirical\ntemplates, such as broken power-law models. We demonstrate this capability by\nsuccessfully reconstructing phase transition parameters and their degeneracies\nfrom mock LISA observations using a Hamiltonian Monte Carlo sampler. The code\nis available at: https://github.com/ctian282/DeepSSM."
                },
                "authors": [
                    {
                        "name": "Chi Tian"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Csaba Balzs"
                    }
                ],
                "author_detail": {
                    "name": "Csaba Balzs"
                },
                "author": "Csaba Balzs",
                "arxiv_comment": "20 pages, 5 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10232v1",
                "updated": "2025-01-17T14:54:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    14,
                    54,
                    49,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T14:54:49Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    14,
                    54,
                    49,
                    4,
                    17,
                    0
                ],
                "title": "Underestimation of systolic pressure in cuff-based blood pressure\n  measurement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Underestimation of systolic pressure in cuff-based blood pressure\n  measurement"
                },
                "summary": "High blood pressure (hypertension) is the number one risk factor for\npremature death. Hypertension is asymptomatic, so blood pressure must be\nregularly monitored to diagnose it. In auscultatory blood pressure measurement,\na patient's systolic (maximum) and diastolic (minimum) blood pressure are\ninferred from the pressure in an inflatable cuff wrapped around the arm. This\ntechnique is the gold standard against which all other non-invasive devices are\nvalidated. However, auscultatory measurements systematically underestimate\nsystolic blood pressure and overestimate diastolic blood pressure.\nOverestimation is attributed to the increased cuff pressure needed to occlude\nthe artery because of the surrounding tissue and arterial stiffness. In\ncontrast, the cause of systolic underestimation, which leads to potentially a\nthird of systolic hypertension cases being missed, has remained unclear. When\nthe cuff is inflated beyond the systolic blood pressure, the blood flow to the\nvessels downstream of the cuff is cut off. The pressure in these downstream\nvessels drops to a low plateau. We have developed a novel experimental rig that\nshows that the low downstream pressure is the key cause of the underestimation\nof systolic blood pressure. The lower the downstream pressure, the greater the\nunderestimation. Our results yield a simple physical model for the\nunderestimation of systolic pressure in our rig and in the human body.\nUnderstanding the physics behind the underestimation of systolic blood pressure\npaves the way for developing strategies to mitigate this error.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High blood pressure (hypertension) is the number one risk factor for\npremature death. Hypertension is asymptomatic, so blood pressure must be\nregularly monitored to diagnose it. In auscultatory blood pressure measurement,\na patient's systolic (maximum) and diastolic (minimum) blood pressure are\ninferred from the pressure in an inflatable cuff wrapped around the arm. This\ntechnique is the gold standard against which all other non-invasive devices are\nvalidated. However, auscultatory measurements systematically underestimate\nsystolic blood pressure and overestimate diastolic blood pressure.\nOverestimation is attributed to the increased cuff pressure needed to occlude\nthe artery because of the surrounding tissue and arterial stiffness. In\ncontrast, the cause of systolic underestimation, which leads to potentially a\nthird of systolic hypertension cases being missed, has remained unclear. When\nthe cuff is inflated beyond the systolic blood pressure, the blood flow to the\nvessels downstream of the cuff is cut off. The pressure in these downstream\nvessels drops to a low plateau. We have developed a novel experimental rig that\nshows that the low downstream pressure is the key cause of the underestimation\nof systolic blood pressure. The lower the downstream pressure, the greater the\nunderestimation. Our results yield a simple physical model for the\nunderestimation of systolic pressure in our rig and in the human body.\nUnderstanding the physics behind the underestimation of systolic blood pressure\npaves the way for developing strategies to mitigate this error."
                },
                "authors": [
                    {
                        "name": "Kate Bassil"
                    },
                    {
                        "name": "Anurag Agarwal"
                    }
                ],
                "author_detail": {
                    "name": "Anurag Agarwal"
                },
                "author": "Anurag Agarwal",
                "arxiv_comment": "15 pages (of which 9 are main text and 6 are supplementary material).\n  12 figures (of which 7 are in the main text and 5 are supplementary)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10229v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10229v1",
                "updated": "2025-01-17T14:51:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    14,
                    51,
                    3,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T14:51:03Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    14,
                    51,
                    3,
                    4,
                    17,
                    0
                ],
                "title": "Amortized Bayesian Mixture Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amortized Bayesian Mixture Models"
                },
                "summary": "Finite mixtures are a broad class of models useful in scenarios where\nobserved data is generated by multiple distinct processes but without explicit\ninformation about the responsible process for each data point. Estimating\nBayesian mixture models is computationally challenging due to issues such as\nhigh-dimensional posterior inference and label switching. Furthermore,\ntraditional methods such as MCMC are applicable only if the likelihoods for\neach mixture component are analytically tractable.\n  Amortized Bayesian Inference (ABI) is a simulation-based framework for\nestimating Bayesian models using generative neural networks. This allows the\nfitting of models without explicit likelihoods, and provides fast inference.\nABI is therefore an attractive framework for estimating mixture models. This\npaper introduces a novel extension of ABI tailored to mixture models. We\nfactorize the posterior into a distribution of the parameters and a\ndistribution of (categorical) mixture indicators, which allows us to use a\ncombination of generative neural networks for parameter inference, and\nclassification networks for mixture membership identification. The proposed\nframework accommodates both independent and dependent mixture models, enabling\nfiltering and smoothing. We validate and demonstrate our approach through\nsynthetic and real-world datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finite mixtures are a broad class of models useful in scenarios where\nobserved data is generated by multiple distinct processes but without explicit\ninformation about the responsible process for each data point. Estimating\nBayesian mixture models is computationally challenging due to issues such as\nhigh-dimensional posterior inference and label switching. Furthermore,\ntraditional methods such as MCMC are applicable only if the likelihoods for\neach mixture component are analytically tractable.\n  Amortized Bayesian Inference (ABI) is a simulation-based framework for\nestimating Bayesian models using generative neural networks. This allows the\nfitting of models without explicit likelihoods, and provides fast inference.\nABI is therefore an attractive framework for estimating mixture models. This\npaper introduces a novel extension of ABI tailored to mixture models. We\nfactorize the posterior into a distribution of the parameters and a\ndistribution of (categorical) mixture indicators, which allows us to use a\ncombination of generative neural networks for parameter inference, and\nclassification networks for mixture membership identification. The proposed\nframework accommodates both independent and dependent mixture models, enabling\nfiltering and smoothing. We validate and demonstrate our approach through\nsynthetic and real-world datasets."
                },
                "authors": [
                    {
                        "name": "imon Kucharsk"
                    },
                    {
                        "name": "Paul Christian Brkner"
                    }
                ],
                "author_detail": {
                    "name": "Paul Christian Brkner"
                },
                "author": "Paul Christian Brkner",
                "arxiv_comment": "34 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10229v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10229v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17217v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17217v2",
                "updated": "2025-01-17T14:42:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    14,
                    42,
                    41,
                    4,
                    17,
                    0
                ],
                "published": "2024-08-30T11:48:51Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    11,
                    48,
                    51,
                    4,
                    243,
                    0
                ],
                "title": "The FLAMINGO Project: An assessment of the systematic errors in the\n  predictions of models for galaxy cluster counts used to infer cosmological\n  parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The FLAMINGO Project: An assessment of the systematic errors in the\n  predictions of models for galaxy cluster counts used to infer cosmological\n  parameters"
                },
                "summary": "Galaxy cluster counts have historically been important for the measurement of\ncosmological parameters and upcoming surveys will greatly reduce the\nstatistical errors. To exploit the potential of current and future cluster\nsurveys, theoretical uncertainties on the predicted abundance must be smaller\nthan the statistical errors. Models used to predict cluster counts typically\ncombine a model for the dark matter only (DMO) halo mass function (HMF) with an\nobservable - mass relation that is assumed to be a power-law with lognormal\nscatter. We use the FLAMINGO suite of cosmological hydrodynamical simulations\nto quantify the biases in the cluster counts and cosmological parameters\nresulting from the different ingredients of conventional models. For the\nobservable mass proxy we focus on the Compton-Y parameter quantifying the\nthermal Sunyaev-Zel'dovich effect, which is expected to result in cluster\nsamples that are relatively close to mass-selected samples. We construct three\nmock samples based on existing (Planck and SPT) and upcoming (Simons\nObservatory) surveys. We ignore measurement uncertainties and compare the\nbiases in the counts and inferred cosmological parameters to each survey's\nPoisson errors. We find that widely used models for the DMO HMF differ\nsignificantly from each other and from the DMO version of FLAMINGO, leading to\nsignificant biases for all three surveys. For upcoming surveys, dramatic\nimprovements are needed for all additional model ingredients, i.e. the\nfunctional forms of the fits to the observable-mass scaling relation and the\nassociated scatter, the priors on the scaling relation and the prior on\nbaryonic effects associated with feedback processes on the HMF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Galaxy cluster counts have historically been important for the measurement of\ncosmological parameters and upcoming surveys will greatly reduce the\nstatistical errors. To exploit the potential of current and future cluster\nsurveys, theoretical uncertainties on the predicted abundance must be smaller\nthan the statistical errors. Models used to predict cluster counts typically\ncombine a model for the dark matter only (DMO) halo mass function (HMF) with an\nobservable - mass relation that is assumed to be a power-law with lognormal\nscatter. We use the FLAMINGO suite of cosmological hydrodynamical simulations\nto quantify the biases in the cluster counts and cosmological parameters\nresulting from the different ingredients of conventional models. For the\nobservable mass proxy we focus on the Compton-Y parameter quantifying the\nthermal Sunyaev-Zel'dovich effect, which is expected to result in cluster\nsamples that are relatively close to mass-selected samples. We construct three\nmock samples based on existing (Planck and SPT) and upcoming (Simons\nObservatory) surveys. We ignore measurement uncertainties and compare the\nbiases in the counts and inferred cosmological parameters to each survey's\nPoisson errors. We find that widely used models for the DMO HMF differ\nsignificantly from each other and from the DMO version of FLAMINGO, leading to\nsignificant biases for all three surveys. For upcoming surveys, dramatic\nimprovements are needed for all additional model ingredients, i.e. the\nfunctional forms of the fits to the observable-mass scaling relation and the\nassociated scatter, the priors on the scaling relation and the prior on\nbaryonic effects associated with feedback processes on the HMF."
                },
                "authors": [
                    {
                        "name": "Roi Kugel"
                    },
                    {
                        "name": "Joop Schaye"
                    },
                    {
                        "name": "Matthieu Schaller"
                    },
                    {
                        "name": "Victor J. Forouhar Moreno"
                    },
                    {
                        "name": "Robert J. McGibbon"
                    }
                ],
                "author_detail": {
                    "name": "Robert J. McGibbon"
                },
                "author": "Robert J. McGibbon",
                "arxiv_comment": "19 pages, 8 figures. (Updates w.r.t. version 1) Accepted for\n  publication in MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17217v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17217v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10733v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10733v5",
                "updated": "2025-01-17T14:22:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    14,
                    22,
                    6,
                    4,
                    17,
                    0
                ],
                "published": "2024-10-14T17:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    15,
                    7,
                    0,
                    288,
                    0
                ],
                "title": "Deep Compression Autoencoder for Efficient High-Resolution Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Compression Autoencoder for Efficient High-Resolution Diffusion\n  Models"
                },
                "summary": "We present Deep Compression Autoencoder (DC-AE), a new family of autoencoder\nmodels for accelerating high-resolution diffusion models. Existing autoencoder\nmodels have demonstrated impressive results at a moderate spatial compression\nratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy for\nhigh spatial compression ratios (e.g., 64x). We address this challenge by\nintroducing two key techniques: (1) Residual Autoencoding, where we design our\nmodels to learn residuals based on the space-to-channel transformed features to\nalleviate the optimization difficulty of high spatial-compression autoencoders;\n(2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phases\ntraining strategy for mitigating the generalization penalty of high\nspatial-compression autoencoders. With these designs, we improve the\nautoencoder's spatial compression ratio up to 128 while maintaining the\nreconstruction quality. Applying our DC-AE to latent diffusion models, we\nachieve significant speedup without accuracy drop. For example, on ImageNet\n512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedup\non H100 GPU for UViT-H while achieving a better FID, compared with the widely\nused SD-VAE-f8 autoencoder. Our code is available at\nhttps://github.com/mit-han-lab/efficientvit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Deep Compression Autoencoder (DC-AE), a new family of autoencoder\nmodels for accelerating high-resolution diffusion models. Existing autoencoder\nmodels have demonstrated impressive results at a moderate spatial compression\nratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy for\nhigh spatial compression ratios (e.g., 64x). We address this challenge by\nintroducing two key techniques: (1) Residual Autoencoding, where we design our\nmodels to learn residuals based on the space-to-channel transformed features to\nalleviate the optimization difficulty of high spatial-compression autoencoders;\n(2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phases\ntraining strategy for mitigating the generalization penalty of high\nspatial-compression autoencoders. With these designs, we improve the\nautoencoder's spatial compression ratio up to 128 while maintaining the\nreconstruction quality. Applying our DC-AE to latent diffusion models, we\nachieve significant speedup without accuracy drop. For example, on ImageNet\n512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedup\non H100 GPU for UViT-H while achieving a better FID, compared with the widely\nused SD-VAE-f8 autoencoder. Our code is available at\nhttps://github.com/mit-han-lab/efficientvit."
                },
                "authors": [
                    {
                        "name": "Junyu Chen"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Junsong Chen"
                    },
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Preprint. First two authors contributed equally to this work. Update:\n  fix typo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10733v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10733v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.12482v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.12482v4",
                "updated": "2025-01-17T14:13:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    14,
                    13,
                    28,
                    4,
                    17,
                    0
                ],
                "published": "2023-04-24T22:45:28Z",
                "published_parsed": [
                    2023,
                    4,
                    24,
                    22,
                    45,
                    28,
                    0,
                    114,
                    0
                ],
                "title": "Information Theory for Complex Systems Scientists",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information Theory for Complex Systems Scientists"
                },
                "summary": "In the 21st century, many of the crucial scientific and technical issues\nfacing humanity can be understood as problems associated with understanding,\nmodelling, and ultimately controlling complex systems: systems comprised of a\nlarge number of non-trivially interacting components whose collective behaviour\ncan be difficult to predict. Information theory, a branch of mathematics\nhistorically associated with questions about encoding and decoding messages,\nhas emerged as something of a lingua franca for those studying complex systems,\nfar exceeding its original narrow domain of communication systems engineering.\nIn the context of complexity science, information theory provides a set of\ntools which allow researchers to uncover the statistical and effective\ndependencies between interacting components; relationships between systems and\ntheir environment; mereological whole-part relationships; and is sensitive to\nnon-linearities missed by commonly parametric statistical models.\n  In this review, we aim to provide an accessible introduction to the core of\nmodern information theory, aimed specifically at aspiring (and established)\ncomplex systems scientists. This includes standard measures, such as Shannon\nentropy, relative entropy, and mutual information, before building to more\nadvanced topics, including: information dynamics, measures of statistical\ncomplexity, information decomposition, and effective network inference. In\naddition to detailing the formal definitions, in this review we make an effort\nto discuss how information theory can be interpreted and develop the intuition\nbehind abstract concepts like \"entropy,\" in the hope that this will enable\ninterested readers to understand what information is, and how it is used, at a\nmore fundamental level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the 21st century, many of the crucial scientific and technical issues\nfacing humanity can be understood as problems associated with understanding,\nmodelling, and ultimately controlling complex systems: systems comprised of a\nlarge number of non-trivially interacting components whose collective behaviour\ncan be difficult to predict. Information theory, a branch of mathematics\nhistorically associated with questions about encoding and decoding messages,\nhas emerged as something of a lingua franca for those studying complex systems,\nfar exceeding its original narrow domain of communication systems engineering.\nIn the context of complexity science, information theory provides a set of\ntools which allow researchers to uncover the statistical and effective\ndependencies between interacting components; relationships between systems and\ntheir environment; mereological whole-part relationships; and is sensitive to\nnon-linearities missed by commonly parametric statistical models.\n  In this review, we aim to provide an accessible introduction to the core of\nmodern information theory, aimed specifically at aspiring (and established)\ncomplex systems scientists. This includes standard measures, such as Shannon\nentropy, relative entropy, and mutual information, before building to more\nadvanced topics, including: information dynamics, measures of statistical\ncomplexity, information decomposition, and effective network inference. In\naddition to detailing the formal definitions, in this review we make an effort\nto discuss how information theory can be interpreted and develop the intuition\nbehind abstract concepts like \"entropy,\" in the hope that this will enable\ninterested readers to understand what information is, and how it is used, at a\nmore fundamental level."
                },
                "authors": [
                    {
                        "name": "Thomas F. Varley"
                    }
                ],
                "author_detail": {
                    "name": "Thomas F. Varley"
                },
                "author": "Thomas F. Varley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.12482v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.12482v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01477v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01477v2",
                "updated": "2025-01-17T14:10:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    14,
                    10,
                    15,
                    4,
                    17,
                    0
                ],
                "published": "2024-11-03T08:30:29Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    8,
                    30,
                    29,
                    6,
                    308,
                    0
                ],
                "title": "DPCL-Diff: The Temporal Knowledge Graph Reasoning Based on Graph Node\n  Diffusion Model with Dual-Domain Periodic Contrastive Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DPCL-Diff: The Temporal Knowledge Graph Reasoning Based on Graph Node\n  Diffusion Model with Dual-Domain Periodic Contrastive Learning"
                },
                "summary": "Temporal knowledge graph (TKG) reasoning that infers future missing facts is\nan essential and challenging task. Predicting future events typically relies on\nclosely related historical facts, yielding more accurate results for repetitive\nor periodic events. However, for future events with sparse historical\ninteractions, the effectiveness of this method, which focuses on leveraging\nhigh-frequency historical information, diminishes. Recently, the capabilities\nof diffusion models in image generation have opened new opportunities for TKG\nreasoning. Therefore, we propose a graph node diffusion model with dual-domain\nperiodic contrastive learning (DPCL-Diff). Graph node diffusion model (GNDiff)\nintroduces noise into sparsely related events to simulate new events,\ngenerating high-quality data that better conforms to the actual distribution.\nThis generative mechanism significantly enhances the model's ability to reason\nabout new events. Additionally, the dual-domain periodic contrastive learning\n(DPCL) maps periodic and non-periodic event entities to Poincar\\'e and\nEuclidean spaces, leveraging their characteristics to distinguish similar\nperiodic events effectively. Experimental results on four public datasets\ndemonstrate that DPCL-Diff significantly outperforms state-of-the-art TKG\nmodels in event prediction, demonstrating our approach's effectiveness. This\nstudy also investigates the combined effectiveness of GNDiff and DPCL in TKG\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal knowledge graph (TKG) reasoning that infers future missing facts is\nan essential and challenging task. Predicting future events typically relies on\nclosely related historical facts, yielding more accurate results for repetitive\nor periodic events. However, for future events with sparse historical\ninteractions, the effectiveness of this method, which focuses on leveraging\nhigh-frequency historical information, diminishes. Recently, the capabilities\nof diffusion models in image generation have opened new opportunities for TKG\nreasoning. Therefore, we propose a graph node diffusion model with dual-domain\nperiodic contrastive learning (DPCL-Diff). Graph node diffusion model (GNDiff)\nintroduces noise into sparsely related events to simulate new events,\ngenerating high-quality data that better conforms to the actual distribution.\nThis generative mechanism significantly enhances the model's ability to reason\nabout new events. Additionally, the dual-domain periodic contrastive learning\n(DPCL) maps periodic and non-periodic event entities to Poincar\\'e and\nEuclidean spaces, leveraging their characteristics to distinguish similar\nperiodic events effectively. Experimental results on four public datasets\ndemonstrate that DPCL-Diff significantly outperforms state-of-the-art TKG\nmodels in event prediction, demonstrating our approach's effectiveness. This\nstudy also investigates the combined effectiveness of GNDiff and DPCL in TKG\ntasks."
                },
                "authors": [
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Lisheng Wang"
                    },
                    {
                        "name": "Luobin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Luobin Huang"
                },
                "author": "Luobin Huang",
                "arxiv_comment": "11 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01477v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01477v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14393v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14393v4",
                "updated": "2025-01-17T13:56:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    13,
                    56,
                    50,
                    4,
                    17,
                    0
                ],
                "published": "2024-06-20T15:12:27Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    15,
                    12,
                    27,
                    3,
                    172,
                    0
                ],
                "title": "Jailbreaking as a Reward Misspecification Problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking as a Reward Misspecification Problem"
                },
                "summary": "The widespread adoption of large language models (LLMs) has raised concerns\nabout their safety and reliability, particularly regarding their vulnerability\nto adversarial attacks. In this paper, we propose a novel perspective that\nattributes this vulnerability to reward misspecification during the alignment\nprocess. This misspecification occurs when the reward function fails to\naccurately capture the intended behavior, leading to misaligned model outputs.\nWe introduce a metric ReGap to quantify the extent of reward misspecification\nand demonstrate its effectiveness and robustness in detecting harmful backdoor\nprompts. Building upon these insights, we present ReMiss, a system for\nautomated red teaming that generates adversarial prompts in a\nreward-misspecified space. ReMiss achieves state-of-the-art attack success\nrates on the AdvBench benchmark against various target aligned LLMs while\npreserving the human readability of the generated prompts. Furthermore, these\nattacks on open-source models demonstrate high transferability to closed-source\nmodels like GPT-4o and out-of-distribution tasks from HarmBench. Detailed\nanalysis highlights the unique advantages of the proposed reward\nmisspecification objective compared to previous methods, offering new insights\nfor improving LLM safety and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of large language models (LLMs) has raised concerns\nabout their safety and reliability, particularly regarding their vulnerability\nto adversarial attacks. In this paper, we propose a novel perspective that\nattributes this vulnerability to reward misspecification during the alignment\nprocess. This misspecification occurs when the reward function fails to\naccurately capture the intended behavior, leading to misaligned model outputs.\nWe introduce a metric ReGap to quantify the extent of reward misspecification\nand demonstrate its effectiveness and robustness in detecting harmful backdoor\nprompts. Building upon these insights, we present ReMiss, a system for\nautomated red teaming that generates adversarial prompts in a\nreward-misspecified space. ReMiss achieves state-of-the-art attack success\nrates on the AdvBench benchmark against various target aligned LLMs while\npreserving the human readability of the generated prompts. Furthermore, these\nattacks on open-source models demonstrate high transferability to closed-source\nmodels like GPT-4o and out-of-distribution tasks from HarmBench. Detailed\nanalysis highlights the unique advantages of the proposed reward\nmisspecification objective compared to previous methods, offering new insights\nfor improving LLM safety and robustness."
                },
                "authors": [
                    {
                        "name": "Zhihui Xie"
                    },
                    {
                        "name": "Jiahui Gao"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Lingpeng Kong"
                    }
                ],
                "author_detail": {
                    "name": "Lingpeng Kong"
                },
                "author": "Lingpeng Kong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14393v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14393v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10200v1",
                "updated": "2025-01-17T13:48:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    13,
                    48,
                    32,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T13:48:32Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    13,
                    48,
                    32,
                    4,
                    17,
                    0
                ],
                "title": "Test Wars: A Comparative Study of SBST, Symbolic Execution, and\n  LLM-Based Approaches to Unit Test Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test Wars: A Comparative Study of SBST, Symbolic Execution, and\n  LLM-Based Approaches to Unit Test Generation"
                },
                "summary": "Generating tests automatically is a key and ongoing area of focus in software\nengineering research. The emergence of Large Language Models (LLMs) has opened\nup new opportunities, given their ability to perform a wide spectrum of tasks.\nHowever, the effectiveness of LLM-based approaches compared to traditional\ntechniques such as search-based software testing (SBST) and symbolic execution\nremains uncertain. In this paper, we perform an extensive study of automatic\ntest generation approaches based on three tools: EvoSuite for SBST, Kex for\nsymbolic execution, and TestSpark for LLM-based test generation. We evaluate\ntools performance on the GitBug Java dataset and compare them using various\nexecution-based and feature-based metrics. Our results show that while\nLLM-based test generation is promising, it falls behind traditional methods in\nterms of coverage. However, it significantly outperforms them in mutation\nscores, suggesting that LLMs provide a deeper semantic understanding of code.\nLLM-based approach also performed worse than SBST and symbolic execution-based\napproaches w.r.t. fault detection capabilities. Additionally, our feature-based\nanalysis shows that all tools are primarily affected by the complexity and\ninternal dependencies of the class under test (CUT), with LLM-based approaches\nbeing especially sensitive to the CUT size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating tests automatically is a key and ongoing area of focus in software\nengineering research. The emergence of Large Language Models (LLMs) has opened\nup new opportunities, given their ability to perform a wide spectrum of tasks.\nHowever, the effectiveness of LLM-based approaches compared to traditional\ntechniques such as search-based software testing (SBST) and symbolic execution\nremains uncertain. In this paper, we perform an extensive study of automatic\ntest generation approaches based on three tools: EvoSuite for SBST, Kex for\nsymbolic execution, and TestSpark for LLM-based test generation. We evaluate\ntools performance on the GitBug Java dataset and compare them using various\nexecution-based and feature-based metrics. Our results show that while\nLLM-based test generation is promising, it falls behind traditional methods in\nterms of coverage. However, it significantly outperforms them in mutation\nscores, suggesting that LLMs provide a deeper semantic understanding of code.\nLLM-based approach also performed worse than SBST and symbolic execution-based\napproaches w.r.t. fault detection capabilities. Additionally, our feature-based\nanalysis shows that all tools are primarily affected by the complexity and\ninternal dependencies of the class under test (CUT), with LLM-based approaches\nbeing especially sensitive to the CUT size."
                },
                "authors": [
                    {
                        "name": "Azat Abdullin"
                    },
                    {
                        "name": "Pouria Derakhshanfar"
                    },
                    {
                        "name": "Annibale Panichella"
                    }
                ],
                "author_detail": {
                    "name": "Annibale Panichella"
                },
                "author": "Annibale Panichella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02573v2",
                "updated": "2025-01-17T13:48:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    13,
                    48,
                    15,
                    4,
                    17,
                    0
                ],
                "published": "2024-08-05T15:48:14Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    15,
                    48,
                    14,
                    0,
                    218,
                    0
                ],
                "title": "Testing identifying assumptions in Tobit Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing identifying assumptions in Tobit Models"
                },
                "summary": "This paper develops sharp testable implications for Tobit and IV-Tobit\nmodels' identifying assumptions: linear index specification, (joint) normality\nof latent errors, and treatment (instrument) exogeneity and relevance. The new\nsharp testable equalities can detect all possible observable violations of the\nidentifying conditions. We propose a testing procedure for the model's validity\nusing existing inference methods for intersection bounds. Simulation results\nsuggests proper size for large samples and that the test is powerful to detect\nlarge violation of the exogeneity assumption and violations in the error\nstructure. Finally, we review and propose new alternative paths to partially\nidentify the parameters of interest under less restrictive assumptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper develops sharp testable implications for Tobit and IV-Tobit\nmodels' identifying assumptions: linear index specification, (joint) normality\nof latent errors, and treatment (instrument) exogeneity and relevance. The new\nsharp testable equalities can detect all possible observable violations of the\nidentifying conditions. We propose a testing procedure for the model's validity\nusing existing inference methods for intersection bounds. Simulation results\nsuggests proper size for large samples and that the test is powerful to detect\nlarge violation of the exogeneity assumption and violations in the error\nstructure. Finally, we review and propose new alternative paths to partially\nidentify the parameters of interest under less restrictive assumptions."
                },
                "authors": [
                    {
                        "name": "Santiago Acerenza"
                    },
                    {
                        "name": "Otvio Bartalotti"
                    },
                    {
                        "name": "Federico Veneri"
                    }
                ],
                "author_detail": {
                    "name": "Federico Veneri"
                },
                "author": "Federico Veneri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10199v1",
                "updated": "2025-01-17T13:48:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    13,
                    48,
                    4,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T13:48:04Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    13,
                    48,
                    4,
                    4,
                    17,
                    0
                ],
                "title": "Adaptive Clustering for Efficient Phenotype Segmentation of UAV\n  Hyperspectral Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Clustering for Efficient Phenotype Segmentation of UAV\n  Hyperspectral Data"
                },
                "summary": "Unmanned Aerial Vehicles (UAVs) combined with Hyperspectral imaging (HSI)\noffer potential for environmental and agricultural applications by capturing\ndetailed spectral information that enables the prediction of invisible features\nlike biochemical leaf properties. However, the data-intensive nature of HSI\nposes challenges for remote devices, which have limited computational resources\nand storage. This paper introduces an Online Hyperspectral Simple Linear\nIterative Clustering algorithm (OHSLIC) framework for real-time tree phenotype\nsegmentation. OHSLIC reduces inherent noise and computational demands through\nadaptive incremental clustering and a lightweight neural network, which\nphenotypes trees using leaf contents such as chlorophyll, carotenoids, and\nanthocyanins. A hyperspectral dataset is created using a custom simulator that\nincorporates realistic leaf parameters, and light interactions. Results\ndemonstrate that OHSLIC achieves superior regression accuracy and segmentation\nperformance compared to pixel- or window-based methods while significantly\nreducing inference time. The method`s adaptive clustering enables dynamic\ntrade-offs between computational efficiency and accuracy, paving the way for\nscalable edge-device deployment in HSI applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned Aerial Vehicles (UAVs) combined with Hyperspectral imaging (HSI)\noffer potential for environmental and agricultural applications by capturing\ndetailed spectral information that enables the prediction of invisible features\nlike biochemical leaf properties. However, the data-intensive nature of HSI\nposes challenges for remote devices, which have limited computational resources\nand storage. This paper introduces an Online Hyperspectral Simple Linear\nIterative Clustering algorithm (OHSLIC) framework for real-time tree phenotype\nsegmentation. OHSLIC reduces inherent noise and computational demands through\nadaptive incremental clustering and a lightweight neural network, which\nphenotypes trees using leaf contents such as chlorophyll, carotenoids, and\nanthocyanins. A hyperspectral dataset is created using a custom simulator that\nincorporates realistic leaf parameters, and light interactions. Results\ndemonstrate that OHSLIC achieves superior regression accuracy and segmentation\nperformance compared to pixel- or window-based methods while significantly\nreducing inference time. The method`s adaptive clustering enables dynamic\ntrade-offs between computational efficiency and accuracy, paving the way for\nscalable edge-device deployment in HSI applications."
                },
                "authors": [
                    {
                        "name": "Ciem Cornelissen"
                    },
                    {
                        "name": "Sam Leroux"
                    },
                    {
                        "name": "Pieter Simoens"
                    }
                ],
                "author_detail": {
                    "name": "Pieter Simoens"
                },
                "author": "Pieter Simoens",
                "arxiv_comment": "accepted WACV 2025 GeoCV workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.4; I.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10198v1",
                "updated": "2025-01-17T13:47:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    13,
                    47,
                    46,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T13:47:46Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    13,
                    47,
                    46,
                    4,
                    17,
                    0
                ],
                "title": "Time-Resolved Measurements of Cumulative Effects in Gas Dynamics Induced\n  by High-Repetition-Rate Femtosecond Laser Filamentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Resolved Measurements of Cumulative Effects in Gas Dynamics Induced\n  by High-Repetition-Rate Femtosecond Laser Filamentation"
                },
                "summary": "The advent of high-average-power, ultrafast ytterbium-based lasers allows us\nto generate laser filaments at repetition rates ranging from 10s of kHz up to\n100s of kHz. At such high repetition rates, the inter-pulse time lies below the\ntime required for the total diffusion of the deposited heat by each laser\npulse, leading to cumulative hydrodynamic effects that have so far been rarely\nstudied. Here, we present, to the best of our knowledge, the first experimental\ntime-resolved measurements of these dynamics in air for laser repetition rates\nbetween 1 kHz and 100 kHz. We measure the change in the air refractive index\ncaused by the localized heat deposition and the length of the\nfilament-generated plasma channel, with which we can infer the corresponding\nchange in air density. We observe that at repetition rates above 10 kHz,\nstationary density depletions with vanishing dynamics emerge. Our findings are\nof wide relevance for the fields of high-repetition-rate laser filamentation\nand its applications, as well as THz generation from laser-induced plasma\nsources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of high-average-power, ultrafast ytterbium-based lasers allows us\nto generate laser filaments at repetition rates ranging from 10s of kHz up to\n100s of kHz. At such high repetition rates, the inter-pulse time lies below the\ntime required for the total diffusion of the deposited heat by each laser\npulse, leading to cumulative hydrodynamic effects that have so far been rarely\nstudied. Here, we present, to the best of our knowledge, the first experimental\ntime-resolved measurements of these dynamics in air for laser repetition rates\nbetween 1 kHz and 100 kHz. We measure the change in the air refractive index\ncaused by the localized heat deposition and the length of the\nfilament-generated plasma channel, with which we can infer the corresponding\nchange in air density. We observe that at repetition rates above 10 kHz,\nstationary density depletions with vanishing dynamics emerge. Our findings are\nof wide relevance for the fields of high-repetition-rate laser filamentation\nand its applications, as well as THz generation from laser-induced plasma\nsources."
                },
                "authors": [
                    {
                        "name": "Robin Lscher"
                    },
                    {
                        "name": "Malte C. Schroeder"
                    },
                    {
                        "name": "Alan Omar"
                    },
                    {
                        "name": "Clara J. Saraceno"
                    }
                ],
                "author_detail": {
                    "name": "Clara J. Saraceno"
                },
                "author": "Clara J. Saraceno",
                "arxiv_comment": "8 pages, 4 figurees",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10189v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10189v1",
                "updated": "2025-01-17T13:37:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    13,
                    37,
                    16,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T13:37:16Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    13,
                    37,
                    16,
                    4,
                    17,
                    0
                ],
                "title": "Optimizing Structured-Sparse Matrix Multiplication in RISC-V Vector\n  Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Structured-Sparse Matrix Multiplication in RISC-V Vector\n  Processors"
                },
                "summary": "Structured sparsity has been proposed as an efficient way to prune the\ncomplexity of Machine Learning (ML) applications and to simplify the handling\nof sparse data in hardware. Accelerating ML models, whether for training, or\ninference, heavily relies on matrix multiplications that can be efficiently\nexecuted on vector processors, or custom matrix engines. This work aims to\nintegrate the simplicity of structured sparsity into vector execution to speed\nup the corresponding matrix multiplications. Initially, the implementation of\nstructured-sparse matrix multiplication using the current RISC-V instruction\nset vector extension is comprehensively explored. Critical parameters that\naffect performance, such as the impact of data distribution across the scalar\nand vector register files, data locality, and the effectiveness of loop\nunrolling are analyzed both qualitatively and quantitatively. Furthermore, it\nis demonstrated that the addition of a single new instruction would reap even\nhigher performance. The newly proposed instruction is called vindexmac, i.e.,\nvector index-multiply-accumulate. It allows for indirect reads from the vector\nregister file and it reduces the number of instructions executed per matrix\nmultiplication iteration, without introducing additional dependencies that\nwould limit loop unrolling. The proposed new instruction was integrated in a\ndecoupled RISC-V vector processor with negligible hardware cost. Experimental\nresults demonstrate the runtime efficiency and the scalability offered by the\nintroduced optimizations and the new instruction for the execution of\nstate-of-the-art Convolutional Neural Networks. More particularly, the addition\nof a custom instruction improves runtime by 25% and 33% when compared with\nhighly-optimized vectorized kernels that use only the currently defined RISC-V\ninstructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured sparsity has been proposed as an efficient way to prune the\ncomplexity of Machine Learning (ML) applications and to simplify the handling\nof sparse data in hardware. Accelerating ML models, whether for training, or\ninference, heavily relies on matrix multiplications that can be efficiently\nexecuted on vector processors, or custom matrix engines. This work aims to\nintegrate the simplicity of structured sparsity into vector execution to speed\nup the corresponding matrix multiplications. Initially, the implementation of\nstructured-sparse matrix multiplication using the current RISC-V instruction\nset vector extension is comprehensively explored. Critical parameters that\naffect performance, such as the impact of data distribution across the scalar\nand vector register files, data locality, and the effectiveness of loop\nunrolling are analyzed both qualitatively and quantitatively. Furthermore, it\nis demonstrated that the addition of a single new instruction would reap even\nhigher performance. The newly proposed instruction is called vindexmac, i.e.,\nvector index-multiply-accumulate. It allows for indirect reads from the vector\nregister file and it reduces the number of instructions executed per matrix\nmultiplication iteration, without introducing additional dependencies that\nwould limit loop unrolling. The proposed new instruction was integrated in a\ndecoupled RISC-V vector processor with negligible hardware cost. Experimental\nresults demonstrate the runtime efficiency and the scalability offered by the\nintroduced optimizations and the new instruction for the execution of\nstate-of-the-art Convolutional Neural Networks. More particularly, the addition\nof a custom instruction improves runtime by 25% and 33% when compared with\nhighly-optimized vectorized kernels that use only the currently defined RISC-V\ninstructions."
                },
                "authors": [
                    {
                        "name": "Vasileios Titopoulos"
                    },
                    {
                        "name": "Kosmas Alexandridis"
                    },
                    {
                        "name": "Christodoulos Peltekis"
                    },
                    {
                        "name": "Chrysostomos Nicopoulos"
                    },
                    {
                        "name": "Giorgos Dimitrakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Dimitrakopoulos"
                },
                "author": "Giorgos Dimitrakopoulos",
                "arxiv_comment": "Accepted for publication at IEEE Transactions on Computers. arXiv\n  admin note: text overlap with arXiv:2311.07241",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10189v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10186v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10186v1",
                "updated": "2025-01-17T13:32:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    13,
                    32,
                    19,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T13:32:19Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    13,
                    32,
                    19,
                    4,
                    17,
                    0
                ],
                "title": "Generative Artificial Intelligence: Implications for Biomedical and\n  Health Professions Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Artificial Intelligence: Implications for Biomedical and\n  Health Professions Education"
                },
                "summary": "Generative AI has had a profound impact on biomedicine and health, both in\nprofessional work and in education. Based on large language models (LLMs),\ngenerative AI has been found to perform as well as humans in simulated\nsituations taking medical board exams, answering clinical questions, solving\nclinical cases, applying clinical reasoning, and summarizing information.\nGenerative AI is also being used widely in education, performing well in\nacademic courses and their assessments. This review summarizes the successes of\nLLMs and highlights some of their challenges in the context of education, most\nnotably aspects that may undermines the acquisition of knowledge and skills for\nprofessional work. It then provides recommendations for best practices\novercoming shortcomings for LLM use in education. Although there are challenges\nfor use of generative AI in education, all students and faculty, in biomedicine\nand health and beyond, must have understanding and be competent in its use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI has had a profound impact on biomedicine and health, both in\nprofessional work and in education. Based on large language models (LLMs),\ngenerative AI has been found to perform as well as humans in simulated\nsituations taking medical board exams, answering clinical questions, solving\nclinical cases, applying clinical reasoning, and summarizing information.\nGenerative AI is also being used widely in education, performing well in\nacademic courses and their assessments. This review summarizes the successes of\nLLMs and highlights some of their challenges in the context of education, most\nnotably aspects that may undermines the acquisition of knowledge and skills for\nprofessional work. It then provides recommendations for best practices\novercoming shortcomings for LLM use in education. Although there are challenges\nfor use of generative AI in education, all students and faculty, in biomedicine\nand health and beyond, must have understanding and be competent in its use."
                },
                "authors": [
                    {
                        "name": "William Hersh"
                    }
                ],
                "author_detail": {
                    "name": "William Hersh"
                },
                "author": "William Hersh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10186v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06879v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06879v2",
                "updated": "2025-01-17T13:20:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    13,
                    20,
                    39,
                    4,
                    17,
                    0
                ],
                "published": "2024-12-09T19:00:00Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    19,
                    0,
                    0,
                    0,
                    344,
                    0
                ],
                "title": "Prospects of a statistical detection of the 21-cm forest and its\n  potential to constrain the thermal state of the neutral IGM during\n  reionization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prospects of a statistical detection of the 21-cm forest and its\n  potential to constrain the thermal state of the neutral IGM during\n  reionization"
                },
                "summary": "The 21-cm forest signal is a promising probe of the Epoch of Reionization\ncomplementary to other 21-cm line observables and Ly$\\alpha$ forest signal.\nProspects of detecting it have significantly improved in the last decade thanks\nto the discovery of more than 30 radio-loud quasars at these redshifts,\nupgrades to telescope facilities, and the notion that neutral hydrogen islands\npersist down to $z\\lesssim 5.5$. We forward-model the 21-cm forest signal using\nsemi-numerical simulations and incorporate various instrumental features to\nexplore the potential of detecting the 21-cm forest at $z=6$, both directly and\nstatistically, with the currently available (uGMRT) and forthcoming (SKA1-low)\nobservatories. We show that it is possible to detect the 1D power spectrum of\nthe 21-cm forest spectrum, especially at large scales of $k\\lesssim8.5\\,\\rm\nMHz^{-1}$ with the $500\\,\\rm hr$ of the uGMRT time and $k\\lesssim32.4\\,\\rm\nMHz^{-1}$ with the SKA1-low over $50\\,\\rm hr$ if the intergalactic medium (IGM)\nis $25\\%$ neutral and these neutral hydrogen regions have a spin temperature of\n$\\lesssim30\\,\\rm K$. On the other hand, we infer that a null-detection of the\nsignal with such observations of 10 radio-loud sources at $z\\approx6$ can be\ntranslated into constraints on the thermal and ionization state of the IGM\nwhich are tighter than the currently available measurements. Moreover, a\nnull-detection of the 1D 21-cm forest power spectrum with only $50\\,\\rm hr$ of\nthe uGMRT observations of 10 radio-loud sources can already be competitive with\nthe Ly$\\alpha$ forest and 21-cm tomographic observations in disfavouring models\nof significantly neutral and cold IGM at $z=6$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The 21-cm forest signal is a promising probe of the Epoch of Reionization\ncomplementary to other 21-cm line observables and Ly$\\alpha$ forest signal.\nProspects of detecting it have significantly improved in the last decade thanks\nto the discovery of more than 30 radio-loud quasars at these redshifts,\nupgrades to telescope facilities, and the notion that neutral hydrogen islands\npersist down to $z\\lesssim 5.5$. We forward-model the 21-cm forest signal using\nsemi-numerical simulations and incorporate various instrumental features to\nexplore the potential of detecting the 21-cm forest at $z=6$, both directly and\nstatistically, with the currently available (uGMRT) and forthcoming (SKA1-low)\nobservatories. We show that it is possible to detect the 1D power spectrum of\nthe 21-cm forest spectrum, especially at large scales of $k\\lesssim8.5\\,\\rm\nMHz^{-1}$ with the $500\\,\\rm hr$ of the uGMRT time and $k\\lesssim32.4\\,\\rm\nMHz^{-1}$ with the SKA1-low over $50\\,\\rm hr$ if the intergalactic medium (IGM)\nis $25\\%$ neutral and these neutral hydrogen regions have a spin temperature of\n$\\lesssim30\\,\\rm K$. On the other hand, we infer that a null-detection of the\nsignal with such observations of 10 radio-loud sources at $z\\approx6$ can be\ntranslated into constraints on the thermal and ionization state of the IGM\nwhich are tighter than the currently available measurements. Moreover, a\nnull-detection of the 1D 21-cm forest power spectrum with only $50\\,\\rm hr$ of\nthe uGMRT observations of 10 radio-loud sources can already be competitive with\nthe Ly$\\alpha$ forest and 21-cm tomographic observations in disfavouring models\nof significantly neutral and cold IGM at $z=6$."
                },
                "authors": [
                    {
                        "name": "Tom oltinsk"
                    },
                    {
                        "name": "Girish Kulkarni"
                    },
                    {
                        "name": "Shriharsh P. Tendulkar"
                    },
                    {
                        "name": "James S. Bolton"
                    }
                ],
                "author_detail": {
                    "name": "James S. Bolton"
                },
                "arxiv_affiliation": "University of Nottingham",
                "author": "James S. Bolton",
                "arxiv_doi": "10.1093/mnras/staf026",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/staf026",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.06879v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06879v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 16 figures. Accepted for publiction in MNRAS",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10175v1",
                "updated": "2025-01-17T13:17:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    13,
                    17,
                    42,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T13:17:42Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    13,
                    17,
                    42,
                    4,
                    17,
                    0
                ],
                "title": "Multi-stage Training of Bilingual Islamic LLM for Neural Passage\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-stage Training of Bilingual Islamic LLM for Neural Passage\n  Retrieval"
                },
                "summary": "This study examines the use of Natural Language Processing (NLP) technology\nwithin the Islamic domain, focusing on developing an Islamic neural retrieval\nmodel. By leveraging the robust XLM-R model, the research employs a language\nreduction technique to create a lightweight bilingual large language model\n(LLM). Our approach for domain adaptation addresses the unique challenges faced\nin the Islamic domain, where substantial in-domain corpora exist only in Arabic\nwhile limited in other languages, including English.\n  The work utilizes a multi-stage training process for retrieval models,\nincorporating large retrieval datasets, such as MS MARCO, and smaller,\nin-domain datasets to improve retrieval performance. Additionally, we have\ncurated an in-domain retrieval dataset in English by employing data\naugmentation techniques and involving a reliable Islamic source. This approach\nenhances the domain-specific dataset for retrieval, leading to further\nperformance gains.\n  The findings suggest that combining domain adaptation and a multi-stage\ntraining method for the bilingual Islamic neural retrieval model enables it to\noutperform monolingual models on downstream retrieval tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examines the use of Natural Language Processing (NLP) technology\nwithin the Islamic domain, focusing on developing an Islamic neural retrieval\nmodel. By leveraging the robust XLM-R model, the research employs a language\nreduction technique to create a lightweight bilingual large language model\n(LLM). Our approach for domain adaptation addresses the unique challenges faced\nin the Islamic domain, where substantial in-domain corpora exist only in Arabic\nwhile limited in other languages, including English.\n  The work utilizes a multi-stage training process for retrieval models,\nincorporating large retrieval datasets, such as MS MARCO, and smaller,\nin-domain datasets to improve retrieval performance. Additionally, we have\ncurated an in-domain retrieval dataset in English by employing data\naugmentation techniques and involving a reliable Islamic source. This approach\nenhances the domain-specific dataset for retrieval, leading to further\nperformance gains.\n  The findings suggest that combining domain adaptation and a multi-stage\ntraining method for the bilingual Islamic neural retrieval model enables it to\noutperform monolingual models on downstream retrieval tasks."
                },
                "authors": [
                    {
                        "name": "Vera Pavlova"
                    }
                ],
                "author_detail": {
                    "name": "Vera Pavlova"
                },
                "author": "Vera Pavlova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10174v1",
                "updated": "2025-01-17T13:17:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    13,
                    17,
                    2,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T13:17:02Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    13,
                    17,
                    2,
                    4,
                    17,
                    0
                ],
                "title": "Michscan: Black-Box Neural Network Integrity Checking at Runtime Through\n  Power Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Michscan: Black-Box Neural Network Integrity Checking at Runtime Through\n  Power Analysis"
                },
                "summary": "As neural networks are increasingly used for critical decision-making tasks,\nthe threat of integrity attacks, where an adversary maliciously alters a model,\nhas become a significant security and safety concern. These concerns are\ncompounded by the use of licensed models, where end-users purchase third-party\nmodels with only black-box access to protect model intellectual property (IP).\nIn such scenarios, conventional approaches to verify model integrity require\nknowledge of model parameters or cooperative model owners. To address this\nchallenge, we propose Michscan, a methodology leveraging power analysis to\nverify the integrity of black-box TinyML neural networks designed for\nresource-constrained devices. Michscan is based on the observation that\nmodifications to model parameters impact the instantaneous power consumption of\nthe device. We leverage this observation to develop a runtime model\nintegrity-checking methodology that employs correlational power analysis using\na golden template or signature to mathematically quantify the likelihood of\nmodel integrity violations at runtime through the Mann-Whitney U-Test. Michscan\noperates in a black-box environment and does not require a cooperative or\ntrustworthy model owner. We evaluated Michscan using an STM32F303RC\nmicrocontroller with an ARM Cortex-M4 running four TinyML models in the\npresence of three model integrity violations. Michscan successfully detected\nall integrity violations at runtime using power data from five inferences. All\ndetected violations had a negligible probability P < 10^(-5) of being produced\nfrom an unmodified model (i.e., false positive).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As neural networks are increasingly used for critical decision-making tasks,\nthe threat of integrity attacks, where an adversary maliciously alters a model,\nhas become a significant security and safety concern. These concerns are\ncompounded by the use of licensed models, where end-users purchase third-party\nmodels with only black-box access to protect model intellectual property (IP).\nIn such scenarios, conventional approaches to verify model integrity require\nknowledge of model parameters or cooperative model owners. To address this\nchallenge, we propose Michscan, a methodology leveraging power analysis to\nverify the integrity of black-box TinyML neural networks designed for\nresource-constrained devices. Michscan is based on the observation that\nmodifications to model parameters impact the instantaneous power consumption of\nthe device. We leverage this observation to develop a runtime model\nintegrity-checking methodology that employs correlational power analysis using\na golden template or signature to mathematically quantify the likelihood of\nmodel integrity violations at runtime through the Mann-Whitney U-Test. Michscan\noperates in a black-box environment and does not require a cooperative or\ntrustworthy model owner. We evaluated Michscan using an STM32F303RC\nmicrocontroller with an ARM Cortex-M4 running four TinyML models in the\npresence of three model integrity violations. Michscan successfully detected\nall integrity violations at runtime using power data from five inferences. All\ndetected violations had a negligible probability P < 10^(-5) of being produced\nfrom an unmodified model (i.e., false positive)."
                },
                "authors": [
                    {
                        "name": "Robi Paul"
                    },
                    {
                        "name": "Michael Zuzak"
                    }
                ],
                "author_detail": {
                    "name": "Michael Zuzak"
                },
                "author": "Michael Zuzak",
                "arxiv_comment": "11 pages, 7 figures. To appear in IEEE International Symposium on\n  Hardware Oriented Security and Trust (HOST) 2025. This material is based upon\n  work supported by the National Science Foundation under Grant No. 2245573",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10172v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10172v1",
                "updated": "2025-01-17T13:07:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    13,
                    7,
                    52,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T13:07:52Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    13,
                    7,
                    52,
                    4,
                    17,
                    0
                ],
                "title": "Mean and Variance Estimation Complexity in Arbitrary Distributions via\n  Wasserstein Minimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mean and Variance Estimation Complexity in Arbitrary Distributions via\n  Wasserstein Minimization"
                },
                "summary": "Parameter estimation is a fundamental challenge in machine learning, crucial\nfor tasks such as neural network weight fitting and Bayesian inference. This\npaper focuses on the complexity of estimating translation $\\boldsymbol{\\mu} \\in\n\\mathbb{R}^l$ and shrinkage $\\sigma \\in \\mathbb{R}_{++}$ parameters for a\ndistribution of the form $\\frac{1}{\\sigma^l} f_0 \\left( \\frac{\\boldsymbol{x} -\n\\boldsymbol{\\mu}}{\\sigma} \\right)$, where $f_0$ is a known density in\n$\\mathbb{R}^l$ given $n$ samples. We highlight that while the problem is\nNP-hard for Maximum Likelihood Estimation (MLE), it is possible to obtain\n$\\varepsilon$-approximations for arbitrary $\\varepsilon > 0$ within\n$\\text{poly} \\left( \\frac{1}{\\varepsilon} \\right)$ time using the Wasserstein\ndistance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter estimation is a fundamental challenge in machine learning, crucial\nfor tasks such as neural network weight fitting and Bayesian inference. This\npaper focuses on the complexity of estimating translation $\\boldsymbol{\\mu} \\in\n\\mathbb{R}^l$ and shrinkage $\\sigma \\in \\mathbb{R}_{++}$ parameters for a\ndistribution of the form $\\frac{1}{\\sigma^l} f_0 \\left( \\frac{\\boldsymbol{x} -\n\\boldsymbol{\\mu}}{\\sigma} \\right)$, where $f_0$ is a known density in\n$\\mathbb{R}^l$ given $n$ samples. We highlight that while the problem is\nNP-hard for Maximum Likelihood Estimation (MLE), it is possible to obtain\n$\\varepsilon$-approximations for arbitrary $\\varepsilon > 0$ within\n$\\text{poly} \\left( \\frac{1}{\\varepsilon} \\right)$ time using the Wasserstein\ndistance."
                },
                "authors": [
                    {
                        "name": "Valentio Iverson"
                    },
                    {
                        "name": "Stephen Vavasis"
                    }
                ],
                "author_detail": {
                    "name": "Stephen Vavasis"
                },
                "author": "Stephen Vavasis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10172v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00329v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00329v2",
                "updated": "2025-01-17T12:53:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    53,
                    37,
                    4,
                    17,
                    0
                ],
                "published": "2024-11-30T03:02:50Z",
                "published_parsed": [
                    2024,
                    11,
                    30,
                    3,
                    2,
                    50,
                    5,
                    335,
                    0
                ],
                "title": "Language Models in Software Development Tasks: An Experimental Analysis\n  of Energy and Accuracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models in Software Development Tasks: An Experimental Analysis\n  of Energy and Accuracy"
                },
                "summary": "The use of generative AI-based coding assistants like ChatGPT and Github\nCopilot is a reality in contemporary software development. Many of these tools\nare provided as remote APIs. Using third-party APIs raises data privacy and\nsecurity concerns for client companies, which motivates the use of\nlocally-deployed language models. In this study, we explore the trade-off\nbetween model accuracy and energy consumption, aiming to provide valuable\ninsights to help developers make informed decisions when selecting a language\nmodel. We investigate the performance of 18 families of LLMs in typical\nsoftware development tasks on two real-world infrastructures, a commodity GPU\nand a powerful AI-specific GPU. Given that deploying LLMs locally requires\npowerful infrastructure which might not be affordable for everyone, we consider\nboth full-precision and quantized models. Our findings reveal that employing a\nbig LLM with a higher energy budget does not always translate to significantly\nimproved accuracy. Additionally, quantized versions of large models generally\noffer better efficiency and accuracy compared to full-precision versions of\nmedium-sized ones. Apart from that, not a single model is suitable for all\ntypes of software development tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of generative AI-based coding assistants like ChatGPT and Github\nCopilot is a reality in contemporary software development. Many of these tools\nare provided as remote APIs. Using third-party APIs raises data privacy and\nsecurity concerns for client companies, which motivates the use of\nlocally-deployed language models. In this study, we explore the trade-off\nbetween model accuracy and energy consumption, aiming to provide valuable\ninsights to help developers make informed decisions when selecting a language\nmodel. We investigate the performance of 18 families of LLMs in typical\nsoftware development tasks on two real-world infrastructures, a commodity GPU\nand a powerful AI-specific GPU. Given that deploying LLMs locally requires\npowerful infrastructure which might not be affordable for everyone, we consider\nboth full-precision and quantized models. Our findings reveal that employing a\nbig LLM with a higher energy budget does not always translate to significantly\nimproved accuracy. Additionally, quantized versions of large models generally\noffer better efficiency and accuracy compared to full-precision versions of\nmedium-sized ones. Apart from that, not a single model is suitable for all\ntypes of software development tasks."
                },
                "authors": [
                    {
                        "name": "Negar Alizadeh"
                    },
                    {
                        "name": "Boris Belchev"
                    },
                    {
                        "name": "Nishant Saurabh"
                    },
                    {
                        "name": "Patricia Kelbert"
                    },
                    {
                        "name": "Fernando Castor"
                    }
                ],
                "author_detail": {
                    "name": "Fernando Castor"
                },
                "author": "Fernando Castor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00329v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00329v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09233v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09233v2",
                "updated": "2025-01-17T12:48:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    48,
                    9,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-16T01:35:16Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    1,
                    35,
                    16,
                    3,
                    16,
                    0
                ],
                "title": "Redefining Affordance via Computational Rationality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Redefining Affordance via Computational Rationality"
                },
                "summary": "Affordances, a foundational concept in human-computer interaction and design,\nhave traditionally been explained by direct-perception theories, which assume\nthat individuals perceive action possibilities directly from the environment.\nHowever, these theories fall short of explaining how affordances are perceived,\nlearned, refined, or misperceived, and how users choose between multiple\naffordances in dynamic contexts. This paper introduces a novel affordance\ntheory grounded in Computational Rationality, positing that humans construct\ninternal representations of the world based on bounded sensory inputs. Within\nthese internal models, affordances are inferred through two core mechanisms:\nfeature recognition and hypothetical motion trajectories. Our theory redefines\naffordance perception as a decision-making process, driven by two components:\nconfidence (the perceived likelihood of successfully executing an action) and\npredicted utility (the expected value of the outcome). By balancing these\nfactors, individuals make informed decisions about which actions to take. Our\ntheory frames affordances perception as dynamic, continuously learned, and\nrefined through reinforcement and feedback. We validate the theory via thought\nexperiments and demonstrate its applicability across diverse types of\naffordances (e.g., physical, digital, social). Beyond clarifying and\ngeneralizing the understanding of affordances across contexts, our theory\nserves as a foundation for improving design communication and guiding the\ndevelopment of more adaptive and intuitive systems that evolve with user\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affordances, a foundational concept in human-computer interaction and design,\nhave traditionally been explained by direct-perception theories, which assume\nthat individuals perceive action possibilities directly from the environment.\nHowever, these theories fall short of explaining how affordances are perceived,\nlearned, refined, or misperceived, and how users choose between multiple\naffordances in dynamic contexts. This paper introduces a novel affordance\ntheory grounded in Computational Rationality, positing that humans construct\ninternal representations of the world based on bounded sensory inputs. Within\nthese internal models, affordances are inferred through two core mechanisms:\nfeature recognition and hypothetical motion trajectories. Our theory redefines\naffordance perception as a decision-making process, driven by two components:\nconfidence (the perceived likelihood of successfully executing an action) and\npredicted utility (the expected value of the outcome). By balancing these\nfactors, individuals make informed decisions about which actions to take. Our\ntheory frames affordances perception as dynamic, continuously learned, and\nrefined through reinforcement and feedback. We validate the theory via thought\nexperiments and demonstrate its applicability across diverse types of\naffordances (e.g., physical, digital, social). Beyond clarifying and\ngeneralizing the understanding of affordances across contexts, our theory\nserves as a foundation for improving design communication and guiding the\ndevelopment of more adaptive and intuitive systems that evolve with user\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Yi-Chi Liao"
                    },
                    {
                        "name": "Christian Holz"
                    }
                ],
                "author_detail": {
                    "name": "Christian Holz"
                },
                "author": "Christian Holz",
                "arxiv_doi": "10.1145/3708359.3712114",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3708359.3712114",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.09233v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09233v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "IUI 2025 Paper",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16302v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16302v2",
                "updated": "2025-01-17T12:27:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    27,
                    40,
                    4,
                    17,
                    0
                ],
                "published": "2024-09-10T11:00:24Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    11,
                    0,
                    24,
                    1,
                    254,
                    0
                ],
                "title": "How Redundant Is the Transformer Stack in Speech Representation Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Redundant Is the Transformer Stack in Speech Representation Models?"
                },
                "summary": "Self-supervised speech representation models, particularly those leveraging\ntransformer architectures, have demonstrated remarkable performance across\nvarious tasks such as speech recognition, speaker identification, and emotion\ndetection. Recent studies on transformer models revealed a high redundancy\nbetween layers and the potential for significant pruning, which we will\ninvestigate here for transformer-based speech representation models. We perform\na detailed analysis of layer similarity in speech representation models using\nthree similarity metrics: cosine similarity, centered kernel alignment, and\nmutual nearest-neighbor alignment. Our findings reveal a block-like structure\nof high similarity, suggesting two main processing steps and significant\nredundancy of layers. We demonstrate the effectiveness of pruning\ntransformer-based speech representation models without the need for\npost-training, achieving up to 40% reduction in transformer layers while\nmaintaining over 95% of the model's predictive capacity. Furthermore, we employ\na knowledge distillation method to substitute the entire transformer stack with\nmimicking layers, reducing the network size 95-98% and the inference time by up\nto 94%. This substantial decrease in computational load occurs without\nconsiderable performance loss, suggesting that the transformer stack is almost\ncompletely redundant for downstream applications of speech representation\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-supervised speech representation models, particularly those leveraging\ntransformer architectures, have demonstrated remarkable performance across\nvarious tasks such as speech recognition, speaker identification, and emotion\ndetection. Recent studies on transformer models revealed a high redundancy\nbetween layers and the potential for significant pruning, which we will\ninvestigate here for transformer-based speech representation models. We perform\na detailed analysis of layer similarity in speech representation models using\nthree similarity metrics: cosine similarity, centered kernel alignment, and\nmutual nearest-neighbor alignment. Our findings reveal a block-like structure\nof high similarity, suggesting two main processing steps and significant\nredundancy of layers. We demonstrate the effectiveness of pruning\ntransformer-based speech representation models without the need for\npost-training, achieving up to 40% reduction in transformer layers while\nmaintaining over 95% of the model's predictive capacity. Furthermore, we employ\na knowledge distillation method to substitute the entire transformer stack with\nmimicking layers, reducing the network size 95-98% and the inference time by up\nto 94%. This substantial decrease in computational load occurs without\nconsiderable performance loss, suggesting that the transformer stack is almost\ncompletely redundant for downstream applications of speech representation\nmodels."
                },
                "authors": [
                    {
                        "name": "Teresa Dorszewski"
                    },
                    {
                        "name": "Albert Kjller Jacobsen"
                    },
                    {
                        "name": "Lenka Ttkov"
                    },
                    {
                        "name": "Lars Kai Hansen"
                    }
                ],
                "author_detail": {
                    "name": "Lars Kai Hansen"
                },
                "author": "Lars Kai Hansen",
                "arxiv_comment": "To appear at ICASSP 2025 (excluding appendix)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16302v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16302v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02957v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02957v3",
                "updated": "2025-01-17T11:59:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    11,
                    59,
                    23,
                    4,
                    17,
                    0
                ],
                "published": "2024-05-05T14:53:51Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    14,
                    53,
                    51,
                    6,
                    126,
                    0
                ],
                "title": "Agent Hospital: A Simulacrum of Hospital with Evolvable Medical Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent Hospital: A Simulacrum of Hospital with Evolvable Medical Agents"
                },
                "summary": "The recent rapid development of large language models (LLMs) has sparked a\nnew wave of technological revolution in medical artificial intelligence (AI).\nWhile LLMs are designed to understand and generate text like a human,\nautonomous agents that utilize LLMs as their \"brain\" have exhibited\ncapabilities beyond text processing such as planning, reflection, and using\ntools by enabling their \"bodies\" to interact with the environment. We introduce\na simulacrum of hospital called Agent Hospital that simulates the entire\nprocess of treating illness, in which all patients, nurses, and doctors are\nLLM-powered autonomous agents. Within the simulacrum, doctor agents are able to\nevolve by treating a large number of patient agents without the need to label\ntraining data manually. After treating tens of thousands of patient agents in\nthe simulacrum (human doctors may take several years in the real world), the\nevolved doctor agents outperform state-of-the-art medical agent methods on the\nMedQA benchmark comprising US Medical Licensing Examination (USMLE) test\nquestions. Our methods of simulacrum construction and agent evolution have the\npotential in benefiting a broad range of applications beyond medical AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent rapid development of large language models (LLMs) has sparked a\nnew wave of technological revolution in medical artificial intelligence (AI).\nWhile LLMs are designed to understand and generate text like a human,\nautonomous agents that utilize LLMs as their \"brain\" have exhibited\ncapabilities beyond text processing such as planning, reflection, and using\ntools by enabling their \"bodies\" to interact with the environment. We introduce\na simulacrum of hospital called Agent Hospital that simulates the entire\nprocess of treating illness, in which all patients, nurses, and doctors are\nLLM-powered autonomous agents. Within the simulacrum, doctor agents are able to\nevolve by treating a large number of patient agents without the need to label\ntraining data manually. After treating tens of thousands of patient agents in\nthe simulacrum (human doctors may take several years in the real world), the\nevolved doctor agents outperform state-of-the-art medical agent methods on the\nMedQA benchmark comprising US Medical Licensing Examination (USMLE) test\nquestions. Our methods of simulacrum construction and agent evolution have the\npotential in benefiting a broad range of applications beyond medical AI."
                },
                "authors": [
                    {
                        "name": "Junkai Li"
                    },
                    {
                        "name": "Yunghwei Lai"
                    },
                    {
                        "name": "Weitao Li"
                    },
                    {
                        "name": "Jingyi Ren"
                    },
                    {
                        "name": "Meng Zhang"
                    },
                    {
                        "name": "Xinhui Kang"
                    },
                    {
                        "name": "Siyu Wang"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Ya-Qin Zhang"
                    },
                    {
                        "name": "Weizhi Ma"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02957v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02957v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10134v1",
                "updated": "2025-01-17T11:49:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    11,
                    49,
                    49,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T11:49:49Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    11,
                    49,
                    49,
                    4,
                    17,
                    0
                ],
                "title": "Exploring the Impact of Generative Artificial Intelligence in Education:\n  A Thematic Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Impact of Generative Artificial Intelligence in Education:\n  A Thematic Analysis"
                },
                "summary": "The recent advancements in Generative Artificial intelligence (GenAI)\ntechnology have been transformative for the field of education. Large Language\nModels (LLMs) such as ChatGPT and Bard can be leveraged to automate boilerplate\ntasks, create content for personalised teaching, and handle repetitive tasks to\nallow more time for creative thinking. However, it is important to develop\nguidelines, policies, and assessment methods in the education sector to ensure\nthe responsible integration of these tools. In this article, thematic analysis\nhas been performed on seven essays obtained from professionals in the education\nsector to understand the advantages and pitfalls of using GenAI models such as\nChatGPT and Bard in education. Exploratory Data Analysis (EDA) has been\nperformed on the essays to extract further insights from the text. The study\nfound several themes which highlight benefits and drawbacks of GenAI tools, as\nwell as suggestions to overcome these limitations and ensure that students are\nusing these tools in a responsible and ethical manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advancements in Generative Artificial intelligence (GenAI)\ntechnology have been transformative for the field of education. Large Language\nModels (LLMs) such as ChatGPT and Bard can be leveraged to automate boilerplate\ntasks, create content for personalised teaching, and handle repetitive tasks to\nallow more time for creative thinking. However, it is important to develop\nguidelines, policies, and assessment methods in the education sector to ensure\nthe responsible integration of these tools. In this article, thematic analysis\nhas been performed on seven essays obtained from professionals in the education\nsector to understand the advantages and pitfalls of using GenAI models such as\nChatGPT and Bard in education. Exploratory Data Analysis (EDA) has been\nperformed on the essays to extract further insights from the text. The study\nfound several themes which highlight benefits and drawbacks of GenAI tools, as\nwell as suggestions to overcome these limitations and ensure that students are\nusing these tools in a responsible and ethical manner."
                },
                "authors": [
                    {
                        "name": "Abhishek Kaushik"
                    },
                    {
                        "name": "Sargam Yadav"
                    },
                    {
                        "name": "Andrew Browne"
                    },
                    {
                        "name": "David Lillis"
                    },
                    {
                        "name": "David Williams"
                    },
                    {
                        "name": "Jack Mc Donnell"
                    },
                    {
                        "name": "Peadar Grant"
                    },
                    {
                        "name": "Siobhan Connolly Kernan"
                    },
                    {
                        "name": "Shubham Sharma"
                    },
                    {
                        "name": "Mansi Arora"
                    }
                ],
                "author_detail": {
                    "name": "Mansi Arora"
                },
                "arxiv_affiliation": "Jagan Institute of Management Studies, Rohini, Delhi, Delhi, India",
                "author": "Mansi Arora",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09636v2",
                "updated": "2025-01-17T11:44:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    11,
                    44,
                    53,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-16T16:25:30Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    16,
                    25,
                    30,
                    3,
                    16,
                    0
                ],
                "title": "LLM-Based Routing in Mixture of Experts: A Novel Framework for Trading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Routing in Mixture of Experts: A Novel Framework for Trading"
                },
                "summary": "Recent advances in deep learning and large language models (LLMs) have\nfacilitated the deployment of the mixture-of-experts (MoE) mechanism in the\nstock investment domain. While these models have demonstrated promising trading\nperformance, they are often unimodal, neglecting the wealth of information\navailable in other modalities, such as textual data. Moreover, the traditional\nneural network-based router selection mechanism fails to consider contextual\nand real-world nuances, resulting in suboptimal expert selection. To address\nthese limitations, we propose LLMoE, a novel framework that employs LLMs as the\nrouter within the MoE architecture. Specifically, we replace the conventional\nneural network-based router with LLMs, leveraging their extensive world\nknowledge and reasoning capabilities to select experts based on historical\nprice data and stock news. This approach provides a more effective and\ninterpretable selection mechanism. Our experiments on multimodal real-world\nstock datasets demonstrate that LLMoE outperforms state-of-the-art MoE models\nand other deep neural network approaches. Additionally, the flexible\narchitecture of LLMoE allows for easy adaptation to various downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in deep learning and large language models (LLMs) have\nfacilitated the deployment of the mixture-of-experts (MoE) mechanism in the\nstock investment domain. While these models have demonstrated promising trading\nperformance, they are often unimodal, neglecting the wealth of information\navailable in other modalities, such as textual data. Moreover, the traditional\nneural network-based router selection mechanism fails to consider contextual\nand real-world nuances, resulting in suboptimal expert selection. To address\nthese limitations, we propose LLMoE, a novel framework that employs LLMs as the\nrouter within the MoE architecture. Specifically, we replace the conventional\nneural network-based router with LLMs, leveraging their extensive world\nknowledge and reasoning capabilities to select experts based on historical\nprice data and stock news. This approach provides a more effective and\ninterpretable selection mechanism. Our experiments on multimodal real-world\nstock datasets demonstrate that LLMoE outperforms state-of-the-art MoE models\nand other deep neural network approaches. Additionally, the flexible\narchitecture of LLMoE allows for easy adaptation to various downstream tasks."
                },
                "authors": [
                    {
                        "name": "Kuan-Ming Liu"
                    },
                    {
                        "name": "Ming-Chih Lo"
                    }
                ],
                "author_detail": {
                    "name": "Ming-Chih Lo"
                },
                "author": "Ming-Chih Lo",
                "arxiv_comment": "Accepted by AAAI 2025 Workshop on AI for Social Impact - Bridging\n  Innovations in Finance, Social Media, and Crime Prevention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10132v1",
                "updated": "2025-01-17T11:41:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    11,
                    41,
                    53,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T11:41:53Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    11,
                    41,
                    53,
                    4,
                    17,
                    0
                ],
                "title": "ComplexFuncBench: Exploring Multi-Step and Constrained Function Calling\n  under Long-Context Scenario",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ComplexFuncBench: Exploring Multi-Step and Constrained Function Calling\n  under Long-Context Scenario"
                },
                "summary": "Enhancing large language models (LLMs) with real-time APIs can help generate\nmore accurate and up-to-date responses. However, evaluating the function\ncalling abilities of LLMs in real-world scenarios remains under-explored due to\nthe complexity of data collection and evaluation. In this work, we introduce\nComplexFuncBench, a benchmark for complex function calling across five\nreal-world scenarios. Compared to existing benchmarks, ComplexFuncBench\nencompasses multi-step and constrained function calling, which requires\nlong-parameter filing, parameter value reasoning, and 128k long context.\nAdditionally, we propose an automatic framework, ComplexEval, for\nquantitatively evaluating complex function calling tasks. Through comprehensive\nexperiments, we demonstrate the deficiencies of state-of-the-art LLMs in\nfunction calling and suggest future directions for optimizing these\ncapabilities. The data and code are available at\n\\url{https://github.com/THUDM/ComplexFuncBench}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing large language models (LLMs) with real-time APIs can help generate\nmore accurate and up-to-date responses. However, evaluating the function\ncalling abilities of LLMs in real-world scenarios remains under-explored due to\nthe complexity of data collection and evaluation. In this work, we introduce\nComplexFuncBench, a benchmark for complex function calling across five\nreal-world scenarios. Compared to existing benchmarks, ComplexFuncBench\nencompasses multi-step and constrained function calling, which requires\nlong-parameter filing, parameter value reasoning, and 128k long context.\nAdditionally, we propose an automatic framework, ComplexEval, for\nquantitatively evaluating complex function calling tasks. Through comprehensive\nexperiments, we demonstrate the deficiencies of state-of-the-art LLMs in\nfunction calling and suggest future directions for optimizing these\ncapabilities. The data and code are available at\n\\url{https://github.com/THUDM/ComplexFuncBench}."
                },
                "authors": [
                    {
                        "name": "Lucen Zhong"
                    },
                    {
                        "name": "Zhengxiao Du"
                    },
                    {
                        "name": "Xiaohan Zhang"
                    },
                    {
                        "name": "Haiyi Hu"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03123v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03123v2",
                "updated": "2025-01-17T11:36:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    11,
                    36,
                    7,
                    4,
                    17,
                    0
                ],
                "published": "2024-07-03T14:06:32Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    14,
                    6,
                    32,
                    2,
                    185,
                    0
                ],
                "title": "Accelerating quantum imaginary-time evolution with random measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating quantum imaginary-time evolution with random measurements"
                },
                "summary": "Quantum imaginary-time evolution (QITE) is a promising tool to prepare\nthermal or ground states of Hamiltonians, as convergence is guaranteed when the\nevolved state overlaps with the ground state. However, its implementation using\na a hybrid quantum/classical approach, where the dynamics of the parameters of\nthe quantum circuit are derived by McLachlan's variational principle is\nimpractical as the number of parameters $m$ increases, since each step in the\nevolution takes $\\Theta(m^2)$ state preparations to calculate the quantum\nFisher information matrix (QFIM). In this work, we accelerate QITE by rapid\nestimation of the QFIM, while conserving the convergence guarantees to the\nextent possible. To this end, we prove that if a parameterized state is rotated\nby a 2-design and measured in the computational basis, then the QFIM can be\ninferred from partial derivative cross correlations of the probability\noutcomes. One sample estimate costs only $\\Theta(m)$ state preparations,\nleading to rapid QFIM estimation when a few samples suffice. The second family\nof estimators take greater liberties and replace QFIMs with averaged classical\nFisher information matrices (CFIMs). In an extreme special case optimized for\nrapid (over accurate) descent, just one CFIM sample is drawn. We justify the\nsecond estimator family by proving rapid descent. Guided by these results, we\npropose the random-measurement imaginary-time evolution (RMITE) algorithm,\nwhich we showcase and test in several molecular systems, with the goal of\npreparing ground states.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum imaginary-time evolution (QITE) is a promising tool to prepare\nthermal or ground states of Hamiltonians, as convergence is guaranteed when the\nevolved state overlaps with the ground state. However, its implementation using\na a hybrid quantum/classical approach, where the dynamics of the parameters of\nthe quantum circuit are derived by McLachlan's variational principle is\nimpractical as the number of parameters $m$ increases, since each step in the\nevolution takes $\\Theta(m^2)$ state preparations to calculate the quantum\nFisher information matrix (QFIM). In this work, we accelerate QITE by rapid\nestimation of the QFIM, while conserving the convergence guarantees to the\nextent possible. To this end, we prove that if a parameterized state is rotated\nby a 2-design and measured in the computational basis, then the QFIM can be\ninferred from partial derivative cross correlations of the probability\noutcomes. One sample estimate costs only $\\Theta(m)$ state preparations,\nleading to rapid QFIM estimation when a few samples suffice. The second family\nof estimators take greater liberties and replace QFIMs with averaged classical\nFisher information matrices (CFIMs). In an extreme special case optimized for\nrapid (over accurate) descent, just one CFIM sample is drawn. We justify the\nsecond estimator family by proving rapid descent. Guided by these results, we\npropose the random-measurement imaginary-time evolution (RMITE) algorithm,\nwhich we showcase and test in several molecular systems, with the goal of\npreparing ground states."
                },
                "authors": [
                    {
                        "name": "Ioannis Kolotouros"
                    },
                    {
                        "name": "David Joseph"
                    },
                    {
                        "name": "Anand Kumar Narayanan"
                    }
                ],
                "author_detail": {
                    "name": "Anand Kumar Narayanan"
                },
                "author": "Anand Kumar Narayanan",
                "arxiv_doi": "10.1103/PhysRevA.111.012424",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevA.111.012424",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.03123v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03123v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 8 figures; v2 published version",
                "arxiv_journal_ref": "Phys. Rev. A 111, 012424 (2025)",
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10124v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10124v1",
                "updated": "2025-01-17T11:27:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    11,
                    27,
                    58,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T11:27:58Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    11,
                    27,
                    58,
                    4,
                    17,
                    0
                ],
                "title": "Gene Regulatory Network Inference in the Presence of Selection Bias and\n  Latent Confounders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gene Regulatory Network Inference in the Presence of Selection Bias and\n  Latent Confounders"
                },
                "summary": "Gene Regulatory Network Inference (GRNI) aims to identify causal\nrelationships among genes using gene expression data, providing insights into\nregulatory mechanisms. A significant yet often overlooked challenge is\nselection bias, a process where only cells meeting specific criteria, such as\ngene expression thresholds, survive or are observed, distorting the true joint\ndistribution of genes and thus biasing GRNI results. Furthermore, gene\nexpression is influenced by latent confounders, such as non-coding RNAs, which\nadd complexity to GRNI. To address these challenges, we propose GISL (Gene\nRegulatory Network Inference in the presence of Selection bias and Latent\nconfounders), a novel algorithm to infer true regulatory relationships in the\npresence of selection and confounding issues. Leveraging data obtained via\nmultiple gene perturbation experiments, we show that the true regulatory\nrelationships, as well as selection processes and latent confounders can be\npartially identified without strong parametric models and under mild graphical\nassumptions. Experimental results on both synthetic and real-world single-cell\ngene expression datasets demonstrate the superiority of GISL over existing\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gene Regulatory Network Inference (GRNI) aims to identify causal\nrelationships among genes using gene expression data, providing insights into\nregulatory mechanisms. A significant yet often overlooked challenge is\nselection bias, a process where only cells meeting specific criteria, such as\ngene expression thresholds, survive or are observed, distorting the true joint\ndistribution of genes and thus biasing GRNI results. Furthermore, gene\nexpression is influenced by latent confounders, such as non-coding RNAs, which\nadd complexity to GRNI. To address these challenges, we propose GISL (Gene\nRegulatory Network Inference in the presence of Selection bias and Latent\nconfounders), a novel algorithm to infer true regulatory relationships in the\npresence of selection and confounding issues. Leveraging data obtained via\nmultiple gene perturbation experiments, we show that the true regulatory\nrelationships, as well as selection processes and latent confounders can be\npartially identified without strong parametric models and under mild graphical\nassumptions. Experimental results on both synthetic and real-world single-cell\ngene expression datasets demonstrate the superiority of GISL over existing\nmethods."
                },
                "authors": [
                    {
                        "name": "Gongxu Luo"
                    },
                    {
                        "name": "Haoyue Dai"
                    },
                    {
                        "name": "Boyang Sun"
                    },
                    {
                        "name": "Loka Li"
                    },
                    {
                        "name": "Biwei Huang"
                    },
                    {
                        "name": "Petar Stojanov"
                    },
                    {
                        "name": "Kun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kun Zhang"
                },
                "author": "Kun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10124v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10124v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.00900v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.00900v3",
                "updated": "2025-01-17T11:18:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    11,
                    18,
                    37,
                    4,
                    17,
                    0
                ],
                "published": "2023-09-02T10:32:53Z",
                "published_parsed": [
                    2023,
                    9,
                    2,
                    10,
                    32,
                    53,
                    5,
                    245,
                    0
                ],
                "title": "Large Process Models: A Vision for Business Process Management in the\n  Age of Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Process Models: A Vision for Business Process Management in the\n  Age of Generative AI"
                },
                "summary": "The continued success of Large Language Models (LLMs) and other generative\nartificial intelligence approaches highlights the advantages that large\ninformation corpora can have over rigidly defined symbolic models, but also\nserves as a proof-point of the challenges that purely statistics-based\napproaches have in terms of safety and trustworthiness. As a framework for\ncontextualizing the potential, as well as the limitations of LLMs and other\nfoundation model-based technologies, we propose the concept of a Large Process\nModel (LPM) that combines the correlation power of LLMs with the analytical\nprecision and reliability of knowledge-based systems and automated reasoning\napproaches. LPMs are envisioned to directly utilize the wealth of process\nmanagement experience that experts have accumulated, as well as process\nperformance data of organizations with diverse characteristics, e.g.,\\\nregarding size, region, or industry. In this vision, the proposed LPM would\nallow organizations to receive context-specific (tailored) process and other\nbusiness models, analytical deep-dives, and improvement recommendations. As\nsuch, they would allow to substantially decrease the time and effort required\nfor business transformation, while also allowing for deeper, more impactful,\nand more actionable insights than previously possible. We argue that\nimplementing an LPM is feasible, but also highlight limitations and research\nchallenges that need to be solved to implement particular aspects of the LPM\nvision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The continued success of Large Language Models (LLMs) and other generative\nartificial intelligence approaches highlights the advantages that large\ninformation corpora can have over rigidly defined symbolic models, but also\nserves as a proof-point of the challenges that purely statistics-based\napproaches have in terms of safety and trustworthiness. As a framework for\ncontextualizing the potential, as well as the limitations of LLMs and other\nfoundation model-based technologies, we propose the concept of a Large Process\nModel (LPM) that combines the correlation power of LLMs with the analytical\nprecision and reliability of knowledge-based systems and automated reasoning\napproaches. LPMs are envisioned to directly utilize the wealth of process\nmanagement experience that experts have accumulated, as well as process\nperformance data of organizations with diverse characteristics, e.g.,\\\nregarding size, region, or industry. In this vision, the proposed LPM would\nallow organizations to receive context-specific (tailored) process and other\nbusiness models, analytical deep-dives, and improvement recommendations. As\nsuch, they would allow to substantially decrease the time and effort required\nfor business transformation, while also allowing for deeper, more impactful,\nand more actionable insights than previously possible. We argue that\nimplementing an LPM is feasible, but also highlight limitations and research\nchallenges that need to be solved to implement particular aspects of the LPM\nvision."
                },
                "authors": [
                    {
                        "name": "Timotheus Kampik"
                    },
                    {
                        "name": "Christian Warmuth"
                    },
                    {
                        "name": "Adrian Rebmann"
                    },
                    {
                        "name": "Ron Agam"
                    },
                    {
                        "name": "Lukas N. P. Egger"
                    },
                    {
                        "name": "Andreas Gerber"
                    },
                    {
                        "name": "Johannes Hoffart"
                    },
                    {
                        "name": "Jonas Kolk"
                    },
                    {
                        "name": "Philipp Herzig"
                    },
                    {
                        "name": "Gero Decker"
                    },
                    {
                        "name": "Han van der Aa"
                    },
                    {
                        "name": "Artem Polyvyanyy"
                    },
                    {
                        "name": "Stefanie Rinderle-Ma"
                    },
                    {
                        "name": "Ingo Weber"
                    },
                    {
                        "name": "Matthias Weidlich"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Weidlich"
                },
                "author": "Matthias Weidlich",
                "arxiv_doi": "10.1007/s13218-024-00863-8",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s13218-024-00863-8",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.00900v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.00900v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "K\\\"unstl Intell (2024)",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10120v1",
                "updated": "2025-01-17T11:12:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    11,
                    12,
                    28,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T11:12:28Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    11,
                    12,
                    28,
                    4,
                    17,
                    0
                ],
                "title": "PaSa: An LLM Agent for Comprehensive Academic Paper Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PaSa: An LLM Agent for Comprehensive Academic Paper Search"
                },
                "summary": "We introduce PaSa, an advanced Paper Search agent powered by large language\nmodels. PaSa can autonomously make a series of decisions, including invoking\nsearch tools, reading papers, and selecting relevant references, to ultimately\nobtain comprehensive and accurate results for complex scholarly queries. We\noptimize PaSa using reinforcement learning with a synthetic dataset,\nAutoScholarQuery, which includes 35k fine-grained academic queries and\ncorresponding papers sourced from top-tier AI conference publications.\nAdditionally, we develop RealScholarQuery, a benchmark collecting real-world\nacademic queries to assess PaSa performance in more realistic scenarios.\nDespite being trained on synthetic data, PaSa significantly outperforms\nexisting baselines on RealScholarQuery, including Google, Google Scholar,\nGoogle with GPT-4 for paraphrased queries, chatGPT (search-enabled GPT-4o),\nGPT-o1, and PaSa-GPT-4o (PaSa implemented by prompting GPT-4o). Notably,\nPaSa-7B surpasses the best Google-based baseline, Google with GPT-4o, by 37.78%\nin recall@20 and 39.90% in recall@50. It also exceeds PaSa-GPT-4o by 30.36% in\nrecall and 4.25% in precision. Model, datasets, and code are available at\nhttps://github.com/bytedance/pasa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce PaSa, an advanced Paper Search agent powered by large language\nmodels. PaSa can autonomously make a series of decisions, including invoking\nsearch tools, reading papers, and selecting relevant references, to ultimately\nobtain comprehensive and accurate results for complex scholarly queries. We\noptimize PaSa using reinforcement learning with a synthetic dataset,\nAutoScholarQuery, which includes 35k fine-grained academic queries and\ncorresponding papers sourced from top-tier AI conference publications.\nAdditionally, we develop RealScholarQuery, a benchmark collecting real-world\nacademic queries to assess PaSa performance in more realistic scenarios.\nDespite being trained on synthetic data, PaSa significantly outperforms\nexisting baselines on RealScholarQuery, including Google, Google Scholar,\nGoogle with GPT-4 for paraphrased queries, chatGPT (search-enabled GPT-4o),\nGPT-o1, and PaSa-GPT-4o (PaSa implemented by prompting GPT-4o). Notably,\nPaSa-7B surpasses the best Google-based baseline, Google with GPT-4o, by 37.78%\nin recall@20 and 39.90% in recall@50. It also exceeds PaSa-GPT-4o by 30.36% in\nrecall and 4.25% in precision. Model, datasets, and code are available at\nhttps://github.com/bytedance/pasa."
                },
                "authors": [
                    {
                        "name": "Yichen He"
                    },
                    {
                        "name": "Guanhua Huang"
                    },
                    {
                        "name": "Peiyuan Feng"
                    },
                    {
                        "name": "Yuan Lin"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Weinan E"
                    }
                ],
                "author_detail": {
                    "name": "Weinan E"
                },
                "author": "Weinan E",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12750v2",
                "updated": "2025-01-17T11:10:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    11,
                    10,
                    5,
                    4,
                    17,
                    0
                ],
                "published": "2024-05-21T13:02:27Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    13,
                    2,
                    27,
                    1,
                    142,
                    0
                ],
                "title": "Generative AI in Cybersecurity: A Comprehensive Review of LLM\n  Applications and Vulnerabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI in Cybersecurity: A Comprehensive Review of LLM\n  Applications and Vulnerabilities"
                },
                "summary": "This paper provides a comprehensive review of the future of cybersecurity\nthrough Generative AI and Large Language Models (LLMs). We explore LLM\napplications across various domains, including hardware design security,\nintrusion detection, software engineering, design verification, cyber threat\nintelligence, malware detection, and phishing detection. We present an overview\nof LLM evolution and its current state, focusing on advancements in models such\nas GPT-4, GPT-3.5, Mixtral-8x7B, BERT, Falcon2, and LLaMA. Our analysis extends\nto LLM vulnerabilities, such as prompt injection, insecure output handling,\ndata poisoning, DDoS attacks, and adversarial instructions. We delve into\nmitigation strategies to protect these models, providing a comprehensive look\nat potential attack scenarios and prevention techniques. Furthermore, we\nevaluate the performance of 42 LLM models in cybersecurity knowledge and\nhardware security, highlighting their strengths and weaknesses. We thoroughly\nevaluate cybersecurity datasets for LLM training and testing, covering the\nlifecycle from data creation to usage and identifying gaps for future research.\nIn addition, we review new strategies for leveraging LLMs, including techniques\nlike Half-Quadratic Quantization (HQQ), Reinforcement Learning with Human\nFeedback (RLHF), Direct Preference Optimization (DPO), Quantized Low-Rank\nAdapters (QLoRA), and Retrieval-Augmented Generation (RAG). These insights aim\nto enhance real-time cybersecurity defenses and improve the sophistication of\nLLM applications in threat detection and response. Our paper provides a\nfoundational understanding and strategic direction for integrating LLMs into\nfuture cybersecurity frameworks, emphasizing innovation and robust model\ndeployment to safeguard against evolving cyber threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides a comprehensive review of the future of cybersecurity\nthrough Generative AI and Large Language Models (LLMs). We explore LLM\napplications across various domains, including hardware design security,\nintrusion detection, software engineering, design verification, cyber threat\nintelligence, malware detection, and phishing detection. We present an overview\nof LLM evolution and its current state, focusing on advancements in models such\nas GPT-4, GPT-3.5, Mixtral-8x7B, BERT, Falcon2, and LLaMA. Our analysis extends\nto LLM vulnerabilities, such as prompt injection, insecure output handling,\ndata poisoning, DDoS attacks, and adversarial instructions. We delve into\nmitigation strategies to protect these models, providing a comprehensive look\nat potential attack scenarios and prevention techniques. Furthermore, we\nevaluate the performance of 42 LLM models in cybersecurity knowledge and\nhardware security, highlighting their strengths and weaknesses. We thoroughly\nevaluate cybersecurity datasets for LLM training and testing, covering the\nlifecycle from data creation to usage and identifying gaps for future research.\nIn addition, we review new strategies for leveraging LLMs, including techniques\nlike Half-Quadratic Quantization (HQQ), Reinforcement Learning with Human\nFeedback (RLHF), Direct Preference Optimization (DPO), Quantized Low-Rank\nAdapters (QLoRA), and Retrieval-Augmented Generation (RAG). These insights aim\nto enhance real-time cybersecurity defenses and improve the sophistication of\nLLM applications in threat detection and response. Our paper provides a\nfoundational understanding and strategic direction for integrating LLMs into\nfuture cybersecurity frameworks, emphasizing innovation and robust model\ndeployment to safeguard against evolving cyber threats."
                },
                "authors": [
                    {
                        "name": "Mohamed Amine Ferrag"
                    },
                    {
                        "name": "Fatima Alwahedi"
                    },
                    {
                        "name": "Ammar Battah"
                    },
                    {
                        "name": "Bilel Cherif"
                    },
                    {
                        "name": "Abdechakour Mechri"
                    },
                    {
                        "name": "Norbert Tihanyi"
                    },
                    {
                        "name": "Tamas Bisztray"
                    },
                    {
                        "name": "Merouane Debbah"
                    }
                ],
                "author_detail": {
                    "name": "Merouane Debbah"
                },
                "author": "Merouane Debbah",
                "arxiv_comment": "52 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10117v1",
                "updated": "2025-01-17T11:05:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    11,
                    5,
                    42,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T11:05:42Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    11,
                    5,
                    42,
                    4,
                    17,
                    0
                ],
                "title": "Prediction Sets and Conformal Inference with Censored Outcomes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prediction Sets and Conformal Inference with Censored Outcomes"
                },
                "summary": "Given data on a scalar random variable $Y$, a prediction set for $Y$ with\nmiscoverage level $\\alpha$ is a set of values for $Y$ that contains a randomly\ndrawn $Y$ with probability $1 - \\alpha$, where $\\alpha \\in (0,1)$. Among all\nprediction sets that satisfy this coverage property, the oracle prediction set\nis the one with the smallest volume. This paper provides estimation methods of\nsuch prediction sets given observed conditioning covariates when $Y$ is\ncensored or measured in intervals. We first characterise the oracle prediction\nset under interval censoring and develop a consistent estimator for the\nshortest prediction interval that satisfies this coverage property. We then\nextend these consistency results to accommodate cases where the prediction set\nconsists of multiple disjoint intervals. Second, we use conformal inference to\nconstruct a prediction set that achieves a particular notion of finite-sample\nvalidity under censoring and maintains consistency as sample size increases.\nThis notion exploits exchangeability to obtain finite sample guarantees on\ncoverage using a specially constructed conformity score function. The procedure\naccomodates the prediction uncertainty that is irreducible (due to the\nstochastic nature of outcomes), the modelling uncertainty due to partial\nidentification and also sampling uncertainty that gets reduced as samples get\nlarger. We conduct a set of Monte Carlo simulations and an application to data\nfrom the Current Population Survey. The results highlight the robustness and\nefficiency of the proposed methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given data on a scalar random variable $Y$, a prediction set for $Y$ with\nmiscoverage level $\\alpha$ is a set of values for $Y$ that contains a randomly\ndrawn $Y$ with probability $1 - \\alpha$, where $\\alpha \\in (0,1)$. Among all\nprediction sets that satisfy this coverage property, the oracle prediction set\nis the one with the smallest volume. This paper provides estimation methods of\nsuch prediction sets given observed conditioning covariates when $Y$ is\ncensored or measured in intervals. We first characterise the oracle prediction\nset under interval censoring and develop a consistent estimator for the\nshortest prediction interval that satisfies this coverage property. We then\nextend these consistency results to accommodate cases where the prediction set\nconsists of multiple disjoint intervals. Second, we use conformal inference to\nconstruct a prediction set that achieves a particular notion of finite-sample\nvalidity under censoring and maintains consistency as sample size increases.\nThis notion exploits exchangeability to obtain finite sample guarantees on\ncoverage using a specially constructed conformity score function. The procedure\naccomodates the prediction uncertainty that is irreducible (due to the\nstochastic nature of outcomes), the modelling uncertainty due to partial\nidentification and also sampling uncertainty that gets reduced as samples get\nlarger. We conduct a set of Monte Carlo simulations and an application to data\nfrom the Current Population Survey. The results highlight the robustness and\nefficiency of the proposed methods."
                },
                "authors": [
                    {
                        "name": "Weiguang Liu"
                    },
                    {
                        "name": "ureo de Paula"
                    },
                    {
                        "name": "Elie Tamer"
                    }
                ],
                "author_detail": {
                    "name": "Elie Tamer"
                },
                "author": "Elie Tamer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10106v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10106v1",
                "updated": "2025-01-17T10:47:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    10,
                    47,
                    11,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T10:47:11Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    10,
                    47,
                    11,
                    4,
                    17,
                    0
                ],
                "title": "LLM Reasoner and Automated Planner: A new NPC approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Reasoner and Automated Planner: A new NPC approach"
                },
                "summary": "In domains requiring intelligent agents to emulate plausible human-like\nbehaviour, such as formative simulations, traditional techniques like behaviour\ntrees encounter significant challenges. Large Language Models (LLMs), despite\nnot always yielding optimal solutions, usually offer plausible and human-like\nresponses to a given problem. In this paper, we exploit this capability and\npropose a novel architecture that integrates an LLM for decision-making with a\nclassical automated planner that can generate sound plans for that decision.\nThe combination aims to equip an agent with the ability to make decisions in\nvarious situations, even if they were not anticipated during the design phase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In domains requiring intelligent agents to emulate plausible human-like\nbehaviour, such as formative simulations, traditional techniques like behaviour\ntrees encounter significant challenges. Large Language Models (LLMs), despite\nnot always yielding optimal solutions, usually offer plausible and human-like\nresponses to a given problem. In this paper, we exploit this capability and\npropose a novel architecture that integrates an LLM for decision-making with a\nclassical automated planner that can generate sound plans for that decision.\nThe combination aims to equip an agent with the ability to make decisions in\nvarious situations, even if they were not anticipated during the design phase."
                },
                "authors": [
                    {
                        "name": "Israel Puerta-Merino"
                    },
                    {
                        "name": "Jordi Sabater-Mir"
                    }
                ],
                "author_detail": {
                    "name": "Jordi Sabater-Mir"
                },
                "author": "Jordi Sabater-Mir",
                "arxiv_doi": "10.3233/FAIA240443",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3233/FAIA240443",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.10106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10106v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 7 figures, extended version of the homonymous paper\n  submitted to the Catalan Conference on Artificial Intelligent (CCIA) 2025",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04520v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04520v2",
                "updated": "2025-01-17T10:38:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    10,
                    38,
                    0,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-08T14:13:12Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    14,
                    13,
                    12,
                    2,
                    8,
                    0
                ],
                "title": "Inferring resource competition in microbial communities from time series",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring resource competition in microbial communities from time series"
                },
                "summary": "The competition for resources is a defining feature of microbial communities.\nIn many contexts, from soils to host-associated communities, highly diverse\nmicrobes are organized into metabolic groups or guilds with similar resource\npreferences. The resource preferences of individual taxa that give rise to\nthese guilds are critical for understanding fluxes of resources through the\ncommunity and the structure of diversity in the system. However, inferring the\nmetabolic capabilities of individual taxa, and their competition with other\ntaxa, within a community is challenging and unresolved. Here we address this\ngap in knowledge by leveraging dynamic measurements of abundances in\ncommunities. We show that simple correlations are often misleading in\npredicting resource competition. We show that spectral methods such as the\ncross-power spectral density (CPSD) and coherence that account for time-delayed\neffects are superior metrics for inferring the structure of resource\ncompetition in communities. We first demonstrate this fact on synthetic data\ngenerated from consumer-resource models with time-dependent resource\navailability, where taxa are organized into groups or guilds with similar\nresource preferences. By applying spectral methods to oceanic plankton\ntime-series data, we demonstrate that these methods detect interaction\nstructures among species with similar genomic sequences. Our results indicate\nthat analyzing temporal data across multiple timescales can reveal the\nunderlying structure of resource competition within communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The competition for resources is a defining feature of microbial communities.\nIn many contexts, from soils to host-associated communities, highly diverse\nmicrobes are organized into metabolic groups or guilds with similar resource\npreferences. The resource preferences of individual taxa that give rise to\nthese guilds are critical for understanding fluxes of resources through the\ncommunity and the structure of diversity in the system. However, inferring the\nmetabolic capabilities of individual taxa, and their competition with other\ntaxa, within a community is challenging and unresolved. Here we address this\ngap in knowledge by leveraging dynamic measurements of abundances in\ncommunities. We show that simple correlations are often misleading in\npredicting resource competition. We show that spectral methods such as the\ncross-power spectral density (CPSD) and coherence that account for time-delayed\neffects are superior metrics for inferring the structure of resource\ncompetition in communities. We first demonstrate this fact on synthetic data\ngenerated from consumer-resource models with time-dependent resource\navailability, where taxa are organized into groups or guilds with similar\nresource preferences. By applying spectral methods to oceanic plankton\ntime-series data, we demonstrate that these methods detect interaction\nstructures among species with similar genomic sequences. Our results indicate\nthat analyzing temporal data across multiple timescales can reveal the\nunderlying structure of resource competition within communities."
                },
                "authors": [
                    {
                        "name": "Xiaowen Chen"
                    },
                    {
                        "name": "Kyle Crocker"
                    },
                    {
                        "name": "Seppe Kuehn"
                    },
                    {
                        "name": "Aleksandra M. Walczak"
                    },
                    {
                        "name": "Thierry Mora"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Mora"
                },
                "author": "Thierry Mora",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04520v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04520v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10088v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10088v1",
                "updated": "2025-01-17T10:15:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    10,
                    15,
                    3,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T10:15:03Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    10,
                    15,
                    3,
                    4,
                    17,
                    0
                ],
                "title": "A recursive Bayesian neural network for constitutive modeling of sands\n  under monotonic loading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A recursive Bayesian neural network for constitutive modeling of sands\n  under monotonic loading"
                },
                "summary": "In geotechnical engineering, constitutive models play a crucial role in\ndescribing soil behavior under varying loading conditions. Data-driven deep\nlearning (DL) models offer a promising alternative for developing predictive\nconstitutive models. When prediction is the primary focus, quantifying the\npredictive uncertainty of a trained DL model and communicating this uncertainty\nto end users is crucial for informed decision-making.\n  This study proposes a recursive Bayesian neural network (rBNN) framework,\nwhich builds upon recursive feedforward neural networks (rFFNNs) by introducing\ngeneralized Bayesian inference for uncertainty quantification. A significant\ncontribution of this work is the incorporation of a sliding window approach in\nrFFNNs, allowing the models to effectively capture temporal dependencies across\nload steps. The rBNN extends this framework by treating model parameters as\nrandom variables, with their posterior distributions inferred using generalized\nvariational inference.\n  The proposed framework is validated on two datasets: (i) a numerically\nsimulated consolidated drained (CD) triaxial dataset employing a hardening soil\nmodel and (ii) an experimental dataset comprising 28 CD triaxial tests on\nBaskarp sand. Comparative analyses with LSTM, Bi-LSTM, and GRU models\ndemonstrate that the deterministic rFFNN achieves superior predictive accuracy,\nattributed to its transparent structure and sliding window design. While the\nrBNN marginally trails in accuracy for the experimental case, it provides\nrobust confidence intervals, addressing data sparsity and measurement noise in\nexperimental conditions. The study underscores the trade-offs between\ndeterministic and probabilistic approaches and the potential of rBNNs for\nuncertainty-aware constitutive modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In geotechnical engineering, constitutive models play a crucial role in\ndescribing soil behavior under varying loading conditions. Data-driven deep\nlearning (DL) models offer a promising alternative for developing predictive\nconstitutive models. When prediction is the primary focus, quantifying the\npredictive uncertainty of a trained DL model and communicating this uncertainty\nto end users is crucial for informed decision-making.\n  This study proposes a recursive Bayesian neural network (rBNN) framework,\nwhich builds upon recursive feedforward neural networks (rFFNNs) by introducing\ngeneralized Bayesian inference for uncertainty quantification. A significant\ncontribution of this work is the incorporation of a sliding window approach in\nrFFNNs, allowing the models to effectively capture temporal dependencies across\nload steps. The rBNN extends this framework by treating model parameters as\nrandom variables, with their posterior distributions inferred using generalized\nvariational inference.\n  The proposed framework is validated on two datasets: (i) a numerically\nsimulated consolidated drained (CD) triaxial dataset employing a hardening soil\nmodel and (ii) an experimental dataset comprising 28 CD triaxial tests on\nBaskarp sand. Comparative analyses with LSTM, Bi-LSTM, and GRU models\ndemonstrate that the deterministic rFFNN achieves superior predictive accuracy,\nattributed to its transparent structure and sliding window design. While the\nrBNN marginally trails in accuracy for the experimental case, it provides\nrobust confidence intervals, addressing data sparsity and measurement noise in\nexperimental conditions. The study underscores the trade-offs between\ndeterministic and probabilistic approaches and the potential of rBNNs for\nuncertainty-aware constitutive modeling."
                },
                "authors": [
                    {
                        "name": "Toiba Noor"
                    },
                    {
                        "name": "Soban Nasir Lone"
                    },
                    {
                        "name": "G. V. Ramana"
                    },
                    {
                        "name": "Rajdip Nayek"
                    }
                ],
                "author_detail": {
                    "name": "Rajdip Nayek"
                },
                "author": "Rajdip Nayek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10088v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10088v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07548v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07548v2",
                "updated": "2025-01-17T09:42:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    42,
                    59,
                    4,
                    17,
                    0
                ],
                "published": "2024-12-10T14:39:51Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    14,
                    39,
                    51,
                    1,
                    345,
                    0
                ],
                "title": "Automatic Database Configuration Debugging using Retrieval-Augmented\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Database Configuration Debugging using Retrieval-Augmented\n  Language Models"
                },
                "summary": "Database management system (DBMS) configuration debugging, e.g., diagnosing\npoorly configured DBMS knobs and generating troubleshooting recommendations, is\ncrucial in optimizing DBMS performance. However, the configuration debugging\nprocess is tedious and, sometimes challenging, even for seasoned database\nadministrators (DBAs) with sufficient experience in DBMS configurations and\ngood understandings of the DBMS internals (e.g., MySQL or Oracle). To address\nthis difficulty, we propose Andromeda, a framework that utilizes large language\nmodels (LLMs) to enable automatic DBMS configuration debugging. Andromeda\nserves as a natural surrogate of DBAs to answer a wide range of natural\nlanguage (NL) questions on DBMS configuration issues, and to generate\ndiagnostic suggestions to fix these issues. Nevertheless, directly prompting\nLLMs with these professional questions may result in overly generic and often\nunsatisfying answers. To this end, we propose a retrieval-augmented generation\n(RAG) strategy that effectively provides matched domain-specific contexts for\nthe question from multiple sources. They come from related historical\nquestions, troubleshooting manuals and DBMS telemetries, which significantly\nimprove the performance of configuration debugging. To support the RAG\nstrategy, we develop a document retrieval mechanism addressing heterogeneous\ndocuments and design an effective method for telemetry analysis. Extensive\nexperiments on real-world DBMS configuration debugging datasets show that\nAndromeda significantly outperforms existing solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Database management system (DBMS) configuration debugging, e.g., diagnosing\npoorly configured DBMS knobs and generating troubleshooting recommendations, is\ncrucial in optimizing DBMS performance. However, the configuration debugging\nprocess is tedious and, sometimes challenging, even for seasoned database\nadministrators (DBAs) with sufficient experience in DBMS configurations and\ngood understandings of the DBMS internals (e.g., MySQL or Oracle). To address\nthis difficulty, we propose Andromeda, a framework that utilizes large language\nmodels (LLMs) to enable automatic DBMS configuration debugging. Andromeda\nserves as a natural surrogate of DBAs to answer a wide range of natural\nlanguage (NL) questions on DBMS configuration issues, and to generate\ndiagnostic suggestions to fix these issues. Nevertheless, directly prompting\nLLMs with these professional questions may result in overly generic and often\nunsatisfying answers. To this end, we propose a retrieval-augmented generation\n(RAG) strategy that effectively provides matched domain-specific contexts for\nthe question from multiple sources. They come from related historical\nquestions, troubleshooting manuals and DBMS telemetries, which significantly\nimprove the performance of configuration debugging. To support the RAG\nstrategy, we develop a document retrieval mechanism addressing heterogeneous\ndocuments and design an effective method for telemetry analysis. Extensive\nexperiments on real-world DBMS configuration debugging datasets show that\nAndromeda significantly outperforms existing solutions."
                },
                "authors": [
                    {
                        "name": "Sibei Chen"
                    },
                    {
                        "name": "Ju Fan"
                    },
                    {
                        "name": "Bin Wu"
                    },
                    {
                        "name": "Nan Tang"
                    },
                    {
                        "name": "Chao Deng"
                    },
                    {
                        "name": "Pengyi Wang"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Jian Tan"
                    },
                    {
                        "name": "Feifei Li"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Xiaoyong Du"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyong Du"
                },
                "author": "Xiaoyong Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07548v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07548v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10069v1",
                "updated": "2025-01-17T09:42:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    42,
                    48,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T09:42:48Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    42,
                    48,
                    4,
                    17,
                    0
                ],
                "title": "A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling,\n  Search Algorithms, and Relevant Frameworks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling,\n  Search Algorithms, and Relevant Frameworks"
                },
                "summary": "LLM test-time compute (or LLM inference) via search has emerged as a\npromising research area with rapid developments. However, current frameworks\noften adopt distinct perspectives on three key aspects (task definition, LLM\nprofiling, and search procedures), making direct comparisons challenging.\nMoreover, the search algorithms employed often diverge from standard\nimplementations, and their specific characteristics are not thoroughly\nspecified. In this survey, we provide a comprehensive technical review that\nunifies task definitions and provides modular definitions of LLM profiling and\nsearch procedures. The definitions enable precise comparisons of various LLM\ninference frameworks while highlighting their departures from conventional\nsearch algorithms. We also discuss the applicability, performance, and\nefficiency of these methods. For further details and ongoing updates, please\nrefer to our GitHub repository:\nhttps://github.com/xinzhel/LLM-Agent-Survey/blob/main/search.md",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM test-time compute (or LLM inference) via search has emerged as a\npromising research area with rapid developments. However, current frameworks\noften adopt distinct perspectives on three key aspects (task definition, LLM\nprofiling, and search procedures), making direct comparisons challenging.\nMoreover, the search algorithms employed often diverge from standard\nimplementations, and their specific characteristics are not thoroughly\nspecified. In this survey, we provide a comprehensive technical review that\nunifies task definitions and provides modular definitions of LLM profiling and\nsearch procedures. The definitions enable precise comparisons of various LLM\ninference frameworks while highlighting their departures from conventional\nsearch algorithms. We also discuss the applicability, performance, and\nefficiency of these methods. For further details and ongoing updates, please\nrefer to our GitHub repository:\nhttps://github.com/xinzhel/LLM-Agent-Survey/blob/main/search.md"
                },
                "authors": [
                    {
                        "name": "Xinzhe Li"
                    }
                ],
                "author_detail": {
                    "name": "Xinzhe Li"
                },
                "author": "Xinzhe Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07124v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07124v3",
                "updated": "2025-01-17T09:39:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    39,
                    17,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-13T08:26:43Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    8,
                    26,
                    43,
                    0,
                    13,
                    0
                ],
                "title": "LLM360 K2: Building a 65B 360-Open-Source Large Language Model from\n  Scratch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM360 K2: Building a 65B 360-Open-Source Large Language Model from\n  Scratch"
                },
                "summary": "We detail the training of the LLM360 K2-65B model, scaling up our 360-degree\nOPEN SOURCE approach to the largest and most powerful models under project\nLLM360. While open-source LLMs continue to advance, the answer to \"How are the\nlargest LLMs trained?\" remains unclear within the community. The implementation\ndetails for such high-capacity models are often protected due to business\nconsiderations associated with their high cost. This lack of transparency\nprevents LLM researchers from leveraging valuable insights from prior\nexperience, e.g., \"What are the best practices for addressing loss spikes?\" The\nLLM360 K2 project addresses this gap by providing full transparency and access\nto resources accumulated during the training of LLMs at the largest scale. This\nreport highlights key elements of the K2 project, including our first model, K2\nDIAMOND, a 65 billion-parameter LLM that surpasses LLaMA-65B and rivals\nLLaMA2-70B, while requiring fewer FLOPs and tokens. We detail the\nimplementation steps and present a longitudinal analysis of K2 DIAMOND's\ncapabilities throughout its training process. We also outline ongoing projects\nsuch as TXT360, setting the stage for future models in the series. By offering\npreviously unavailable resources, the K2 project also resonates with the\n360-degree OPEN SOURCE principles of transparency, reproducibility, and\naccessibility, which we believe are vital in the era of resource-intensive AI\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We detail the training of the LLM360 K2-65B model, scaling up our 360-degree\nOPEN SOURCE approach to the largest and most powerful models under project\nLLM360. While open-source LLMs continue to advance, the answer to \"How are the\nlargest LLMs trained?\" remains unclear within the community. The implementation\ndetails for such high-capacity models are often protected due to business\nconsiderations associated with their high cost. This lack of transparency\nprevents LLM researchers from leveraging valuable insights from prior\nexperience, e.g., \"What are the best practices for addressing loss spikes?\" The\nLLM360 K2 project addresses this gap by providing full transparency and access\nto resources accumulated during the training of LLMs at the largest scale. This\nreport highlights key elements of the K2 project, including our first model, K2\nDIAMOND, a 65 billion-parameter LLM that surpasses LLaMA-65B and rivals\nLLaMA2-70B, while requiring fewer FLOPs and tokens. We detail the\nimplementation steps and present a longitudinal analysis of K2 DIAMOND's\ncapabilities throughout its training process. We also outline ongoing projects\nsuch as TXT360, setting the stage for future models in the series. By offering\npreviously unavailable resources, the K2 project also resonates with the\n360-degree OPEN SOURCE principles of transparency, reproducibility, and\naccessibility, which we believe are vital in the era of resource-intensive AI\nresearch."
                },
                "authors": [
                    {
                        "name": "Zhengzhong Liu"
                    },
                    {
                        "name": "Bowen Tan"
                    },
                    {
                        "name": "Hongyi Wang"
                    },
                    {
                        "name": "Willie Neiswanger"
                    },
                    {
                        "name": "Tianhua Tao"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Fajri Koto"
                    },
                    {
                        "name": "Yuqi Wang"
                    },
                    {
                        "name": "Suqi Sun"
                    },
                    {
                        "name": "Omkar Pangarkar"
                    },
                    {
                        "name": "Richard Fan"
                    },
                    {
                        "name": "Yi Gu"
                    },
                    {
                        "name": "Victor Miller"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Liping Tang"
                    },
                    {
                        "name": "Nikhil Ranjan"
                    },
                    {
                        "name": "Yonghao Zhuang"
                    },
                    {
                        "name": "Guowei He"
                    },
                    {
                        "name": "Renxi Wang"
                    },
                    {
                        "name": "Mingkai Deng"
                    },
                    {
                        "name": "Robin Algayres"
                    },
                    {
                        "name": "Yuanzhi Li"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Eric Xing"
                    }
                ],
                "author_detail": {
                    "name": "Eric Xing"
                },
                "author": "Eric Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07124v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07124v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03594v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03594v2",
                "updated": "2025-01-17T09:37:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    37,
                    36,
                    4,
                    17,
                    0
                ],
                "published": "2024-11-29T05:57:37Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    57,
                    37,
                    4,
                    334,
                    0
                ],
                "title": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching"
                },
                "summary": "Large language models (LLMs) increasingly play an important role in a wide\nrange of information processing and management tasks. Many of these tasks are\nperformed in large batches or even offline, and the performance indictor for\nwhich is throughput. These tasks usually show the characteristic of prefix\nsharing, where different prompt input can partially show the common prefix.\nHowever, the existing LLM inference engines tend to optimize the streaming\nrequests and show limitations of supporting the large batched tasks with the\nprefix sharing characteristic. The existing solutions use the LRU-based cache\nto reuse the KV context of common prefix between requests. The KV context that\nare about to be reused may prematurely evicted with the implicit cache\nmanagement. Besides, the streaming oriented systems do not leverage the\nrequest-batch information and can not mix the decoding tokens with the prefill\nchunks to the best for the batched scenarios, and thus fails to saturate the\nGPU. We propose BatchLLM to address the above problems. BatchLLM explicitly\nidentifies the common prefixes globally. The requests sharing the same prefix\nwill be scheduled together to reuse the KV context the best. BatchLLM reorders\nthe requests and schedules the requests with larger ratio of decoding first to\nbetter mix the decoding tokens with the latter prefill chunks, and applies\nmemory-centric token batching to enlarge the token-batch sizes, which helps to\nincrease the GPU utilization. Finally, BatchLLM optimizes the prefix-shared\nAttention kernel with horizontal fusion to reduce tail effect and kernel launch\noverhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang\nby 1.3$\\times$ to 10.8$\\times$ on a set of microbenchmarks and a typical\nindustry workload under different hardware environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly play an important role in a wide\nrange of information processing and management tasks. Many of these tasks are\nperformed in large batches or even offline, and the performance indictor for\nwhich is throughput. These tasks usually show the characteristic of prefix\nsharing, where different prompt input can partially show the common prefix.\nHowever, the existing LLM inference engines tend to optimize the streaming\nrequests and show limitations of supporting the large batched tasks with the\nprefix sharing characteristic. The existing solutions use the LRU-based cache\nto reuse the KV context of common prefix between requests. The KV context that\nare about to be reused may prematurely evicted with the implicit cache\nmanagement. Besides, the streaming oriented systems do not leverage the\nrequest-batch information and can not mix the decoding tokens with the prefill\nchunks to the best for the batched scenarios, and thus fails to saturate the\nGPU. We propose BatchLLM to address the above problems. BatchLLM explicitly\nidentifies the common prefixes globally. The requests sharing the same prefix\nwill be scheduled together to reuse the KV context the best. BatchLLM reorders\nthe requests and schedules the requests with larger ratio of decoding first to\nbetter mix the decoding tokens with the latter prefill chunks, and applies\nmemory-centric token batching to enlarge the token-batch sizes, which helps to\nincrease the GPU utilization. Finally, BatchLLM optimizes the prefix-shared\nAttention kernel with horizontal fusion to reduce tail effect and kernel launch\noverhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang\nby 1.3$\\times$ to 10.8$\\times$ on a set of microbenchmarks and a typical\nindustry workload under different hardware environments."
                },
                "authors": [
                    {
                        "name": "Zhen Zheng"
                    },
                    {
                        "name": "Xin Ji"
                    },
                    {
                        "name": "Taosong Fang"
                    },
                    {
                        "name": "Fanghao Zhou"
                    },
                    {
                        "name": "Chuanjie Liu"
                    },
                    {
                        "name": "Gang Peng"
                    }
                ],
                "author_detail": {
                    "name": "Gang Peng"
                },
                "author": "Gang Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03594v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.20262v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.20262v3",
                "updated": "2025-01-17T09:32:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    32,
                    54,
                    4,
                    17,
                    0
                ],
                "published": "2024-03-29T16:13:31Z",
                "published_parsed": [
                    2024,
                    3,
                    29,
                    16,
                    13,
                    31,
                    4,
                    89,
                    0
                ],
                "title": "ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language\n  Models"
                },
                "summary": "Research on Large Language Models (LLMs) has recently witnessed an increasing\ninterest in extending the models' context size to better capture dependencies\nwithin long documents. While benchmarks have been proposed to assess long-range\nabilities, existing efforts primarily considered generic tasks that are not\nnecessarily aligned with real-world applications. In contrast, we propose a new\nbenchmark for long-context LLMs focused on a practical meeting assistant\nscenario in which the long contexts consist of transcripts obtained by\nautomatic speech recognition, presenting unique challenges for LLMs due to the\ninherent noisiness and oral nature of such data. Our benchmark, ELITR-Bench,\naugments the existing ELITR corpus by adding 271 manually crafted questions\nwith their ground-truth answers, as well as noisy versions of meeting\ntranscripts altered to target different Word Error Rate levels. Our experiments\nwith 12 long-context LLMs on ELITR-Bench confirm the progress made across\nsuccessive generations of both proprietary and open models, and point out their\ndiscrepancies in terms of robustness to transcript noise. We also provide a\nthorough analysis of our GPT-4-based evaluation, including insights from a\ncrowdsourcing study. Our findings indicate that while GPT-4's scores align with\nhuman judges, its ability to distinguish beyond three score levels may be\nlimited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on Large Language Models (LLMs) has recently witnessed an increasing\ninterest in extending the models' context size to better capture dependencies\nwithin long documents. While benchmarks have been proposed to assess long-range\nabilities, existing efforts primarily considered generic tasks that are not\nnecessarily aligned with real-world applications. In contrast, we propose a new\nbenchmark for long-context LLMs focused on a practical meeting assistant\nscenario in which the long contexts consist of transcripts obtained by\nautomatic speech recognition, presenting unique challenges for LLMs due to the\ninherent noisiness and oral nature of such data. Our benchmark, ELITR-Bench,\naugments the existing ELITR corpus by adding 271 manually crafted questions\nwith their ground-truth answers, as well as noisy versions of meeting\ntranscripts altered to target different Word Error Rate levels. Our experiments\nwith 12 long-context LLMs on ELITR-Bench confirm the progress made across\nsuccessive generations of both proprietary and open models, and point out their\ndiscrepancies in terms of robustness to transcript noise. We also provide a\nthorough analysis of our GPT-4-based evaluation, including insights from a\ncrowdsourcing study. Our findings indicate that while GPT-4's scores align with\nhuman judges, its ability to distinguish beyond three score levels may be\nlimited."
                },
                "authors": [
                    {
                        "name": "Thibaut Thonet"
                    },
                    {
                        "name": "Jos Rozen"
                    },
                    {
                        "name": "Laurent Besacier"
                    }
                ],
                "author_detail": {
                    "name": "Laurent Besacier"
                },
                "author": "Laurent Besacier",
                "arxiv_comment": "Published in COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.20262v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.20262v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.17296v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.17296v8",
                "updated": "2025-01-17T09:28:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    28,
                    45,
                    4,
                    17,
                    0
                ],
                "published": "2023-12-28T16:25:52Z",
                "published_parsed": [
                    2023,
                    12,
                    28,
                    16,
                    25,
                    52,
                    3,
                    362,
                    0
                ],
                "title": "Structured Packing in LLM Training Improves Long Context Utilization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured Packing in LLM Training Improves Long Context Utilization"
                },
                "summary": "Recent advancements in long-context large language models have attracted\nsignificant attention, yet their practical applications often suffer from\nsuboptimal context utilization. This study investigates structuring training\ndata to enhance semantic interdependence, demonstrating that this approach\neffectively improves context utilization. To this end, we introduce the\nStructured Packing for Long Context (SPLiCe) method, which utilizes retrieval\nto collate mutually relevant documents into long and coherent training\nexamples. We validate SPLiCe empirically across models of varying sizes -- 3B,\n7B, and 13B -- achieving improved performance in long-context tasks, such as\nQasper and HotpotQA. Remarkably, even brief fine-tuning with SPLiCe is\nsufficient to realize these benefits. Additionally, SPLiCe effectively\nmitigates the lost-in-middle phenomenon often observed in large models. Our\ncomprehensive analysis of SPLiCe explores its design choices and reveals\nintriguing transfer effects; for instance, training on programming code\nenhances performance on natural language tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in long-context large language models have attracted\nsignificant attention, yet their practical applications often suffer from\nsuboptimal context utilization. This study investigates structuring training\ndata to enhance semantic interdependence, demonstrating that this approach\neffectively improves context utilization. To this end, we introduce the\nStructured Packing for Long Context (SPLiCe) method, which utilizes retrieval\nto collate mutually relevant documents into long and coherent training\nexamples. We validate SPLiCe empirically across models of varying sizes -- 3B,\n7B, and 13B -- achieving improved performance in long-context tasks, such as\nQasper and HotpotQA. Remarkably, even brief fine-tuning with SPLiCe is\nsufficient to realize these benefits. Additionally, SPLiCe effectively\nmitigates the lost-in-middle phenomenon often observed in large models. Our\ncomprehensive analysis of SPLiCe explores its design choices and reveals\nintriguing transfer effects; for instance, training on programming code\nenhances performance on natural language tasks."
                },
                "authors": [
                    {
                        "name": "Konrad Staniszewski"
                    },
                    {
                        "name": "Szymon Tworkowski"
                    },
                    {
                        "name": "Sebastian Jaszczur"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Henryk Michalewski"
                    },
                    {
                        "name": "ukasz Kuciski"
                    },
                    {
                        "name": "Piotr Mio"
                    }
                ],
                "author_detail": {
                    "name": "Piotr Mio"
                },
                "author": "Piotr Mio",
                "arxiv_comment": "AAAI'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.17296v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.17296v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10054v1",
                "updated": "2025-01-17T09:20:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    20,
                    56,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T09:20:56Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    20,
                    56,
                    4,
                    17,
                    0
                ],
                "title": "Accelerating Large Language Models through Partially Linear Feed-Forward\n  Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Large Language Models through Partially Linear Feed-Forward\n  Network"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable capabilities but face\ndeployment challenges due to their massive parameter counts. While existing\ncompression techniques like pruning can reduce model size, it leads to\nsignificant accuracy degradation under high compression ratios. We present a\nnovel perspective inspired by constant folding in compiler optimization. Our\napproach enables parameter reduction by treating activation functions in LLMs\nas linear functions.\n  However, recent LLMs use complex non-linear activations like GELU that\nprevent direct application of this technique. We propose TARDIS, which enables\noptimization of LLMs with non-linear activations by partially approximating\nthem with linear functions in frequently occurring input ranges. For outlier\ninputs, TARDIS employs an online predictor to dynamically fall back to original\ncomputations.\n  Our experiments demonstrate that TARDIS achieves 80% parameter reduction in\nfeed-forward networks, while significantly outperforming state-of-the-art\npruning methods Wanda and RIA with up to 65% higher accuracy. In practical\ndeployments for a 7B model, TARDIS achieves 1.6x end-to-end inference speedup\nwhen integrated with the vLLM serving system, and 1.4x speedup with the widely\nadopted HuggingFace implementation, while incurring only a 10.9% accuracy\ntrade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable capabilities but face\ndeployment challenges due to their massive parameter counts. While existing\ncompression techniques like pruning can reduce model size, it leads to\nsignificant accuracy degradation under high compression ratios. We present a\nnovel perspective inspired by constant folding in compiler optimization. Our\napproach enables parameter reduction by treating activation functions in LLMs\nas linear functions.\n  However, recent LLMs use complex non-linear activations like GELU that\nprevent direct application of this technique. We propose TARDIS, which enables\noptimization of LLMs with non-linear activations by partially approximating\nthem with linear functions in frequently occurring input ranges. For outlier\ninputs, TARDIS employs an online predictor to dynamically fall back to original\ncomputations.\n  Our experiments demonstrate that TARDIS achieves 80% parameter reduction in\nfeed-forward networks, while significantly outperforming state-of-the-art\npruning methods Wanda and RIA with up to 65% higher accuracy. In practical\ndeployments for a 7B model, TARDIS achieves 1.6x end-to-end inference speedup\nwhen integrated with the vLLM serving system, and 1.4x speedup with the widely\nadopted HuggingFace implementation, while incurring only a 10.9% accuracy\ntrade-off."
                },
                "authors": [
                    {
                        "name": "Gansen Hu"
                    },
                    {
                        "name": "Zhaoguo Wang"
                    },
                    {
                        "name": "Jinglin Wei"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4; I.2; D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17482v2",
                "updated": "2025-01-17T09:17:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    17,
                    30,
                    4,
                    17,
                    0
                ],
                "published": "2024-07-02T08:07:27Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    8,
                    7,
                    27,
                    1,
                    184,
                    0
                ],
                "title": "Reinforcement Learning from Human Feedback: Whose Culture, Whose Values,\n  Whose Perspectives?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback: Whose Culture, Whose Values,\n  Whose Perspectives?"
                },
                "summary": "We argue for the epistemic and ethical advantages of pluralism in\nReinforcement Learning from Human Feedback (RLHF) in the context of Large\nLanguage Models (LLM). Drawing on social epistemology and pluralist philosophy\nof science, we suggest ways in which RHLF can be made more responsive to human\nneeds and how we can address challenges along the way. The paper concludes with\nan agenda for change, i.e. concrete, actionable steps to improve LLM\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We argue for the epistemic and ethical advantages of pluralism in\nReinforcement Learning from Human Feedback (RLHF) in the context of Large\nLanguage Models (LLM). Drawing on social epistemology and pluralist philosophy\nof science, we suggest ways in which RHLF can be made more responsive to human\nneeds and how we can address challenges along the way. The paper concludes with\nan agenda for change, i.e. concrete, actionable steps to improve LLM\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Kristian Gonzlez Barman"
                    },
                    {
                        "name": "Simon Lohse"
                    },
                    {
                        "name": "Henk de Regt"
                    }
                ],
                "author_detail": {
                    "name": "Henk de Regt"
                },
                "author": "Henk de Regt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10053v1",
                "updated": "2025-01-17T09:16:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    16,
                    13,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T09:16:13Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    16,
                    13,
                    4,
                    17,
                    0
                ],
                "title": "AirRAG: Activating Intrinsic Reasoning for Retrieval Augmented\n  Generation via Tree-based Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirRAG: Activating Intrinsic Reasoning for Retrieval Augmented\n  Generation via Tree-based Search"
                },
                "summary": "Leveraging the autonomous decision-making capabilities of large language\nmodels (LLMs) demonstrates superior performance in reasoning tasks. Despite the\nsuccesses of iterative or recursive retrieval-augmented generation (RAG), they\noften are trapped in a single solution space when confronted with complex\ntasks. In this paper, we propose a novel thinking pattern in RAG which\nintegrates system analysis with efficient reasoning actions, significantly\nactivating intrinsic reasoning capabilities and expanding the solution space of\nspecific tasks via Monte Carlo Tree Search (MCTS), dubbed AirRAG. Specifically,\nour approach designs five fundamental reasoning actions that are expanded to a\nwide tree-based reasoning spaces using MCTS. The extension also uses\nself-consistency verification to explore potential reasoning paths and\nimplement inference scaling. In addition, computationally optimal strategies\nare used to apply more inference computation to key actions to achieve further\nperformance improvements. Experimental results demonstrate the effectiveness of\nAirRAG through considerable performance gains over complex QA datasets.\nFurthermore, AirRAG is flexible and lightweight, making it easy to integrate\nwith other advanced technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging the autonomous decision-making capabilities of large language\nmodels (LLMs) demonstrates superior performance in reasoning tasks. Despite the\nsuccesses of iterative or recursive retrieval-augmented generation (RAG), they\noften are trapped in a single solution space when confronted with complex\ntasks. In this paper, we propose a novel thinking pattern in RAG which\nintegrates system analysis with efficient reasoning actions, significantly\nactivating intrinsic reasoning capabilities and expanding the solution space of\nspecific tasks via Monte Carlo Tree Search (MCTS), dubbed AirRAG. Specifically,\nour approach designs five fundamental reasoning actions that are expanded to a\nwide tree-based reasoning spaces using MCTS. The extension also uses\nself-consistency verification to explore potential reasoning paths and\nimplement inference scaling. In addition, computationally optimal strategies\nare used to apply more inference computation to key actions to achieve further\nperformance improvements. Experimental results demonstrate the effectiveness of\nAirRAG through considerable performance gains over complex QA datasets.\nFurthermore, AirRAG is flexible and lightweight, making it easy to integrate\nwith other advanced technologies."
                },
                "authors": [
                    {
                        "name": "Wenfeng Feng"
                    },
                    {
                        "name": "Chuzhan Hao"
                    },
                    {
                        "name": "Yuewei Zhang"
                    },
                    {
                        "name": "Jingyi Song"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "arxiv_comment": "17 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10052v1",
                "updated": "2025-01-17T09:15:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    15,
                    53,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T09:15:53Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    15,
                    53,
                    4,
                    17,
                    0
                ],
                "title": "Conditional Latent Diffusion-Based Speech Enhancement Via Dual Context\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditional Latent Diffusion-Based Speech Enhancement Via Dual Context\n  Learning"
                },
                "summary": "Recently, the application of diffusion probabilistic models has advanced\nspeech enhancement through generative approaches. However, existing\ndiffusion-based methods have focused on the generation process in\nhigh-dimensional waveform or spectral domains, leading to increased generation\ncomplexity and slower inference speeds. Additionally, these methods have\nprimarily modelled clean speech distributions, with limited exploration of\nnoise distributions, thereby constraining the discriminative capability of\ndiffusion models for speech enhancement. To address these issues, we propose a\nnovel approach that integrates a conditional latent diffusion model (cLDM) with\ndual-context learning (DCL). Our method utilizes a variational autoencoder\n(VAE) to compress mel-spectrograms into a low-dimensional latent space. We then\napply cLDM to transform the latent representations of both clean speech and\nbackground noise into Gaussian noise by the DCL process, and a parameterized\nmodel is trained to reverse this process, conditioned on noisy latent\nrepresentations and text embeddings. By operating in a lower-dimensional space,\nthe latent representations reduce the complexity of the generation process,\nwhile the DCL process enhances the model's ability to handle diverse and unseen\nnoise environments. Our experiments demonstrate the strong performance of the\nproposed approach compared to existing diffusion-based methods, even with fewer\niterative steps, and highlight the superior generalization capability of our\nmodels to out-of-domain noise datasets\n(https://github.com/modelscope/ClearerVoice-Studio).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, the application of diffusion probabilistic models has advanced\nspeech enhancement through generative approaches. However, existing\ndiffusion-based methods have focused on the generation process in\nhigh-dimensional waveform or spectral domains, leading to increased generation\ncomplexity and slower inference speeds. Additionally, these methods have\nprimarily modelled clean speech distributions, with limited exploration of\nnoise distributions, thereby constraining the discriminative capability of\ndiffusion models for speech enhancement. To address these issues, we propose a\nnovel approach that integrates a conditional latent diffusion model (cLDM) with\ndual-context learning (DCL). Our method utilizes a variational autoencoder\n(VAE) to compress mel-spectrograms into a low-dimensional latent space. We then\napply cLDM to transform the latent representations of both clean speech and\nbackground noise into Gaussian noise by the DCL process, and a parameterized\nmodel is trained to reverse this process, conditioned on noisy latent\nrepresentations and text embeddings. By operating in a lower-dimensional space,\nthe latent representations reduce the complexity of the generation process,\nwhile the DCL process enhances the model's ability to handle diverse and unseen\nnoise environments. Our experiments demonstrate the strong performance of the\nproposed approach compared to existing diffusion-based methods, even with fewer\niterative steps, and highlight the superior generalization capability of our\nmodels to out-of-domain noise datasets\n(https://github.com/modelscope/ClearerVoice-Studio)."
                },
                "authors": [
                    {
                        "name": "Shengkui Zhao"
                    },
                    {
                        "name": "Zexu Pan"
                    },
                    {
                        "name": "Kun Zhou"
                    },
                    {
                        "name": "Yukun Ma"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Bin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Bin Ma"
                },
                "author": "Bin Ma",
                "arxiv_comment": "5 pages, 1 figure, accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02178v2",
                "updated": "2025-01-17T08:44:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    8,
                    44,
                    57,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-04T04:02:23Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    4,
                    2,
                    23,
                    5,
                    4,
                    0
                ],
                "title": "The Application of Large Language Models in Recommendation Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Application of Large Language Models in Recommendation Systems"
                },
                "summary": "The integration of Large Language Models into recommendation frameworks\npresents key advantages for personalization and adaptability of experiences to\nthe users. Classic methods of recommendations, such as collaborative filtering\nand content-based filtering, are seriously limited in the solution of\ncold-start problems, sparsity of data, and lack of diversity in information\nconsidered. LLMs, of which GPT-4 is a good example, have emerged as powerful\ntools that enable recommendation frameworks to tap into unstructured data\nsources such as user reviews, social interactions, and text-based content. By\nanalyzing these data sources, LLMs improve the accuracy and relevance of\nrecommendations, thereby overcoming some of the limitations of traditional\napproaches. This work discusses applications of LLMs in recommendation systems,\nespecially in electronic commerce, social media platforms, streaming services,\nand educational technologies. This showcases how LLMs enrich recommendation\ndiversity, user engagement, and the system's adaptability; yet it also looks\ninto the challenges connected to their technical implementation. This can also\nbe presented as a study that shows the potential of LLMs for changing user\nexperiences and making innovation possible in industries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models into recommendation frameworks\npresents key advantages for personalization and adaptability of experiences to\nthe users. Classic methods of recommendations, such as collaborative filtering\nand content-based filtering, are seriously limited in the solution of\ncold-start problems, sparsity of data, and lack of diversity in information\nconsidered. LLMs, of which GPT-4 is a good example, have emerged as powerful\ntools that enable recommendation frameworks to tap into unstructured data\nsources such as user reviews, social interactions, and text-based content. By\nanalyzing these data sources, LLMs improve the accuracy and relevance of\nrecommendations, thereby overcoming some of the limitations of traditional\napproaches. This work discusses applications of LLMs in recommendation systems,\nespecially in electronic commerce, social media platforms, streaming services,\nand educational technologies. This showcases how LLMs enrich recommendation\ndiversity, user engagement, and the system's adaptability; yet it also looks\ninto the challenges connected to their technical implementation. This can also\nbe presented as a study that shows the potential of LLMs for changing user\nexperiences and making innovation possible in industries."
                },
                "authors": [
                    {
                        "name": "Peiyang Yu"
                    },
                    {
                        "name": "Zeqiu Xu"
                    },
                    {
                        "name": "Jiani Wang"
                    },
                    {
                        "name": "Xiaochuan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaochuan Xu"
                },
                "author": "Xiaochuan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09368v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09368v2",
                "updated": "2025-01-17T08:23:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    8,
                    23,
                    3,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-16T08:27:40Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    27,
                    40,
                    3,
                    16,
                    0
                ],
                "title": "Aligning Instruction Tuning with Pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Instruction Tuning with Pre-training"
                },
                "summary": "Instruction tuning enhances large language models (LLMs) to follow human\ninstructions across diverse tasks, relying on high-quality datasets to guide\nbehavior. However, these datasets, whether manually curated or synthetically\ngenerated, are often narrowly focused and misaligned with the broad\ndistributions captured during pre-training, limiting LLM generalization and\neffective use of pre-trained knowledge. We propose *Aligning Instruction Tuning\nwith Pre-training* (AITP), a method that bridges this gap by identifying\ncoverage shortfalls in instruction-tuning datasets and rewriting\nunderrepresented pre-training data into high-quality instruction-response\npairs. This approach enriches dataset diversity while preserving task-specific\nobjectives. Evaluations on three fully open LLMs across eight benchmarks\ndemonstrate consistent performance improvements with AITP. Ablations highlight\nthe benefits of adaptive data selection, controlled rewriting, and balanced\nintegration, emphasizing the importance of aligning instruction tuning with\npre-training distributions to unlock the full potential of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning enhances large language models (LLMs) to follow human\ninstructions across diverse tasks, relying on high-quality datasets to guide\nbehavior. However, these datasets, whether manually curated or synthetically\ngenerated, are often narrowly focused and misaligned with the broad\ndistributions captured during pre-training, limiting LLM generalization and\neffective use of pre-trained knowledge. We propose *Aligning Instruction Tuning\nwith Pre-training* (AITP), a method that bridges this gap by identifying\ncoverage shortfalls in instruction-tuning datasets and rewriting\nunderrepresented pre-training data into high-quality instruction-response\npairs. This approach enriches dataset diversity while preserving task-specific\nobjectives. Evaluations on three fully open LLMs across eight benchmarks\ndemonstrate consistent performance improvements with AITP. Ablations highlight\nthe benefits of adaptive data selection, controlled rewriting, and balanced\nintegration, emphasizing the importance of aligning instruction tuning with\npre-training distributions to unlock the full potential of LLMs."
                },
                "authors": [
                    {
                        "name": "Yiming Liang"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Lei Ma"
                    },
                    {
                        "name": "Wenhu Chen"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09368v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09368v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.16732v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.16732v2",
                "updated": "2025-01-17T08:13:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    8,
                    13,
                    35,
                    4,
                    17,
                    0
                ],
                "published": "2024-01-30T04:11:24Z",
                "published_parsed": [
                    2024,
                    1,
                    30,
                    4,
                    11,
                    24,
                    1,
                    30,
                    0
                ],
                "title": "Flash: A Hybrid Private Inference Protocol for Deep CNNs with High\n  Accuracy and Low Latency on CPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flash: A Hybrid Private Inference Protocol for Deep CNNs with High\n  Accuracy and Low Latency on CPU"
                },
                "summary": "This paper presents Flash, an optimized private inference (PI) hybrid\nprotocol utilizing both homomorphic encryption (HE) and secure two-party\ncomputation (2PC), which can reduce the end-to-end PI latency for deep CNN\nmodels less than 1 minute with CPU. To this end, first, Flash proposes a\nlow-latency convolution algorithm built upon a fast slot rotation operation and\na novel data encoding scheme, which results in 4-94x performance gain over the\nstate-of-the-art. Second, to minimize the communication cost introduced by the\nstandard nonlinear activation function ReLU, Flash replaces the entire ReLUs\nwith the polynomial $x^2+x$ and trains deep CNN models with the new training\nstrategy. The trained models improve the inference accuracy for CIFAR-10/100\nand TinyImageNet by 16% on average (up to 40% for ResNet-32) compared to prior\nart. Last, Flash proposes an efficient 2PC-based $x^2+x$ evaluation protocol\nthat does not require any offline communication and that reduces the total\ncommunication cost to process the activation layer by 84-196x over the\nstate-of-the-art. As a result, the end-to-end PI latency of Flash implemented\non CPU is 0.02 minute for CIFAR-100 and 0.57 minute for TinyImageNet\nclassification, while the total data communication is 0.07GB for CIFAR-100 and\n0.22GB for TinyImageNet. Flash improves the state-of-the-art PI by 16-45x in\nlatency and 84-196x in communication cost. Moreover, even for ImageNet, Flash\ncan deliver the latency less than 1 minute on CPU with the total communication\nless than 1GB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents Flash, an optimized private inference (PI) hybrid\nprotocol utilizing both homomorphic encryption (HE) and secure two-party\ncomputation (2PC), which can reduce the end-to-end PI latency for deep CNN\nmodels less than 1 minute with CPU. To this end, first, Flash proposes a\nlow-latency convolution algorithm built upon a fast slot rotation operation and\na novel data encoding scheme, which results in 4-94x performance gain over the\nstate-of-the-art. Second, to minimize the communication cost introduced by the\nstandard nonlinear activation function ReLU, Flash replaces the entire ReLUs\nwith the polynomial $x^2+x$ and trains deep CNN models with the new training\nstrategy. The trained models improve the inference accuracy for CIFAR-10/100\nand TinyImageNet by 16% on average (up to 40% for ResNet-32) compared to prior\nart. Last, Flash proposes an efficient 2PC-based $x^2+x$ evaluation protocol\nthat does not require any offline communication and that reduces the total\ncommunication cost to process the activation layer by 84-196x over the\nstate-of-the-art. As a result, the end-to-end PI latency of Flash implemented\non CPU is 0.02 minute for CIFAR-100 and 0.57 minute for TinyImageNet\nclassification, while the total data communication is 0.07GB for CIFAR-100 and\n0.22GB for TinyImageNet. Flash improves the state-of-the-art PI by 16-45x in\nlatency and 84-196x in communication cost. Moreover, even for ImageNet, Flash\ncan deliver the latency less than 1 minute on CPU with the total communication\nless than 1GB."
                },
                "authors": [
                    {
                        "name": "Hyeri Roh"
                    },
                    {
                        "name": "Jinsu Yeo"
                    },
                    {
                        "name": "Yeongil Ko"
                    },
                    {
                        "name": "Gu-Yeon Wei"
                    },
                    {
                        "name": "David Brooks"
                    },
                    {
                        "name": "Woo-Seok Choi"
                    }
                ],
                "author_detail": {
                    "name": "Woo-Seok Choi"
                },
                "author": "Woo-Seok Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.16732v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.16732v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12688v2",
                "updated": "2025-01-17T08:04:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    8,
                    4,
                    59,
                    4,
                    17,
                    0
                ],
                "published": "2024-04-19T07:41:43Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    7,
                    41,
                    43,
                    4,
                    110,
                    0
                ],
                "title": "Change of Measure for Bayesian Field Inversion with Hierarchical\n  Hyperparameters Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Change of Measure for Bayesian Field Inversion with Hierarchical\n  Hyperparameters Sampling"
                },
                "summary": "This paper proposes an effective treatment of hyperparameters in the Bayesian\ninference of a scalar field from indirect observations. Obtaining the joint\nposterior distribution of the field and its hyperparameters is challenging. The\ninfinite dimensionality of the field requires a finite parametrization that\nusually involves hyperparameters to reflect the limited prior knowledge. In the\npresent work, we consider a Karhunen-Lo{\\`e}ve(KL) decomposition for the random\nfield and hyperparameters to account for the lack of prior knowledge of its\nautocovariance function. The hyperparameters must be inferred. To efficiently\nsample jointly the KL coordinates of the field and the autocovariance\nhyperparameters, we introduce a change of measure to reformulate the joint\nposterior distribution into a hierarchical Bayesian form. The likelihood\ndepends only onthe field's coordinates in a fixed KL basis, with a prior\nconditioned on the hyperparameters. We exploit this structure to derive an\nefficient Markov Chain Monte Carlo (MCMC) sampling scheme based on an adapted\nMetropolis-Hasting algorithm. We rely on surrogate models (Polynomial Chaos\nexpansions) of the forward model predictions to further accelerate the MCMC\nsampling. A first application to a transient diffusionproblem shows that our\nmethod is consistent with other approaches based on a change of coordinates\n(Sraj et al., 2016). A second application to a seismic traveltime tomography\nhighlights the importance of inferring the hyperparameters. A third application\nto a 2D anisotropic groundwater flow problem illustrates the method on a more\ncomplex geometry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes an effective treatment of hyperparameters in the Bayesian\ninference of a scalar field from indirect observations. Obtaining the joint\nposterior distribution of the field and its hyperparameters is challenging. The\ninfinite dimensionality of the field requires a finite parametrization that\nusually involves hyperparameters to reflect the limited prior knowledge. In the\npresent work, we consider a Karhunen-Lo{\\`e}ve(KL) decomposition for the random\nfield and hyperparameters to account for the lack of prior knowledge of its\nautocovariance function. The hyperparameters must be inferred. To efficiently\nsample jointly the KL coordinates of the field and the autocovariance\nhyperparameters, we introduce a change of measure to reformulate the joint\nposterior distribution into a hierarchical Bayesian form. The likelihood\ndepends only onthe field's coordinates in a fixed KL basis, with a prior\nconditioned on the hyperparameters. We exploit this structure to derive an\nefficient Markov Chain Monte Carlo (MCMC) sampling scheme based on an adapted\nMetropolis-Hasting algorithm. We rely on surrogate models (Polynomial Chaos\nexpansions) of the forward model predictions to further accelerate the MCMC\nsampling. A first application to a transient diffusionproblem shows that our\nmethod is consistent with other approaches based on a change of coordinates\n(Sraj et al., 2016). A second application to a seismic traveltime tomography\nhighlights the importance of inferring the hyperparameters. A third application\nto a 2D anisotropic groundwater flow problem illustrates the method on a more\ncomplex geometry."
                },
                "authors": [
                    {
                        "name": "Nadge Polette"
                    },
                    {
                        "name": "Olivier Le Matre"
                    },
                    {
                        "name": "Pierre Sochala"
                    },
                    {
                        "name": "Alexandrine Gesret"
                    }
                ],
                "author_detail": {
                    "name": "Alexandrine Gesret"
                },
                "arxiv_affiliation": "GEOSCIENCES",
                "author": "Alexandrine Gesret",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10018v1",
                "updated": "2025-01-17T08:03:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    8,
                    3,
                    2,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T08:03:02Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    8,
                    3,
                    2,
                    4,
                    17,
                    0
                ],
                "title": "DiffuEraser: A Diffusion Model for Video Inpainting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffuEraser: A Diffusion Model for Video Inpainting"
                },
                "summary": "Recent video inpainting algorithms integrate flow-based pixel propagation\nwith transformer-based generation to leverage optical flow for restoring\ntextures and objects using information from neighboring frames, while\ncompleting masked regions through visual Transformers. However, these\napproaches often encounter blurring and temporal inconsistencies when dealing\nwith large masks, highlighting the need for models with enhanced generative\ncapabilities. Recently, diffusion models have emerged as a prominent technique\nin image and video generation due to their impressive performance. In this\npaper, we introduce DiffuEraser, a video inpainting model based on stable\ndiffusion, designed to fill masked regions with greater details and more\ncoherent structures. We incorporate prior information to provide initialization\nand weak conditioning,which helps mitigate noisy artifacts and suppress\nhallucinations. Additionally, to improve temporal consistency during\nlong-sequence inference, we expand the temporal receptive fields of both the\nprior model and DiffuEraser, and further enhance consistency by leveraging the\ntemporal smoothing property of Video Diffusion Models. Experimental results\ndemonstrate that our proposed method outperforms state-of-the-art techniques in\nboth content completeness and temporal consistency while maintaining acceptable\nefficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent video inpainting algorithms integrate flow-based pixel propagation\nwith transformer-based generation to leverage optical flow for restoring\ntextures and objects using information from neighboring frames, while\ncompleting masked regions through visual Transformers. However, these\napproaches often encounter blurring and temporal inconsistencies when dealing\nwith large masks, highlighting the need for models with enhanced generative\ncapabilities. Recently, diffusion models have emerged as a prominent technique\nin image and video generation due to their impressive performance. In this\npaper, we introduce DiffuEraser, a video inpainting model based on stable\ndiffusion, designed to fill masked regions with greater details and more\ncoherent structures. We incorporate prior information to provide initialization\nand weak conditioning,which helps mitigate noisy artifacts and suppress\nhallucinations. Additionally, to improve temporal consistency during\nlong-sequence inference, we expand the temporal receptive fields of both the\nprior model and DiffuEraser, and further enhance consistency by leveraging the\ntemporal smoothing property of Video Diffusion Models. Experimental results\ndemonstrate that our proposed method outperforms state-of-the-art techniques in\nboth content completeness and temporal consistency while maintaining acceptable\nefficiency."
                },
                "authors": [
                    {
                        "name": "Xiaowen Li"
                    },
                    {
                        "name": "Haolan Xue"
                    },
                    {
                        "name": "Peiran Ren"
                    },
                    {
                        "name": "Liefeng Bo"
                    }
                ],
                "author_detail": {
                    "name": "Liefeng Bo"
                },
                "author": "Liefeng Bo",
                "arxiv_comment": "11pages, 13figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.07450v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.07450v2",
                "updated": "2025-01-17T07:52:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    7,
                    52,
                    3,
                    4,
                    17,
                    0
                ],
                "published": "2023-12-12T17:23:07Z",
                "published_parsed": [
                    2023,
                    12,
                    12,
                    17,
                    23,
                    7,
                    1,
                    346,
                    0
                ],
                "title": "The Pfaffian Structure of CFN Phylogenetic Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Pfaffian Structure of CFN Phylogenetic Networks"
                },
                "summary": "Algebraic techniques in phylogenetics have historically been successful at\nproving identifiability results and have also led to novel reconstruction\nalgorithms. In this paper, we study the ideal of phylogenetic invariants of the\nCavender-Farris-Neyman (CFN) model on a phylogenetic network with the goal of\nproviding a description of the invariants which is useful for network\ninference. It was previously shown that to characterize the invariants of any\nlevel-1 network, it suffices to understand all sunlet networks, which are those\nconsisting of a single cycle with a leaf adjacent to each cycle vertex. We show\nthat the parameterization of an affine open patch of the CFN sunlet model,\nwhich intersects the probability simplex factors through the space of\nskew-symmetric matrices via Pfaffians. We then show that this affine patch is\nisomorphic to a determinantal variety and give an explicit Gr{\\\"o}bner basis\nfor the associated ideal, which involves only polynomially many coordinates.\nLastly, we show that sunlet networks with at least 6 leaves are identifiable\nusing only these polynomials and run extensive simulations, which show that\nthese polynomials can be used to accurately infer the correct network from DNA\nsequence data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algebraic techniques in phylogenetics have historically been successful at\nproving identifiability results and have also led to novel reconstruction\nalgorithms. In this paper, we study the ideal of phylogenetic invariants of the\nCavender-Farris-Neyman (CFN) model on a phylogenetic network with the goal of\nproviding a description of the invariants which is useful for network\ninference. It was previously shown that to characterize the invariants of any\nlevel-1 network, it suffices to understand all sunlet networks, which are those\nconsisting of a single cycle with a leaf adjacent to each cycle vertex. We show\nthat the parameterization of an affine open patch of the CFN sunlet model,\nwhich intersects the probability simplex factors through the space of\nskew-symmetric matrices via Pfaffians. We then show that this affine patch is\nisomorphic to a determinantal variety and give an explicit Gr{\\\"o}bner basis\nfor the associated ideal, which involves only polynomially many coordinates.\nLastly, we show that sunlet networks with at least 6 leaves are identifiable\nusing only these polynomials and run extensive simulations, which show that\nthese polynomials can be used to accurately infer the correct network from DNA\nsequence data."
                },
                "authors": [
                    {
                        "name": "Joseph Cummings"
                    },
                    {
                        "name": "Elizabeth Gross"
                    },
                    {
                        "name": "Benjamin Hollering"
                    },
                    {
                        "name": "Samuel Martin"
                    },
                    {
                        "name": "Ikenna Nometa"
                    }
                ],
                "author_detail": {
                    "name": "Ikenna Nometa"
                },
                "author": "Ikenna Nometa",
                "arxiv_comment": "26 pages, 3 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.07450v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.07450v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92B10, 62R01, 13P25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09831v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09831v2",
                "updated": "2025-01-17T07:48:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    7,
                    48,
                    39,
                    4,
                    17,
                    0
                ],
                "published": "2024-08-19T09:27:45Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    27,
                    45,
                    0,
                    232,
                    0
                ],
                "title": "Ranking Generated Answers: On the Agreement of Retrieval Models with\n  Humans on Consumer Health Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ranking Generated Answers: On the Agreement of Retrieval Models with\n  Humans on Consumer Health Questions"
                },
                "summary": "Evaluating the output of generative large language models (LLMs) is\nchallenging and difficult to scale. Many evaluations of LLMs focus on tasks\nsuch as single-choice question-answering or text classification. These tasks\nare not suitable for assessing open-ended question-answering capabilities,\nwhich are critical in domains where expertise is required. One such domain is\nhealth, where misleading or incorrect answers can have a negative impact on a\nuser's well-being. Using human experts to evaluate the quality of LLM answers\nis generally considered the gold standard, but expert annotation is costly and\nslow. We present a method for evaluating LLM answers that uses ranking models\ntrained on annotated document collections as a substitute for explicit\nrelevance judgements and apply it to the CLEF 2021 eHealth dataset. In a user\nstudy, our method correlates with the preferences of a human expert (Kendall's\n$\\tau=0.64$). It is also consistent with previous findings in that the quality\nof generated answers improves with the size of the model and more sophisticated\nprompting strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the output of generative large language models (LLMs) is\nchallenging and difficult to scale. Many evaluations of LLMs focus on tasks\nsuch as single-choice question-answering or text classification. These tasks\nare not suitable for assessing open-ended question-answering capabilities,\nwhich are critical in domains where expertise is required. One such domain is\nhealth, where misleading or incorrect answers can have a negative impact on a\nuser's well-being. Using human experts to evaluate the quality of LLM answers\nis generally considered the gold standard, but expert annotation is costly and\nslow. We present a method for evaluating LLM answers that uses ranking models\ntrained on annotated document collections as a substitute for explicit\nrelevance judgements and apply it to the CLEF 2021 eHealth dataset. In a user\nstudy, our method correlates with the preferences of a human expert (Kendall's\n$\\tau=0.64$). It is also consistent with previous findings in that the quality\nof generated answers improves with the size of the model and more sophisticated\nprompting strategies."
                },
                "authors": [
                    {
                        "name": "Sebastian Heineking"
                    },
                    {
                        "name": "Jonas Probst"
                    },
                    {
                        "name": "Daniel Steinbach"
                    },
                    {
                        "name": "Martin Potthast"
                    },
                    {
                        "name": "Harrisen Scells"
                    }
                ],
                "author_detail": {
                    "name": "Harrisen Scells"
                },
                "author": "Harrisen Scells",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09831v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09831v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10011v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10011v1",
                "updated": "2025-01-17T07:48:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    7,
                    48,
                    37,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T07:48:37Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    7,
                    48,
                    37,
                    4,
                    17,
                    0
                ],
                "title": "Mitigating Hallucinations on Object Attributes using Multiview Images\n  and Negative Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Hallucinations on Object Attributes using Multiview Images\n  and Negative Instructions"
                },
                "summary": "Current popular Large Vision-Language Models (LVLMs) are suffering from\nHallucinations on Object Attributes (HoOA), leading to incorrect determination\nof fine-grained attributes in the input images. Leveraging significant\nadvancements in 3D generation from a single image, this paper proposes a novel\nmethod to mitigate HoOA in LVLMs. This method utilizes multiview images sampled\nfrom generated 3D representations as visual prompts for LVLMs, thereby\nproviding more visual information from other viewpoints. Furthermore, we\nobserve the input order of multiple multiview images significantly affects the\nperformance of LVLMs. Consequently, we have devised Multiview Image Augmented\nVLM (MIAVLM), incorporating a Multiview Attributes Perceiver (MAP) submodule\ncapable of simultaneously eliminating the influence of input image order and\naligning visual information from multiview images with Large Language Models\n(LLMs). Besides, we designed and employed negative instructions to mitigate\nLVLMs' bias towards ``Yes\" responses. Comprehensive experiments demonstrate the\neffectiveness of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current popular Large Vision-Language Models (LVLMs) are suffering from\nHallucinations on Object Attributes (HoOA), leading to incorrect determination\nof fine-grained attributes in the input images. Leveraging significant\nadvancements in 3D generation from a single image, this paper proposes a novel\nmethod to mitigate HoOA in LVLMs. This method utilizes multiview images sampled\nfrom generated 3D representations as visual prompts for LVLMs, thereby\nproviding more visual information from other viewpoints. Furthermore, we\nobserve the input order of multiple multiview images significantly affects the\nperformance of LVLMs. Consequently, we have devised Multiview Image Augmented\nVLM (MIAVLM), incorporating a Multiview Attributes Perceiver (MAP) submodule\ncapable of simultaneously eliminating the influence of input image order and\naligning visual information from multiview images with Large Language Models\n(LLMs). Besides, we designed and employed negative instructions to mitigate\nLVLMs' bias towards ``Yes\" responses. Comprehensive experiments demonstrate the\neffectiveness of our method."
                },
                "authors": [
                    {
                        "name": "Zhijie Tan"
                    },
                    {
                        "name": "Yuzhi Li"
                    },
                    {
                        "name": "Shengwei Meng"
                    },
                    {
                        "name": "Xiang Yuan"
                    },
                    {
                        "name": "Weiping Li"
                    },
                    {
                        "name": "Tong Mo"
                    },
                    {
                        "name": "Bingce Wang"
                    },
                    {
                        "name": "Xu Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chu"
                },
                "author": "Xu Chu",
                "arxiv_comment": "2025 IEEE International Conference on Acoustics, Speech, and Signal\n  Processing (ICASSP 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10011v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09997v1",
                "updated": "2025-01-17T07:30:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    7,
                    30,
                    1,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T07:30:01Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    7,
                    30,
                    1,
                    4,
                    17,
                    0
                ],
                "title": "Attention-guided Self-reflection for Zero-shot Hallucination Detection\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-guided Self-reflection for Zero-shot Hallucination Detection\n  in Large Language Models"
                },
                "summary": "Hallucination has emerged as a significant barrier to the effective\napplication of Large Language Models (LLMs). In this work, we introduce a novel\nAttention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination\ndetection in LLMs. The AGSER method utilizes attention contributions to\ncategorize the input query into attentive and non-attentive queries. Each query\nis then processed separately through the LLMs, allowing us to compute\nconsistency scores between the generated responses and the original answer. The\ndifference between the two consistency scores serves as a hallucination\nestimator. In addition to its efficacy in detecting hallucinations, AGSER\nnotably reduces computational complexity, requiring only three passes through\nthe LLM and utilizing two sets of tokens. We have conducted extensive\nexperiments with four widely-used LLMs across three different hallucination\nbenchmarks, demonstrating that our approach significantly outperforms existing\nmethods in zero-shot hallucination detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination has emerged as a significant barrier to the effective\napplication of Large Language Models (LLMs). In this work, we introduce a novel\nAttention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination\ndetection in LLMs. The AGSER method utilizes attention contributions to\ncategorize the input query into attentive and non-attentive queries. Each query\nis then processed separately through the LLMs, allowing us to compute\nconsistency scores between the generated responses and the original answer. The\ndifference between the two consistency scores serves as a hallucination\nestimator. In addition to its efficacy in detecting hallucinations, AGSER\nnotably reduces computational complexity, requiring only three passes through\nthe LLM and utilizing two sets of tokens. We have conducted extensive\nexperiments with four widely-used LLMs across three different hallucination\nbenchmarks, demonstrating that our approach significantly outperforms existing\nmethods in zero-shot hallucination detection."
                },
                "authors": [
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Xinlong Chen"
                    },
                    {
                        "name": "Yue Ding"
                    },
                    {
                        "name": "Shizhen Xu"
                    },
                    {
                        "name": "Shu Wu"
                    },
                    {
                        "name": "Liang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wang"
                },
                "author": "Liang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09993v1",
                "updated": "2025-01-17T07:23:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    7,
                    23,
                    6,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T07:23:06Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    7,
                    23,
                    6,
                    4,
                    17,
                    0
                ],
                "title": "Agent-as-Judge for Factual Summarization of Long Narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-as-Judge for Factual Summarization of Long Narratives"
                },
                "summary": "Large Language Models (LLMs) have demonstrated near-human performance in\nsummarization tasks based on traditional metrics such as ROUGE and BERTScore.\nHowever, these metrics do not adequately capture critical aspects of\nsummarization quality, such as factual accuracy, particularly for long\nnarratives (>100K tokens). Recent advances, such as LLM-as-a-Judge, address the\nlimitations of metrics based on lexical similarity but still exhibit factual\ninconsistencies, especially in understanding character relationships and\nstates. In this work, we introduce NarrativeFactScore, a novel\n\"Agent-as-a-Judge\" framework for evaluating and refining summaries. By\nleveraging a Character Knowledge Graph (CKG) extracted from input and generated\nsummaries, NarrativeFactScore assesses the factual consistency and provides\nactionable guidance for refinement, such as identifying missing or erroneous\nfacts. We demonstrate the effectiveness of NarrativeFactScore through a\ndetailed workflow illustration and extensive validation on widely adopted\nbenchmarks, achieving superior performance compared to competitive methods. Our\nresults highlight the potential of agent-driven evaluation systems to improve\nthe factual reliability of LLM-generated summaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated near-human performance in\nsummarization tasks based on traditional metrics such as ROUGE and BERTScore.\nHowever, these metrics do not adequately capture critical aspects of\nsummarization quality, such as factual accuracy, particularly for long\nnarratives (>100K tokens). Recent advances, such as LLM-as-a-Judge, address the\nlimitations of metrics based on lexical similarity but still exhibit factual\ninconsistencies, especially in understanding character relationships and\nstates. In this work, we introduce NarrativeFactScore, a novel\n\"Agent-as-a-Judge\" framework for evaluating and refining summaries. By\nleveraging a Character Knowledge Graph (CKG) extracted from input and generated\nsummaries, NarrativeFactScore assesses the factual consistency and provides\nactionable guidance for refinement, such as identifying missing or erroneous\nfacts. We demonstrate the effectiveness of NarrativeFactScore through a\ndetailed workflow illustration and extensive validation on widely adopted\nbenchmarks, achieving superior performance compared to competitive methods. Our\nresults highlight the potential of agent-driven evaluation systems to improve\nthe factual reliability of LLM-generated summaries."
                },
                "authors": [
                    {
                        "name": "Yeonseok Jeong"
                    },
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Seung-won Hwang"
                    },
                    {
                        "name": "Byung-Hak Kim"
                    }
                ],
                "author_detail": {
                    "name": "Byung-Hak Kim"
                },
                "author": "Byung-Hak Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.10083v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.10083v3",
                "updated": "2025-01-17T06:49:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    6,
                    49,
                    46,
                    4,
                    17,
                    0
                ],
                "published": "2023-04-20T04:34:18Z",
                "published_parsed": [
                    2023,
                    4,
                    20,
                    4,
                    34,
                    18,
                    3,
                    110,
                    0
                ],
                "title": "Improved methodology for deep aquifer characterization using\n  hydrogeological, self-potential, and magnetotellurics data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved methodology for deep aquifer characterization using\n  hydrogeological, self-potential, and magnetotellurics data"
                },
                "summary": "Estimating subsurface properties like hydraulic conductivity using\nhydrogeological data alone is challenging in field sites with sparse wells.\nGeophysical data, including Self-potential (SP) and Magnetotelluric (MT), can\nimprove understanding of hydrogeological structures and interpolate data\nbetween wells. However, determining hydraulic conductivity requires a proper\npetrophysical relationship between hydraulic conductivity and inferred\ngeophysical properties, which may not exist or be unique. In this work, we\npropose a joint-inversion approach without assuming petrophysical\nrelationships, using self-potential data to connect groundwater flow velocity\nto electrical potential differences, and MT data to estimate hydraulic\nconductivity and electrical conductivity. A spectral method is employed for the\nself-potential forward problem. To accelerate joint data inversion, a dimension\nreduction technique through the Principal Component Geostatistical Approach is\nused. The applicability and robustness of the joint-inversion method are\ndemonstrated through inversion tests using hydrogeophysical data sets generated\nfrom subsurface models with and without petrophysical relationships. The joint\nhydraulic head-SP-MT data inversion can reasonably estimate hydraulic\nconductivity and electrical resistivity, even without knowledge of a one-to-one\npetrophysical relationship. On average, joint inversion yields a 25%\nimprovement in hydraulic conductivity estimates compared to single data-type\ninversion. Our proposed joint inversion approach, with SP-data compensating for\nthe absence of a known petrophysical relationship, provided close agreement\nwith joint inversion of head and MT data using a known petrophysical\nrelationship. Successful inversion tests demonstrate the usefulness of SP data\nin connecting hydrogeological properties and geophysical data without requiring\na petrophysical relationship.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating subsurface properties like hydraulic conductivity using\nhydrogeological data alone is challenging in field sites with sparse wells.\nGeophysical data, including Self-potential (SP) and Magnetotelluric (MT), can\nimprove understanding of hydrogeological structures and interpolate data\nbetween wells. However, determining hydraulic conductivity requires a proper\npetrophysical relationship between hydraulic conductivity and inferred\ngeophysical properties, which may not exist or be unique. In this work, we\npropose a joint-inversion approach without assuming petrophysical\nrelationships, using self-potential data to connect groundwater flow velocity\nto electrical potential differences, and MT data to estimate hydraulic\nconductivity and electrical conductivity. A spectral method is employed for the\nself-potential forward problem. To accelerate joint data inversion, a dimension\nreduction technique through the Principal Component Geostatistical Approach is\nused. The applicability and robustness of the joint-inversion method are\ndemonstrated through inversion tests using hydrogeophysical data sets generated\nfrom subsurface models with and without petrophysical relationships. The joint\nhydraulic head-SP-MT data inversion can reasonably estimate hydraulic\nconductivity and electrical resistivity, even without knowledge of a one-to-one\npetrophysical relationship. On average, joint inversion yields a 25%\nimprovement in hydraulic conductivity estimates compared to single data-type\ninversion. Our proposed joint inversion approach, with SP-data compensating for\nthe absence of a known petrophysical relationship, provided close agreement\nwith joint inversion of head and MT data using a known petrophysical\nrelationship. Successful inversion tests demonstrate the usefulness of SP data\nin connecting hydrogeological properties and geophysical data without requiring\na petrophysical relationship."
                },
                "authors": [
                    {
                        "name": "Young-Ho Seo"
                    },
                    {
                        "name": "Jonghyun Lee"
                    },
                    {
                        "name": "Aly I. El-Kadi"
                    },
                    {
                        "name": "Niels Grobbe"
                    }
                ],
                "author_detail": {
                    "name": "Niels Grobbe"
                },
                "author": "Niels Grobbe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.10083v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.10083v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09967v1",
                "updated": "2025-01-17T06:16:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    6,
                    16,
                    57,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T06:16:57Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    6,
                    16,
                    57,
                    4,
                    17,
                    0
                ],
                "title": "Explainable artificial intelligence (XAI): from inherent explainability\n  to large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable artificial intelligence (XAI): from inherent explainability\n  to large language models"
                },
                "summary": "Artificial Intelligence (AI) has continued to achieve tremendous success in\nrecent times. However, the decision logic of these frameworks is often not\ntransparent, making it difficult for stakeholders to understand, interpret or\nexplain their behavior. This limitation hinders trust in machine learning\nsystems and causes a general reluctance towards their adoption in practical\napplications, particularly in mission-critical domains like healthcare and\nautonomous driving. Explainable AI (XAI) techniques facilitate the\nexplainability or interpretability of machine learning models, enabling users\nto discern the basis of the decision and possibly avert undesirable behavior.\nThis comprehensive survey details the advancements of explainable AI methods,\nfrom inherently interpretable models to modern approaches for achieving\ninterpretability of various black box models, including large language models\n(LLMs). Additionally, we review explainable AI techniques that leverage LLM and\nvision-language model (VLM) frameworks to automate or improve the\nexplainability of other machine learning models. The use of LLM and VLM as\ninterpretability methods particularly enables high-level, semantically\nmeaningful explanations of model decisions and behavior. Throughout the paper,\nwe highlight the scientific principles, strengths and weaknesses of\nstate-of-the-art methods and outline different areas of improvement. Where\nappropriate, we also present qualitative and quantitative comparison results of\nvarious methods to show how they compare. Finally, we discuss the key\nchallenges of XAI and directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) has continued to achieve tremendous success in\nrecent times. However, the decision logic of these frameworks is often not\ntransparent, making it difficult for stakeholders to understand, interpret or\nexplain their behavior. This limitation hinders trust in machine learning\nsystems and causes a general reluctance towards their adoption in practical\napplications, particularly in mission-critical domains like healthcare and\nautonomous driving. Explainable AI (XAI) techniques facilitate the\nexplainability or interpretability of machine learning models, enabling users\nto discern the basis of the decision and possibly avert undesirable behavior.\nThis comprehensive survey details the advancements of explainable AI methods,\nfrom inherently interpretable models to modern approaches for achieving\ninterpretability of various black box models, including large language models\n(LLMs). Additionally, we review explainable AI techniques that leverage LLM and\nvision-language model (VLM) frameworks to automate or improve the\nexplainability of other machine learning models. The use of LLM and VLM as\ninterpretability methods particularly enables high-level, semantically\nmeaningful explanations of model decisions and behavior. Throughout the paper,\nwe highlight the scientific principles, strengths and weaknesses of\nstate-of-the-art methods and outline different areas of improvement. Where\nappropriate, we also present qualitative and quantitative comparison results of\nvarious methods to show how they compare. Finally, we discuss the key\nchallenges of XAI and directions for future research."
                },
                "authors": [
                    {
                        "name": "Fuseini Mumuni"
                    },
                    {
                        "name": "Alhassan Mumuni"
                    }
                ],
                "author_detail": {
                    "name": "Alhassan Mumuni"
                },
                "author": "Alhassan Mumuni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.13632v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.13632v4",
                "updated": "2025-01-17T06:09:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    6,
                    9,
                    13,
                    4,
                    17,
                    0
                ],
                "published": "2023-12-21T07:48:54Z",
                "published_parsed": [
                    2023,
                    12,
                    21,
                    7,
                    48,
                    54,
                    3,
                    355,
                    0
                ],
                "title": "TraceFL: Interpretability-Driven Debugging in Federated Learning via\n  Neuron Provenance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TraceFL: Interpretability-Driven Debugging in Federated Learning via\n  Neuron Provenance"
                },
                "summary": "In Federated Learning, clients train models on local data and send updates to\na central server, which aggregates them into a global model using a fusion\nalgorithm. This collaborative yet privacy-preserving training comes at a cost.\nFL developers face significant challenges in attributing global model\npredictions to specific clients. Localizing responsible clients is a crucial\nstep towards (a) excluding clients primarily responsible for incorrect\npredictions and (b) encouraging clients who contributed high-quality models to\ncontinue participating in the future. Existing ML debugging approaches are\ninherently inapplicable as they are designed for single-model, centralized\ntraining.\n  We introduce TraceFL, a fine-grained neuron provenance capturing mechanism\nthat identifies clients responsible for a global model's prediction by tracking\nthe flow of information from individual clients to the global model. Since\ninference on different inputs activates a different set of neurons of the\nglobal model, TraceFL dynamically quantifies the significance of the global\nmodel's neurons in a given prediction, identifying the most crucial neurons in\nthe global model. It then maps them to the corresponding neurons in every\nparticipating client to determine each client's contribution, ultimately\nlocalizing the responsible client. We evaluate TraceFL on six datasets,\nincluding two real-world medical imaging datasets and four neural networks,\nincluding advanced models such as GPT. TraceFL achieves 99% accuracy in\nlocalizing the responsible client in FL tasks spanning both image and text\nclassification tasks. At a time when state-of-the-artML debugging approaches\nare mostly domain-specific (e.g., image classification only), TraceFL is the\nfirst technique to enable highly accurate automated reasoning across a wide\nrange of FL applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Federated Learning, clients train models on local data and send updates to\na central server, which aggregates them into a global model using a fusion\nalgorithm. This collaborative yet privacy-preserving training comes at a cost.\nFL developers face significant challenges in attributing global model\npredictions to specific clients. Localizing responsible clients is a crucial\nstep towards (a) excluding clients primarily responsible for incorrect\npredictions and (b) encouraging clients who contributed high-quality models to\ncontinue participating in the future. Existing ML debugging approaches are\ninherently inapplicable as they are designed for single-model, centralized\ntraining.\n  We introduce TraceFL, a fine-grained neuron provenance capturing mechanism\nthat identifies clients responsible for a global model's prediction by tracking\nthe flow of information from individual clients to the global model. Since\ninference on different inputs activates a different set of neurons of the\nglobal model, TraceFL dynamically quantifies the significance of the global\nmodel's neurons in a given prediction, identifying the most crucial neurons in\nthe global model. It then maps them to the corresponding neurons in every\nparticipating client to determine each client's contribution, ultimately\nlocalizing the responsible client. We evaluate TraceFL on six datasets,\nincluding two real-world medical imaging datasets and four neural networks,\nincluding advanced models such as GPT. TraceFL achieves 99% accuracy in\nlocalizing the responsible client in FL tasks spanning both image and text\nclassification tasks. At a time when state-of-the-artML debugging approaches\nare mostly domain-specific (e.g., image classification only), TraceFL is the\nfirst technique to enable highly accurate automated reasoning across a wide\nrange of FL applications."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Ali Anwar"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ali Gulzar"
                },
                "arxiv_affiliation": "Virginia Tech",
                "author": "Muhammad Ali Gulzar",
                "arxiv_comment": "Accepted at 2025 IEEE/ACM 47th International Conference on Software\n  Engineering (ICSE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.13632v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.13632v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05844v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05844v2",
                "updated": "2025-01-17T05:33:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    5,
                    33,
                    54,
                    4,
                    17,
                    0
                ],
                "published": "2024-11-06T15:32:28Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    15,
                    32,
                    28,
                    2,
                    311,
                    0
                ],
                "title": "LEGO-GraphRAG: Modularizing Graph-based Retrieval-Augmented Generation\n  for Design Space Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEGO-GraphRAG: Modularizing Graph-based Retrieval-Augmented Generation\n  for Design Space Exploration"
                },
                "summary": "GraphRAG integrates (knowledge) graphs with large language models (LLMs) to\nimprove reasoning accuracy and contextual relevance. Despite its promising\napplications and strong relevance to multiple research communities, such as\ndatabases and natural language processing, GraphRAG currently lacks modular\nworkflow analysis, systematic solution frameworks, and insightful empirical\nstudies. To bridge these gaps, we propose LEGO-GraphRAG, a modular framework\nthat enables: 1) fine-grained decomposition of the GraphRAG workflow, 2)\nsystematic classification of existing techniques and implemented GraphRAG\ninstances, and 3) creation of new GraphRAG instances. Our framework facilitates\ncomprehensive empirical studies of GraphRAG on large-scale real-world graphs\nand diverse query sets, revealing insights into balancing reasoning quality,\nruntime efficiency, and token or GPU cost, that are essential for building\nadvanced GraphRAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphRAG integrates (knowledge) graphs with large language models (LLMs) to\nimprove reasoning accuracy and contextual relevance. Despite its promising\napplications and strong relevance to multiple research communities, such as\ndatabases and natural language processing, GraphRAG currently lacks modular\nworkflow analysis, systematic solution frameworks, and insightful empirical\nstudies. To bridge these gaps, we propose LEGO-GraphRAG, a modular framework\nthat enables: 1) fine-grained decomposition of the GraphRAG workflow, 2)\nsystematic classification of existing techniques and implemented GraphRAG\ninstances, and 3) creation of new GraphRAG instances. Our framework facilitates\ncomprehensive empirical studies of GraphRAG on large-scale real-world graphs\nand diverse query sets, revealing insights into balancing reasoning quality,\nruntime efficiency, and token or GPU cost, that are essential for building\nadvanced GraphRAG systems."
                },
                "authors": [
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Zengyi Gao"
                    },
                    {
                        "name": "Zhiyang Li"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "Kevin Zhou"
                    },
                    {
                        "name": "Jianliang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jianliang Xu"
                },
                "author": "Jianliang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05844v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05844v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09959v1",
                "updated": "2025-01-17T05:21:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    5,
                    21,
                    49,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T05:21:49Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    5,
                    21,
                    49,
                    4,
                    17,
                    0
                ],
                "title": "A Survey on Multi-Turn Interaction Capabilities of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Multi-Turn Interaction Capabilities of Large Language Models"
                },
                "summary": "Multi-turn interaction in the dialogue system research refers to a system's\nability to maintain context across multiple dialogue turns, enabling it to\ngenerate coherent and contextually relevant responses. Recent advancements in\nlarge language models (LLMs) have significantly expanded the scope of\nmulti-turn interaction, moving beyond chatbots to enable more dynamic agentic\ninteractions with users or environments. In this paper, we provide a focused\nreview of the multi-turn capabilities of LLMs, which are critical for a wide\nrange of downstream applications, including conversational search and\nrecommendation, consultation services, and interactive tutoring. This survey\nexplores four key aspects: (1) the core model capabilities that contribute to\neffective multi-turn interaction, (2) how multi-turn interaction is evaluated\nin current practice, (3) the general algorithms used to enhance multi-turn\ninteraction, and (4) potential future directions for research in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn interaction in the dialogue system research refers to a system's\nability to maintain context across multiple dialogue turns, enabling it to\ngenerate coherent and contextually relevant responses. Recent advancements in\nlarge language models (LLMs) have significantly expanded the scope of\nmulti-turn interaction, moving beyond chatbots to enable more dynamic agentic\ninteractions with users or environments. In this paper, we provide a focused\nreview of the multi-turn capabilities of LLMs, which are critical for a wide\nrange of downstream applications, including conversational search and\nrecommendation, consultation services, and interactive tutoring. This survey\nexplores four key aspects: (1) the core model capabilities that contribute to\neffective multi-turn interaction, (2) how multi-turn interaction is evaluated\nin current practice, (3) the general algorithms used to enhance multi-turn\ninteraction, and (4) potential future directions for research in this field."
                },
                "authors": [
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Xinyi Dai"
                    },
                    {
                        "name": "Yaxiong Wu"
                    },
                    {
                        "name": "Qu Yang"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "Draft Version, 14 pages, Ongoing refinement over time",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09957v1",
                "updated": "2025-01-17T05:19:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    5,
                    19,
                    14,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T05:19:14Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    5,
                    19,
                    14,
                    4,
                    17,
                    0
                ],
                "title": "FRAG: A Flexible Modular Framework for Retrieval-Augmented Generation\n  based on Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FRAG: A Flexible Modular Framework for Retrieval-Augmented Generation\n  based on Knowledge Graphs"
                },
                "summary": "To mitigate the hallucination and knowledge deficiency in large language\nmodels (LLMs), Knowledge Graph (KG)-based Retrieval-Augmented Generation (RAG)\nhas shown promising potential by utilizing KGs as external resource to enhance\nLLMs reasoning.However, existing KG-RAG approaches struggle with a trade-off\nbetween flexibility and retrieval quality.Modular methods prioritize\nflexibility by avoiding the use of KG-fine-tuned models during retrieval,\nleading to fixed retrieval strategies and suboptimal retrieval\nquality.Conversely, coupled methods embed KG information within models to\nimprove retrieval quality, but at the expense of flexibility.In this paper, we\npropose a novel flexible modular KG-RAG framework, termed FRAG, which\nsynergizes the advantages of both approaches.FRAG estimates the hop range of\nreasoning paths based solely on the query and classify it as either simple or\ncomplex.To match the complexity of the query, tailored pipelines are applied to\nensure efficient and accurate reasoning path retrieval, thus fostering the\nfinal reasoning process.By using the query text instead of the KG to infer the\nstructural information of reasoning paths and employing adaptable retrieval\nstrategies, FRAG improves retrieval quality while maintaining\nflexibility.Moreover, FRAG does not require extra LLMs fine-tuning or calls,\nsignificantly boosting efficiency and conserving resources.Extensive\nexperiments show that FRAG achieves state-of-the-art performance with high\nefficiency and low resource consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To mitigate the hallucination and knowledge deficiency in large language\nmodels (LLMs), Knowledge Graph (KG)-based Retrieval-Augmented Generation (RAG)\nhas shown promising potential by utilizing KGs as external resource to enhance\nLLMs reasoning.However, existing KG-RAG approaches struggle with a trade-off\nbetween flexibility and retrieval quality.Modular methods prioritize\nflexibility by avoiding the use of KG-fine-tuned models during retrieval,\nleading to fixed retrieval strategies and suboptimal retrieval\nquality.Conversely, coupled methods embed KG information within models to\nimprove retrieval quality, but at the expense of flexibility.In this paper, we\npropose a novel flexible modular KG-RAG framework, termed FRAG, which\nsynergizes the advantages of both approaches.FRAG estimates the hop range of\nreasoning paths based solely on the query and classify it as either simple or\ncomplex.To match the complexity of the query, tailored pipelines are applied to\nensure efficient and accurate reasoning path retrieval, thus fostering the\nfinal reasoning process.By using the query text instead of the KG to infer the\nstructural information of reasoning paths and employing adaptable retrieval\nstrategies, FRAG improves retrieval quality while maintaining\nflexibility.Moreover, FRAG does not require extra LLMs fine-tuning or calls,\nsignificantly boosting efficiency and conserving resources.Extensive\nexperiments show that FRAG achieves state-of-the-art performance with high\nefficiency and low resource consumption."
                },
                "authors": [
                    {
                        "name": "Zengyi Gao"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Hairu Wang"
                    },
                    {
                        "name": "Ao Ke"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S Kevin Zhou"
                },
                "author": "S Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09954v1",
                "updated": "2025-01-17T04:57:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    4,
                    57,
                    42,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T04:57:42Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    4,
                    57,
                    42,
                    4,
                    17,
                    0
                ],
                "title": "AIRCHITECT v2: Learning the Hardware Accelerator Design Space through\n  Unified Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIRCHITECT v2: Learning the Hardware Accelerator Design Space through\n  Unified Representations"
                },
                "summary": "Design space exploration (DSE) plays a crucial role in enabling custom\nhardware architectures, particularly for emerging applications like AI, where\noptimized and specialized designs are essential. With the growing complexity of\ndeep neural networks (DNNs) and the introduction of advanced foundational\nmodels (FMs), the design space for DNN accelerators is expanding at an\nexponential rate. Additionally, this space is highly non-uniform and\nnon-convex, making it increasingly difficult to navigate and optimize.\nTraditional DSE techniques rely on search-based methods, which involve\niterative sampling of the design space to find the optimal solution. However,\nthis process is both time-consuming and often fails to converge to the global\noptima for such design spaces. Recently, AIrchitect v1, the first attempt to\naddress the limitations of search-based techniques, transformed DSE into a\nconstant-time classification problem using recommendation networks. In this\nwork, we propose AIrchitect v2, a more accurate and generalizable\nlearning-based DSE technique applicable to large-scale design spaces that\novercomes the shortcomings of earlier approaches. Specifically, we devise an\nencoder-decoder transformer model that (a) encodes the complex design space\ninto a uniform intermediate representation using contrastive learning and (b)\nleverages a novel unified representation blending the advantages of\nclassification and regression to effectively explore the large DSE space\nwithout sacrificing accuracy. Experimental results evaluated on 10^5 real DNN\nworkloads demonstrate that, on average, AIrchitect v2 outperforms existing\ntechniques by 15% in identifying optimal design points. Furthermore, to\ndemonstrate the generalizability of our method, we evaluate performance on\nunseen model workloads (LLMs) and attain a 1.7x improvement in inference\nlatency on the identified hardware architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design space exploration (DSE) plays a crucial role in enabling custom\nhardware architectures, particularly for emerging applications like AI, where\noptimized and specialized designs are essential. With the growing complexity of\ndeep neural networks (DNNs) and the introduction of advanced foundational\nmodels (FMs), the design space for DNN accelerators is expanding at an\nexponential rate. Additionally, this space is highly non-uniform and\nnon-convex, making it increasingly difficult to navigate and optimize.\nTraditional DSE techniques rely on search-based methods, which involve\niterative sampling of the design space to find the optimal solution. However,\nthis process is both time-consuming and often fails to converge to the global\noptima for such design spaces. Recently, AIrchitect v1, the first attempt to\naddress the limitations of search-based techniques, transformed DSE into a\nconstant-time classification problem using recommendation networks. In this\nwork, we propose AIrchitect v2, a more accurate and generalizable\nlearning-based DSE technique applicable to large-scale design spaces that\novercomes the shortcomings of earlier approaches. Specifically, we devise an\nencoder-decoder transformer model that (a) encodes the complex design space\ninto a uniform intermediate representation using contrastive learning and (b)\nleverages a novel unified representation blending the advantages of\nclassification and regression to effectively explore the large DSE space\nwithout sacrificing accuracy. Experimental results evaluated on 10^5 real DNN\nworkloads demonstrate that, on average, AIrchitect v2 outperforms existing\ntechniques by 15% in identifying optimal design points. Furthermore, to\ndemonstrate the generalizability of our method, we evaluate performance on\nunseen model workloads (LLMs) and attain a 1.7x improvement in inference\nlatency on the identified hardware architecture."
                },
                "authors": [
                    {
                        "name": "Jamin Seo"
                    },
                    {
                        "name": "Akshat Ramachandran"
                    },
                    {
                        "name": "Yu-Chuan Chuang"
                    },
                    {
                        "name": "Anirudh Itagi"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "arxiv_comment": "Accepted to DATE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00771v2",
                "updated": "2025-01-17T04:47:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    4,
                    47,
                    11,
                    4,
                    17,
                    0
                ],
                "published": "2024-10-01T15:07:07Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    15,
                    7,
                    7,
                    1,
                    275,
                    0
                ],
                "title": "Empowering Large Language Model for Continual Video Question Answering\n  with Collaborative Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering Large Language Model for Continual Video Question Answering\n  with Collaborative Prompting"
                },
                "summary": "In recent years, the rapid increase in online video content has underscored\nthe limitations of static Video Question Answering (VideoQA) models trained on\nfixed datasets, as they struggle to adapt to new questions or tasks posed by\nnewly available content. In this paper, we explore the novel challenge of\nVideoQA within a continual learning framework, and empirically identify a\ncritical issue: fine-tuning a large language model (LLM) for a sequence of\ntasks often results in catastrophic forgetting. To address this, we propose\nCollaborative Prompting (ColPro), which integrates specific question constraint\nprompting, knowledge acquisition prompting, and visual temporal awareness\nprompting. These prompts aim to capture textual question context, visual\ncontent, and video temporal dynamics in VideoQA, a perspective underexplored in\nprior research. Experimental results on the NExT-QA and DramaQA datasets show\nthat ColPro achieves superior performance compared to existing approaches,\nachieving 55.14\\% accuracy on NExT-QA and 71.24\\% accuracy on DramaQA,\nhighlighting its practical relevance and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the rapid increase in online video content has underscored\nthe limitations of static Video Question Answering (VideoQA) models trained on\nfixed datasets, as they struggle to adapt to new questions or tasks posed by\nnewly available content. In this paper, we explore the novel challenge of\nVideoQA within a continual learning framework, and empirically identify a\ncritical issue: fine-tuning a large language model (LLM) for a sequence of\ntasks often results in catastrophic forgetting. To address this, we propose\nCollaborative Prompting (ColPro), which integrates specific question constraint\nprompting, knowledge acquisition prompting, and visual temporal awareness\nprompting. These prompts aim to capture textual question context, visual\ncontent, and video temporal dynamics in VideoQA, a perspective underexplored in\nprior research. Experimental results on the NExT-QA and DramaQA datasets show\nthat ColPro achieves superior performance compared to existing approaches,\nachieving 55.14\\% accuracy on NExT-QA and 71.24\\% accuracy on DramaQA,\nhighlighting its practical relevance and effectiveness."
                },
                "authors": [
                    {
                        "name": "Chen Cai"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Jianjun Gao"
                    },
                    {
                        "name": "Wenyang Liu"
                    },
                    {
                        "name": "Ye Lu"
                    },
                    {
                        "name": "Runzhong Zhang"
                    },
                    {
                        "name": "Kim-Hui Yap"
                    }
                ],
                "author_detail": {
                    "name": "Kim-Hui Yap"
                },
                "author": "Kim-Hui Yap",
                "arxiv_comment": "Accepted by main EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.10444v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.10444v5",
                "updated": "2025-01-17T04:45:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    4,
                    45,
                    45,
                    4,
                    17,
                    0
                ],
                "published": "2023-09-19T09:04:15Z",
                "published_parsed": [
                    2023,
                    9,
                    19,
                    9,
                    4,
                    15,
                    1,
                    262,
                    0
                ],
                "title": "Exploring Iterative Enhancement for Improving Learnersourced\n  Multiple-Choice Question Explanations with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Iterative Enhancement for Improving Learnersourced\n  Multiple-Choice Question Explanations with Large Language Models"
                },
                "summary": "Large language models exhibit superior capabilities in processing and\nunderstanding language, yet their applications in educational contexts remain\nunderexplored. Learnersourcing enhances learning by engaging students in\ncreating their own educational content. When learnersourcing multiple-choice\nquestions, creating explanations for the solution of a question is a crucial\nstep; it helps other students understand the solution and promotes a deeper\nunderstanding of related concepts. However, it is often difficult for students\nto craft effective solution explanations, due to limited subject understanding.\nTo help scaffold the task of automated explanation generation, we present and\nevaluate a framework called \"ILearner-LLM\", that iteratively enhances the\ngenerated explanations for the given questions with large language models.\nComprising an explanation generation model and an explanation evaluation model,\nthe framework generates high-quality student-aligned explanations by\niteratively feeding the quality rating score from the evaluation model back\ninto the instruction prompt of the explanation generation model. Experimental\nresults demonstrate the effectiveness of our ILearner-LLM on LLaMA2-13B and\nGPT-4 to generate higher quality explanations that are closer to those written\nby students on five PeerWise datasets. Our findings represent a promising path\nto enrich the learnersourcing experience for students and to enhance the\ncapabilities of large language models for educational applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models exhibit superior capabilities in processing and\nunderstanding language, yet their applications in educational contexts remain\nunderexplored. Learnersourcing enhances learning by engaging students in\ncreating their own educational content. When learnersourcing multiple-choice\nquestions, creating explanations for the solution of a question is a crucial\nstep; it helps other students understand the solution and promotes a deeper\nunderstanding of related concepts. However, it is often difficult for students\nto craft effective solution explanations, due to limited subject understanding.\nTo help scaffold the task of automated explanation generation, we present and\nevaluate a framework called \"ILearner-LLM\", that iteratively enhances the\ngenerated explanations for the given questions with large language models.\nComprising an explanation generation model and an explanation evaluation model,\nthe framework generates high-quality student-aligned explanations by\niteratively feeding the quality rating score from the evaluation model back\ninto the instruction prompt of the explanation generation model. Experimental\nresults demonstrate the effectiveness of our ILearner-LLM on LLaMA2-13B and\nGPT-4 to generate higher quality explanations that are closer to those written\nby students on five PeerWise datasets. Our findings represent a promising path\nto enrich the learnersourcing experience for students and to enhance the\ncapabilities of large language models for educational applications."
                },
                "authors": [
                    {
                        "name": "Qiming Bao"
                    },
                    {
                        "name": "Juho Leinonen"
                    },
                    {
                        "name": "Alex Yuxuan Peng"
                    },
                    {
                        "name": "Wanjun Zhong"
                    },
                    {
                        "name": "Gal Gendron"
                    },
                    {
                        "name": "Timothy Pistotti"
                    },
                    {
                        "name": "Alice Huang"
                    },
                    {
                        "name": "Paul Denny"
                    },
                    {
                        "name": "Michael Witbrock"
                    },
                    {
                        "name": "Jiamou Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiamou Liu"
                },
                "author": "Jiamou Liu",
                "arxiv_comment": "The short version (v4) has been accepted as a non-archival workshop\n  paper at AGI@ICLR 2024, and the full version has been accepted by the main\n  track of AAAI/EAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.10444v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.10444v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.09430v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.09430v5",
                "updated": "2025-01-17T04:39:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    4,
                    39,
                    38,
                    4,
                    17,
                    0
                ],
                "published": "2023-10-13T22:29:15Z",
                "published_parsed": [
                    2023,
                    10,
                    13,
                    22,
                    29,
                    15,
                    4,
                    286,
                    0
                ],
                "title": "Assessing and Enhancing the Robustness of Large Language Models with\n  Task Structure Variations for Logical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing and Enhancing the Robustness of Large Language Models with\n  Task Structure Variations for Logical Reasoning"
                },
                "summary": "Large language models (LLMs), such as LLaMA, Alpaca, Vicuna, GPT-3.5 and\nGPT-4, have advanced the performance of AI systems on various natural language\nprocessing tasks to human-like levels. However, their generalisation and\nrobustness when performing logical reasoning has not been sufficiently\nassessed. To comprehensively evaluate this ability, we develop three new\nlogical reasoning datasets named \"ReClor-plus\", \"LogiQA-plus\" and\n\"LogiQAv2-plus\" that extend standard logical reasoning datasets to evaluate the\nrobustness of the LLM's reasoning. For each, we create three subsets: the first\nwith randomly shuffled options, the second with the correct choices replaced by\n\"none of the other options is correct\", and the third with a combination of\nshuffling and substitution. Experiments on these datasets show that these\nsimple augmentations greatly hinder the models' performance. Despite their high\nperformance on the original publicly available datasets, we find that all\nmodels perform poorly on these newly constructed datasets. We also demonstrate\nthat introducing task variations into the training set can markedly improve the\nmodel's performance on both the original and our developed datasets. Finally,\nwe show that applying logic-driven data augmentation for fine-tuning and\nprompting can enhance generalisation in both discriminative and generative\nmodels, offering a path to improving their robustness for tasks involving\nlogical reasoning. Source code and data are made publicly available at\nhttps://github.com/Strong-AI-Lab/Logical-and-abstract-reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), such as LLaMA, Alpaca, Vicuna, GPT-3.5 and\nGPT-4, have advanced the performance of AI systems on various natural language\nprocessing tasks to human-like levels. However, their generalisation and\nrobustness when performing logical reasoning has not been sufficiently\nassessed. To comprehensively evaluate this ability, we develop three new\nlogical reasoning datasets named \"ReClor-plus\", \"LogiQA-plus\" and\n\"LogiQAv2-plus\" that extend standard logical reasoning datasets to evaluate the\nrobustness of the LLM's reasoning. For each, we create three subsets: the first\nwith randomly shuffled options, the second with the correct choices replaced by\n\"none of the other options is correct\", and the third with a combination of\nshuffling and substitution. Experiments on these datasets show that these\nsimple augmentations greatly hinder the models' performance. Despite their high\nperformance on the original publicly available datasets, we find that all\nmodels perform poorly on these newly constructed datasets. We also demonstrate\nthat introducing task variations into the training set can markedly improve the\nmodel's performance on both the original and our developed datasets. Finally,\nwe show that applying logic-driven data augmentation for fine-tuning and\nprompting can enhance generalisation in both discriminative and generative\nmodels, offering a path to improving their robustness for tasks involving\nlogical reasoning. Source code and data are made publicly available at\nhttps://github.com/Strong-AI-Lab/Logical-and-abstract-reasoning."
                },
                "authors": [
                    {
                        "name": "Qiming Bao"
                    },
                    {
                        "name": "Gael Gendron"
                    },
                    {
                        "name": "Alex Yuxuan Peng"
                    },
                    {
                        "name": "Wanjun Zhong"
                    },
                    {
                        "name": "Neset Tan"
                    },
                    {
                        "name": "Yang Chen"
                    },
                    {
                        "name": "Michael Witbrock"
                    },
                    {
                        "name": "Jiamou Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiamou Liu"
                },
                "author": "Jiamou Liu",
                "arxiv_comment": "The short version (v3) was accepted for oral presentation at the\n  first LLM@IJCAI 2023 non-archival symposium, and the full version was\n  accepted by ICONIP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.09430v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.09430v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01892v2",
                "updated": "2025-01-17T04:29:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    4,
                    29,
                    47,
                    4,
                    17,
                    0
                ],
                "published": "2024-07-02T02:27:46Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    2,
                    27,
                    46,
                    1,
                    184,
                    0
                ],
                "title": "GRASP: A Grid-Based Benchmark for Evaluating Commonsense Spatial\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRASP: A Grid-Based Benchmark for Evaluating Commonsense Spatial\n  Reasoning"
                },
                "summary": "Spatial reasoning, an important faculty of human cognition with many\npractical applications, is one of the core commonsense skills that is not\npurely language-based and, for satisfying (as opposed to optimal) solutions,\nrequires some minimum degree of planning. Existing benchmarks of Commonsense\nSpatial Reasoning (CSR) tend to evaluate how Large Language Models (LLMs)\ninterpret text-based spatial $\\textit{descriptions}$ rather than directly\nevaluate a plan produced by the LLM in response to a $\\textit{specific}$\nspatial reasoning problem. In this paper, we construct a large-scale benchmark\ncalled GRASP, which consists of 16,000 grid-based environments where the agent\nis tasked with an energy collection problem. These environments include 100\ngrid instances instantiated using each of the 160 different grid settings,\ninvolving five different energy distributions, two modes of agent starting\nposition, and two distinct obstacle configurations, as well as three kinds of\nagent constraints. Using GRASP, we compare classic baseline approaches, such as\nrandom walk and greedy search methods, with advanced LLMs like GPT-3.5-Turbo,\nGPT-4o, and GPT-o1-mini. The experimental results indicate that even these\nadvanced LLMs struggle to consistently achieve satisfactory solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial reasoning, an important faculty of human cognition with many\npractical applications, is one of the core commonsense skills that is not\npurely language-based and, for satisfying (as opposed to optimal) solutions,\nrequires some minimum degree of planning. Existing benchmarks of Commonsense\nSpatial Reasoning (CSR) tend to evaluate how Large Language Models (LLMs)\ninterpret text-based spatial $\\textit{descriptions}$ rather than directly\nevaluate a plan produced by the LLM in response to a $\\textit{specific}$\nspatial reasoning problem. In this paper, we construct a large-scale benchmark\ncalled GRASP, which consists of 16,000 grid-based environments where the agent\nis tasked with an energy collection problem. These environments include 100\ngrid instances instantiated using each of the 160 different grid settings,\ninvolving five different energy distributions, two modes of agent starting\nposition, and two distinct obstacle configurations, as well as three kinds of\nagent constraints. Using GRASP, we compare classic baseline approaches, such as\nrandom walk and greedy search methods, with advanced LLMs like GPT-3.5-Turbo,\nGPT-4o, and GPT-o1-mini. The experimental results indicate that even these\nadvanced LLMs struggle to consistently achieve satisfactory solutions."
                },
                "authors": [
                    {
                        "name": "Zhisheng Tang"
                    },
                    {
                        "name": "Mayank Kejriwal"
                    }
                ],
                "author_detail": {
                    "name": "Mayank Kejriwal"
                },
                "author": "Mayank Kejriwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17640v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17640v2",
                "updated": "2025-01-17T04:26:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    4,
                    26,
                    44,
                    4,
                    17,
                    0
                ],
                "published": "2024-09-26T08:44:38Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    44,
                    38,
                    3,
                    270,
                    0
                ],
                "title": "T3: A Novel Zero-shot Transfer Learning Framework Iteratively Training\n  on an Assistant Task for a Target Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T3: A Novel Zero-shot Transfer Learning Framework Iteratively Training\n  on an Assistant Task for a Target Task"
                },
                "summary": "Long text summarization, gradually being essential for efficiently processing\nlarge volumes of information, stays challenging for Large Language Models\n(LLMs) such as GPT and LLaMA families because of the insufficient open-sourced\ntraining datasets and the high requirement of contextual details dealing. To\naddress the issue, we design a novel zero-shot transfer learning framework,\nabbreviated as T3, to iteratively training a baseline LLM on an assistant task\nfor the target task, where the former should own richer data resources and\nshare structural or semantic similarity with the latter. In practice, T3 is\napproached to deal with the long text summarization task by utilizing question\nanswering as the assistant task, and further validated its effectiveness on the\nBBC summary, NarraSum, FairytaleQA, and NLQuAD datasets, with up to nearly 14%\nimprovement in ROUGE, 35% improvement in BLEU, and 16% improvement in Factscore\ncompared to three baseline LLMs, demonstrating its potential for more\nassistant-target task combinations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long text summarization, gradually being essential for efficiently processing\nlarge volumes of information, stays challenging for Large Language Models\n(LLMs) such as GPT and LLaMA families because of the insufficient open-sourced\ntraining datasets and the high requirement of contextual details dealing. To\naddress the issue, we design a novel zero-shot transfer learning framework,\nabbreviated as T3, to iteratively training a baseline LLM on an assistant task\nfor the target task, where the former should own richer data resources and\nshare structural or semantic similarity with the latter. In practice, T3 is\napproached to deal with the long text summarization task by utilizing question\nanswering as the assistant task, and further validated its effectiveness on the\nBBC summary, NarraSum, FairytaleQA, and NLQuAD datasets, with up to nearly 14%\nimprovement in ROUGE, 35% improvement in BLEU, and 16% improvement in Factscore\ncompared to three baseline LLMs, demonstrating its potential for more\nassistant-target task combinations."
                },
                "authors": [
                    {
                        "name": "Xindi Tong"
                    },
                    {
                        "name": "Yujin Zhu"
                    },
                    {
                        "name": "Shijian Fan"
                    },
                    {
                        "name": "Liang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Liang Xu"
                },
                "author": "Liang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17640v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17640v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.11156v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.11156v4",
                "updated": "2025-01-17T04:21:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    4,
                    21,
                    47,
                    4,
                    17,
                    0
                ],
                "published": "2023-03-17T17:53:19Z",
                "published_parsed": [
                    2023,
                    3,
                    17,
                    17,
                    53,
                    19,
                    4,
                    76,
                    0
                ],
                "title": "Can AI-Generated Text be Reliably Detected?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can AI-Generated Text be Reliably Detected?"
                },
                "summary": "Large Language Models (LLMs) perform impressively well in various\napplications. However, the potential for misuse of these models in activities\nsuch as plagiarism, generating fake news, and spamming has raised concern about\ntheir responsible use. Consequently, the reliable detection of AI-generated\ntext has become a critical area of research. AI text detectors have shown to be\neffective under their specific settings. In this paper, we stress-test the\nrobustness of these AI text detectors in the presence of an attacker. We\nintroduce recursive paraphrasing attack to stress test a wide range of\ndetection schemes, including the ones using the watermarking as well as neural\nnetwork-based detectors, zero shot classifiers, and retrieval-based detectors.\nOur experiments conducted on passages, each approximately 300 tokens long,\nreveal the varying sensitivities of these detectors to our attacks. Our\nfindings indicate that while our recursive paraphrasing method can\nsignificantly reduce detection rates, it only slightly degrades text quality in\nmany cases, highlighting potential vulnerabilities in current detection systems\nin the presence of an attacker. Additionally, we investigate the susceptibility\nof watermarked LLMs to spoofing attacks aimed at misclassifying human-written\ntext as AI-generated. We demonstrate that an attacker can infer hidden AI text\nsignatures without white-box access to the detection method, potentially\nleading to reputational risks for LLM developers. Finally, we provide a\ntheoretical framework connecting the AUROC of the best possible detector to the\nTotal Variation distance between human and AI text distributions. This analysis\noffers insights into the fundamental challenges of reliable detection as\nlanguage models continue to advance. Our code is publicly available at\nhttps://github.com/vinusankars/Reliability-of-AI-text-detectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) perform impressively well in various\napplications. However, the potential for misuse of these models in activities\nsuch as plagiarism, generating fake news, and spamming has raised concern about\ntheir responsible use. Consequently, the reliable detection of AI-generated\ntext has become a critical area of research. AI text detectors have shown to be\neffective under their specific settings. In this paper, we stress-test the\nrobustness of these AI text detectors in the presence of an attacker. We\nintroduce recursive paraphrasing attack to stress test a wide range of\ndetection schemes, including the ones using the watermarking as well as neural\nnetwork-based detectors, zero shot classifiers, and retrieval-based detectors.\nOur experiments conducted on passages, each approximately 300 tokens long,\nreveal the varying sensitivities of these detectors to our attacks. Our\nfindings indicate that while our recursive paraphrasing method can\nsignificantly reduce detection rates, it only slightly degrades text quality in\nmany cases, highlighting potential vulnerabilities in current detection systems\nin the presence of an attacker. Additionally, we investigate the susceptibility\nof watermarked LLMs to spoofing attacks aimed at misclassifying human-written\ntext as AI-generated. We demonstrate that an attacker can infer hidden AI text\nsignatures without white-box access to the detection method, potentially\nleading to reputational risks for LLM developers. Finally, we provide a\ntheoretical framework connecting the AUROC of the best possible detector to the\nTotal Variation distance between human and AI text distributions. This analysis\noffers insights into the fundamental challenges of reliable detection as\nlanguage models continue to advance. Our code is publicly available at\nhttps://github.com/vinusankars/Reliability-of-AI-text-detectors."
                },
                "authors": [
                    {
                        "name": "Vinu Sankar Sadasivan"
                    },
                    {
                        "name": "Aounon Kumar"
                    },
                    {
                        "name": "Sriram Balasubramanian"
                    },
                    {
                        "name": "Wenxiao Wang"
                    },
                    {
                        "name": "Soheil Feizi"
                    }
                ],
                "author_detail": {
                    "name": "Soheil Feizi"
                },
                "author": "Soheil Feizi",
                "arxiv_comment": "Published in Transactions on Machine Learning Research (TMLR)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.11156v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.11156v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09948v1",
                "updated": "2025-01-17T04:20:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    4,
                    20,
                    43,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T04:20:43Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    4,
                    20,
                    43,
                    4,
                    17,
                    0
                ],
                "title": "AI Explainability for Power Electronics: From a Lipschitz Continuity\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Explainability for Power Electronics: From a Lipschitz Continuity\n  Perspective"
                },
                "summary": "Lifecycle management of power converters continues to thrive with emerging\nartificial intelligence (AI) solutions, yet AI mathematical explainability\nremains unexplored in power electronics (PE) community. The lack of theoretical\nrigor challenges adoption in mission-critical applications. Therefore, this\nletter proposes a generic framework to evaluate mathematical explainability,\nhighlighting inference stability and training convergence from a Lipschitz\ncontinuity perspective. Inference stability governs consistent outputs under\ninput perturbations, essential for robust real-time control and fault\ndiagnosis. Training convergence guarantees stable learning dynamics,\nfacilitating accurate modeling in PE contexts. Additionally, a Lipschitz-aware\nlearning rate selection strategy is introduced to accelerate convergence while\nmitigating overshoots and oscillations. The feasibility of the proposed\nLipschitz-oriented framework is demonstrated by validating the mathematical\nexplainability of a state-of-the-art physics-in-architecture neural network,\nand substantiated through empirical case studies on dual-active-bridge\nconverters. This letter serves as a clarion call for the PE community to\nembrace mathematical explainability, heralding a transformative era of\ntrustworthy and explainable AI solutions that potentially redefine the future\nof power electronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lifecycle management of power converters continues to thrive with emerging\nartificial intelligence (AI) solutions, yet AI mathematical explainability\nremains unexplored in power electronics (PE) community. The lack of theoretical\nrigor challenges adoption in mission-critical applications. Therefore, this\nletter proposes a generic framework to evaluate mathematical explainability,\nhighlighting inference stability and training convergence from a Lipschitz\ncontinuity perspective. Inference stability governs consistent outputs under\ninput perturbations, essential for robust real-time control and fault\ndiagnosis. Training convergence guarantees stable learning dynamics,\nfacilitating accurate modeling in PE contexts. Additionally, a Lipschitz-aware\nlearning rate selection strategy is introduced to accelerate convergence while\nmitigating overshoots and oscillations. The feasibility of the proposed\nLipschitz-oriented framework is demonstrated by validating the mathematical\nexplainability of a state-of-the-art physics-in-architecture neural network,\nand substantiated through empirical case studies on dual-active-bridge\nconverters. This letter serves as a clarion call for the PE community to\nembrace mathematical explainability, heralding a transformative era of\ntrustworthy and explainable AI solutions that potentially redefine the future\nof power electronics."
                },
                "authors": [
                    {
                        "name": "Xinze Li"
                    },
                    {
                        "name": "Fanfan Lin"
                    },
                    {
                        "name": "Homer Alan Mantooth"
                    },
                    {
                        "name": "Juan Jos Rodrguez-Andina"
                    }
                ],
                "author_detail": {
                    "name": "Juan Jos Rodrguez-Andina"
                },
                "author": "Juan Jos Rodrguez-Andina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16239v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16239v3",
                "updated": "2025-01-17T04:19:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    4,
                    19,
                    43,
                    4,
                    17,
                    0
                ],
                "published": "2024-11-25T09:54:42Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    54,
                    42,
                    0,
                    330,
                    0
                ],
                "title": "CS-Eval: A Comprehensive Large Language Model Benchmark for\n  CyberSecurity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CS-Eval: A Comprehensive Large Language Model Benchmark for\n  CyberSecurity"
                },
                "summary": "Over the past year, there has been a notable rise in the use of large\nlanguage models (LLMs) for academic research and industrial practices within\nthe cybersecurity field. However, it remains a lack of comprehensive and\npublicly accessible benchmarks to evaluate the performance of LLMs on\ncybersecurity tasks. To address this gap, we introduce CS-Eval, a publicly\naccessible, comprehensive and bilingual LLM benchmark specifically designed for\ncybersecurity. CS-Eval synthesizes the research hotspots from academia and\npractical applications from industry, curating a diverse set of high-quality\nquestions across 42 categories within cybersecurity, systematically organized\ninto three cognitive levels: knowledge, ability, and application. Through an\nextensive evaluation of a wide range of LLMs using CS-Eval, we have uncovered\nvaluable insights. For instance, while GPT-4 generally excels overall, other\nmodels may outperform it in certain specific subcategories. Additionally, by\nconducting evaluations over several months, we observed significant\nimprovements in many LLMs' abilities to solve cybersecurity tasks. The\nbenchmarks are now publicly available at https://github.com/CS-EVAL/CS-Eval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past year, there has been a notable rise in the use of large\nlanguage models (LLMs) for academic research and industrial practices within\nthe cybersecurity field. However, it remains a lack of comprehensive and\npublicly accessible benchmarks to evaluate the performance of LLMs on\ncybersecurity tasks. To address this gap, we introduce CS-Eval, a publicly\naccessible, comprehensive and bilingual LLM benchmark specifically designed for\ncybersecurity. CS-Eval synthesizes the research hotspots from academia and\npractical applications from industry, curating a diverse set of high-quality\nquestions across 42 categories within cybersecurity, systematically organized\ninto three cognitive levels: knowledge, ability, and application. Through an\nextensive evaluation of a wide range of LLMs using CS-Eval, we have uncovered\nvaluable insights. For instance, while GPT-4 generally excels overall, other\nmodels may outperform it in certain specific subcategories. Additionally, by\nconducting evaluations over several months, we observed significant\nimprovements in many LLMs' abilities to solve cybersecurity tasks. The\nbenchmarks are now publicly available at https://github.com/CS-EVAL/CS-Eval."
                },
                "authors": [
                    {
                        "name": "Zhengmin Yu"
                    },
                    {
                        "name": "Jiutian Zeng"
                    },
                    {
                        "name": "Siyi Chen"
                    },
                    {
                        "name": "Wenhan Xu"
                    },
                    {
                        "name": "Dandan Xu"
                    },
                    {
                        "name": "Xiangyu Liu"
                    },
                    {
                        "name": "Zonghao Ying"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Min Yang"
                },
                "author": "Min Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16239v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16239v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.03911v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.03911v4",
                "updated": "2025-01-17T04:10:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    4,
                    10,
                    18,
                    4,
                    17,
                    0
                ],
                "published": "2023-11-07T11:50:35Z",
                "published_parsed": [
                    2023,
                    11,
                    7,
                    11,
                    50,
                    35,
                    1,
                    311,
                    0
                ],
                "title": "Distributed Parameter Estimation with Gaussian Observation Noises in\n  Time-varying Digraphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Parameter Estimation with Gaussian Observation Noises in\n  Time-varying Digraphs"
                },
                "summary": "In this paper, we consider the problem of distributed parameter estimation in\nsensor networks. Each sensor makes successive observations of an unknown\n$d$-dimensional parameter, which might be subject to Gaussian random noises.\nThe sensors aim to infer the true value of the unknown parameter by cooperating\nwith each other. To this end, we first generalize the so-called dynamic\nregressor extension and mixing (DREM) algorithm to stochastic systems, with\nwhich the problem of estimating a $d$-dimensional vector parameter is\ntransformed to that of $d$ scalar ones: one for each of the unknown parameters.\nFor each of the scalar problem, both combine-then-adapt (CTA) and\nadapt-then-combine (ATC) diffusion-based estimation algorithms are given, where\neach sensor performs a combination step to fuse the local estimates in its\nin-neighborhood, alongside an adaptation step to process its streaming\nobservations. Under weak conditions on network topology and excitation of\nregressors, we show that the proposed estimators guarantee that each sensor\ninfers the true parameter, even if any individual of them cannot by itself.\nSpecifically, it is required that the union of topologies over an interval with\nfixed length is strongly connected. Moreover, the sensors must collectively\nsatisfy a cooperative persistent excitation (PE) condition, which relaxes the\ntraditional PE condition. Numerical examples are finally provided to illustrate\nthe established results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we consider the problem of distributed parameter estimation in\nsensor networks. Each sensor makes successive observations of an unknown\n$d$-dimensional parameter, which might be subject to Gaussian random noises.\nThe sensors aim to infer the true value of the unknown parameter by cooperating\nwith each other. To this end, we first generalize the so-called dynamic\nregressor extension and mixing (DREM) algorithm to stochastic systems, with\nwhich the problem of estimating a $d$-dimensional vector parameter is\ntransformed to that of $d$ scalar ones: one for each of the unknown parameters.\nFor each of the scalar problem, both combine-then-adapt (CTA) and\nadapt-then-combine (ATC) diffusion-based estimation algorithms are given, where\neach sensor performs a combination step to fuse the local estimates in its\nin-neighborhood, alongside an adaptation step to process its streaming\nobservations. Under weak conditions on network topology and excitation of\nregressors, we show that the proposed estimators guarantee that each sensor\ninfers the true parameter, even if any individual of them cannot by itself.\nSpecifically, it is required that the union of topologies over an interval with\nfixed length is strongly connected. Moreover, the sensors must collectively\nsatisfy a cooperative persistent excitation (PE) condition, which relaxes the\ntraditional PE condition. Numerical examples are finally provided to illustrate\nthe established results."
                },
                "authors": [
                    {
                        "name": "Jiaqi Yan"
                    },
                    {
                        "name": "Hideaki Ishii"
                    }
                ],
                "author_detail": {
                    "name": "Hideaki Ishii"
                },
                "author": "Hideaki Ishii",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.03911v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.03911v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16950v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16950v5",
                "updated": "2025-01-17T03:43:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    3,
                    43,
                    53,
                    4,
                    17,
                    0
                ],
                "published": "2024-03-25T17:11:28Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    17,
                    11,
                    28,
                    0,
                    85,
                    0
                ],
                "title": "Aligning with Human Judgement: The Role of Pairwise Preference in Large\n  Language Model Evaluators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning with Human Judgement: The Role of Pairwise Preference in Large\n  Language Model Evaluators"
                },
                "summary": "Large Language Models (LLMs) have demonstrated promising capabilities as\nautomatic evaluators in assessing the quality of generated natural language.\nHowever, LLMs still exhibit biases in evaluation and often struggle to generate\ncoherent evaluations that align with human assessments. In this work, we first\nconduct a systematic study of the misalignment between LLM evaluators and human\nevaluation, revealing that existing calibration methods aimed at mitigating\nbiases of LLMs are insufficient for effectively aligning LLM evaluators.\nInspired by the use of preference data in RLHF, we formulate the evaluation as\na ranking problem and introduce Pairwise-preference Search (PAIRS), an\nuncertainty-guided search-based rank aggregation method that employs LLMs to\nconduct pairwise comparisons locally and efficiently ranks candidate texts\nglobally. PAIRS achieves state-of-the-art performance on representative\nevaluation tasks in long-form generations and demonstrates significant\nimprovements over direct scoring. Furthermore, we provide insights into the\nrole of pairwise preference in quantifying the transitivity of LLMs and\ndemonstrate how PAIRS benefits from calibration using debiased pairwise\nevaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated promising capabilities as\nautomatic evaluators in assessing the quality of generated natural language.\nHowever, LLMs still exhibit biases in evaluation and often struggle to generate\ncoherent evaluations that align with human assessments. In this work, we first\nconduct a systematic study of the misalignment between LLM evaluators and human\nevaluation, revealing that existing calibration methods aimed at mitigating\nbiases of LLMs are insufficient for effectively aligning LLM evaluators.\nInspired by the use of preference data in RLHF, we formulate the evaluation as\na ranking problem and introduce Pairwise-preference Search (PAIRS), an\nuncertainty-guided search-based rank aggregation method that employs LLMs to\nconduct pairwise comparisons locally and efficiently ranks candidate texts\nglobally. PAIRS achieves state-of-the-art performance on representative\nevaluation tasks in long-form generations and demonstrates significant\nimprovements over direct scoring. Furthermore, we provide insights into the\nrole of pairwise preference in quantifying the transitivity of LLMs and\ndemonstrate how PAIRS benefits from calibration using debiased pairwise\nevaluations."
                },
                "authors": [
                    {
                        "name": "Yinhong Liu"
                    },
                    {
                        "name": "Han Zhou"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Ehsan Shareghi"
                    },
                    {
                        "name": "Ivan Vuli"
                    },
                    {
                        "name": "Anna Korhonen"
                    },
                    {
                        "name": "Nigel Collier"
                    }
                ],
                "author_detail": {
                    "name": "Nigel Collier"
                },
                "author": "Nigel Collier",
                "arxiv_comment": "This paper has been accepted by COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16950v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16950v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02933v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02933v4",
                "updated": "2025-01-17T03:19:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    3,
                    19,
                    16,
                    4,
                    17,
                    0
                ],
                "published": "2024-04-03T01:09:41Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    1,
                    9,
                    41,
                    2,
                    94,
                    0
                ],
                "title": "NL2KQL: From Natural Language to Kusto Query",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NL2KQL: From Natural Language to Kusto Query"
                },
                "summary": "Data is growing rapidly in volume and complexity. Proficiency in database\nquery languages is pivotal for crafting effective queries. As coding assistants\nbecome more prevalent, there is significant opportunity to enhance database\nquery languages. The Kusto Query Language (KQL) is a widely used query language\nfor large semi-structured data such as logs, telemetries, and time-series for\nbig data analytics platforms. This paper introduces NL2KQL an innovative\nframework that uses large language models (LLMs) to convert natural language\nqueries (NLQs) to KQL queries. The proposed NL2KQL framework includes several\nkey components: Schema Refiner which narrows down the schema to its most\npertinent elements; the Few-shot Selector which dynamically selects relevant\nexamples from a few-shot dataset; and the Query Refiner which repairs syntactic\nand semantic errors in KQL queries. Additionally, this study outlines a method\nfor generating large datasets of synthetic NLQ-KQL pairs which are valid within\na specific database contexts. To validate NL2KQL's performance, we utilize an\narray of online (based on query execution) and offline (based on query parsing)\nmetrics. Through ablation studies, the significance of each framework component\nis examined, and the datasets used for benchmarking are made publicly\navailable. This work is the first of its kind and is compared with available\nbaselines to demonstrate its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data is growing rapidly in volume and complexity. Proficiency in database\nquery languages is pivotal for crafting effective queries. As coding assistants\nbecome more prevalent, there is significant opportunity to enhance database\nquery languages. The Kusto Query Language (KQL) is a widely used query language\nfor large semi-structured data such as logs, telemetries, and time-series for\nbig data analytics platforms. This paper introduces NL2KQL an innovative\nframework that uses large language models (LLMs) to convert natural language\nqueries (NLQs) to KQL queries. The proposed NL2KQL framework includes several\nkey components: Schema Refiner which narrows down the schema to its most\npertinent elements; the Few-shot Selector which dynamically selects relevant\nexamples from a few-shot dataset; and the Query Refiner which repairs syntactic\nand semantic errors in KQL queries. Additionally, this study outlines a method\nfor generating large datasets of synthetic NLQ-KQL pairs which are valid within\na specific database contexts. To validate NL2KQL's performance, we utilize an\narray of online (based on query execution) and offline (based on query parsing)\nmetrics. Through ablation studies, the significance of each framework component\nis examined, and the datasets used for benchmarking are made publicly\navailable. This work is the first of its kind and is compared with available\nbaselines to demonstrate its effectiveness."
                },
                "authors": [
                    {
                        "name": "Xinye Tang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Jeremias Eichelbaum"
                    },
                    {
                        "name": "Mahan Das"
                    },
                    {
                        "name": "Alex Klein"
                    },
                    {
                        "name": "Nihal Irmak Pakis"
                    },
                    {
                        "name": "William Blum"
                    },
                    {
                        "name": "Daniel L Mace"
                    },
                    {
                        "name": "Tanvi Raja"
                    },
                    {
                        "name": "Namrata Padmanabhan"
                    },
                    {
                        "name": "Ye Xing"
                    }
                ],
                "author_detail": {
                    "name": "Ye Xing"
                },
                "author": "Ye Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02933v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02933v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09933v1",
                "updated": "2025-01-17T03:14:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    3,
                    14,
                    43,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T03:14:43Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    3,
                    14,
                    43,
                    4,
                    17,
                    0
                ],
                "title": "Statistical Inference for Sequential Feature Selection after Domain\n  Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Inference for Sequential Feature Selection after Domain\n  Adaptation"
                },
                "summary": "In high-dimensional regression, feature selection methods, such as sequential\nfeature selection (SeqFS), are commonly used to identify relevant features.\nWhen data is limited, domain adaptation (DA) becomes crucial for transferring\nknowledge from a related source domain to a target domain, improving\ngeneralization performance. Although SeqFS after DA is an important task in\nmachine learning, none of the existing methods can guarantee the reliability of\nits results. In this paper, we propose a novel method for testing the features\nselected by SeqFS-DA. The main advantage of the proposed method is its\ncapability to control the false positive rate (FPR) below a significance level\n$\\alpha$ (e.g., 0.05). Additionally, a strategic approach is introduced to\nenhance the statistical power of the test. Furthermore, we provide extensions\nof the proposed method to SeqFS with model selection criteria including AIC,\nBIC, and adjusted R-squared. Extensive experiments are conducted on both\nsynthetic and real-world datasets to validate the theoretical results and\ndemonstrate the proposed method's superior performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In high-dimensional regression, feature selection methods, such as sequential\nfeature selection (SeqFS), are commonly used to identify relevant features.\nWhen data is limited, domain adaptation (DA) becomes crucial for transferring\nknowledge from a related source domain to a target domain, improving\ngeneralization performance. Although SeqFS after DA is an important task in\nmachine learning, none of the existing methods can guarantee the reliability of\nits results. In this paper, we propose a novel method for testing the features\nselected by SeqFS-DA. The main advantage of the proposed method is its\ncapability to control the false positive rate (FPR) below a significance level\n$\\alpha$ (e.g., 0.05). Additionally, a strategic approach is introduced to\nenhance the statistical power of the test. Furthermore, we provide extensions\nof the proposed method to SeqFS with model selection criteria including AIC,\nBIC, and adjusted R-squared. Extensive experiments are conducted on both\nsynthetic and real-world datasets to validate the theoretical results and\ndemonstrate the proposed method's superior performance."
                },
                "authors": [
                    {
                        "name": "Duong Tan Loc"
                    },
                    {
                        "name": "Nguyen Thang Loi"
                    },
                    {
                        "name": "Vo Nguyen Le Duy"
                    }
                ],
                "author_detail": {
                    "name": "Vo Nguyen Le Duy"
                },
                "author": "Vo Nguyen Le Duy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10960v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10960v4",
                "updated": "2025-01-17T03:09:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    3,
                    9,
                    24,
                    4,
                    17,
                    0
                ],
                "published": "2024-07-15T17:55:42Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    55,
                    42,
                    0,
                    197,
                    0
                ],
                "title": "Fast Matrix Multiplications for Lookup Table-Quantized LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Matrix Multiplications for Lookup Table-Quantized LLMs"
                },
                "summary": "The deployment of large language models (LLMs) is often constrained by memory\nbandwidth, where the primary bottleneck is the cost of transferring model\nparameters from the GPU's global memory to its registers. When coupled with\ncustom kernels that fuse the dequantization and matmul operations, weight-only\nquantization can thus enable faster inference by reducing the amount of memory\nmovement. However, developing high-performance kernels for weight-quantized\nLLMs presents substantial challenges, especially when the weights are\ncompressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform,\nlookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup\ntable engine for LUT-quantized LLMs, which uses offline restructuring of the\nquantized weight matrix to minimize bit manipulations associated with\nunpacking, and vectorization and duplication of the lookup table to mitigate\nshared memory bandwidth constraints. At batch sizes < 32 and quantization group\nsize of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster\nthan existing GEMM kernels. As an application of FLUTE, we explore a simple\nextension to lookup table-based NormalFloat quantization and apply it to\nquantize LLaMA3 to various configurations, obtaining competitive quantization\nperformance against strong baselines while obtaining an end-to-end throughput\nincrease of 1.5 to 2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often constrained by memory\nbandwidth, where the primary bottleneck is the cost of transferring model\nparameters from the GPU's global memory to its registers. When coupled with\ncustom kernels that fuse the dequantization and matmul operations, weight-only\nquantization can thus enable faster inference by reducing the amount of memory\nmovement. However, developing high-performance kernels for weight-quantized\nLLMs presents substantial challenges, especially when the weights are\ncompressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform,\nlookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup\ntable engine for LUT-quantized LLMs, which uses offline restructuring of the\nquantized weight matrix to minimize bit manipulations associated with\nunpacking, and vectorization and duplication of the lookup table to mitigate\nshared memory bandwidth constraints. At batch sizes < 32 and quantization group\nsize of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster\nthan existing GEMM kernels. As an application of FLUTE, we explore a simple\nextension to lookup table-based NormalFloat quantization and apply it to\nquantize LLaMA3 to various configurations, obtaining competitive quantization\nperformance against strong baselines while obtaining an end-to-end throughput\nincrease of 1.5 to 2 times."
                },
                "authors": [
                    {
                        "name": "Han Guo"
                    },
                    {
                        "name": "William Brandon"
                    },
                    {
                        "name": "Radostin Cholakov"
                    },
                    {
                        "name": "Jonathan Ragan-Kelley"
                    },
                    {
                        "name": "Eric P. Xing"
                    },
                    {
                        "name": "Yoon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Yoon Kim"
                },
                "author": "Yoon Kim",
                "arxiv_comment": "EMNLP 2024 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10960v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10960v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09929v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09929v1",
                "updated": "2025-01-17T02:55:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    2,
                    55,
                    23,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T02:55:23Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    2,
                    55,
                    23,
                    4,
                    17,
                    0
                ],
                "title": "Steering Large Language Models with Feature Guided Activation Additions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering Large Language Models with Feature Guided Activation Additions"
                },
                "summary": "Effective and reliable control over large language model (LLM) behavior is a\nsignificant challenge. While activation steering methods, which add steering\nvectors to a model's hidden states, are a promising approach, existing\ntechniques often lack precision and interpretability in how they influence\nmodel outputs. We introduce Feature Guided Activation Additions (FGAA), a novel\nactivation steering method that leverages insights from Contrastive Activation\nAddition (CAA) and Sparse Autoencoder-Targeted Steering (SAE-TS). By operating\nin the latent space of a Sparse Autoencoder (SAE) and employing optimization\ntechniques to select desired SAE features, FGAA constructs precise steering\nvectors that provide better steering effects while maintaining coherence of\nsteered model outputs. In this regard, evaluations on Gemma-2-2B and Gemma-2-9B\nmodels across various steering tasks demonstrate that FGAA outperforms existing\nsteering methods of CAA, SAE decoder steering, and SAE-TS. Our results also\nhighlight important trade-offs between steering scale and general model\ncapabilities that are consistent across all tested steering methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective and reliable control over large language model (LLM) behavior is a\nsignificant challenge. While activation steering methods, which add steering\nvectors to a model's hidden states, are a promising approach, existing\ntechniques often lack precision and interpretability in how they influence\nmodel outputs. We introduce Feature Guided Activation Additions (FGAA), a novel\nactivation steering method that leverages insights from Contrastive Activation\nAddition (CAA) and Sparse Autoencoder-Targeted Steering (SAE-TS). By operating\nin the latent space of a Sparse Autoencoder (SAE) and employing optimization\ntechniques to select desired SAE features, FGAA constructs precise steering\nvectors that provide better steering effects while maintaining coherence of\nsteered model outputs. In this regard, evaluations on Gemma-2-2B and Gemma-2-9B\nmodels across various steering tasks demonstrate that FGAA outperforms existing\nsteering methods of CAA, SAE decoder steering, and SAE-TS. Our results also\nhighlight important trade-offs between steering scale and general model\ncapabilities that are consistent across all tested steering methods."
                },
                "authors": [
                    {
                        "name": "Samuel Soo"
                    },
                    {
                        "name": "Wesley Teng"
                    },
                    {
                        "name": "Chandrasekaran Balaganesh"
                    }
                ],
                "author_detail": {
                    "name": "Chandrasekaran Balaganesh"
                },
                "author": "Chandrasekaran Balaganesh",
                "arxiv_comment": "7 maintext pages, 14 appendix pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09929v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09928v1",
                "updated": "2025-01-17T02:48:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    2,
                    48,
                    29,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T02:48:29Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    2,
                    48,
                    29,
                    4,
                    17,
                    0
                ],
                "title": "Dialogue Benchmark Generation from Knowledge Graphs with Cost-Effective\n  Retrieval-Augmented LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue Benchmark Generation from Knowledge Graphs with Cost-Effective\n  Retrieval-Augmented LLMs"
                },
                "summary": "Dialogue benchmarks are crucial in training and evaluating chatbots engaging\nin domain-specific conversations. Knowledge graphs (KGs) represent semantically\nrich and well-organized data spanning various domains, such as DBLP, DBpedia,\nand YAGO. Traditionally, dialogue benchmarks have been manually created from\ndocuments, neglecting the potential of KGs in automating this process. Some\nquestion-answering benchmarks are automatically generated using extensive\npreprocessing from KGs, but they do not support dialogue generation. This paper\nintroduces Chatty-Gen, a novel multi-stage retrieval-augmented generation\nplatform for automatically generating high-quality dialogue benchmarks tailored\nto a specific domain using a KG. Chatty-Gen decomposes the generation process\ninto manageable stages and uses assertion rules for automatic validation\nbetween stages. Our approach enables control over intermediate results to\nprevent time-consuming restarts due to hallucinations. It also reduces reliance\non costly and more powerful commercial LLMs. Chatty-Gen eliminates upfront\nprocessing of the entire KG using efficient query-based retrieval to find\nrepresentative subgraphs based on the dialogue context. Our experiments with\nseveral real and large KGs demonstrate that Chatty-Gen significantly\noutperforms state-of-the-art systems and ensures consistent model and system\nperformance across multiple LLMs of diverse capabilities, such as GPT-4o,\nGemini 1.5, Llama 3, and Mistral.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue benchmarks are crucial in training and evaluating chatbots engaging\nin domain-specific conversations. Knowledge graphs (KGs) represent semantically\nrich and well-organized data spanning various domains, such as DBLP, DBpedia,\nand YAGO. Traditionally, dialogue benchmarks have been manually created from\ndocuments, neglecting the potential of KGs in automating this process. Some\nquestion-answering benchmarks are automatically generated using extensive\npreprocessing from KGs, but they do not support dialogue generation. This paper\nintroduces Chatty-Gen, a novel multi-stage retrieval-augmented generation\nplatform for automatically generating high-quality dialogue benchmarks tailored\nto a specific domain using a KG. Chatty-Gen decomposes the generation process\ninto manageable stages and uses assertion rules for automatic validation\nbetween stages. Our approach enables control over intermediate results to\nprevent time-consuming restarts due to hallucinations. It also reduces reliance\non costly and more powerful commercial LLMs. Chatty-Gen eliminates upfront\nprocessing of the entire KG using efficient query-based retrieval to find\nrepresentative subgraphs based on the dialogue context. Our experiments with\nseveral real and large KGs demonstrate that Chatty-Gen significantly\noutperforms state-of-the-art systems and ensures consistent model and system\nperformance across multiple LLMs of diverse capabilities, such as GPT-4o,\nGemini 1.5, Llama 3, and Mistral."
                },
                "authors": [
                    {
                        "name": "Reham Omar"
                    },
                    {
                        "name": "Omij Mangukiya"
                    },
                    {
                        "name": "Essam Mansour"
                    }
                ],
                "author_detail": {
                    "name": "Essam Mansour"
                },
                "author": "Essam Mansour",
                "arxiv_comment": "The paper is publsihed in SIGMOD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07227v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07227v3",
                "updated": "2025-01-17T02:27:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    2,
                    27,
                    29,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-13T11:28:49Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    11,
                    28,
                    49,
                    0,
                    13,
                    0
                ],
                "title": "MECD+: Unlocking Event-Level Causal Graph Discovery for Video Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MECD+: Unlocking Event-Level Causal Graph Discovery for Video Reasoning"
                },
                "summary": "Video causal reasoning aims to achieve a high-level understanding of videos\nfrom a causal perspective. However, it exhibits limitations in its scope,\nprimarily executed in a question-answering paradigm and focusing on brief video\nsegments containing isolated events and basic causal relations, lacking\ncomprehensive and structured causality analysis for videos with multiple\ninterconnected events. To fill this gap, we introduce a new task and dataset,\nMulti-Event Causal Discovery (MECD). It aims to uncover the causal relations\nbetween events distributed chronologically across long videos. Given visual\nsegments and textual descriptions of events, MECD identifies the causal\nassociations between these events to derive a comprehensive and structured\nevent-level video causal graph explaining why and how the result event\noccurred. To address the challenges of MECD, we devise a novel framework\ninspired by the Granger Causality method, incorporating an efficient mask-based\nevent prediction model to perform an Event Granger Test. It estimates causality\nby comparing the predicted result event when premise events are masked versus\nunmasked. Furthermore, we integrate causal inference techniques such as\nfront-door adjustment and counterfactual inference to mitigate challenges in\nMECD like causality confounding and illusory causality. Additionally, context\nchain reasoning is introduced to conduct more robust and generalized reasoning.\nExperiments validate the effectiveness of our framework in reasoning complete\ncausal relations, outperforming GPT-4o and VideoChat2 by 5.77% and 2.70%,\nrespectively. Further experiments demonstrate that causal relation graphs can\nalso contribute to downstream video understanding tasks such as video question\nanswering and video event prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video causal reasoning aims to achieve a high-level understanding of videos\nfrom a causal perspective. However, it exhibits limitations in its scope,\nprimarily executed in a question-answering paradigm and focusing on brief video\nsegments containing isolated events and basic causal relations, lacking\ncomprehensive and structured causality analysis for videos with multiple\ninterconnected events. To fill this gap, we introduce a new task and dataset,\nMulti-Event Causal Discovery (MECD). It aims to uncover the causal relations\nbetween events distributed chronologically across long videos. Given visual\nsegments and textual descriptions of events, MECD identifies the causal\nassociations between these events to derive a comprehensive and structured\nevent-level video causal graph explaining why and how the result event\noccurred. To address the challenges of MECD, we devise a novel framework\ninspired by the Granger Causality method, incorporating an efficient mask-based\nevent prediction model to perform an Event Granger Test. It estimates causality\nby comparing the predicted result event when premise events are masked versus\nunmasked. Furthermore, we integrate causal inference techniques such as\nfront-door adjustment and counterfactual inference to mitigate challenges in\nMECD like causality confounding and illusory causality. Additionally, context\nchain reasoning is introduced to conduct more robust and generalized reasoning.\nExperiments validate the effectiveness of our framework in reasoning complete\ncausal relations, outperforming GPT-4o and VideoChat2 by 5.77% and 2.70%,\nrespectively. Further experiments demonstrate that causal relation graphs can\nalso contribute to downstream video understanding tasks such as video question\nanswering and video event prediction."
                },
                "authors": [
                    {
                        "name": "Tieyuan Chen"
                    },
                    {
                        "name": "Huabin Liu"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Yihang Chen"
                    },
                    {
                        "name": "Tianyao He"
                    },
                    {
                        "name": "Chaofan Gan"
                    },
                    {
                        "name": "Huanyu He"
                    },
                    {
                        "name": "Weiyao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weiyao Lin"
                },
                "author": "Weiyao Lin",
                "arxiv_comment": "IEEE TPAMI Submission. continuous work of arXiv:2409.17647 (NeurIPS\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07227v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07227v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09918v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09918v1",
                "updated": "2025-01-17T02:20:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    2,
                    20,
                    52,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T02:20:52Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    2,
                    20,
                    52,
                    4,
                    17,
                    0
                ],
                "title": "GenSC-6G: A Prototype Testbed for Integrated Generative AI, Quantum, and\n  Semantic Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenSC-6G: A Prototype Testbed for Integrated Generative AI, Quantum, and\n  Semantic Communication"
                },
                "summary": "We introduce a prototyping testbed, GenSC-6G, developed to generate a\ncomprehensive dataset that supports the integration of generative artificial\nintelligence (AI), quantum computing, and semantic communication for emerging\nsixth-generation (6G) applications. The GenSC-6G dataset is designed with\nnoise-augmented synthetic data optimized for semantic decoding, classification,\nand localization tasks, significantly enhancing flexibility for diverse\nAI-driven communication applications. This adaptable prototype supports\nseamless modifications across baseline models, communication modules, and\ngoal-oriented decoders. Case studies demonstrate its application in lightweight\nclassification, semantic upsampling, and edge-based language inference under\nnoise conditions. The GenSC-6G dataset serves as a scalable and robust resource\nfor developing goal-oriented communication systems tailored to the growing\ndemands of 6G networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a prototyping testbed, GenSC-6G, developed to generate a\ncomprehensive dataset that supports the integration of generative artificial\nintelligence (AI), quantum computing, and semantic communication for emerging\nsixth-generation (6G) applications. The GenSC-6G dataset is designed with\nnoise-augmented synthetic data optimized for semantic decoding, classification,\nand localization tasks, significantly enhancing flexibility for diverse\nAI-driven communication applications. This adaptable prototype supports\nseamless modifications across baseline models, communication modules, and\ngoal-oriented decoders. Case studies demonstrate its application in lightweight\nclassification, semantic upsampling, and edge-based language inference under\nnoise conditions. The GenSC-6G dataset serves as a scalable and robust resource\nfor developing goal-oriented communication systems tailored to the growing\ndemands of 6G networks."
                },
                "authors": [
                    {
                        "name": "Brian E. Arfeto"
                    },
                    {
                        "name": "Shehbaz Tariq"
                    },
                    {
                        "name": "Uman Khalid"
                    },
                    {
                        "name": "Trung Q. Duong"
                    },
                    {
                        "name": "Hyundong Shin"
                    }
                ],
                "author_detail": {
                    "name": "Hyundong Shin"
                },
                "author": "Hyundong Shin",
                "arxiv_comment": "SUBMITTED FOR PUBLICATION IN IEEE COMMUNICATIONS MAGAZINE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09918v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07832v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07832v8",
                "updated": "2025-01-17T02:18:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    2,
                    18,
                    0,
                    4,
                    17,
                    0
                ],
                "published": "2024-07-31T14:49:35Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    49,
                    35,
                    2,
                    213,
                    0
                ],
                "title": "LADDER: Language Driven Slice Discovery and Error Rectification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LADDER: Language Driven Slice Discovery and Error Rectification"
                },
                "summary": "Error slice discovery is crucial to diagnose and mitigate model errors.\nCurrent clustering or discrete attribute-based slice discovery methods face key\nlimitations: 1) clustering results in incoherent slices, while assigning\ndiscrete attributes to slices leads to incomplete coverage of error patterns\ndue to missing or insufficient attributes; 2) these methods lack complex\nreasoning, preventing them from fully explaining model biases; 3) they fail to\nintegrate \\textit{domain knowledge}, limiting their usage in specialized fields\n\\eg radiology. We propose\\ladder (\\underline{La}nguage-\\underline{D}riven\n\\underline{D}iscovery and \\underline{E}rror \\underline{R}ectification), to\naddress the limitations by: (1) leveraging the flexibility of natural language\nto address incompleteness, (2) employing LLM's latent \\textit{domain knowledge}\nand advanced reasoning to analyze sentences and derive testable hypotheses\ndirectly, identifying biased attributes, and form coherent error slices without\nclustering. Existing mitigation methods typically address only the\nworst-performing group, often amplifying errors in other subgroups. In\ncontrast,\\ladder generates pseudo attributes from the discovered hypotheses to\nmitigate errors across all biases without explicit attribute annotations or\nprior knowledge of bias. Rigorous evaluations on 6 datasets spanning natural\nand medical images -- comparing 200+ classifiers with diverse architectures,\npretraining strategies, and LLMs -- show that\\ladder consistently outperforms\nexisting baselines in discovering and mitigating biases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Error slice discovery is crucial to diagnose and mitigate model errors.\nCurrent clustering or discrete attribute-based slice discovery methods face key\nlimitations: 1) clustering results in incoherent slices, while assigning\ndiscrete attributes to slices leads to incomplete coverage of error patterns\ndue to missing or insufficient attributes; 2) these methods lack complex\nreasoning, preventing them from fully explaining model biases; 3) they fail to\nintegrate \\textit{domain knowledge}, limiting their usage in specialized fields\n\\eg radiology. We propose\\ladder (\\underline{La}nguage-\\underline{D}riven\n\\underline{D}iscovery and \\underline{E}rror \\underline{R}ectification), to\naddress the limitations by: (1) leveraging the flexibility of natural language\nto address incompleteness, (2) employing LLM's latent \\textit{domain knowledge}\nand advanced reasoning to analyze sentences and derive testable hypotheses\ndirectly, identifying biased attributes, and form coherent error slices without\nclustering. Existing mitigation methods typically address only the\nworst-performing group, often amplifying errors in other subgroups. In\ncontrast,\\ladder generates pseudo attributes from the discovered hypotheses to\nmitigate errors across all biases without explicit attribute annotations or\nprior knowledge of bias. Rigorous evaluations on 6 datasets spanning natural\nand medical images -- comparing 200+ classifiers with diverse architectures,\npretraining strategies, and LLMs -- show that\\ladder consistently outperforms\nexisting baselines in discovering and mitigating biases."
                },
                "authors": [
                    {
                        "name": "Shantanu Ghosh"
                    },
                    {
                        "name": "Rayan Syed"
                    },
                    {
                        "name": "Chenyu Wang"
                    },
                    {
                        "name": "Clare B. Poynton"
                    },
                    {
                        "name": "Shyam Visweswaran"
                    },
                    {
                        "name": "Kayhan Batmanghelich"
                    }
                ],
                "author_detail": {
                    "name": "Kayhan Batmanghelich"
                },
                "author": "Kayhan Batmanghelich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07832v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07832v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09913v1",
                "updated": "2025-01-17T02:02:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    2,
                    2,
                    12,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T02:02:12Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    2,
                    2,
                    12,
                    4,
                    17,
                    0
                ],
                "title": "Towards A Litmus Test for Common Sense",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards A Litmus Test for Common Sense"
                },
                "summary": "This paper is the second in a planned series aimed at envisioning a path to\nsafe and beneficial artificial intelligence. Building on the conceptual\ninsights of \"Common Sense Is All You Need,\" we propose a more formal litmus\ntest for common sense, adopting an axiomatic approach that combines minimal\nprior knowledge (MPK) constraints with diagonal or Godel-style arguments to\ncreate tasks beyond the agent's known concept set. We discuss how this approach\napplies to the Abstraction and Reasoning Corpus (ARC), acknowledging\ntraining/test data constraints, physical or virtual embodiment, and large\nlanguage models (LLMs). We also integrate observations regarding emergent\ndeceptive hallucinations, in which more capable AI systems may intentionally\nfabricate plausible yet misleading outputs to disguise knowledge gaps. The\noverarching theme is that scaling AI without ensuring common sense risks\nintensifying such deceptive tendencies, thereby undermining safety and trust.\nAligning with the broader goal of developing beneficial AI without causing\nharm, our axiomatic litmus test not only diagnoses whether an AI can handle\ntruly novel concepts but also provides a stepping stone toward an ethical,\nreliable foundation for future safe, beneficial, and aligned artificial\nintelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper is the second in a planned series aimed at envisioning a path to\nsafe and beneficial artificial intelligence. Building on the conceptual\ninsights of \"Common Sense Is All You Need,\" we propose a more formal litmus\ntest for common sense, adopting an axiomatic approach that combines minimal\nprior knowledge (MPK) constraints with diagonal or Godel-style arguments to\ncreate tasks beyond the agent's known concept set. We discuss how this approach\napplies to the Abstraction and Reasoning Corpus (ARC), acknowledging\ntraining/test data constraints, physical or virtual embodiment, and large\nlanguage models (LLMs). We also integrate observations regarding emergent\ndeceptive hallucinations, in which more capable AI systems may intentionally\nfabricate plausible yet misleading outputs to disguise knowledge gaps. The\noverarching theme is that scaling AI without ensuring common sense risks\nintensifying such deceptive tendencies, thereby undermining safety and trust.\nAligning with the broader goal of developing beneficial AI without causing\nharm, our axiomatic litmus test not only diagnoses whether an AI can handle\ntruly novel concepts but also provides a stepping stone toward an ethical,\nreliable foundation for future safe, beneficial, and aligned artificial\nintelligence."
                },
                "authors": [
                    {
                        "name": "Hugo Latapie"
                    }
                ],
                "author_detail": {
                    "name": "Hugo Latapie"
                },
                "author": "Hugo Latapie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09910v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09910v1",
                "updated": "2025-01-17T01:48:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    48,
                    15,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T01:48:15Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    48,
                    15,
                    4,
                    17,
                    0
                ],
                "title": "Chatbot apologies: Beyond bullshit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chatbot apologies: Beyond bullshit"
                },
                "summary": "Apologies serve essential functions for moral agents such as expressing\nremorse, taking responsibility, and repairing trust. LLM-based chatbots\nroutinely produce output that has the linguistic form of an apology. However,\nthey do this simply because they are echoing the kinds of things that humans\nsay. Moreover, there are reasons to think that chatbots are not the kind of\nlinguistic or moral agents capable of apology. To put the point bluntly:\nChatbot apologies are bullshit. This paper offers several arguments for this\nconclusion, drawing on the nature of morally-serious apologies, the linguistic\nagency required to perform them, and the moral agency required for them to\nmatter. We conclude by considering some consequences for how chatbots should be\ndesigned and how we ought to think about them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apologies serve essential functions for moral agents such as expressing\nremorse, taking responsibility, and repairing trust. LLM-based chatbots\nroutinely produce output that has the linguistic form of an apology. However,\nthey do this simply because they are echoing the kinds of things that humans\nsay. Moreover, there are reasons to think that chatbots are not the kind of\nlinguistic or moral agents capable of apology. To put the point bluntly:\nChatbot apologies are bullshit. This paper offers several arguments for this\nconclusion, drawing on the nature of morally-serious apologies, the linguistic\nagency required to perform them, and the moral agency required for them to\nmatter. We conclude by considering some consequences for how chatbots should be\ndesigned and how we ought to think about them."
                },
                "authors": [
                    {
                        "name": "P. D. Magnus"
                    },
                    {
                        "name": "Alessandra Buccella"
                    },
                    {
                        "name": "Jason D'Cruz"
                    }
                ],
                "author_detail": {
                    "name": "Jason D'Cruz"
                },
                "author": "Jason D'Cruz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09910v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09910v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09909v1",
                "updated": "2025-01-17T01:45:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    45,
                    35,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T01:45:35Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    45,
                    35,
                    4,
                    17,
                    0
                ],
                "title": "Demo: Interactive Visualization of Semantic Relationships in a\n  Biomedical Project's Talent Knowledge Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demo: Interactive Visualization of Semantic Relationships in a\n  Biomedical Project's Talent Knowledge Graph"
                },
                "summary": "We present an interactive visualization of the Cell Map for AI Talent\nKnowledge Graph (CM4AI TKG), a detailed semantic space comprising approximately\n28,000 experts and 1,000 datasets focused on the biomedical field. Our tool\nleverages transformer-based embeddings, WebGL visualization techniques, and\ngenerative AI, specifically Large Language Models (LLMs), to provide a\nresponsive and user-friendly interface. This visualization supports the\nexploration of around 29,000 nodes, assisting users in identifying potential\ncollaborators and dataset users within the health and biomedical research\nfields. Our solution transcends the limitations of conventional graph\nvisualization tools like Gephi, particularly in handling large-scale\ninteractive graphs. We utilize GPT-4o to furnish detailed justifications for\nrecommended collaborators and dataset users, promoting informed\ndecision-making. Key functionalities include responsive search and exploration,\nas well as GenAI-driven recommendations, all contributing to a nuanced\nrepresentation of the convergence between biomedical and AI research\nlandscapes. In addition to benefiting the Bridge2AI and CM4AI communities, this\nadaptable visualization framework can be extended to other biomedical knowledge\ngraphs, fostering advancements in medical AI and healthcare innovation through\nimproved user interaction and data exploration. The demonstration is available\nat: https://jiawei-alpha.vercel.app/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an interactive visualization of the Cell Map for AI Talent\nKnowledge Graph (CM4AI TKG), a detailed semantic space comprising approximately\n28,000 experts and 1,000 datasets focused on the biomedical field. Our tool\nleverages transformer-based embeddings, WebGL visualization techniques, and\ngenerative AI, specifically Large Language Models (LLMs), to provide a\nresponsive and user-friendly interface. This visualization supports the\nexploration of around 29,000 nodes, assisting users in identifying potential\ncollaborators and dataset users within the health and biomedical research\nfields. Our solution transcends the limitations of conventional graph\nvisualization tools like Gephi, particularly in handling large-scale\ninteractive graphs. We utilize GPT-4o to furnish detailed justifications for\nrecommended collaborators and dataset users, promoting informed\ndecision-making. Key functionalities include responsive search and exploration,\nas well as GenAI-driven recommendations, all contributing to a nuanced\nrepresentation of the convergence between biomedical and AI research\nlandscapes. In addition to benefiting the Bridge2AI and CM4AI communities, this\nadaptable visualization framework can be extended to other biomedical knowledge\ngraphs, fostering advancements in medical AI and healthcare innovation through\nimproved user interaction and data exploration. The demonstration is available\nat: https://jiawei-alpha.vercel.app/."
                },
                "authors": [
                    {
                        "name": "Jiawei Xu"
                    },
                    {
                        "name": "Zhandos Sembay"
                    },
                    {
                        "name": "Swathi Thaker"
                    },
                    {
                        "name": "Pamela Payne-Foster"
                    },
                    {
                        "name": "Jake Yue Chen"
                    },
                    {
                        "name": "Ying Ding"
                    }
                ],
                "author_detail": {
                    "name": "Ying Ding"
                },
                "author": "Ying Ding",
                "arxiv_comment": "Accepted by GenAI for Health Workshop @ NeurIPS 2024, Vancouver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.10360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10360v1",
                "updated": "2025-01-17T18:59:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    18,
                    59,
                    55,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T18:59:55Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    18,
                    59,
                    55,
                    4,
                    17,
                    0
                ],
                "title": "FaceXBench: Evaluating Multimodal LLMs on Face Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaceXBench: Evaluating Multimodal LLMs on Face Understanding"
                },
                "summary": "Multimodal Large Language Models (MLLMs) demonstrate impressive\nproblem-solving abilities across a wide range of tasks and domains. However,\ntheir capacity for face understanding has not been systematically studied. To\naddress this gap, we introduce FaceXBench, a comprehensive benchmark designed\nto evaluate MLLMs on complex face understanding tasks. FaceXBench includes\n5,000 multimodal multiple-choice questions derived from 25 public datasets and\na newly created dataset, FaceXAPI. These questions cover 14 tasks across 6\nbroad categories, assessing MLLMs' face understanding abilities in bias and\nfairness, face authentication, recognition, analysis, localization and tool\nretrieval. Using FaceXBench, we conduct an extensive evaluation of 26\nopen-source MLLMs alongside 2 proprietary models, revealing the unique\nchallenges in complex face understanding tasks. We analyze the models across\nthree evaluation settings: zero-shot, in-context task description, and\nchain-of-thought prompting. Our detailed analysis reveals that current MLLMs,\nincluding advanced models like GPT-4o, and GeminiPro 1.5, show significant room\nfor improvement. We believe FaceXBench will be a crucial resource for\ndeveloping MLLMs equipped to perform sophisticated face understanding. Code:\nhttps://github.com/Kartik-3004/facexbench",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) demonstrate impressive\nproblem-solving abilities across a wide range of tasks and domains. However,\ntheir capacity for face understanding has not been systematically studied. To\naddress this gap, we introduce FaceXBench, a comprehensive benchmark designed\nto evaluate MLLMs on complex face understanding tasks. FaceXBench includes\n5,000 multimodal multiple-choice questions derived from 25 public datasets and\na newly created dataset, FaceXAPI. These questions cover 14 tasks across 6\nbroad categories, assessing MLLMs' face understanding abilities in bias and\nfairness, face authentication, recognition, analysis, localization and tool\nretrieval. Using FaceXBench, we conduct an extensive evaluation of 26\nopen-source MLLMs alongside 2 proprietary models, revealing the unique\nchallenges in complex face understanding tasks. We analyze the models across\nthree evaluation settings: zero-shot, in-context task description, and\nchain-of-thought prompting. Our detailed analysis reveals that current MLLMs,\nincluding advanced models like GPT-4o, and GeminiPro 1.5, show significant room\nfor improvement. We believe FaceXBench will be a crucial resource for\ndeveloping MLLMs equipped to perform sophisticated face understanding. Code:\nhttps://github.com/Kartik-3004/facexbench"
                },
                "authors": [
                    {
                        "name": "Kartik Narayan"
                    },
                    {
                        "name": "Vibashan VS"
                    },
                    {
                        "name": "Vishal M. Patel"
                    }
                ],
                "author_detail": {
                    "name": "Vishal M. Patel"
                },
                "author": "Vishal M. Patel",
                "arxiv_comment": "Project Page: https://kartik-3004.github.io/facexbench/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10332v1",
                "updated": "2025-01-17T18:05:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    18,
                    5,
                    4,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T18:05:04Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    18,
                    5,
                    4,
                    4,
                    17,
                    0
                ],
                "title": "Agent4Edu: Generating Learner Response Data by Generative Agents for\n  Intelligent Education Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent4Edu: Generating Learner Response Data by Generative Agents for\n  Intelligent Education Systems"
                },
                "summary": "Personalized learning represents a promising educational strategy within\nintelligent educational systems, aiming to enhance learners' practice\nefficiency. However, the discrepancy between offline metrics and online\nperformance significantly impedes their progress. To address this challenge, we\nintroduce Agent4Edu, a novel personalized learning simulator leveraging recent\nadvancements in human intelligence through large language models (LLMs).\nAgent4Edu features LLM-powered generative agents equipped with learner profile,\nmemory, and action modules tailored to personalized learning algorithms. The\nlearner profiles are initialized using real-world response data, capturing\npractice styles and cognitive factors. Inspired by human psychology theory, the\nmemory module records practice facts and high-level summaries, integrating\nreflection mechanisms. The action module supports various behaviors, including\nexercise understanding, analysis, and response generation. Each agent can\ninteract with personalized learning algorithms, such as computerized adaptive\ntesting, enabling a multifaceted evaluation and enhancement of customized\nservices. Through a comprehensive assessment, we explore the strengths and\nweaknesses of Agent4Edu, emphasizing the consistency and discrepancies in\nresponses between agents and human learners. The code, data, and appendix are\npublicly available at https://github.com/bigdata-ustc/Agent4Edu.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized learning represents a promising educational strategy within\nintelligent educational systems, aiming to enhance learners' practice\nefficiency. However, the discrepancy between offline metrics and online\nperformance significantly impedes their progress. To address this challenge, we\nintroduce Agent4Edu, a novel personalized learning simulator leveraging recent\nadvancements in human intelligence through large language models (LLMs).\nAgent4Edu features LLM-powered generative agents equipped with learner profile,\nmemory, and action modules tailored to personalized learning algorithms. The\nlearner profiles are initialized using real-world response data, capturing\npractice styles and cognitive factors. Inspired by human psychology theory, the\nmemory module records practice facts and high-level summaries, integrating\nreflection mechanisms. The action module supports various behaviors, including\nexercise understanding, analysis, and response generation. Each agent can\ninteract with personalized learning algorithms, such as computerized adaptive\ntesting, enabling a multifaceted evaluation and enhancement of customized\nservices. Through a comprehensive assessment, we explore the strengths and\nweaknesses of Agent4Edu, emphasizing the consistency and discrepancies in\nresponses between agents and human learners. The code, data, and appendix are\npublicly available at https://github.com/bigdata-ustc/Agent4Edu."
                },
                "authors": [
                    {
                        "name": "Weibo Gao"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Linan Yue"
                    },
                    {
                        "name": "Fangzhou Yao"
                    },
                    {
                        "name": "Rui Lv"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Zhenya Huang"
                    }
                ],
                "author_detail": {
                    "name": "Zhenya Huang"
                },
                "author": "Zhenya Huang",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10326v1",
                "updated": "2025-01-17T17:56:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    17,
                    56,
                    58,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T17:56:58Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    17,
                    56,
                    58,
                    4,
                    17,
                    0
                ],
                "title": "Large language models for automated scholarly paper review: A survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models for automated scholarly paper review: A survey"
                },
                "summary": "Large language models (LLMs) have significantly impacted human society,\ninfluencing various domains. Among them, academia is not simply a domain\naffected by LLMs, but it is also the pivotal force in the development of LLMs.\nIn academic publications, this phenomenon is represented during the\nincorporation of LLMs into the peer review mechanism for reviewing manuscripts.\nWe proposed the concept of automated scholarly paper review (ASPR) in our\nprevious paper. As the incorporation grows, it now enters the coexistence phase\nof ASPR and peer review, which is described in that paper. LLMs hold\ntransformative potential for the full-scale implementation of ASPR, but they\nalso pose new issues and challenges that need to be addressed. In this survey\npaper, we aim to provide a holistic view of ASPR in the era of LLMs. We begin\nwith a survey to find out which LLMs are used to conduct ASPR. Then, we review\nwhat ASPR-related technological bottlenecks have been solved with the\nincorporation of LLM technology. After that, we move on to explore new methods,\nnew datasets, new source code, and new online systems that come with LLMs for\nASPR. Furthermore, we summarize the performance and issues of LLMs in ASPR, and\ninvestigate the attitudes and reactions of publishers and academia to ASPR.\nLastly, we discuss the challenges associated with the development of LLMs for\nASPR. We hope this survey can serve as an inspirational reference for the\nresearchers and promote the progress of ASPR for its actual implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly impacted human society,\ninfluencing various domains. Among them, academia is not simply a domain\naffected by LLMs, but it is also the pivotal force in the development of LLMs.\nIn academic publications, this phenomenon is represented during the\nincorporation of LLMs into the peer review mechanism for reviewing manuscripts.\nWe proposed the concept of automated scholarly paper review (ASPR) in our\nprevious paper. As the incorporation grows, it now enters the coexistence phase\nof ASPR and peer review, which is described in that paper. LLMs hold\ntransformative potential for the full-scale implementation of ASPR, but they\nalso pose new issues and challenges that need to be addressed. In this survey\npaper, we aim to provide a holistic view of ASPR in the era of LLMs. We begin\nwith a survey to find out which LLMs are used to conduct ASPR. Then, we review\nwhat ASPR-related technological bottlenecks have been solved with the\nincorporation of LLM technology. After that, we move on to explore new methods,\nnew datasets, new source code, and new online systems that come with LLMs for\nASPR. Furthermore, we summarize the performance and issues of LLMs in ASPR, and\ninvestigate the attitudes and reactions of publishers and academia to ASPR.\nLastly, we discuss the challenges associated with the development of LLMs for\nASPR. We hope this survey can serve as an inspirational reference for the\nresearchers and promote the progress of ASPR for its actual implementation."
                },
                "authors": [
                    {
                        "name": "Zhenzhen Zhuang"
                    },
                    {
                        "name": "Jiandong Chen"
                    },
                    {
                        "name": "Hongfeng Xu"
                    },
                    {
                        "name": "Yuwen Jiang"
                    },
                    {
                        "name": "Jialiang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jialiang Lin"
                },
                "author": "Jialiang Lin",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10321v1",
                "updated": "2025-01-17T17:51:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    17,
                    51,
                    22,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T17:51:22Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    17,
                    51,
                    22,
                    4,
                    17,
                    0
                ],
                "title": "Towards Human-Guided, Data-Centric LLM Co-Pilots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Human-Guided, Data-Centric LLM Co-Pilots"
                },
                "summary": "Machine learning (ML) has the potential to revolutionize healthcare, but its\nadoption is often hindered by the disconnect between the needs of domain\nexperts and translating these needs into robust and valid ML tools. Despite\nrecent advances in LLM-based co-pilots to democratize ML for non-technical\ndomain experts, these systems remain predominantly focused on model-centric\naspects while overlooking critical data-centric challenges. This limitation is\nproblematic in complex real-world settings where raw data often contains\ncomplex issues, such as missing values, label noise, and domain-specific\nnuances requiring tailored handling. To address this we introduce CliMB-DC, a\nhuman-guided, data-centric framework for LLM co-pilots that combines advanced\ndata-centric tools with LLM-driven reasoning to enable robust, context-aware\ndata processing. At its core, CliMB-DC introduces a novel, multi-agent\nreasoning system that combines a strategic coordinator for dynamic planning and\nadaptation with a specialized worker agent for precise execution. Domain\nexpertise is then systematically incorporated to guide the reasoning process\nusing a human-in-the-loop approach. To guide development, we formalize a\ntaxonomy of key data-centric challenges that co-pilots must address.\nThereafter, to address the dimensions of the taxonomy, we integrate\nstate-of-the-art data-centric tools into an extensible, open-source\narchitecture, facilitating the addition of new tools from the research\ncommunity. Empirically, using real-world healthcare datasets we demonstrate\nCliMB-DC's ability to transform uncurated datasets into ML-ready formats,\nsignificantly outperforming existing co-pilot baselines for handling\ndata-centric challenges. CliMB-DC promises to empower domain experts from\ndiverse domains -- healthcare, finance, social sciences and more -- to actively\nparticipate in driving real-world impact using ML.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) has the potential to revolutionize healthcare, but its\nadoption is often hindered by the disconnect between the needs of domain\nexperts and translating these needs into robust and valid ML tools. Despite\nrecent advances in LLM-based co-pilots to democratize ML for non-technical\ndomain experts, these systems remain predominantly focused on model-centric\naspects while overlooking critical data-centric challenges. This limitation is\nproblematic in complex real-world settings where raw data often contains\ncomplex issues, such as missing values, label noise, and domain-specific\nnuances requiring tailored handling. To address this we introduce CliMB-DC, a\nhuman-guided, data-centric framework for LLM co-pilots that combines advanced\ndata-centric tools with LLM-driven reasoning to enable robust, context-aware\ndata processing. At its core, CliMB-DC introduces a novel, multi-agent\nreasoning system that combines a strategic coordinator for dynamic planning and\nadaptation with a specialized worker agent for precise execution. Domain\nexpertise is then systematically incorporated to guide the reasoning process\nusing a human-in-the-loop approach. To guide development, we formalize a\ntaxonomy of key data-centric challenges that co-pilots must address.\nThereafter, to address the dimensions of the taxonomy, we integrate\nstate-of-the-art data-centric tools into an extensible, open-source\narchitecture, facilitating the addition of new tools from the research\ncommunity. Empirically, using real-world healthcare datasets we demonstrate\nCliMB-DC's ability to transform uncurated datasets into ML-ready formats,\nsignificantly outperforming existing co-pilot baselines for handling\ndata-centric challenges. CliMB-DC promises to empower domain experts from\ndiverse domains -- healthcare, finance, social sciences and more -- to actively\nparticipate in driving real-world impact using ML."
                },
                "authors": [
                    {
                        "name": "Evgeny Saveliev"
                    },
                    {
                        "name": "Jiashuo Liu"
                    },
                    {
                        "name": "Nabeel Seedat"
                    },
                    {
                        "name": "Anders Boyd"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "arxiv_comment": "Saveliev, Liu & Seedat contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03545v2",
                "updated": "2025-01-17T17:47:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    17,
                    47,
                    24,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-07T05:43:23Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    5,
                    43,
                    23,
                    1,
                    7,
                    0
                ],
                "title": "Beyond Factual Accuracy: Evaluating Coverage of Diverse Factual\n  Information in Long-form Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Factual Accuracy: Evaluating Coverage of Diverse Factual\n  Information in Long-form Text Generation"
                },
                "summary": "This paper presents ICAT, an evaluation framework for measuring coverage of\ndiverse factual information in long-form text generation. ICAT breaks down a\nlong output text into a list of atomic claims and not only verifies each claim\nthrough retrieval from a (reliable) knowledge source, but also computes the\nalignment between the atomic factual claims and various aspects expected to be\npresented in the output. We study three implementations of the ICAT framework,\neach with a different assumption on the availability of aspects and alignment\nmethod. By adopting data from the diversification task in the TREC Web Track\nand the ClueWeb corpus, we evaluate the ICAT framework. We demonstrate strong\ncorrelation with human judgments and provide comprehensive evaluation across\nmultiple state-of-the-art LLMs. Our framework further offers interpretable and\nfine-grained analysis of diversity and coverage. Its modular design allows for\neasy adaptation to different domains and datasets, making it a valuable tool\nfor evaluating the qualitative aspects of long-form responses produced by LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents ICAT, an evaluation framework for measuring coverage of\ndiverse factual information in long-form text generation. ICAT breaks down a\nlong output text into a list of atomic claims and not only verifies each claim\nthrough retrieval from a (reliable) knowledge source, but also computes the\nalignment between the atomic factual claims and various aspects expected to be\npresented in the output. We study three implementations of the ICAT framework,\neach with a different assumption on the availability of aspects and alignment\nmethod. By adopting data from the diversification task in the TREC Web Track\nand the ClueWeb corpus, we evaluate the ICAT framework. We demonstrate strong\ncorrelation with human judgments and provide comprehensive evaluation across\nmultiple state-of-the-art LLMs. Our framework further offers interpretable and\nfine-grained analysis of diversity and coverage. Its modular design allows for\neasy adaptation to different domains and datasets, making it a valuable tool\nfor evaluating the qualitative aspects of long-form responses produced by LLMs."
                },
                "authors": [
                    {
                        "name": "Chris Samarinas"
                    },
                    {
                        "name": "Alexander Krubner"
                    },
                    {
                        "name": "Alireza Salemi"
                    },
                    {
                        "name": "Youngwoo Kim"
                    },
                    {
                        "name": "Hamed Zamani"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Zamani"
                },
                "author": "Hamed Zamani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10316v1",
                "updated": "2025-01-17T17:40:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    17,
                    40,
                    12,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T17:40:12Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    17,
                    40,
                    12,
                    4,
                    17,
                    0
                ],
                "title": "Towards Preventing Overreliance on Task-Oriented Conversational AI\n  Through Accountability Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Preventing Overreliance on Task-Oriented Conversational AI\n  Through Accountability Modeling"
                },
                "summary": "Recent LLMs have enabled significant advancements for conversational agents.\nHowever, they are also well-known to hallucinate, i.e., they often produce\nresponses that seem plausible but are not factually correct. On the other hand,\nusers tend to over-rely on LLM-based AI agents; they accept the AI's suggestion\neven when it is wrong. Adding good friction, such as explanations or getting\nuser confirmations, has been proposed as a mitigation in AI-supported\ndecision-making systems. In this paper, we propose an accountability model for\nLLM-based task-oriented dialogue agents to address user overreliance via\nfriction turns in cases of model uncertainty and errors associated with\ndialogue state tracking (DST). The accountability model is an augmented LLM\nwith an additional accountability head, which functions as a binary classifier\nto predict the slots of the dialogue states. We perform our experiments with\nthree backbone LLMs (Llama, Mistral, Gemma) on two established task-oriented\ndatasets (MultiWOZ and Snips). Our empirical findings demonstrate that this\napproach not only enables reliable estimation of AI agent errors but also\nguides the LLM decoder in generating more accurate actions. We observe around\n3% absolute improvement in joint goal accuracy by incorporating accountability\nheads in modern LLMs for the MultiWOZ dataset. We also show that this method\nenables the agent to self-correct its actions, further boosting its performance\nby 3%. Finally, we discuss the application of accountability modeling to\nprevent user overreliance by introducing friction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent LLMs have enabled significant advancements for conversational agents.\nHowever, they are also well-known to hallucinate, i.e., they often produce\nresponses that seem plausible but are not factually correct. On the other hand,\nusers tend to over-rely on LLM-based AI agents; they accept the AI's suggestion\neven when it is wrong. Adding good friction, such as explanations or getting\nuser confirmations, has been proposed as a mitigation in AI-supported\ndecision-making systems. In this paper, we propose an accountability model for\nLLM-based task-oriented dialogue agents to address user overreliance via\nfriction turns in cases of model uncertainty and errors associated with\ndialogue state tracking (DST). The accountability model is an augmented LLM\nwith an additional accountability head, which functions as a binary classifier\nto predict the slots of the dialogue states. We perform our experiments with\nthree backbone LLMs (Llama, Mistral, Gemma) on two established task-oriented\ndatasets (MultiWOZ and Snips). Our empirical findings demonstrate that this\napproach not only enables reliable estimation of AI agent errors but also\nguides the LLM decoder in generating more accurate actions. We observe around\n3% absolute improvement in joint goal accuracy by incorporating accountability\nheads in modern LLMs for the MultiWOZ dataset. We also show that this method\nenables the agent to self-correct its actions, further boosting its performance\nby 3%. Finally, we discuss the application of accountability modeling to\nprevent user overreliance by introducing friction."
                },
                "authors": [
                    {
                        "name": "Suvodip Dey"
                    },
                    {
                        "name": "Yi-Jyun Sun"
                    },
                    {
                        "name": "Gokhan Tur"
                    },
                    {
                        "name": "Dilek Hakkani-Tur"
                    }
                ],
                "author_detail": {
                    "name": "Dilek Hakkani-Tur"
                },
                "author": "Dilek Hakkani-Tur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10313v1",
                "updated": "2025-01-17T17:35:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    17,
                    35,
                    14,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T17:35:14Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    17,
                    35,
                    14,
                    4,
                    17,
                    0
                ],
                "title": "Addressing Popularity Bias in Third-Party Library Recommendations Using\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing Popularity Bias in Third-Party Library Recommendations Using\n  LLMs"
                },
                "summary": "Recommender systems for software engineering (RSSE) play a crucial role in\nautomating development tasks by providing relevant suggestions according to the\ndeveloper's context. However, they suffer from the so-called popularity bias,\ni.e., the phenomenon of recommending popular items that might be irrelevant to\nthe current task. In particular, the long-tail effect can hamper the system's\nperformance in terms of accuracy, thus leading to false positives in the\nprovided recommendations. Foundation models are the most advanced generative\nAI-based models that achieve relevant results in several SE tasks.\n  This paper aims to investigate the capability of large language models (LLMs)\nto address the popularity bias in recommender systems of third-party libraries\n(TPLs). We conduct an ablation study experimenting with state-of-the-art\ntechniques to mitigate the popularity bias, including fine-tuning and\npopularity penalty mechanisms. Our findings reveal that the considered LLMs\ncannot address the popularity bias in TPL recommenders, even though fine-tuning\nand post-processing penalty mechanism contributes to increasing the overall\ndiversity of the provided recommendations. In addition, we discuss the\nlimitations of LLMs in this context and suggest potential improvements to\naddress the popularity bias in TPL recommenders, thus paving the way for\nadditional experiments in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems for software engineering (RSSE) play a crucial role in\nautomating development tasks by providing relevant suggestions according to the\ndeveloper's context. However, they suffer from the so-called popularity bias,\ni.e., the phenomenon of recommending popular items that might be irrelevant to\nthe current task. In particular, the long-tail effect can hamper the system's\nperformance in terms of accuracy, thus leading to false positives in the\nprovided recommendations. Foundation models are the most advanced generative\nAI-based models that achieve relevant results in several SE tasks.\n  This paper aims to investigate the capability of large language models (LLMs)\nto address the popularity bias in recommender systems of third-party libraries\n(TPLs). We conduct an ablation study experimenting with state-of-the-art\ntechniques to mitigate the popularity bias, including fine-tuning and\npopularity penalty mechanisms. Our findings reveal that the considered LLMs\ncannot address the popularity bias in TPL recommenders, even though fine-tuning\nand post-processing penalty mechanism contributes to increasing the overall\ndiversity of the provided recommendations. In addition, we discuss the\nlimitations of LLMs in this context and suggest potential improvements to\naddress the popularity bias in TPL recommenders, thus paving the way for\nadditional experiments in this direction."
                },
                "authors": [
                    {
                        "name": "Claudio Di Sipio"
                    },
                    {
                        "name": "Juri Di Rocco"
                    },
                    {
                        "name": "Davide Di Ruscio"
                    },
                    {
                        "name": "Vladyslav Bulhakov"
                    }
                ],
                "author_detail": {
                    "name": "Vladyslav Bulhakov"
                },
                "author": "Vladyslav Bulhakov",
                "arxiv_comment": "Accepted at the 1st International Workshop on Fairness in Software\n  Systems, co-located with SANER2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09594v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09594v2",
                "updated": "2025-01-17T16:44:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    16,
                    44,
                    35,
                    4,
                    17,
                    0
                ],
                "published": "2024-08-18T20:59:59Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    20,
                    59,
                    59,
                    6,
                    231,
                    0
                ],
                "title": "Moonshine: Distilling Game Content Generators into Steerable Generative\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moonshine: Distilling Game Content Generators into Steerable Generative\n  Models"
                },
                "summary": "Procedural Content Generation via Machine Learning (PCGML) has enhanced game\ncontent creation, yet challenges in controllability and limited training data\npersist. This study addresses these issues by distilling a constructive PCG\nalgorithm into a controllable PCGML model. We first generate a large amount of\ncontent with a constructive algorithm and label it using a Large Language Model\n(LLM). We use these synthetic labels to condition two PCGML models for\ncontent-specific generation, a diffusion model and the five-dollar model. This\nneural network distillation process ensures that the generation aligns with the\noriginal algorithm while introducing controllability through plain text. We\ndefine this text-conditioned PCGML as a Text-to-game-Map (T2M) task, offering\nan alternative to prevalent text-to-image multi-modal tasks. We compare our\ndistilled models with the baseline constructive algorithm. Our analysis of the\nvariety, accuracy, and quality of our generation demonstrates the efficacy of\ndistilling constructive methods into controllable text-conditioned PCGML\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Procedural Content Generation via Machine Learning (PCGML) has enhanced game\ncontent creation, yet challenges in controllability and limited training data\npersist. This study addresses these issues by distilling a constructive PCG\nalgorithm into a controllable PCGML model. We first generate a large amount of\ncontent with a constructive algorithm and label it using a Large Language Model\n(LLM). We use these synthetic labels to condition two PCGML models for\ncontent-specific generation, a diffusion model and the five-dollar model. This\nneural network distillation process ensures that the generation aligns with the\noriginal algorithm while introducing controllability through plain text. We\ndefine this text-conditioned PCGML as a Text-to-game-Map (T2M) task, offering\nan alternative to prevalent text-to-image multi-modal tasks. We compare our\ndistilled models with the baseline constructive algorithm. Our analysis of the\nvariety, accuracy, and quality of our generation demonstrates the efficacy of\ndistilling constructive methods into controllable text-conditioned PCGML\nmodels."
                },
                "authors": [
                    {
                        "name": "Yuhe Nie"
                    },
                    {
                        "name": "Michael Middleton"
                    },
                    {
                        "name": "Tim Merino"
                    },
                    {
                        "name": "Nidhushan Kanagaraja"
                    },
                    {
                        "name": "Ashutosh Kumar"
                    },
                    {
                        "name": "Zhan Zhuang"
                    },
                    {
                        "name": "Julian Togelius"
                    }
                ],
                "author_detail": {
                    "name": "Julian Togelius"
                },
                "author": "Julian Togelius",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09594v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10282v1",
                "updated": "2025-01-17T16:21:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    16,
                    21,
                    18,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T16:21:18Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    16,
                    21,
                    18,
                    4,
                    17,
                    0
                ],
                "title": "Computational Protein Science in the Era of Large Language Models (LLMs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Protein Science in the Era of Large Language Models (LLMs)"
                },
                "summary": "Considering the significance of proteins, computational protein science has\nalways been a critical scientific field, dedicated to revealing knowledge and\ndeveloping applications within the protein sequence-structure-function\nparadigm. In the last few decades, Artificial Intelligence (AI) has made\nsignificant impacts in computational protein science, leading to notable\nsuccesses in specific protein modeling tasks. However, those previous AI models\nstill meet limitations, such as the difficulty in comprehending the semantics\nof protein sequences, and the inability to generalize across a wide range of\nprotein modeling tasks. Recently, LLMs have emerged as a milestone in AI due to\ntheir unprecedented language processing & generalization capability. They can\npromote comprehensive progress in fields rather than solving individual tasks.\nAs a result, researchers have actively introduced LLM techniques in\ncomputational protein science, developing protein Language Models (pLMs) that\nskillfully grasp the foundational knowledge of proteins and can be effectively\ngeneralized to solve a diversity of sequence-structure-function reasoning\nproblems. While witnessing prosperous developments, it's necessary to present a\nsystematic overview of computational protein science empowered by LLM\ntechniques. First, we summarize existing pLMs into categories based on their\nmastered protein knowledge, i.e., underlying sequence patterns, explicit\nstructural and functional information, and external scientific languages.\nSecond, we introduce the utilization and adaptation of pLMs, highlighting their\nremarkable achievements in promoting protein structure prediction, protein\nfunction prediction, and protein design studies. Then, we describe the\npractical application of pLMs in antibody design, enzyme design, and drug\ndiscovery. Finally, we specifically discuss the promising future directions in\nthis fast-growing field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Considering the significance of proteins, computational protein science has\nalways been a critical scientific field, dedicated to revealing knowledge and\ndeveloping applications within the protein sequence-structure-function\nparadigm. In the last few decades, Artificial Intelligence (AI) has made\nsignificant impacts in computational protein science, leading to notable\nsuccesses in specific protein modeling tasks. However, those previous AI models\nstill meet limitations, such as the difficulty in comprehending the semantics\nof protein sequences, and the inability to generalize across a wide range of\nprotein modeling tasks. Recently, LLMs have emerged as a milestone in AI due to\ntheir unprecedented language processing & generalization capability. They can\npromote comprehensive progress in fields rather than solving individual tasks.\nAs a result, researchers have actively introduced LLM techniques in\ncomputational protein science, developing protein Language Models (pLMs) that\nskillfully grasp the foundational knowledge of proteins and can be effectively\ngeneralized to solve a diversity of sequence-structure-function reasoning\nproblems. While witnessing prosperous developments, it's necessary to present a\nsystematic overview of computational protein science empowered by LLM\ntechniques. First, we summarize existing pLMs into categories based on their\nmastered protein knowledge, i.e., underlying sequence patterns, explicit\nstructural and functional information, and external scientific languages.\nSecond, we introduce the utilization and adaptation of pLMs, highlighting their\nremarkable achievements in promoting protein structure prediction, protein\nfunction prediction, and protein design studies. Then, we describe the\npractical application of pLMs in antibody design, enzyme design, and drug\ndiscovery. Finally, we specifically discuss the promising future directions in\nthis fast-growing field."
                },
                "authors": [
                    {
                        "name": "Wenqi Fan"
                    },
                    {
                        "name": "Yi Zhou"
                    },
                    {
                        "name": "Shijie Wang"
                    },
                    {
                        "name": "Yuyao Yan"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Qian Zhao"
                    },
                    {
                        "name": "Le Song"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10262v1",
                "updated": "2025-01-17T15:43:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    15,
                    43,
                    49,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T15:43:49Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    15,
                    43,
                    49,
                    4,
                    17,
                    0
                ],
                "title": "Deployment of an Aerial Multi-agent System for Automated Task Execution\n  in Large-scale Underground Mining Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deployment of an Aerial Multi-agent System for Automated Task Execution\n  in Large-scale Underground Mining Environments"
                },
                "summary": "In this article, we present a framework for deploying an aerial multi-agent\nsystem in large-scale subterranean environments with minimal infrastructure for\nsupporting multi-agent operations. The multi-agent objective is to optimally\nand reactively allocate and execute inspection tasks in a mine, which are\nentered by a mine operator on-the-fly. The assignment of currently available\ntasks to the team of agents is accomplished through an auction-based system,\nwhere the agents bid for available tasks, which are used by a central\nauctioneer to optimally assigns tasks to agents. A mobile Wi-Fi mesh supports\ninter-agent communication and bi-directional communication between the agents\nand the task allocator, while the task execution is performed completely\ninfrastructure-free. Given a task to be accomplished, a reliable and modular\nagent behavior is synthesized by generating behavior trees from a pool of agent\ncapabilities, using a back-chaining approach. The auction system in the\nproposed framework is reactive and supports addition of new operator-specified\ntasks on-the-go, at any point through a user-friendly operator interface. The\nframework has been validated in a real underground mining environment using\nthree aerial agents, with several inspection locations spread in an environment\nof almost 200 meters. The proposed framework can be utilized for missions\ninvolving rapid inspection, gas detection, distributed sensing and mapping etc.\nin a subterranean environment. The proposed framework and its field deployment\ncontributes towards furthering reliable automation in large-scale subterranean\nenvironments to offload both routine and dangerous tasks from human operators\nto autonomous aerial robots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article, we present a framework for deploying an aerial multi-agent\nsystem in large-scale subterranean environments with minimal infrastructure for\nsupporting multi-agent operations. The multi-agent objective is to optimally\nand reactively allocate and execute inspection tasks in a mine, which are\nentered by a mine operator on-the-fly. The assignment of currently available\ntasks to the team of agents is accomplished through an auction-based system,\nwhere the agents bid for available tasks, which are used by a central\nauctioneer to optimally assigns tasks to agents. A mobile Wi-Fi mesh supports\ninter-agent communication and bi-directional communication between the agents\nand the task allocator, while the task execution is performed completely\ninfrastructure-free. Given a task to be accomplished, a reliable and modular\nagent behavior is synthesized by generating behavior trees from a pool of agent\ncapabilities, using a back-chaining approach. The auction system in the\nproposed framework is reactive and supports addition of new operator-specified\ntasks on-the-go, at any point through a user-friendly operator interface. The\nframework has been validated in a real underground mining environment using\nthree aerial agents, with several inspection locations spread in an environment\nof almost 200 meters. The proposed framework can be utilized for missions\ninvolving rapid inspection, gas detection, distributed sensing and mapping etc.\nin a subterranean environment. The proposed framework and its field deployment\ncontributes towards furthering reliable automation in large-scale subterranean\nenvironments to offload both routine and dangerous tasks from human operators\nto autonomous aerial robots."
                },
                "authors": [
                    {
                        "name": "Niklas Dahlquist"
                    },
                    {
                        "name": "Samuel Nordstrm"
                    },
                    {
                        "name": "Nikolaos Stathoulopoulos"
                    },
                    {
                        "name": "Bjrn Lindqvist"
                    },
                    {
                        "name": "Akshit Saradagi"
                    },
                    {
                        "name": "George Nikolakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "George Nikolakopoulos"
                },
                "author": "George Nikolakopoulos",
                "arxiv_comment": "Submitted to IEEE Transactions on Field Robotics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09686v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09686v2",
                "updated": "2025-01-17T15:24:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    15,
                    24,
                    53,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-16T17:37:58Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    17,
                    37,
                    58,
                    3,
                    16,
                    0
                ],
                "title": "Towards Large Reasoning Models: A Survey on Scaling LLM Reasoning\n  Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Large Reasoning Models: A Survey on Scaling LLM Reasoning\n  Capabilities"
                },
                "summary": "Language has long been conceived as an essential tool for human reasoning.\nThe breakthrough of Large Language Models (LLMs) has sparked significant\nresearch interest in leveraging these models to tackle complex reasoning tasks.\nResearchers have moved beyond simple autoregressive token generation by\nintroducing the concept of \"thought\" -- a sequence of tokens representing\nintermediate steps in the reasoning process. This innovative paradigm enables\nLLMs' to mimic complex human reasoning processes, such as tree search and\nreflective thinking. Recently, an emerging trend of learning to reason has\napplied reinforcement learning (RL) to train LLMs to master reasoning\nprocesses. This approach enables the automatic generation of high-quality\nreasoning trajectories through trial-and-error search algorithms, significantly\nexpanding LLMs' reasoning capacity by providing substantially more training\ndata. Furthermore, recent studies demonstrate that encouraging LLMs to \"think\"\nwith more tokens during test-time inference can further significantly boost\nreasoning accuracy. Therefore, the train-time and test-time scaling combined to\nshow a new research frontier -- a path toward Large Reasoning Model. The\nintroduction of OpenAI's o1 series marks a significant milestone in this\nresearch direction. In this survey, we present a comprehensive review of recent\nprogress in LLM reasoning. We begin by introducing the foundational background\nof LLMs and then explore the key technical components driving the development\nof large reasoning models, with a focus on automated data construction,\nlearning-to-reason techniques, and test-time scaling. We also analyze popular\nopen-source projects at building large reasoning models, and conclude with open\nchallenges and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language has long been conceived as an essential tool for human reasoning.\nThe breakthrough of Large Language Models (LLMs) has sparked significant\nresearch interest in leveraging these models to tackle complex reasoning tasks.\nResearchers have moved beyond simple autoregressive token generation by\nintroducing the concept of \"thought\" -- a sequence of tokens representing\nintermediate steps in the reasoning process. This innovative paradigm enables\nLLMs' to mimic complex human reasoning processes, such as tree search and\nreflective thinking. Recently, an emerging trend of learning to reason has\napplied reinforcement learning (RL) to train LLMs to master reasoning\nprocesses. This approach enables the automatic generation of high-quality\nreasoning trajectories through trial-and-error search algorithms, significantly\nexpanding LLMs' reasoning capacity by providing substantially more training\ndata. Furthermore, recent studies demonstrate that encouraging LLMs to \"think\"\nwith more tokens during test-time inference can further significantly boost\nreasoning accuracy. Therefore, the train-time and test-time scaling combined to\nshow a new research frontier -- a path toward Large Reasoning Model. The\nintroduction of OpenAI's o1 series marks a significant milestone in this\nresearch direction. In this survey, we present a comprehensive review of recent\nprogress in LLM reasoning. We begin by introducing the foundational background\nof LLMs and then explore the key technical components driving the development\nof large reasoning models, with a focus on automated data construction,\nlearning-to-reason techniques, and test-time scaling. We also analyze popular\nopen-source projects at building large reasoning models, and conclude with open\nchallenges and future research directions."
                },
                "authors": [
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Qianyue Hao"
                    },
                    {
                        "name": "Zefang Zong"
                    },
                    {
                        "name": "Jingwei Wang"
                    },
                    {
                        "name": "Yunke Zhang"
                    },
                    {
                        "name": "Jingyi Wang"
                    },
                    {
                        "name": "Xiaochong Lan"
                    },
                    {
                        "name": "Jiahui Gong"
                    },
                    {
                        "name": "Tianjian Ouyang"
                    },
                    {
                        "name": "Fanjin Meng"
                    },
                    {
                        "name": "Chenyang Shao"
                    },
                    {
                        "name": "Yuwei Yan"
                    },
                    {
                        "name": "Qinglong Yang"
                    },
                    {
                        "name": "Yiwen Song"
                    },
                    {
                        "name": "Sijian Ren"
                    },
                    {
                        "name": "Xinyuan Hu"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "36 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09686v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09686v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09274v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09274v2",
                "updated": "2025-01-17T15:22:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    15,
                    22,
                    0,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-16T03:44:16Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    3,
                    44,
                    16,
                    3,
                    16,
                    0
                ],
                "title": "Large Language Model is Secretly a Protein Sequence Optimizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model is Secretly a Protein Sequence Optimizer"
                },
                "summary": "We consider the protein sequence engineering problem, which aims to find\nprotein sequences with high fitness levels, starting from a given wild-type\nsequence. Directed evolution has been a dominating paradigm in this field which\nhas an iterative process to generate variants and select via experimental\nfeedback. We demonstrate large language models (LLMs), despite being trained on\nmassive texts, are secretly protein sequence optimizers. With a directed\nevolutionary method, LLM can perform protein engineering through Pareto and\nexperiment-budget constrained optimization, demonstrating success on both\nsynthetic and experimental fitness landscapes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the protein sequence engineering problem, which aims to find\nprotein sequences with high fitness levels, starting from a given wild-type\nsequence. Directed evolution has been a dominating paradigm in this field which\nhas an iterative process to generate variants and select via experimental\nfeedback. We demonstrate large language models (LLMs), despite being trained on\nmassive texts, are secretly protein sequence optimizers. With a directed\nevolutionary method, LLM can perform protein engineering through Pareto and\nexperiment-budget constrained optimization, demonstrating success on both\nsynthetic and experimental fitness landscapes."
                },
                "authors": [
                    {
                        "name": "Yinkai Wang"
                    },
                    {
                        "name": "Jiaxing He"
                    },
                    {
                        "name": "Yuanqi Du"
                    },
                    {
                        "name": "Xiaohui Chen"
                    },
                    {
                        "name": "Jianan Canal Li"
                    },
                    {
                        "name": "Li-Ping Liu"
                    },
                    {
                        "name": "Xiaolin Xu"
                    },
                    {
                        "name": "Soha Hassoun"
                    }
                ],
                "author_detail": {
                    "name": "Soha Hassoun"
                },
                "author": "Soha Hassoun",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09274v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09274v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14393v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14393v4",
                "updated": "2025-01-17T13:56:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    13,
                    56,
                    50,
                    4,
                    17,
                    0
                ],
                "published": "2024-06-20T15:12:27Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    15,
                    12,
                    27,
                    3,
                    172,
                    0
                ],
                "title": "Jailbreaking as a Reward Misspecification Problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking as a Reward Misspecification Problem"
                },
                "summary": "The widespread adoption of large language models (LLMs) has raised concerns\nabout their safety and reliability, particularly regarding their vulnerability\nto adversarial attacks. In this paper, we propose a novel perspective that\nattributes this vulnerability to reward misspecification during the alignment\nprocess. This misspecification occurs when the reward function fails to\naccurately capture the intended behavior, leading to misaligned model outputs.\nWe introduce a metric ReGap to quantify the extent of reward misspecification\nand demonstrate its effectiveness and robustness in detecting harmful backdoor\nprompts. Building upon these insights, we present ReMiss, a system for\nautomated red teaming that generates adversarial prompts in a\nreward-misspecified space. ReMiss achieves state-of-the-art attack success\nrates on the AdvBench benchmark against various target aligned LLMs while\npreserving the human readability of the generated prompts. Furthermore, these\nattacks on open-source models demonstrate high transferability to closed-source\nmodels like GPT-4o and out-of-distribution tasks from HarmBench. Detailed\nanalysis highlights the unique advantages of the proposed reward\nmisspecification objective compared to previous methods, offering new insights\nfor improving LLM safety and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of large language models (LLMs) has raised concerns\nabout their safety and reliability, particularly regarding their vulnerability\nto adversarial attacks. In this paper, we propose a novel perspective that\nattributes this vulnerability to reward misspecification during the alignment\nprocess. This misspecification occurs when the reward function fails to\naccurately capture the intended behavior, leading to misaligned model outputs.\nWe introduce a metric ReGap to quantify the extent of reward misspecification\nand demonstrate its effectiveness and robustness in detecting harmful backdoor\nprompts. Building upon these insights, we present ReMiss, a system for\nautomated red teaming that generates adversarial prompts in a\nreward-misspecified space. ReMiss achieves state-of-the-art attack success\nrates on the AdvBench benchmark against various target aligned LLMs while\npreserving the human readability of the generated prompts. Furthermore, these\nattacks on open-source models demonstrate high transferability to closed-source\nmodels like GPT-4o and out-of-distribution tasks from HarmBench. Detailed\nanalysis highlights the unique advantages of the proposed reward\nmisspecification objective compared to previous methods, offering new insights\nfor improving LLM safety and robustness."
                },
                "authors": [
                    {
                        "name": "Zhihui Xie"
                    },
                    {
                        "name": "Jiahui Gao"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Lingpeng Kong"
                    }
                ],
                "author_detail": {
                    "name": "Lingpeng Kong"
                },
                "author": "Lingpeng Kong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14393v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14393v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13265v2",
                "updated": "2025-01-17T13:51:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    13,
                    51,
                    21,
                    4,
                    17,
                    0
                ],
                "published": "2024-12-17T19:00:52Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    19,
                    0,
                    52,
                    1,
                    352,
                    0
                ],
                "title": "Nods of Agreement: Webcam-Driven Avatars Improve Meeting Outcomes and\n  Avatar Satisfaction Over Audio-Driven or Static Avatars in All-Avatar Work\n  Videoconferencing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nods of Agreement: Webcam-Driven Avatars Improve Meeting Outcomes and\n  Avatar Satisfaction Over Audio-Driven or Static Avatars in All-Avatar Work\n  Videoconferencing"
                },
                "summary": "Avatars are edging into mainstream videoconferencing, but evaluation of how\navatar animation modalities contribute to work meeting outcomes has been\nlimited. We report a within-group videoconferencing experiment in which 68\nemployees of a global technology company, in 16 groups, used the same stylized\navatars in three modalities (static picture, audio-animation, and\nwebcam-animation) to complete collaborative decision-making tasks.\nQuantitatively, for meeting outcomes, webcam-animated avatars improved meeting\neffectiveness over the picture modality and were also reported to be more\ncomfortable and inclusive than both other modalities. In terms of avatar\nsatisfaction, there was a similar preference for webcam animation as compared\nto both other modalities. Our qualitative analysis shows participants\nexpressing a preference for the holistic motion of webcam animation, and that\nmeaningful movement outweighs realism for meeting outcomes, as evidenced\nthrough a systematic overview of ten thematic factors. We discuss implications\nfor research and commercial deployment and conclude that webcam-animated\navatars are a plausible alternative to video in work meetings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Avatars are edging into mainstream videoconferencing, but evaluation of how\navatar animation modalities contribute to work meeting outcomes has been\nlimited. We report a within-group videoconferencing experiment in which 68\nemployees of a global technology company, in 16 groups, used the same stylized\navatars in three modalities (static picture, audio-animation, and\nwebcam-animation) to complete collaborative decision-making tasks.\nQuantitatively, for meeting outcomes, webcam-animated avatars improved meeting\neffectiveness over the picture modality and were also reported to be more\ncomfortable and inclusive than both other modalities. In terms of avatar\nsatisfaction, there was a similar preference for webcam animation as compared\nto both other modalities. Our qualitative analysis shows participants\nexpressing a preference for the holistic motion of webcam animation, and that\nmeaningful movement outweighs realism for meeting outcomes, as evidenced\nthrough a systematic overview of ten thematic factors. We discuss implications\nfor research and commercial deployment and conclude that webcam-animated\navatars are a plausible alternative to video in work meetings."
                },
                "authors": [
                    {
                        "name": "Fang Ma"
                    },
                    {
                        "name": "Ju Zhang"
                    },
                    {
                        "name": "Lev Tankelevitch"
                    },
                    {
                        "name": "Payod Panda"
                    },
                    {
                        "name": "Torang Asadi"
                    },
                    {
                        "name": "Charlie Hewitt"
                    },
                    {
                        "name": "Lohit Petikam"
                    },
                    {
                        "name": "James Clemoes"
                    },
                    {
                        "name": "Marco Gillies"
                    },
                    {
                        "name": "Xueni Pan"
                    },
                    {
                        "name": "Sean Rintel"
                    },
                    {
                        "name": "Marta Wilczkowiak"
                    }
                ],
                "author_detail": {
                    "name": "Marta Wilczkowiak"
                },
                "author": "Marta Wilczkowiak",
                "arxiv_doi": "10.1145/3711040",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711040",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.13265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "to be published in PACM HCI",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10200v1",
                "updated": "2025-01-17T13:48:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    13,
                    48,
                    32,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T13:48:32Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    13,
                    48,
                    32,
                    4,
                    17,
                    0
                ],
                "title": "Test Wars: A Comparative Study of SBST, Symbolic Execution, and\n  LLM-Based Approaches to Unit Test Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test Wars: A Comparative Study of SBST, Symbolic Execution, and\n  LLM-Based Approaches to Unit Test Generation"
                },
                "summary": "Generating tests automatically is a key and ongoing area of focus in software\nengineering research. The emergence of Large Language Models (LLMs) has opened\nup new opportunities, given their ability to perform a wide spectrum of tasks.\nHowever, the effectiveness of LLM-based approaches compared to traditional\ntechniques such as search-based software testing (SBST) and symbolic execution\nremains uncertain. In this paper, we perform an extensive study of automatic\ntest generation approaches based on three tools: EvoSuite for SBST, Kex for\nsymbolic execution, and TestSpark for LLM-based test generation. We evaluate\ntools performance on the GitBug Java dataset and compare them using various\nexecution-based and feature-based metrics. Our results show that while\nLLM-based test generation is promising, it falls behind traditional methods in\nterms of coverage. However, it significantly outperforms them in mutation\nscores, suggesting that LLMs provide a deeper semantic understanding of code.\nLLM-based approach also performed worse than SBST and symbolic execution-based\napproaches w.r.t. fault detection capabilities. Additionally, our feature-based\nanalysis shows that all tools are primarily affected by the complexity and\ninternal dependencies of the class under test (CUT), with LLM-based approaches\nbeing especially sensitive to the CUT size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating tests automatically is a key and ongoing area of focus in software\nengineering research. The emergence of Large Language Models (LLMs) has opened\nup new opportunities, given their ability to perform a wide spectrum of tasks.\nHowever, the effectiveness of LLM-based approaches compared to traditional\ntechniques such as search-based software testing (SBST) and symbolic execution\nremains uncertain. In this paper, we perform an extensive study of automatic\ntest generation approaches based on three tools: EvoSuite for SBST, Kex for\nsymbolic execution, and TestSpark for LLM-based test generation. We evaluate\ntools performance on the GitBug Java dataset and compare them using various\nexecution-based and feature-based metrics. Our results show that while\nLLM-based test generation is promising, it falls behind traditional methods in\nterms of coverage. However, it significantly outperforms them in mutation\nscores, suggesting that LLMs provide a deeper semantic understanding of code.\nLLM-based approach also performed worse than SBST and symbolic execution-based\napproaches w.r.t. fault detection capabilities. Additionally, our feature-based\nanalysis shows that all tools are primarily affected by the complexity and\ninternal dependencies of the class under test (CUT), with LLM-based approaches\nbeing especially sensitive to the CUT size."
                },
                "authors": [
                    {
                        "name": "Azat Abdullin"
                    },
                    {
                        "name": "Pouria Derakhshanfar"
                    },
                    {
                        "name": "Annibale Panichella"
                    }
                ],
                "author_detail": {
                    "name": "Annibale Panichella"
                },
                "author": "Annibale Panichella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10199v1",
                "updated": "2025-01-17T13:48:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    13,
                    48,
                    4,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T13:48:04Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    13,
                    48,
                    4,
                    4,
                    17,
                    0
                ],
                "title": "Adaptive Clustering for Efficient Phenotype Segmentation of UAV\n  Hyperspectral Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Clustering for Efficient Phenotype Segmentation of UAV\n  Hyperspectral Data"
                },
                "summary": "Unmanned Aerial Vehicles (UAVs) combined with Hyperspectral imaging (HSI)\noffer potential for environmental and agricultural applications by capturing\ndetailed spectral information that enables the prediction of invisible features\nlike biochemical leaf properties. However, the data-intensive nature of HSI\nposes challenges for remote devices, which have limited computational resources\nand storage. This paper introduces an Online Hyperspectral Simple Linear\nIterative Clustering algorithm (OHSLIC) framework for real-time tree phenotype\nsegmentation. OHSLIC reduces inherent noise and computational demands through\nadaptive incremental clustering and a lightweight neural network, which\nphenotypes trees using leaf contents such as chlorophyll, carotenoids, and\nanthocyanins. A hyperspectral dataset is created using a custom simulator that\nincorporates realistic leaf parameters, and light interactions. Results\ndemonstrate that OHSLIC achieves superior regression accuracy and segmentation\nperformance compared to pixel- or window-based methods while significantly\nreducing inference time. The method`s adaptive clustering enables dynamic\ntrade-offs between computational efficiency and accuracy, paving the way for\nscalable edge-device deployment in HSI applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned Aerial Vehicles (UAVs) combined with Hyperspectral imaging (HSI)\noffer potential for environmental and agricultural applications by capturing\ndetailed spectral information that enables the prediction of invisible features\nlike biochemical leaf properties. However, the data-intensive nature of HSI\nposes challenges for remote devices, which have limited computational resources\nand storage. This paper introduces an Online Hyperspectral Simple Linear\nIterative Clustering algorithm (OHSLIC) framework for real-time tree phenotype\nsegmentation. OHSLIC reduces inherent noise and computational demands through\nadaptive incremental clustering and a lightweight neural network, which\nphenotypes trees using leaf contents such as chlorophyll, carotenoids, and\nanthocyanins. A hyperspectral dataset is created using a custom simulator that\nincorporates realistic leaf parameters, and light interactions. Results\ndemonstrate that OHSLIC achieves superior regression accuracy and segmentation\nperformance compared to pixel- or window-based methods while significantly\nreducing inference time. The method`s adaptive clustering enables dynamic\ntrade-offs between computational efficiency and accuracy, paving the way for\nscalable edge-device deployment in HSI applications."
                },
                "authors": [
                    {
                        "name": "Ciem Cornelissen"
                    },
                    {
                        "name": "Sam Leroux"
                    },
                    {
                        "name": "Pieter Simoens"
                    }
                ],
                "author_detail": {
                    "name": "Pieter Simoens"
                },
                "author": "Pieter Simoens",
                "arxiv_comment": "accepted WACV 2025 GeoCV workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.4; I.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10186v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10186v1",
                "updated": "2025-01-17T13:32:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    13,
                    32,
                    19,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T13:32:19Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    13,
                    32,
                    19,
                    4,
                    17,
                    0
                ],
                "title": "Generative Artificial Intelligence: Implications for Biomedical and\n  Health Professions Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Artificial Intelligence: Implications for Biomedical and\n  Health Professions Education"
                },
                "summary": "Generative AI has had a profound impact on biomedicine and health, both in\nprofessional work and in education. Based on large language models (LLMs),\ngenerative AI has been found to perform as well as humans in simulated\nsituations taking medical board exams, answering clinical questions, solving\nclinical cases, applying clinical reasoning, and summarizing information.\nGenerative AI is also being used widely in education, performing well in\nacademic courses and their assessments. This review summarizes the successes of\nLLMs and highlights some of their challenges in the context of education, most\nnotably aspects that may undermines the acquisition of knowledge and skills for\nprofessional work. It then provides recommendations for best practices\novercoming shortcomings for LLM use in education. Although there are challenges\nfor use of generative AI in education, all students and faculty, in biomedicine\nand health and beyond, must have understanding and be competent in its use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI has had a profound impact on biomedicine and health, both in\nprofessional work and in education. Based on large language models (LLMs),\ngenerative AI has been found to perform as well as humans in simulated\nsituations taking medical board exams, answering clinical questions, solving\nclinical cases, applying clinical reasoning, and summarizing information.\nGenerative AI is also being used widely in education, performing well in\nacademic courses and their assessments. This review summarizes the successes of\nLLMs and highlights some of their challenges in the context of education, most\nnotably aspects that may undermines the acquisition of knowledge and skills for\nprofessional work. It then provides recommendations for best practices\novercoming shortcomings for LLM use in education. Although there are challenges\nfor use of generative AI in education, all students and faculty, in biomedicine\nand health and beyond, must have understanding and be competent in its use."
                },
                "authors": [
                    {
                        "name": "William Hersh"
                    }
                ],
                "author_detail": {
                    "name": "William Hersh"
                },
                "author": "William Hersh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10186v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10175v1",
                "updated": "2025-01-17T13:17:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    13,
                    17,
                    42,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T13:17:42Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    13,
                    17,
                    42,
                    4,
                    17,
                    0
                ],
                "title": "Multi-stage Training of Bilingual Islamic LLM for Neural Passage\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-stage Training of Bilingual Islamic LLM for Neural Passage\n  Retrieval"
                },
                "summary": "This study examines the use of Natural Language Processing (NLP) technology\nwithin the Islamic domain, focusing on developing an Islamic neural retrieval\nmodel. By leveraging the robust XLM-R model, the research employs a language\nreduction technique to create a lightweight bilingual large language model\n(LLM). Our approach for domain adaptation addresses the unique challenges faced\nin the Islamic domain, where substantial in-domain corpora exist only in Arabic\nwhile limited in other languages, including English.\n  The work utilizes a multi-stage training process for retrieval models,\nincorporating large retrieval datasets, such as MS MARCO, and smaller,\nin-domain datasets to improve retrieval performance. Additionally, we have\ncurated an in-domain retrieval dataset in English by employing data\naugmentation techniques and involving a reliable Islamic source. This approach\nenhances the domain-specific dataset for retrieval, leading to further\nperformance gains.\n  The findings suggest that combining domain adaptation and a multi-stage\ntraining method for the bilingual Islamic neural retrieval model enables it to\noutperform monolingual models on downstream retrieval tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examines the use of Natural Language Processing (NLP) technology\nwithin the Islamic domain, focusing on developing an Islamic neural retrieval\nmodel. By leveraging the robust XLM-R model, the research employs a language\nreduction technique to create a lightweight bilingual large language model\n(LLM). Our approach for domain adaptation addresses the unique challenges faced\nin the Islamic domain, where substantial in-domain corpora exist only in Arabic\nwhile limited in other languages, including English.\n  The work utilizes a multi-stage training process for retrieval models,\nincorporating large retrieval datasets, such as MS MARCO, and smaller,\nin-domain datasets to improve retrieval performance. Additionally, we have\ncurated an in-domain retrieval dataset in English by employing data\naugmentation techniques and involving a reliable Islamic source. This approach\nenhances the domain-specific dataset for retrieval, leading to further\nperformance gains.\n  The findings suggest that combining domain adaptation and a multi-stage\ntraining method for the bilingual Islamic neural retrieval model enables it to\noutperform monolingual models on downstream retrieval tasks."
                },
                "authors": [
                    {
                        "name": "Vera Pavlova"
                    }
                ],
                "author_detail": {
                    "name": "Vera Pavlova"
                },
                "author": "Vera Pavlova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00329v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00329v2",
                "updated": "2025-01-17T12:53:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    53,
                    37,
                    4,
                    17,
                    0
                ],
                "published": "2024-11-30T03:02:50Z",
                "published_parsed": [
                    2024,
                    11,
                    30,
                    3,
                    2,
                    50,
                    5,
                    335,
                    0
                ],
                "title": "Language Models in Software Development Tasks: An Experimental Analysis\n  of Energy and Accuracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models in Software Development Tasks: An Experimental Analysis\n  of Energy and Accuracy"
                },
                "summary": "The use of generative AI-based coding assistants like ChatGPT and Github\nCopilot is a reality in contemporary software development. Many of these tools\nare provided as remote APIs. Using third-party APIs raises data privacy and\nsecurity concerns for client companies, which motivates the use of\nlocally-deployed language models. In this study, we explore the trade-off\nbetween model accuracy and energy consumption, aiming to provide valuable\ninsights to help developers make informed decisions when selecting a language\nmodel. We investigate the performance of 18 families of LLMs in typical\nsoftware development tasks on two real-world infrastructures, a commodity GPU\nand a powerful AI-specific GPU. Given that deploying LLMs locally requires\npowerful infrastructure which might not be affordable for everyone, we consider\nboth full-precision and quantized models. Our findings reveal that employing a\nbig LLM with a higher energy budget does not always translate to significantly\nimproved accuracy. Additionally, quantized versions of large models generally\noffer better efficiency and accuracy compared to full-precision versions of\nmedium-sized ones. Apart from that, not a single model is suitable for all\ntypes of software development tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of generative AI-based coding assistants like ChatGPT and Github\nCopilot is a reality in contemporary software development. Many of these tools\nare provided as remote APIs. Using third-party APIs raises data privacy and\nsecurity concerns for client companies, which motivates the use of\nlocally-deployed language models. In this study, we explore the trade-off\nbetween model accuracy and energy consumption, aiming to provide valuable\ninsights to help developers make informed decisions when selecting a language\nmodel. We investigate the performance of 18 families of LLMs in typical\nsoftware development tasks on two real-world infrastructures, a commodity GPU\nand a powerful AI-specific GPU. Given that deploying LLMs locally requires\npowerful infrastructure which might not be affordable for everyone, we consider\nboth full-precision and quantized models. Our findings reveal that employing a\nbig LLM with a higher energy budget does not always translate to significantly\nimproved accuracy. Additionally, quantized versions of large models generally\noffer better efficiency and accuracy compared to full-precision versions of\nmedium-sized ones. Apart from that, not a single model is suitable for all\ntypes of software development tasks."
                },
                "authors": [
                    {
                        "name": "Negar Alizadeh"
                    },
                    {
                        "name": "Boris Belchev"
                    },
                    {
                        "name": "Nishant Saurabh"
                    },
                    {
                        "name": "Patricia Kelbert"
                    },
                    {
                        "name": "Fernando Castor"
                    }
                ],
                "author_detail": {
                    "name": "Fernando Castor"
                },
                "author": "Fernando Castor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00329v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00329v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02957v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02957v3",
                "updated": "2025-01-17T11:59:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    11,
                    59,
                    23,
                    4,
                    17,
                    0
                ],
                "published": "2024-05-05T14:53:51Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    14,
                    53,
                    51,
                    6,
                    126,
                    0
                ],
                "title": "Agent Hospital: A Simulacrum of Hospital with Evolvable Medical Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent Hospital: A Simulacrum of Hospital with Evolvable Medical Agents"
                },
                "summary": "The recent rapid development of large language models (LLMs) has sparked a\nnew wave of technological revolution in medical artificial intelligence (AI).\nWhile LLMs are designed to understand and generate text like a human,\nautonomous agents that utilize LLMs as their \"brain\" have exhibited\ncapabilities beyond text processing such as planning, reflection, and using\ntools by enabling their \"bodies\" to interact with the environment. We introduce\na simulacrum of hospital called Agent Hospital that simulates the entire\nprocess of treating illness, in which all patients, nurses, and doctors are\nLLM-powered autonomous agents. Within the simulacrum, doctor agents are able to\nevolve by treating a large number of patient agents without the need to label\ntraining data manually. After treating tens of thousands of patient agents in\nthe simulacrum (human doctors may take several years in the real world), the\nevolved doctor agents outperform state-of-the-art medical agent methods on the\nMedQA benchmark comprising US Medical Licensing Examination (USMLE) test\nquestions. Our methods of simulacrum construction and agent evolution have the\npotential in benefiting a broad range of applications beyond medical AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent rapid development of large language models (LLMs) has sparked a\nnew wave of technological revolution in medical artificial intelligence (AI).\nWhile LLMs are designed to understand and generate text like a human,\nautonomous agents that utilize LLMs as their \"brain\" have exhibited\ncapabilities beyond text processing such as planning, reflection, and using\ntools by enabling their \"bodies\" to interact with the environment. We introduce\na simulacrum of hospital called Agent Hospital that simulates the entire\nprocess of treating illness, in which all patients, nurses, and doctors are\nLLM-powered autonomous agents. Within the simulacrum, doctor agents are able to\nevolve by treating a large number of patient agents without the need to label\ntraining data manually. After treating tens of thousands of patient agents in\nthe simulacrum (human doctors may take several years in the real world), the\nevolved doctor agents outperform state-of-the-art medical agent methods on the\nMedQA benchmark comprising US Medical Licensing Examination (USMLE) test\nquestions. Our methods of simulacrum construction and agent evolution have the\npotential in benefiting a broad range of applications beyond medical AI."
                },
                "authors": [
                    {
                        "name": "Junkai Li"
                    },
                    {
                        "name": "Yunghwei Lai"
                    },
                    {
                        "name": "Weitao Li"
                    },
                    {
                        "name": "Jingyi Ren"
                    },
                    {
                        "name": "Meng Zhang"
                    },
                    {
                        "name": "Xinhui Kang"
                    },
                    {
                        "name": "Siyu Wang"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Ya-Qin Zhang"
                    },
                    {
                        "name": "Weizhi Ma"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02957v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02957v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10134v1",
                "updated": "2025-01-17T11:49:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    11,
                    49,
                    49,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T11:49:49Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    11,
                    49,
                    49,
                    4,
                    17,
                    0
                ],
                "title": "Exploring the Impact of Generative Artificial Intelligence in Education:\n  A Thematic Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Impact of Generative Artificial Intelligence in Education:\n  A Thematic Analysis"
                },
                "summary": "The recent advancements in Generative Artificial intelligence (GenAI)\ntechnology have been transformative for the field of education. Large Language\nModels (LLMs) such as ChatGPT and Bard can be leveraged to automate boilerplate\ntasks, create content for personalised teaching, and handle repetitive tasks to\nallow more time for creative thinking. However, it is important to develop\nguidelines, policies, and assessment methods in the education sector to ensure\nthe responsible integration of these tools. In this article, thematic analysis\nhas been performed on seven essays obtained from professionals in the education\nsector to understand the advantages and pitfalls of using GenAI models such as\nChatGPT and Bard in education. Exploratory Data Analysis (EDA) has been\nperformed on the essays to extract further insights from the text. The study\nfound several themes which highlight benefits and drawbacks of GenAI tools, as\nwell as suggestions to overcome these limitations and ensure that students are\nusing these tools in a responsible and ethical manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advancements in Generative Artificial intelligence (GenAI)\ntechnology have been transformative for the field of education. Large Language\nModels (LLMs) such as ChatGPT and Bard can be leveraged to automate boilerplate\ntasks, create content for personalised teaching, and handle repetitive tasks to\nallow more time for creative thinking. However, it is important to develop\nguidelines, policies, and assessment methods in the education sector to ensure\nthe responsible integration of these tools. In this article, thematic analysis\nhas been performed on seven essays obtained from professionals in the education\nsector to understand the advantages and pitfalls of using GenAI models such as\nChatGPT and Bard in education. Exploratory Data Analysis (EDA) has been\nperformed on the essays to extract further insights from the text. The study\nfound several themes which highlight benefits and drawbacks of GenAI tools, as\nwell as suggestions to overcome these limitations and ensure that students are\nusing these tools in a responsible and ethical manner."
                },
                "authors": [
                    {
                        "name": "Abhishek Kaushik"
                    },
                    {
                        "name": "Sargam Yadav"
                    },
                    {
                        "name": "Andrew Browne"
                    },
                    {
                        "name": "David Lillis"
                    },
                    {
                        "name": "David Williams"
                    },
                    {
                        "name": "Jack Mc Donnell"
                    },
                    {
                        "name": "Peadar Grant"
                    },
                    {
                        "name": "Siobhan Connolly Kernan"
                    },
                    {
                        "name": "Shubham Sharma"
                    },
                    {
                        "name": "Mansi Arora"
                    }
                ],
                "author_detail": {
                    "name": "Mansi Arora"
                },
                "arxiv_affiliation": "Jagan Institute of Management Studies, Rohini, Delhi, Delhi, India",
                "author": "Mansi Arora",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09636v2",
                "updated": "2025-01-17T11:44:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    11,
                    44,
                    53,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-16T16:25:30Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    16,
                    25,
                    30,
                    3,
                    16,
                    0
                ],
                "title": "LLM-Based Routing in Mixture of Experts: A Novel Framework for Trading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Routing in Mixture of Experts: A Novel Framework for Trading"
                },
                "summary": "Recent advances in deep learning and large language models (LLMs) have\nfacilitated the deployment of the mixture-of-experts (MoE) mechanism in the\nstock investment domain. While these models have demonstrated promising trading\nperformance, they are often unimodal, neglecting the wealth of information\navailable in other modalities, such as textual data. Moreover, the traditional\nneural network-based router selection mechanism fails to consider contextual\nand real-world nuances, resulting in suboptimal expert selection. To address\nthese limitations, we propose LLMoE, a novel framework that employs LLMs as the\nrouter within the MoE architecture. Specifically, we replace the conventional\nneural network-based router with LLMs, leveraging their extensive world\nknowledge and reasoning capabilities to select experts based on historical\nprice data and stock news. This approach provides a more effective and\ninterpretable selection mechanism. Our experiments on multimodal real-world\nstock datasets demonstrate that LLMoE outperforms state-of-the-art MoE models\nand other deep neural network approaches. Additionally, the flexible\narchitecture of LLMoE allows for easy adaptation to various downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in deep learning and large language models (LLMs) have\nfacilitated the deployment of the mixture-of-experts (MoE) mechanism in the\nstock investment domain. While these models have demonstrated promising trading\nperformance, they are often unimodal, neglecting the wealth of information\navailable in other modalities, such as textual data. Moreover, the traditional\nneural network-based router selection mechanism fails to consider contextual\nand real-world nuances, resulting in suboptimal expert selection. To address\nthese limitations, we propose LLMoE, a novel framework that employs LLMs as the\nrouter within the MoE architecture. Specifically, we replace the conventional\nneural network-based router with LLMs, leveraging their extensive world\nknowledge and reasoning capabilities to select experts based on historical\nprice data and stock news. This approach provides a more effective and\ninterpretable selection mechanism. Our experiments on multimodal real-world\nstock datasets demonstrate that LLMoE outperforms state-of-the-art MoE models\nand other deep neural network approaches. Additionally, the flexible\narchitecture of LLMoE allows for easy adaptation to various downstream tasks."
                },
                "authors": [
                    {
                        "name": "Kuan-Ming Liu"
                    },
                    {
                        "name": "Ming-Chih Lo"
                    }
                ],
                "author_detail": {
                    "name": "Ming-Chih Lo"
                },
                "author": "Ming-Chih Lo",
                "arxiv_comment": "Accepted by AAAI 2025 Workshop on AI for Social Impact - Bridging\n  Innovations in Finance, Social Media, and Crime Prevention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10132v1",
                "updated": "2025-01-17T11:41:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    11,
                    41,
                    53,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T11:41:53Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    11,
                    41,
                    53,
                    4,
                    17,
                    0
                ],
                "title": "ComplexFuncBench: Exploring Multi-Step and Constrained Function Calling\n  under Long-Context Scenario",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ComplexFuncBench: Exploring Multi-Step and Constrained Function Calling\n  under Long-Context Scenario"
                },
                "summary": "Enhancing large language models (LLMs) with real-time APIs can help generate\nmore accurate and up-to-date responses. However, evaluating the function\ncalling abilities of LLMs in real-world scenarios remains under-explored due to\nthe complexity of data collection and evaluation. In this work, we introduce\nComplexFuncBench, a benchmark for complex function calling across five\nreal-world scenarios. Compared to existing benchmarks, ComplexFuncBench\nencompasses multi-step and constrained function calling, which requires\nlong-parameter filing, parameter value reasoning, and 128k long context.\nAdditionally, we propose an automatic framework, ComplexEval, for\nquantitatively evaluating complex function calling tasks. Through comprehensive\nexperiments, we demonstrate the deficiencies of state-of-the-art LLMs in\nfunction calling and suggest future directions for optimizing these\ncapabilities. The data and code are available at\n\\url{https://github.com/THUDM/ComplexFuncBench}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing large language models (LLMs) with real-time APIs can help generate\nmore accurate and up-to-date responses. However, evaluating the function\ncalling abilities of LLMs in real-world scenarios remains under-explored due to\nthe complexity of data collection and evaluation. In this work, we introduce\nComplexFuncBench, a benchmark for complex function calling across five\nreal-world scenarios. Compared to existing benchmarks, ComplexFuncBench\nencompasses multi-step and constrained function calling, which requires\nlong-parameter filing, parameter value reasoning, and 128k long context.\nAdditionally, we propose an automatic framework, ComplexEval, for\nquantitatively evaluating complex function calling tasks. Through comprehensive\nexperiments, we demonstrate the deficiencies of state-of-the-art LLMs in\nfunction calling and suggest future directions for optimizing these\ncapabilities. The data and code are available at\n\\url{https://github.com/THUDM/ComplexFuncBench}."
                },
                "authors": [
                    {
                        "name": "Lucen Zhong"
                    },
                    {
                        "name": "Zhengxiao Du"
                    },
                    {
                        "name": "Xiaohan Zhang"
                    },
                    {
                        "name": "Haiyi Hu"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.00900v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.00900v3",
                "updated": "2025-01-17T11:18:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    11,
                    18,
                    37,
                    4,
                    17,
                    0
                ],
                "published": "2023-09-02T10:32:53Z",
                "published_parsed": [
                    2023,
                    9,
                    2,
                    10,
                    32,
                    53,
                    5,
                    245,
                    0
                ],
                "title": "Large Process Models: A Vision for Business Process Management in the\n  Age of Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Process Models: A Vision for Business Process Management in the\n  Age of Generative AI"
                },
                "summary": "The continued success of Large Language Models (LLMs) and other generative\nartificial intelligence approaches highlights the advantages that large\ninformation corpora can have over rigidly defined symbolic models, but also\nserves as a proof-point of the challenges that purely statistics-based\napproaches have in terms of safety and trustworthiness. As a framework for\ncontextualizing the potential, as well as the limitations of LLMs and other\nfoundation model-based technologies, we propose the concept of a Large Process\nModel (LPM) that combines the correlation power of LLMs with the analytical\nprecision and reliability of knowledge-based systems and automated reasoning\napproaches. LPMs are envisioned to directly utilize the wealth of process\nmanagement experience that experts have accumulated, as well as process\nperformance data of organizations with diverse characteristics, e.g.,\\\nregarding size, region, or industry. In this vision, the proposed LPM would\nallow organizations to receive context-specific (tailored) process and other\nbusiness models, analytical deep-dives, and improvement recommendations. As\nsuch, they would allow to substantially decrease the time and effort required\nfor business transformation, while also allowing for deeper, more impactful,\nand more actionable insights than previously possible. We argue that\nimplementing an LPM is feasible, but also highlight limitations and research\nchallenges that need to be solved to implement particular aspects of the LPM\nvision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The continued success of Large Language Models (LLMs) and other generative\nartificial intelligence approaches highlights the advantages that large\ninformation corpora can have over rigidly defined symbolic models, but also\nserves as a proof-point of the challenges that purely statistics-based\napproaches have in terms of safety and trustworthiness. As a framework for\ncontextualizing the potential, as well as the limitations of LLMs and other\nfoundation model-based technologies, we propose the concept of a Large Process\nModel (LPM) that combines the correlation power of LLMs with the analytical\nprecision and reliability of knowledge-based systems and automated reasoning\napproaches. LPMs are envisioned to directly utilize the wealth of process\nmanagement experience that experts have accumulated, as well as process\nperformance data of organizations with diverse characteristics, e.g.,\\\nregarding size, region, or industry. In this vision, the proposed LPM would\nallow organizations to receive context-specific (tailored) process and other\nbusiness models, analytical deep-dives, and improvement recommendations. As\nsuch, they would allow to substantially decrease the time and effort required\nfor business transformation, while also allowing for deeper, more impactful,\nand more actionable insights than previously possible. We argue that\nimplementing an LPM is feasible, but also highlight limitations and research\nchallenges that need to be solved to implement particular aspects of the LPM\nvision."
                },
                "authors": [
                    {
                        "name": "Timotheus Kampik"
                    },
                    {
                        "name": "Christian Warmuth"
                    },
                    {
                        "name": "Adrian Rebmann"
                    },
                    {
                        "name": "Ron Agam"
                    },
                    {
                        "name": "Lukas N. P. Egger"
                    },
                    {
                        "name": "Andreas Gerber"
                    },
                    {
                        "name": "Johannes Hoffart"
                    },
                    {
                        "name": "Jonas Kolk"
                    },
                    {
                        "name": "Philipp Herzig"
                    },
                    {
                        "name": "Gero Decker"
                    },
                    {
                        "name": "Han van der Aa"
                    },
                    {
                        "name": "Artem Polyvyanyy"
                    },
                    {
                        "name": "Stefanie Rinderle-Ma"
                    },
                    {
                        "name": "Ingo Weber"
                    },
                    {
                        "name": "Matthias Weidlich"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Weidlich"
                },
                "author": "Matthias Weidlich",
                "arxiv_doi": "10.1007/s13218-024-00863-8",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s13218-024-00863-8",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.00900v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.00900v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "K\\\"unstl Intell (2024)",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10120v1",
                "updated": "2025-01-17T11:12:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    11,
                    12,
                    28,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T11:12:28Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    11,
                    12,
                    28,
                    4,
                    17,
                    0
                ],
                "title": "PaSa: An LLM Agent for Comprehensive Academic Paper Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PaSa: An LLM Agent for Comprehensive Academic Paper Search"
                },
                "summary": "We introduce PaSa, an advanced Paper Search agent powered by large language\nmodels. PaSa can autonomously make a series of decisions, including invoking\nsearch tools, reading papers, and selecting relevant references, to ultimately\nobtain comprehensive and accurate results for complex scholarly queries. We\noptimize PaSa using reinforcement learning with a synthetic dataset,\nAutoScholarQuery, which includes 35k fine-grained academic queries and\ncorresponding papers sourced from top-tier AI conference publications.\nAdditionally, we develop RealScholarQuery, a benchmark collecting real-world\nacademic queries to assess PaSa performance in more realistic scenarios.\nDespite being trained on synthetic data, PaSa significantly outperforms\nexisting baselines on RealScholarQuery, including Google, Google Scholar,\nGoogle with GPT-4 for paraphrased queries, chatGPT (search-enabled GPT-4o),\nGPT-o1, and PaSa-GPT-4o (PaSa implemented by prompting GPT-4o). Notably,\nPaSa-7B surpasses the best Google-based baseline, Google with GPT-4o, by 37.78%\nin recall@20 and 39.90% in recall@50. It also exceeds PaSa-GPT-4o by 30.36% in\nrecall and 4.25% in precision. Model, datasets, and code are available at\nhttps://github.com/bytedance/pasa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce PaSa, an advanced Paper Search agent powered by large language\nmodels. PaSa can autonomously make a series of decisions, including invoking\nsearch tools, reading papers, and selecting relevant references, to ultimately\nobtain comprehensive and accurate results for complex scholarly queries. We\noptimize PaSa using reinforcement learning with a synthetic dataset,\nAutoScholarQuery, which includes 35k fine-grained academic queries and\ncorresponding papers sourced from top-tier AI conference publications.\nAdditionally, we develop RealScholarQuery, a benchmark collecting real-world\nacademic queries to assess PaSa performance in more realistic scenarios.\nDespite being trained on synthetic data, PaSa significantly outperforms\nexisting baselines on RealScholarQuery, including Google, Google Scholar,\nGoogle with GPT-4 for paraphrased queries, chatGPT (search-enabled GPT-4o),\nGPT-o1, and PaSa-GPT-4o (PaSa implemented by prompting GPT-4o). Notably,\nPaSa-7B surpasses the best Google-based baseline, Google with GPT-4o, by 37.78%\nin recall@20 and 39.90% in recall@50. It also exceeds PaSa-GPT-4o by 30.36% in\nrecall and 4.25% in precision. Model, datasets, and code are available at\nhttps://github.com/bytedance/pasa."
                },
                "authors": [
                    {
                        "name": "Yichen He"
                    },
                    {
                        "name": "Guanhua Huang"
                    },
                    {
                        "name": "Peiyuan Feng"
                    },
                    {
                        "name": "Yuan Lin"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Weinan E"
                    }
                ],
                "author_detail": {
                    "name": "Weinan E"
                },
                "author": "Weinan E",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12750v2",
                "updated": "2025-01-17T11:10:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    11,
                    10,
                    5,
                    4,
                    17,
                    0
                ],
                "published": "2024-05-21T13:02:27Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    13,
                    2,
                    27,
                    1,
                    142,
                    0
                ],
                "title": "Generative AI in Cybersecurity: A Comprehensive Review of LLM\n  Applications and Vulnerabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI in Cybersecurity: A Comprehensive Review of LLM\n  Applications and Vulnerabilities"
                },
                "summary": "This paper provides a comprehensive review of the future of cybersecurity\nthrough Generative AI and Large Language Models (LLMs). We explore LLM\napplications across various domains, including hardware design security,\nintrusion detection, software engineering, design verification, cyber threat\nintelligence, malware detection, and phishing detection. We present an overview\nof LLM evolution and its current state, focusing on advancements in models such\nas GPT-4, GPT-3.5, Mixtral-8x7B, BERT, Falcon2, and LLaMA. Our analysis extends\nto LLM vulnerabilities, such as prompt injection, insecure output handling,\ndata poisoning, DDoS attacks, and adversarial instructions. We delve into\nmitigation strategies to protect these models, providing a comprehensive look\nat potential attack scenarios and prevention techniques. Furthermore, we\nevaluate the performance of 42 LLM models in cybersecurity knowledge and\nhardware security, highlighting their strengths and weaknesses. We thoroughly\nevaluate cybersecurity datasets for LLM training and testing, covering the\nlifecycle from data creation to usage and identifying gaps for future research.\nIn addition, we review new strategies for leveraging LLMs, including techniques\nlike Half-Quadratic Quantization (HQQ), Reinforcement Learning with Human\nFeedback (RLHF), Direct Preference Optimization (DPO), Quantized Low-Rank\nAdapters (QLoRA), and Retrieval-Augmented Generation (RAG). These insights aim\nto enhance real-time cybersecurity defenses and improve the sophistication of\nLLM applications in threat detection and response. Our paper provides a\nfoundational understanding and strategic direction for integrating LLMs into\nfuture cybersecurity frameworks, emphasizing innovation and robust model\ndeployment to safeguard against evolving cyber threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides a comprehensive review of the future of cybersecurity\nthrough Generative AI and Large Language Models (LLMs). We explore LLM\napplications across various domains, including hardware design security,\nintrusion detection, software engineering, design verification, cyber threat\nintelligence, malware detection, and phishing detection. We present an overview\nof LLM evolution and its current state, focusing on advancements in models such\nas GPT-4, GPT-3.5, Mixtral-8x7B, BERT, Falcon2, and LLaMA. Our analysis extends\nto LLM vulnerabilities, such as prompt injection, insecure output handling,\ndata poisoning, DDoS attacks, and adversarial instructions. We delve into\nmitigation strategies to protect these models, providing a comprehensive look\nat potential attack scenarios and prevention techniques. Furthermore, we\nevaluate the performance of 42 LLM models in cybersecurity knowledge and\nhardware security, highlighting their strengths and weaknesses. We thoroughly\nevaluate cybersecurity datasets for LLM training and testing, covering the\nlifecycle from data creation to usage and identifying gaps for future research.\nIn addition, we review new strategies for leveraging LLMs, including techniques\nlike Half-Quadratic Quantization (HQQ), Reinforcement Learning with Human\nFeedback (RLHF), Direct Preference Optimization (DPO), Quantized Low-Rank\nAdapters (QLoRA), and Retrieval-Augmented Generation (RAG). These insights aim\nto enhance real-time cybersecurity defenses and improve the sophistication of\nLLM applications in threat detection and response. Our paper provides a\nfoundational understanding and strategic direction for integrating LLMs into\nfuture cybersecurity frameworks, emphasizing innovation and robust model\ndeployment to safeguard against evolving cyber threats."
                },
                "authors": [
                    {
                        "name": "Mohamed Amine Ferrag"
                    },
                    {
                        "name": "Fatima Alwahedi"
                    },
                    {
                        "name": "Ammar Battah"
                    },
                    {
                        "name": "Bilel Cherif"
                    },
                    {
                        "name": "Abdechakour Mechri"
                    },
                    {
                        "name": "Norbert Tihanyi"
                    },
                    {
                        "name": "Tamas Bisztray"
                    },
                    {
                        "name": "Merouane Debbah"
                    }
                ],
                "author_detail": {
                    "name": "Merouane Debbah"
                },
                "author": "Merouane Debbah",
                "arxiv_comment": "52 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10106v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10106v1",
                "updated": "2025-01-17T10:47:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    10,
                    47,
                    11,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T10:47:11Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    10,
                    47,
                    11,
                    4,
                    17,
                    0
                ],
                "title": "LLM Reasoner and Automated Planner: A new NPC approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Reasoner and Automated Planner: A new NPC approach"
                },
                "summary": "In domains requiring intelligent agents to emulate plausible human-like\nbehaviour, such as formative simulations, traditional techniques like behaviour\ntrees encounter significant challenges. Large Language Models (LLMs), despite\nnot always yielding optimal solutions, usually offer plausible and human-like\nresponses to a given problem. In this paper, we exploit this capability and\npropose a novel architecture that integrates an LLM for decision-making with a\nclassical automated planner that can generate sound plans for that decision.\nThe combination aims to equip an agent with the ability to make decisions in\nvarious situations, even if they were not anticipated during the design phase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In domains requiring intelligent agents to emulate plausible human-like\nbehaviour, such as formative simulations, traditional techniques like behaviour\ntrees encounter significant challenges. Large Language Models (LLMs), despite\nnot always yielding optimal solutions, usually offer plausible and human-like\nresponses to a given problem. In this paper, we exploit this capability and\npropose a novel architecture that integrates an LLM for decision-making with a\nclassical automated planner that can generate sound plans for that decision.\nThe combination aims to equip an agent with the ability to make decisions in\nvarious situations, even if they were not anticipated during the design phase."
                },
                "authors": [
                    {
                        "name": "Israel Puerta-Merino"
                    },
                    {
                        "name": "Jordi Sabater-Mir"
                    }
                ],
                "author_detail": {
                    "name": "Jordi Sabater-Mir"
                },
                "author": "Jordi Sabater-Mir",
                "arxiv_doi": "10.3233/FAIA240443",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3233/FAIA240443",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.10106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10106v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 7 figures, extended version of the homonymous paper\n  submitted to the Catalan Conference on Artificial Intelligent (CCIA) 2025",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10100v1",
                "updated": "2025-01-17T10:39:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    10,
                    39,
                    9,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T10:39:09Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    10,
                    39,
                    9,
                    4,
                    17,
                    0
                ],
                "title": "Robotic World Model: A Neural Network Simulator for Robust Policy\n  Optimization in Robotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic World Model: A Neural Network Simulator for Robust Policy\n  Optimization in Robotics"
                },
                "summary": "Learning robust and generalizable world models is crucial for enabling\nefficient and scalable robotic control in real-world environments. In this\nwork, we introduce a novel framework for learning world models that accurately\ncapture complex, partially observable, and stochastic dynamics. The proposed\nmethod employs a dual-autoregressive mechanism and self-supervised training to\nachieve reliable long-horizon predictions without relying on domain-specific\ninductive biases, ensuring adaptability across diverse robotic tasks. We\nfurther propose a policy optimization framework that leverages world models for\nefficient training in imagined environments and seamless deployment in\nreal-world systems. Through extensive experiments, our approach consistently\noutperforms state-of-the-art methods, demonstrating superior autoregressive\nprediction accuracy, robustness to noise, and generalization across\nmanipulation and locomotion tasks. Notably, policies trained with our method\nare successfully deployed on ANYmal D hardware in a zero-shot transfer,\nachieving robust performance with minimal sim-to-real performance loss. This\nwork advances model-based reinforcement learning by addressing the challenges\nof long-horizon prediction, error accumulation, and sim-to-real transfer. By\nproviding a scalable and robust framework, the introduced methods pave the way\nfor adaptive and efficient robotic systems in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning robust and generalizable world models is crucial for enabling\nefficient and scalable robotic control in real-world environments. In this\nwork, we introduce a novel framework for learning world models that accurately\ncapture complex, partially observable, and stochastic dynamics. The proposed\nmethod employs a dual-autoregressive mechanism and self-supervised training to\nachieve reliable long-horizon predictions without relying on domain-specific\ninductive biases, ensuring adaptability across diverse robotic tasks. We\nfurther propose a policy optimization framework that leverages world models for\nefficient training in imagined environments and seamless deployment in\nreal-world systems. Through extensive experiments, our approach consistently\noutperforms state-of-the-art methods, demonstrating superior autoregressive\nprediction accuracy, robustness to noise, and generalization across\nmanipulation and locomotion tasks. Notably, policies trained with our method\nare successfully deployed on ANYmal D hardware in a zero-shot transfer,\nachieving robust performance with minimal sim-to-real performance loss. This\nwork advances model-based reinforcement learning by addressing the challenges\nof long-horizon prediction, error accumulation, and sim-to-real transfer. By\nproviding a scalable and robust framework, the introduced methods pave the way\nfor adaptive and efficient robotic systems in real-world applications."
                },
                "authors": [
                    {
                        "name": "Chenhao Li"
                    },
                    {
                        "name": "Andreas Krause"
                    },
                    {
                        "name": "Marco Hutter"
                    }
                ],
                "author_detail": {
                    "name": "Marco Hutter"
                },
                "author": "Marco Hutter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07548v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07548v2",
                "updated": "2025-01-17T09:42:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    42,
                    59,
                    4,
                    17,
                    0
                ],
                "published": "2024-12-10T14:39:51Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    14,
                    39,
                    51,
                    1,
                    345,
                    0
                ],
                "title": "Automatic Database Configuration Debugging using Retrieval-Augmented\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Database Configuration Debugging using Retrieval-Augmented\n  Language Models"
                },
                "summary": "Database management system (DBMS) configuration debugging, e.g., diagnosing\npoorly configured DBMS knobs and generating troubleshooting recommendations, is\ncrucial in optimizing DBMS performance. However, the configuration debugging\nprocess is tedious and, sometimes challenging, even for seasoned database\nadministrators (DBAs) with sufficient experience in DBMS configurations and\ngood understandings of the DBMS internals (e.g., MySQL or Oracle). To address\nthis difficulty, we propose Andromeda, a framework that utilizes large language\nmodels (LLMs) to enable automatic DBMS configuration debugging. Andromeda\nserves as a natural surrogate of DBAs to answer a wide range of natural\nlanguage (NL) questions on DBMS configuration issues, and to generate\ndiagnostic suggestions to fix these issues. Nevertheless, directly prompting\nLLMs with these professional questions may result in overly generic and often\nunsatisfying answers. To this end, we propose a retrieval-augmented generation\n(RAG) strategy that effectively provides matched domain-specific contexts for\nthe question from multiple sources. They come from related historical\nquestions, troubleshooting manuals and DBMS telemetries, which significantly\nimprove the performance of configuration debugging. To support the RAG\nstrategy, we develop a document retrieval mechanism addressing heterogeneous\ndocuments and design an effective method for telemetry analysis. Extensive\nexperiments on real-world DBMS configuration debugging datasets show that\nAndromeda significantly outperforms existing solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Database management system (DBMS) configuration debugging, e.g., diagnosing\npoorly configured DBMS knobs and generating troubleshooting recommendations, is\ncrucial in optimizing DBMS performance. However, the configuration debugging\nprocess is tedious and, sometimes challenging, even for seasoned database\nadministrators (DBAs) with sufficient experience in DBMS configurations and\ngood understandings of the DBMS internals (e.g., MySQL or Oracle). To address\nthis difficulty, we propose Andromeda, a framework that utilizes large language\nmodels (LLMs) to enable automatic DBMS configuration debugging. Andromeda\nserves as a natural surrogate of DBAs to answer a wide range of natural\nlanguage (NL) questions on DBMS configuration issues, and to generate\ndiagnostic suggestions to fix these issues. Nevertheless, directly prompting\nLLMs with these professional questions may result in overly generic and often\nunsatisfying answers. To this end, we propose a retrieval-augmented generation\n(RAG) strategy that effectively provides matched domain-specific contexts for\nthe question from multiple sources. They come from related historical\nquestions, troubleshooting manuals and DBMS telemetries, which significantly\nimprove the performance of configuration debugging. To support the RAG\nstrategy, we develop a document retrieval mechanism addressing heterogeneous\ndocuments and design an effective method for telemetry analysis. Extensive\nexperiments on real-world DBMS configuration debugging datasets show that\nAndromeda significantly outperforms existing solutions."
                },
                "authors": [
                    {
                        "name": "Sibei Chen"
                    },
                    {
                        "name": "Ju Fan"
                    },
                    {
                        "name": "Bin Wu"
                    },
                    {
                        "name": "Nan Tang"
                    },
                    {
                        "name": "Chao Deng"
                    },
                    {
                        "name": "Pengyi Wang"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Jian Tan"
                    },
                    {
                        "name": "Feifei Li"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Xiaoyong Du"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyong Du"
                },
                "author": "Xiaoyong Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07548v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07548v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10069v1",
                "updated": "2025-01-17T09:42:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    42,
                    48,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T09:42:48Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    42,
                    48,
                    4,
                    17,
                    0
                ],
                "title": "A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling,\n  Search Algorithms, and Relevant Frameworks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling,\n  Search Algorithms, and Relevant Frameworks"
                },
                "summary": "LLM test-time compute (or LLM inference) via search has emerged as a\npromising research area with rapid developments. However, current frameworks\noften adopt distinct perspectives on three key aspects (task definition, LLM\nprofiling, and search procedures), making direct comparisons challenging.\nMoreover, the search algorithms employed often diverge from standard\nimplementations, and their specific characteristics are not thoroughly\nspecified. In this survey, we provide a comprehensive technical review that\nunifies task definitions and provides modular definitions of LLM profiling and\nsearch procedures. The definitions enable precise comparisons of various LLM\ninference frameworks while highlighting their departures from conventional\nsearch algorithms. We also discuss the applicability, performance, and\nefficiency of these methods. For further details and ongoing updates, please\nrefer to our GitHub repository:\nhttps://github.com/xinzhel/LLM-Agent-Survey/blob/main/search.md",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM test-time compute (or LLM inference) via search has emerged as a\npromising research area with rapid developments. However, current frameworks\noften adopt distinct perspectives on three key aspects (task definition, LLM\nprofiling, and search procedures), making direct comparisons challenging.\nMoreover, the search algorithms employed often diverge from standard\nimplementations, and their specific characteristics are not thoroughly\nspecified. In this survey, we provide a comprehensive technical review that\nunifies task definitions and provides modular definitions of LLM profiling and\nsearch procedures. The definitions enable precise comparisons of various LLM\ninference frameworks while highlighting their departures from conventional\nsearch algorithms. We also discuss the applicability, performance, and\nefficiency of these methods. For further details and ongoing updates, please\nrefer to our GitHub repository:\nhttps://github.com/xinzhel/LLM-Agent-Survey/blob/main/search.md"
                },
                "authors": [
                    {
                        "name": "Xinzhe Li"
                    }
                ],
                "author_detail": {
                    "name": "Xinzhe Li"
                },
                "author": "Xinzhe Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07124v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07124v3",
                "updated": "2025-01-17T09:39:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    39,
                    17,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-13T08:26:43Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    8,
                    26,
                    43,
                    0,
                    13,
                    0
                ],
                "title": "LLM360 K2: Building a 65B 360-Open-Source Large Language Model from\n  Scratch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM360 K2: Building a 65B 360-Open-Source Large Language Model from\n  Scratch"
                },
                "summary": "We detail the training of the LLM360 K2-65B model, scaling up our 360-degree\nOPEN SOURCE approach to the largest and most powerful models under project\nLLM360. While open-source LLMs continue to advance, the answer to \"How are the\nlargest LLMs trained?\" remains unclear within the community. The implementation\ndetails for such high-capacity models are often protected due to business\nconsiderations associated with their high cost. This lack of transparency\nprevents LLM researchers from leveraging valuable insights from prior\nexperience, e.g., \"What are the best practices for addressing loss spikes?\" The\nLLM360 K2 project addresses this gap by providing full transparency and access\nto resources accumulated during the training of LLMs at the largest scale. This\nreport highlights key elements of the K2 project, including our first model, K2\nDIAMOND, a 65 billion-parameter LLM that surpasses LLaMA-65B and rivals\nLLaMA2-70B, while requiring fewer FLOPs and tokens. We detail the\nimplementation steps and present a longitudinal analysis of K2 DIAMOND's\ncapabilities throughout its training process. We also outline ongoing projects\nsuch as TXT360, setting the stage for future models in the series. By offering\npreviously unavailable resources, the K2 project also resonates with the\n360-degree OPEN SOURCE principles of transparency, reproducibility, and\naccessibility, which we believe are vital in the era of resource-intensive AI\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We detail the training of the LLM360 K2-65B model, scaling up our 360-degree\nOPEN SOURCE approach to the largest and most powerful models under project\nLLM360. While open-source LLMs continue to advance, the answer to \"How are the\nlargest LLMs trained?\" remains unclear within the community. The implementation\ndetails for such high-capacity models are often protected due to business\nconsiderations associated with their high cost. This lack of transparency\nprevents LLM researchers from leveraging valuable insights from prior\nexperience, e.g., \"What are the best practices for addressing loss spikes?\" The\nLLM360 K2 project addresses this gap by providing full transparency and access\nto resources accumulated during the training of LLMs at the largest scale. This\nreport highlights key elements of the K2 project, including our first model, K2\nDIAMOND, a 65 billion-parameter LLM that surpasses LLaMA-65B and rivals\nLLaMA2-70B, while requiring fewer FLOPs and tokens. We detail the\nimplementation steps and present a longitudinal analysis of K2 DIAMOND's\ncapabilities throughout its training process. We also outline ongoing projects\nsuch as TXT360, setting the stage for future models in the series. By offering\npreviously unavailable resources, the K2 project also resonates with the\n360-degree OPEN SOURCE principles of transparency, reproducibility, and\naccessibility, which we believe are vital in the era of resource-intensive AI\nresearch."
                },
                "authors": [
                    {
                        "name": "Zhengzhong Liu"
                    },
                    {
                        "name": "Bowen Tan"
                    },
                    {
                        "name": "Hongyi Wang"
                    },
                    {
                        "name": "Willie Neiswanger"
                    },
                    {
                        "name": "Tianhua Tao"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Fajri Koto"
                    },
                    {
                        "name": "Yuqi Wang"
                    },
                    {
                        "name": "Suqi Sun"
                    },
                    {
                        "name": "Omkar Pangarkar"
                    },
                    {
                        "name": "Richard Fan"
                    },
                    {
                        "name": "Yi Gu"
                    },
                    {
                        "name": "Victor Miller"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Liping Tang"
                    },
                    {
                        "name": "Nikhil Ranjan"
                    },
                    {
                        "name": "Yonghao Zhuang"
                    },
                    {
                        "name": "Guowei He"
                    },
                    {
                        "name": "Renxi Wang"
                    },
                    {
                        "name": "Mingkai Deng"
                    },
                    {
                        "name": "Robin Algayres"
                    },
                    {
                        "name": "Yuanzhi Li"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Eric Xing"
                    }
                ],
                "author_detail": {
                    "name": "Eric Xing"
                },
                "author": "Eric Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07124v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07124v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03594v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03594v2",
                "updated": "2025-01-17T09:37:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    37,
                    36,
                    4,
                    17,
                    0
                ],
                "published": "2024-11-29T05:57:37Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    57,
                    37,
                    4,
                    334,
                    0
                ],
                "title": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching"
                },
                "summary": "Large language models (LLMs) increasingly play an important role in a wide\nrange of information processing and management tasks. Many of these tasks are\nperformed in large batches or even offline, and the performance indictor for\nwhich is throughput. These tasks usually show the characteristic of prefix\nsharing, where different prompt input can partially show the common prefix.\nHowever, the existing LLM inference engines tend to optimize the streaming\nrequests and show limitations of supporting the large batched tasks with the\nprefix sharing characteristic. The existing solutions use the LRU-based cache\nto reuse the KV context of common prefix between requests. The KV context that\nare about to be reused may prematurely evicted with the implicit cache\nmanagement. Besides, the streaming oriented systems do not leverage the\nrequest-batch information and can not mix the decoding tokens with the prefill\nchunks to the best for the batched scenarios, and thus fails to saturate the\nGPU. We propose BatchLLM to address the above problems. BatchLLM explicitly\nidentifies the common prefixes globally. The requests sharing the same prefix\nwill be scheduled together to reuse the KV context the best. BatchLLM reorders\nthe requests and schedules the requests with larger ratio of decoding first to\nbetter mix the decoding tokens with the latter prefill chunks, and applies\nmemory-centric token batching to enlarge the token-batch sizes, which helps to\nincrease the GPU utilization. Finally, BatchLLM optimizes the prefix-shared\nAttention kernel with horizontal fusion to reduce tail effect and kernel launch\noverhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang\nby 1.3$\\times$ to 10.8$\\times$ on a set of microbenchmarks and a typical\nindustry workload under different hardware environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly play an important role in a wide\nrange of information processing and management tasks. Many of these tasks are\nperformed in large batches or even offline, and the performance indictor for\nwhich is throughput. These tasks usually show the characteristic of prefix\nsharing, where different prompt input can partially show the common prefix.\nHowever, the existing LLM inference engines tend to optimize the streaming\nrequests and show limitations of supporting the large batched tasks with the\nprefix sharing characteristic. The existing solutions use the LRU-based cache\nto reuse the KV context of common prefix between requests. The KV context that\nare about to be reused may prematurely evicted with the implicit cache\nmanagement. Besides, the streaming oriented systems do not leverage the\nrequest-batch information and can not mix the decoding tokens with the prefill\nchunks to the best for the batched scenarios, and thus fails to saturate the\nGPU. We propose BatchLLM to address the above problems. BatchLLM explicitly\nidentifies the common prefixes globally. The requests sharing the same prefix\nwill be scheduled together to reuse the KV context the best. BatchLLM reorders\nthe requests and schedules the requests with larger ratio of decoding first to\nbetter mix the decoding tokens with the latter prefill chunks, and applies\nmemory-centric token batching to enlarge the token-batch sizes, which helps to\nincrease the GPU utilization. Finally, BatchLLM optimizes the prefix-shared\nAttention kernel with horizontal fusion to reduce tail effect and kernel launch\noverhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang\nby 1.3$\\times$ to 10.8$\\times$ on a set of microbenchmarks and a typical\nindustry workload under different hardware environments."
                },
                "authors": [
                    {
                        "name": "Zhen Zheng"
                    },
                    {
                        "name": "Xin Ji"
                    },
                    {
                        "name": "Taosong Fang"
                    },
                    {
                        "name": "Fanghao Zhou"
                    },
                    {
                        "name": "Chuanjie Liu"
                    },
                    {
                        "name": "Gang Peng"
                    }
                ],
                "author_detail": {
                    "name": "Gang Peng"
                },
                "author": "Gang Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03594v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.20262v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.20262v3",
                "updated": "2025-01-17T09:32:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    32,
                    54,
                    4,
                    17,
                    0
                ],
                "published": "2024-03-29T16:13:31Z",
                "published_parsed": [
                    2024,
                    3,
                    29,
                    16,
                    13,
                    31,
                    4,
                    89,
                    0
                ],
                "title": "ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language\n  Models"
                },
                "summary": "Research on Large Language Models (LLMs) has recently witnessed an increasing\ninterest in extending the models' context size to better capture dependencies\nwithin long documents. While benchmarks have been proposed to assess long-range\nabilities, existing efforts primarily considered generic tasks that are not\nnecessarily aligned with real-world applications. In contrast, we propose a new\nbenchmark for long-context LLMs focused on a practical meeting assistant\nscenario in which the long contexts consist of transcripts obtained by\nautomatic speech recognition, presenting unique challenges for LLMs due to the\ninherent noisiness and oral nature of such data. Our benchmark, ELITR-Bench,\naugments the existing ELITR corpus by adding 271 manually crafted questions\nwith their ground-truth answers, as well as noisy versions of meeting\ntranscripts altered to target different Word Error Rate levels. Our experiments\nwith 12 long-context LLMs on ELITR-Bench confirm the progress made across\nsuccessive generations of both proprietary and open models, and point out their\ndiscrepancies in terms of robustness to transcript noise. We also provide a\nthorough analysis of our GPT-4-based evaluation, including insights from a\ncrowdsourcing study. Our findings indicate that while GPT-4's scores align with\nhuman judges, its ability to distinguish beyond three score levels may be\nlimited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on Large Language Models (LLMs) has recently witnessed an increasing\ninterest in extending the models' context size to better capture dependencies\nwithin long documents. While benchmarks have been proposed to assess long-range\nabilities, existing efforts primarily considered generic tasks that are not\nnecessarily aligned with real-world applications. In contrast, we propose a new\nbenchmark for long-context LLMs focused on a practical meeting assistant\nscenario in which the long contexts consist of transcripts obtained by\nautomatic speech recognition, presenting unique challenges for LLMs due to the\ninherent noisiness and oral nature of such data. Our benchmark, ELITR-Bench,\naugments the existing ELITR corpus by adding 271 manually crafted questions\nwith their ground-truth answers, as well as noisy versions of meeting\ntranscripts altered to target different Word Error Rate levels. Our experiments\nwith 12 long-context LLMs on ELITR-Bench confirm the progress made across\nsuccessive generations of both proprietary and open models, and point out their\ndiscrepancies in terms of robustness to transcript noise. We also provide a\nthorough analysis of our GPT-4-based evaluation, including insights from a\ncrowdsourcing study. Our findings indicate that while GPT-4's scores align with\nhuman judges, its ability to distinguish beyond three score levels may be\nlimited."
                },
                "authors": [
                    {
                        "name": "Thibaut Thonet"
                    },
                    {
                        "name": "Jos Rozen"
                    },
                    {
                        "name": "Laurent Besacier"
                    }
                ],
                "author_detail": {
                    "name": "Laurent Besacier"
                },
                "author": "Laurent Besacier",
                "arxiv_comment": "Published in COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.20262v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.20262v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.17296v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.17296v8",
                "updated": "2025-01-17T09:28:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    28,
                    45,
                    4,
                    17,
                    0
                ],
                "published": "2023-12-28T16:25:52Z",
                "published_parsed": [
                    2023,
                    12,
                    28,
                    16,
                    25,
                    52,
                    3,
                    362,
                    0
                ],
                "title": "Structured Packing in LLM Training Improves Long Context Utilization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured Packing in LLM Training Improves Long Context Utilization"
                },
                "summary": "Recent advancements in long-context large language models have attracted\nsignificant attention, yet their practical applications often suffer from\nsuboptimal context utilization. This study investigates structuring training\ndata to enhance semantic interdependence, demonstrating that this approach\neffectively improves context utilization. To this end, we introduce the\nStructured Packing for Long Context (SPLiCe) method, which utilizes retrieval\nto collate mutually relevant documents into long and coherent training\nexamples. We validate SPLiCe empirically across models of varying sizes -- 3B,\n7B, and 13B -- achieving improved performance in long-context tasks, such as\nQasper and HotpotQA. Remarkably, even brief fine-tuning with SPLiCe is\nsufficient to realize these benefits. Additionally, SPLiCe effectively\nmitigates the lost-in-middle phenomenon often observed in large models. Our\ncomprehensive analysis of SPLiCe explores its design choices and reveals\nintriguing transfer effects; for instance, training on programming code\nenhances performance on natural language tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in long-context large language models have attracted\nsignificant attention, yet their practical applications often suffer from\nsuboptimal context utilization. This study investigates structuring training\ndata to enhance semantic interdependence, demonstrating that this approach\neffectively improves context utilization. To this end, we introduce the\nStructured Packing for Long Context (SPLiCe) method, which utilizes retrieval\nto collate mutually relevant documents into long and coherent training\nexamples. We validate SPLiCe empirically across models of varying sizes -- 3B,\n7B, and 13B -- achieving improved performance in long-context tasks, such as\nQasper and HotpotQA. Remarkably, even brief fine-tuning with SPLiCe is\nsufficient to realize these benefits. Additionally, SPLiCe effectively\nmitigates the lost-in-middle phenomenon often observed in large models. Our\ncomprehensive analysis of SPLiCe explores its design choices and reveals\nintriguing transfer effects; for instance, training on programming code\nenhances performance on natural language tasks."
                },
                "authors": [
                    {
                        "name": "Konrad Staniszewski"
                    },
                    {
                        "name": "Szymon Tworkowski"
                    },
                    {
                        "name": "Sebastian Jaszczur"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Henryk Michalewski"
                    },
                    {
                        "name": "ukasz Kuciski"
                    },
                    {
                        "name": "Piotr Mio"
                    }
                ],
                "author_detail": {
                    "name": "Piotr Mio"
                },
                "author": "Piotr Mio",
                "arxiv_comment": "AAAI'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.17296v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.17296v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10054v1",
                "updated": "2025-01-17T09:20:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    20,
                    56,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T09:20:56Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    20,
                    56,
                    4,
                    17,
                    0
                ],
                "title": "Accelerating Large Language Models through Partially Linear Feed-Forward\n  Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Large Language Models through Partially Linear Feed-Forward\n  Network"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable capabilities but face\ndeployment challenges due to their massive parameter counts. While existing\ncompression techniques like pruning can reduce model size, it leads to\nsignificant accuracy degradation under high compression ratios. We present a\nnovel perspective inspired by constant folding in compiler optimization. Our\napproach enables parameter reduction by treating activation functions in LLMs\nas linear functions.\n  However, recent LLMs use complex non-linear activations like GELU that\nprevent direct application of this technique. We propose TARDIS, which enables\noptimization of LLMs with non-linear activations by partially approximating\nthem with linear functions in frequently occurring input ranges. For outlier\ninputs, TARDIS employs an online predictor to dynamically fall back to original\ncomputations.\n  Our experiments demonstrate that TARDIS achieves 80% parameter reduction in\nfeed-forward networks, while significantly outperforming state-of-the-art\npruning methods Wanda and RIA with up to 65% higher accuracy. In practical\ndeployments for a 7B model, TARDIS achieves 1.6x end-to-end inference speedup\nwhen integrated with the vLLM serving system, and 1.4x speedup with the widely\nadopted HuggingFace implementation, while incurring only a 10.9% accuracy\ntrade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable capabilities but face\ndeployment challenges due to their massive parameter counts. While existing\ncompression techniques like pruning can reduce model size, it leads to\nsignificant accuracy degradation under high compression ratios. We present a\nnovel perspective inspired by constant folding in compiler optimization. Our\napproach enables parameter reduction by treating activation functions in LLMs\nas linear functions.\n  However, recent LLMs use complex non-linear activations like GELU that\nprevent direct application of this technique. We propose TARDIS, which enables\noptimization of LLMs with non-linear activations by partially approximating\nthem with linear functions in frequently occurring input ranges. For outlier\ninputs, TARDIS employs an online predictor to dynamically fall back to original\ncomputations.\n  Our experiments demonstrate that TARDIS achieves 80% parameter reduction in\nfeed-forward networks, while significantly outperforming state-of-the-art\npruning methods Wanda and RIA with up to 65% higher accuracy. In practical\ndeployments for a 7B model, TARDIS achieves 1.6x end-to-end inference speedup\nwhen integrated with the vLLM serving system, and 1.4x speedup with the widely\nadopted HuggingFace implementation, while incurring only a 10.9% accuracy\ntrade-off."
                },
                "authors": [
                    {
                        "name": "Gansen Hu"
                    },
                    {
                        "name": "Zhaoguo Wang"
                    },
                    {
                        "name": "Jinglin Wei"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4; I.2; D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17482v2",
                "updated": "2025-01-17T09:17:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    17,
                    30,
                    4,
                    17,
                    0
                ],
                "published": "2024-07-02T08:07:27Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    8,
                    7,
                    27,
                    1,
                    184,
                    0
                ],
                "title": "Reinforcement Learning from Human Feedback: Whose Culture, Whose Values,\n  Whose Perspectives?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback: Whose Culture, Whose Values,\n  Whose Perspectives?"
                },
                "summary": "We argue for the epistemic and ethical advantages of pluralism in\nReinforcement Learning from Human Feedback (RLHF) in the context of Large\nLanguage Models (LLM). Drawing on social epistemology and pluralist philosophy\nof science, we suggest ways in which RHLF can be made more responsive to human\nneeds and how we can address challenges along the way. The paper concludes with\nan agenda for change, i.e. concrete, actionable steps to improve LLM\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We argue for the epistemic and ethical advantages of pluralism in\nReinforcement Learning from Human Feedback (RLHF) in the context of Large\nLanguage Models (LLM). Drawing on social epistemology and pluralist philosophy\nof science, we suggest ways in which RHLF can be made more responsive to human\nneeds and how we can address challenges along the way. The paper concludes with\nan agenda for change, i.e. concrete, actionable steps to improve LLM\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Kristian Gonzlez Barman"
                    },
                    {
                        "name": "Simon Lohse"
                    },
                    {
                        "name": "Henk de Regt"
                    }
                ],
                "author_detail": {
                    "name": "Henk de Regt"
                },
                "author": "Henk de Regt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10053v1",
                "updated": "2025-01-17T09:16:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    16,
                    13,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T09:16:13Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    16,
                    13,
                    4,
                    17,
                    0
                ],
                "title": "AirRAG: Activating Intrinsic Reasoning for Retrieval Augmented\n  Generation via Tree-based Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirRAG: Activating Intrinsic Reasoning for Retrieval Augmented\n  Generation via Tree-based Search"
                },
                "summary": "Leveraging the autonomous decision-making capabilities of large language\nmodels (LLMs) demonstrates superior performance in reasoning tasks. Despite the\nsuccesses of iterative or recursive retrieval-augmented generation (RAG), they\noften are trapped in a single solution space when confronted with complex\ntasks. In this paper, we propose a novel thinking pattern in RAG which\nintegrates system analysis with efficient reasoning actions, significantly\nactivating intrinsic reasoning capabilities and expanding the solution space of\nspecific tasks via Monte Carlo Tree Search (MCTS), dubbed AirRAG. Specifically,\nour approach designs five fundamental reasoning actions that are expanded to a\nwide tree-based reasoning spaces using MCTS. The extension also uses\nself-consistency verification to explore potential reasoning paths and\nimplement inference scaling. In addition, computationally optimal strategies\nare used to apply more inference computation to key actions to achieve further\nperformance improvements. Experimental results demonstrate the effectiveness of\nAirRAG through considerable performance gains over complex QA datasets.\nFurthermore, AirRAG is flexible and lightweight, making it easy to integrate\nwith other advanced technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging the autonomous decision-making capabilities of large language\nmodels (LLMs) demonstrates superior performance in reasoning tasks. Despite the\nsuccesses of iterative or recursive retrieval-augmented generation (RAG), they\noften are trapped in a single solution space when confronted with complex\ntasks. In this paper, we propose a novel thinking pattern in RAG which\nintegrates system analysis with efficient reasoning actions, significantly\nactivating intrinsic reasoning capabilities and expanding the solution space of\nspecific tasks via Monte Carlo Tree Search (MCTS), dubbed AirRAG. Specifically,\nour approach designs five fundamental reasoning actions that are expanded to a\nwide tree-based reasoning spaces using MCTS. The extension also uses\nself-consistency verification to explore potential reasoning paths and\nimplement inference scaling. In addition, computationally optimal strategies\nare used to apply more inference computation to key actions to achieve further\nperformance improvements. Experimental results demonstrate the effectiveness of\nAirRAG through considerable performance gains over complex QA datasets.\nFurthermore, AirRAG is flexible and lightweight, making it easy to integrate\nwith other advanced technologies."
                },
                "authors": [
                    {
                        "name": "Wenfeng Feng"
                    },
                    {
                        "name": "Chuzhan Hao"
                    },
                    {
                        "name": "Yuewei Zhang"
                    },
                    {
                        "name": "Jingyi Song"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "arxiv_comment": "17 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10040v1",
                "updated": "2025-01-17T08:56:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    8,
                    56,
                    17,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T08:56:17Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    8,
                    56,
                    17,
                    4,
                    17,
                    0
                ],
                "title": "LWGANet: A Lightweight Group Attention Backbone for Remote Sensing\n  Visual Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LWGANet: A Lightweight Group Attention Backbone for Remote Sensing\n  Visual Tasks"
                },
                "summary": "Remote sensing (RS) visual tasks have gained significant academic and\npractical importance. However, they encounter numerous challenges that hinder\neffective feature extraction, including the detection and recognition of\nmultiple objects exhibiting substantial variations in scale within a single\nimage. While prior dual-branch or multi-branch architectural strategies have\nbeen effective in managing these object variances, they have concurrently\nresulted in considerable increases in computational demands and parameter\ncounts. Consequently, these architectures are rendered less viable for\ndeployment on resource-constrained devices. Contemporary lightweight backbone\nnetworks, designed primarily for natural images, frequently encounter\ndifficulties in effectively extracting features from multi-scale objects, which\ncompromises their efficacy in RS visual tasks. This article introduces LWGANet,\na specialized lightweight backbone network tailored for RS visual tasks,\nincorporating a novel lightweight group attention (LWGA) module designed to\naddress these specific challenges. LWGA module, tailored for RS imagery,\nadeptly harnesses redundant features to extract a wide range of spatial\ninformation, from local to global scales, without introducing additional\ncomplexity or computational overhead. This facilitates precise feature\nextraction across multiple scales within an efficient framework.LWGANet was\nrigorously evaluated across twelve datasets, which span four crucial RS visual\ntasks: scene classification, oriented object detection, semantic segmentation,\nand change detection. The results confirm LWGANet's widespread applicability\nand its ability to maintain an optimal balance between high performance and low\ncomplexity, achieving SOTA results across diverse datasets. LWGANet emerged as\na novel solution for resource-limited scenarios requiring robust RS image\nprocessing capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remote sensing (RS) visual tasks have gained significant academic and\npractical importance. However, they encounter numerous challenges that hinder\neffective feature extraction, including the detection and recognition of\nmultiple objects exhibiting substantial variations in scale within a single\nimage. While prior dual-branch or multi-branch architectural strategies have\nbeen effective in managing these object variances, they have concurrently\nresulted in considerable increases in computational demands and parameter\ncounts. Consequently, these architectures are rendered less viable for\ndeployment on resource-constrained devices. Contemporary lightweight backbone\nnetworks, designed primarily for natural images, frequently encounter\ndifficulties in effectively extracting features from multi-scale objects, which\ncompromises their efficacy in RS visual tasks. This article introduces LWGANet,\na specialized lightweight backbone network tailored for RS visual tasks,\nincorporating a novel lightweight group attention (LWGA) module designed to\naddress these specific challenges. LWGA module, tailored for RS imagery,\nadeptly harnesses redundant features to extract a wide range of spatial\ninformation, from local to global scales, without introducing additional\ncomplexity or computational overhead. This facilitates precise feature\nextraction across multiple scales within an efficient framework.LWGANet was\nrigorously evaluated across twelve datasets, which span four crucial RS visual\ntasks: scene classification, oriented object detection, semantic segmentation,\nand change detection. The results confirm LWGANet's widespread applicability\nand its ability to maintain an optimal balance between high performance and low\ncomplexity, achieving SOTA results across diverse datasets. LWGANet emerged as\na novel solution for resource-limited scenarios requiring robust RS image\nprocessing capabilities."
                },
                "authors": [
                    {
                        "name": "Wei Lu"
                    },
                    {
                        "name": "Si-Bao Chen"
                    },
                    {
                        "name": "Chris H. Q. Ding"
                    },
                    {
                        "name": "Jin Tang"
                    },
                    {
                        "name": "Bin Luo"
                    }
                ],
                "author_detail": {
                    "name": "Bin Luo"
                },
                "author": "Bin Luo",
                "arxiv_comment": "12 pages, 8 figures, Remote sensing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02178v2",
                "updated": "2025-01-17T08:44:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    8,
                    44,
                    57,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-04T04:02:23Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    4,
                    2,
                    23,
                    5,
                    4,
                    0
                ],
                "title": "The Application of Large Language Models in Recommendation Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Application of Large Language Models in Recommendation Systems"
                },
                "summary": "The integration of Large Language Models into recommendation frameworks\npresents key advantages for personalization and adaptability of experiences to\nthe users. Classic methods of recommendations, such as collaborative filtering\nand content-based filtering, are seriously limited in the solution of\ncold-start problems, sparsity of data, and lack of diversity in information\nconsidered. LLMs, of which GPT-4 is a good example, have emerged as powerful\ntools that enable recommendation frameworks to tap into unstructured data\nsources such as user reviews, social interactions, and text-based content. By\nanalyzing these data sources, LLMs improve the accuracy and relevance of\nrecommendations, thereby overcoming some of the limitations of traditional\napproaches. This work discusses applications of LLMs in recommendation systems,\nespecially in electronic commerce, social media platforms, streaming services,\nand educational technologies. This showcases how LLMs enrich recommendation\ndiversity, user engagement, and the system's adaptability; yet it also looks\ninto the challenges connected to their technical implementation. This can also\nbe presented as a study that shows the potential of LLMs for changing user\nexperiences and making innovation possible in industries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models into recommendation frameworks\npresents key advantages for personalization and adaptability of experiences to\nthe users. Classic methods of recommendations, such as collaborative filtering\nand content-based filtering, are seriously limited in the solution of\ncold-start problems, sparsity of data, and lack of diversity in information\nconsidered. LLMs, of which GPT-4 is a good example, have emerged as powerful\ntools that enable recommendation frameworks to tap into unstructured data\nsources such as user reviews, social interactions, and text-based content. By\nanalyzing these data sources, LLMs improve the accuracy and relevance of\nrecommendations, thereby overcoming some of the limitations of traditional\napproaches. This work discusses applications of LLMs in recommendation systems,\nespecially in electronic commerce, social media platforms, streaming services,\nand educational technologies. This showcases how LLMs enrich recommendation\ndiversity, user engagement, and the system's adaptability; yet it also looks\ninto the challenges connected to their technical implementation. This can also\nbe presented as a study that shows the potential of LLMs for changing user\nexperiences and making innovation possible in industries."
                },
                "authors": [
                    {
                        "name": "Peiyang Yu"
                    },
                    {
                        "name": "Zeqiu Xu"
                    },
                    {
                        "name": "Jiani Wang"
                    },
                    {
                        "name": "Xiaochuan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaochuan Xu"
                },
                "author": "Xiaochuan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09368v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09368v2",
                "updated": "2025-01-17T08:23:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    8,
                    23,
                    3,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-16T08:27:40Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    27,
                    40,
                    3,
                    16,
                    0
                ],
                "title": "Aligning Instruction Tuning with Pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Instruction Tuning with Pre-training"
                },
                "summary": "Instruction tuning enhances large language models (LLMs) to follow human\ninstructions across diverse tasks, relying on high-quality datasets to guide\nbehavior. However, these datasets, whether manually curated or synthetically\ngenerated, are often narrowly focused and misaligned with the broad\ndistributions captured during pre-training, limiting LLM generalization and\neffective use of pre-trained knowledge. We propose *Aligning Instruction Tuning\nwith Pre-training* (AITP), a method that bridges this gap by identifying\ncoverage shortfalls in instruction-tuning datasets and rewriting\nunderrepresented pre-training data into high-quality instruction-response\npairs. This approach enriches dataset diversity while preserving task-specific\nobjectives. Evaluations on three fully open LLMs across eight benchmarks\ndemonstrate consistent performance improvements with AITP. Ablations highlight\nthe benefits of adaptive data selection, controlled rewriting, and balanced\nintegration, emphasizing the importance of aligning instruction tuning with\npre-training distributions to unlock the full potential of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning enhances large language models (LLMs) to follow human\ninstructions across diverse tasks, relying on high-quality datasets to guide\nbehavior. However, these datasets, whether manually curated or synthetically\ngenerated, are often narrowly focused and misaligned with the broad\ndistributions captured during pre-training, limiting LLM generalization and\neffective use of pre-trained knowledge. We propose *Aligning Instruction Tuning\nwith Pre-training* (AITP), a method that bridges this gap by identifying\ncoverage shortfalls in instruction-tuning datasets and rewriting\nunderrepresented pre-training data into high-quality instruction-response\npairs. This approach enriches dataset diversity while preserving task-specific\nobjectives. Evaluations on three fully open LLMs across eight benchmarks\ndemonstrate consistent performance improvements with AITP. Ablations highlight\nthe benefits of adaptive data selection, controlled rewriting, and balanced\nintegration, emphasizing the importance of aligning instruction tuning with\npre-training distributions to unlock the full potential of LLMs."
                },
                "authors": [
                    {
                        "name": "Yiming Liang"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Lei Ma"
                    },
                    {
                        "name": "Wenhu Chen"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09368v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09368v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10016v1",
                "updated": "2025-01-17T07:51:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    7,
                    51,
                    26,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T07:51:26Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    7,
                    51,
                    26,
                    4,
                    17,
                    0
                ],
                "title": "Infrastructure Deployment in Vehicular Communication Networks Using a\n  Parallel Multiobjective Evolutionary Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infrastructure Deployment in Vehicular Communication Networks Using a\n  Parallel Multiobjective Evolutionary Algorithm"
                },
                "summary": "This article describes the application of a multiobjective evolutionary\nalgorithm for locating roadside infrastructure for vehicular communication\nnetworks over realistic urban areas. A multiobjective formulation of the\nproblem is introduced, considering quality-of-service and cost objectives. The\nexperimental analysis is performed over a real map of M\\'alaga, using real\ntraffic information and antennas, and scenarios that model different\ncombinations of traffic patterns and applications (text/audio/video) in the\ncommunications. The proposed multiobjective evolutionary algorithm computes\naccurate trade-off solutions, significantly improving over state-of-the-art\nalgorithms previously applied to the problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article describes the application of a multiobjective evolutionary\nalgorithm for locating roadside infrastructure for vehicular communication\nnetworks over realistic urban areas. A multiobjective formulation of the\nproblem is introduced, considering quality-of-service and cost objectives. The\nexperimental analysis is performed over a real map of M\\'alaga, using real\ntraffic information and antennas, and scenarios that model different\ncombinations of traffic patterns and applications (text/audio/video) in the\ncommunications. The proposed multiobjective evolutionary algorithm computes\naccurate trade-off solutions, significantly improving over state-of-the-art\nalgorithms previously applied to the problem."
                },
                "authors": [
                    {
                        "name": "Renzo Massobrio"
                    },
                    {
                        "name": "Jamal Toutouh"
                    },
                    {
                        "name": "Sergio Nesmachniw"
                    },
                    {
                        "name": "Enrique Alba"
                    }
                ],
                "author_detail": {
                    "name": "Enrique Alba"
                },
                "author": "Enrique Alba",
                "arxiv_doi": "10.1002/int.21890",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1002/int.21890",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.10016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IJIS, 32(8), 801-829 (2017)",
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09831v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09831v2",
                "updated": "2025-01-17T07:48:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    7,
                    48,
                    39,
                    4,
                    17,
                    0
                ],
                "published": "2024-08-19T09:27:45Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    27,
                    45,
                    0,
                    232,
                    0
                ],
                "title": "Ranking Generated Answers: On the Agreement of Retrieval Models with\n  Humans on Consumer Health Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ranking Generated Answers: On the Agreement of Retrieval Models with\n  Humans on Consumer Health Questions"
                },
                "summary": "Evaluating the output of generative large language models (LLMs) is\nchallenging and difficult to scale. Many evaluations of LLMs focus on tasks\nsuch as single-choice question-answering or text classification. These tasks\nare not suitable for assessing open-ended question-answering capabilities,\nwhich are critical in domains where expertise is required. One such domain is\nhealth, where misleading or incorrect answers can have a negative impact on a\nuser's well-being. Using human experts to evaluate the quality of LLM answers\nis generally considered the gold standard, but expert annotation is costly and\nslow. We present a method for evaluating LLM answers that uses ranking models\ntrained on annotated document collections as a substitute for explicit\nrelevance judgements and apply it to the CLEF 2021 eHealth dataset. In a user\nstudy, our method correlates with the preferences of a human expert (Kendall's\n$\\tau=0.64$). It is also consistent with previous findings in that the quality\nof generated answers improves with the size of the model and more sophisticated\nprompting strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the output of generative large language models (LLMs) is\nchallenging and difficult to scale. Many evaluations of LLMs focus on tasks\nsuch as single-choice question-answering or text classification. These tasks\nare not suitable for assessing open-ended question-answering capabilities,\nwhich are critical in domains where expertise is required. One such domain is\nhealth, where misleading or incorrect answers can have a negative impact on a\nuser's well-being. Using human experts to evaluate the quality of LLM answers\nis generally considered the gold standard, but expert annotation is costly and\nslow. We present a method for evaluating LLM answers that uses ranking models\ntrained on annotated document collections as a substitute for explicit\nrelevance judgements and apply it to the CLEF 2021 eHealth dataset. In a user\nstudy, our method correlates with the preferences of a human expert (Kendall's\n$\\tau=0.64$). It is also consistent with previous findings in that the quality\nof generated answers improves with the size of the model and more sophisticated\nprompting strategies."
                },
                "authors": [
                    {
                        "name": "Sebastian Heineking"
                    },
                    {
                        "name": "Jonas Probst"
                    },
                    {
                        "name": "Daniel Steinbach"
                    },
                    {
                        "name": "Martin Potthast"
                    },
                    {
                        "name": "Harrisen Scells"
                    }
                ],
                "author_detail": {
                    "name": "Harrisen Scells"
                },
                "author": "Harrisen Scells",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09831v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09831v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10011v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10011v1",
                "updated": "2025-01-17T07:48:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    7,
                    48,
                    37,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T07:48:37Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    7,
                    48,
                    37,
                    4,
                    17,
                    0
                ],
                "title": "Mitigating Hallucinations on Object Attributes using Multiview Images\n  and Negative Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Hallucinations on Object Attributes using Multiview Images\n  and Negative Instructions"
                },
                "summary": "Current popular Large Vision-Language Models (LVLMs) are suffering from\nHallucinations on Object Attributes (HoOA), leading to incorrect determination\nof fine-grained attributes in the input images. Leveraging significant\nadvancements in 3D generation from a single image, this paper proposes a novel\nmethod to mitigate HoOA in LVLMs. This method utilizes multiview images sampled\nfrom generated 3D representations as visual prompts for LVLMs, thereby\nproviding more visual information from other viewpoints. Furthermore, we\nobserve the input order of multiple multiview images significantly affects the\nperformance of LVLMs. Consequently, we have devised Multiview Image Augmented\nVLM (MIAVLM), incorporating a Multiview Attributes Perceiver (MAP) submodule\ncapable of simultaneously eliminating the influence of input image order and\naligning visual information from multiview images with Large Language Models\n(LLMs). Besides, we designed and employed negative instructions to mitigate\nLVLMs' bias towards ``Yes\" responses. Comprehensive experiments demonstrate the\neffectiveness of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current popular Large Vision-Language Models (LVLMs) are suffering from\nHallucinations on Object Attributes (HoOA), leading to incorrect determination\nof fine-grained attributes in the input images. Leveraging significant\nadvancements in 3D generation from a single image, this paper proposes a novel\nmethod to mitigate HoOA in LVLMs. This method utilizes multiview images sampled\nfrom generated 3D representations as visual prompts for LVLMs, thereby\nproviding more visual information from other viewpoints. Furthermore, we\nobserve the input order of multiple multiview images significantly affects the\nperformance of LVLMs. Consequently, we have devised Multiview Image Augmented\nVLM (MIAVLM), incorporating a Multiview Attributes Perceiver (MAP) submodule\ncapable of simultaneously eliminating the influence of input image order and\naligning visual information from multiview images with Large Language Models\n(LLMs). Besides, we designed and employed negative instructions to mitigate\nLVLMs' bias towards ``Yes\" responses. Comprehensive experiments demonstrate the\neffectiveness of our method."
                },
                "authors": [
                    {
                        "name": "Zhijie Tan"
                    },
                    {
                        "name": "Yuzhi Li"
                    },
                    {
                        "name": "Shengwei Meng"
                    },
                    {
                        "name": "Xiang Yuan"
                    },
                    {
                        "name": "Weiping Li"
                    },
                    {
                        "name": "Tong Mo"
                    },
                    {
                        "name": "Bingce Wang"
                    },
                    {
                        "name": "Xu Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chu"
                },
                "author": "Xu Chu",
                "arxiv_comment": "2025 IEEE International Conference on Acoustics, Speech, and Signal\n  Processing (ICASSP 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10011v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09997v1",
                "updated": "2025-01-17T07:30:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    7,
                    30,
                    1,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T07:30:01Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    7,
                    30,
                    1,
                    4,
                    17,
                    0
                ],
                "title": "Attention-guided Self-reflection for Zero-shot Hallucination Detection\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-guided Self-reflection for Zero-shot Hallucination Detection\n  in Large Language Models"
                },
                "summary": "Hallucination has emerged as a significant barrier to the effective\napplication of Large Language Models (LLMs). In this work, we introduce a novel\nAttention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination\ndetection in LLMs. The AGSER method utilizes attention contributions to\ncategorize the input query into attentive and non-attentive queries. Each query\nis then processed separately through the LLMs, allowing us to compute\nconsistency scores between the generated responses and the original answer. The\ndifference between the two consistency scores serves as a hallucination\nestimator. In addition to its efficacy in detecting hallucinations, AGSER\nnotably reduces computational complexity, requiring only three passes through\nthe LLM and utilizing two sets of tokens. We have conducted extensive\nexperiments with four widely-used LLMs across three different hallucination\nbenchmarks, demonstrating that our approach significantly outperforms existing\nmethods in zero-shot hallucination detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination has emerged as a significant barrier to the effective\napplication of Large Language Models (LLMs). In this work, we introduce a novel\nAttention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination\ndetection in LLMs. The AGSER method utilizes attention contributions to\ncategorize the input query into attentive and non-attentive queries. Each query\nis then processed separately through the LLMs, allowing us to compute\nconsistency scores between the generated responses and the original answer. The\ndifference between the two consistency scores serves as a hallucination\nestimator. In addition to its efficacy in detecting hallucinations, AGSER\nnotably reduces computational complexity, requiring only three passes through\nthe LLM and utilizing two sets of tokens. We have conducted extensive\nexperiments with four widely-used LLMs across three different hallucination\nbenchmarks, demonstrating that our approach significantly outperforms existing\nmethods in zero-shot hallucination detection."
                },
                "authors": [
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Xinlong Chen"
                    },
                    {
                        "name": "Yue Ding"
                    },
                    {
                        "name": "Shizhen Xu"
                    },
                    {
                        "name": "Shu Wu"
                    },
                    {
                        "name": "Liang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wang"
                },
                "author": "Liang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09993v1",
                "updated": "2025-01-17T07:23:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    7,
                    23,
                    6,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T07:23:06Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    7,
                    23,
                    6,
                    4,
                    17,
                    0
                ],
                "title": "Agent-as-Judge for Factual Summarization of Long Narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-as-Judge for Factual Summarization of Long Narratives"
                },
                "summary": "Large Language Models (LLMs) have demonstrated near-human performance in\nsummarization tasks based on traditional metrics such as ROUGE and BERTScore.\nHowever, these metrics do not adequately capture critical aspects of\nsummarization quality, such as factual accuracy, particularly for long\nnarratives (>100K tokens). Recent advances, such as LLM-as-a-Judge, address the\nlimitations of metrics based on lexical similarity but still exhibit factual\ninconsistencies, especially in understanding character relationships and\nstates. In this work, we introduce NarrativeFactScore, a novel\n\"Agent-as-a-Judge\" framework for evaluating and refining summaries. By\nleveraging a Character Knowledge Graph (CKG) extracted from input and generated\nsummaries, NarrativeFactScore assesses the factual consistency and provides\nactionable guidance for refinement, such as identifying missing or erroneous\nfacts. We demonstrate the effectiveness of NarrativeFactScore through a\ndetailed workflow illustration and extensive validation on widely adopted\nbenchmarks, achieving superior performance compared to competitive methods. Our\nresults highlight the potential of agent-driven evaluation systems to improve\nthe factual reliability of LLM-generated summaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated near-human performance in\nsummarization tasks based on traditional metrics such as ROUGE and BERTScore.\nHowever, these metrics do not adequately capture critical aspects of\nsummarization quality, such as factual accuracy, particularly for long\nnarratives (>100K tokens). Recent advances, such as LLM-as-a-Judge, address the\nlimitations of metrics based on lexical similarity but still exhibit factual\ninconsistencies, especially in understanding character relationships and\nstates. In this work, we introduce NarrativeFactScore, a novel\n\"Agent-as-a-Judge\" framework for evaluating and refining summaries. By\nleveraging a Character Knowledge Graph (CKG) extracted from input and generated\nsummaries, NarrativeFactScore assesses the factual consistency and provides\nactionable guidance for refinement, such as identifying missing or erroneous\nfacts. We demonstrate the effectiveness of NarrativeFactScore through a\ndetailed workflow illustration and extensive validation on widely adopted\nbenchmarks, achieving superior performance compared to competitive methods. Our\nresults highlight the potential of agent-driven evaluation systems to improve\nthe factual reliability of LLM-generated summaries."
                },
                "authors": [
                    {
                        "name": "Yeonseok Jeong"
                    },
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Seung-won Hwang"
                    },
                    {
                        "name": "Byung-Hak Kim"
                    }
                ],
                "author_detail": {
                    "name": "Byung-Hak Kim"
                },
                "author": "Byung-Hak Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09986v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09986v1",
                "updated": "2025-01-17T06:55:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    6,
                    55,
                    13,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T06:55:13Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    6,
                    55,
                    13,
                    4,
                    17,
                    0
                ],
                "title": "ComptoNet: An End-to-End Deep Learning Framework for Scatter Estimation\n  in Multi-Source Stationary CT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ComptoNet: An End-to-End Deep Learning Framework for Scatter Estimation\n  in Multi-Source Stationary CT"
                },
                "summary": "Multi-source stationary computed tomography (MSS-CT) offers significant\nadvantages in medical and industrial applications due to its gantry-less scan\narchitecture and/or capability of simultaneous multi-source emission. However,\nthe lack of anti-scatter grid deployment in MSS-CT results in severe forward\nand/or cross scatter contamination, presenting a critical challenge that\nnecessitates an accurate and efficient scatter correction. In this work,\nComptoNet, an innovative end-to-end deep learning framework for scatter\nestimation in MSS-CT, is proposed, which integrates Compton-scattering physics\nwith deep learning techniques to address the challenges of scatter estimation\neffectively. Central to ComptoNet is the Compton-map, a novel concept that\ncaptures the distribution of scatter signals outside the scan field of view,\nprimarily consisting of large-angle Compton scatter. In ComptoNet, a reference\nCompton-map and/or spare detector data are used to guide the physics-driven\ndeep estimation of scatter from simultaneous emissions by multiple sources.\nAdditionally, a frequency attention module is employed for enhancing the\nlow-frequency smoothness. Such a multi-source deep scatter estimation framework\ndecouples the cross and forward scatter. It reduces network complexity and\nensures a consistent low-frequency signature with different photon numbers of\nsimulations, as evidenced by mean absolute percentage errors (MAPEs) that are\nless than $1.26\\%$. Conducted by using data generated from Monte Carlo\nsimulations with various phantoms, experiments demonstrate the effectiveness of\nComptoNet, with significant improvements in scatter estimation accuracy (a MAPE\nof $0.84\\%$). After scatter correction, nearly artifact-free CT images are\nobtained, further validating the capability of our proposed ComptoNet in\nmitigating scatter-induced errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-source stationary computed tomography (MSS-CT) offers significant\nadvantages in medical and industrial applications due to its gantry-less scan\narchitecture and/or capability of simultaneous multi-source emission. However,\nthe lack of anti-scatter grid deployment in MSS-CT results in severe forward\nand/or cross scatter contamination, presenting a critical challenge that\nnecessitates an accurate and efficient scatter correction. In this work,\nComptoNet, an innovative end-to-end deep learning framework for scatter\nestimation in MSS-CT, is proposed, which integrates Compton-scattering physics\nwith deep learning techniques to address the challenges of scatter estimation\neffectively. Central to ComptoNet is the Compton-map, a novel concept that\ncaptures the distribution of scatter signals outside the scan field of view,\nprimarily consisting of large-angle Compton scatter. In ComptoNet, a reference\nCompton-map and/or spare detector data are used to guide the physics-driven\ndeep estimation of scatter from simultaneous emissions by multiple sources.\nAdditionally, a frequency attention module is employed for enhancing the\nlow-frequency smoothness. Such a multi-source deep scatter estimation framework\ndecouples the cross and forward scatter. It reduces network complexity and\nensures a consistent low-frequency signature with different photon numbers of\nsimulations, as evidenced by mean absolute percentage errors (MAPEs) that are\nless than $1.26\\%$. Conducted by using data generated from Monte Carlo\nsimulations with various phantoms, experiments demonstrate the effectiveness of\nComptoNet, with significant improvements in scatter estimation accuracy (a MAPE\nof $0.84\\%$). After scatter correction, nearly artifact-free CT images are\nobtained, further validating the capability of our proposed ComptoNet in\nmitigating scatter-induced errors."
                },
                "authors": [
                    {
                        "name": "Yingxian Xia"
                    },
                    {
                        "name": "Zhiqiang Chen"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Yuxiang Xing"
                    },
                    {
                        "name": "Hewei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Hewei Gao"
                },
                "author": "Hewei Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09986v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09986v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09967v1",
                "updated": "2025-01-17T06:16:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    6,
                    16,
                    57,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T06:16:57Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    6,
                    16,
                    57,
                    4,
                    17,
                    0
                ],
                "title": "Explainable artificial intelligence (XAI): from inherent explainability\n  to large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable artificial intelligence (XAI): from inherent explainability\n  to large language models"
                },
                "summary": "Artificial Intelligence (AI) has continued to achieve tremendous success in\nrecent times. However, the decision logic of these frameworks is often not\ntransparent, making it difficult for stakeholders to understand, interpret or\nexplain their behavior. This limitation hinders trust in machine learning\nsystems and causes a general reluctance towards their adoption in practical\napplications, particularly in mission-critical domains like healthcare and\nautonomous driving. Explainable AI (XAI) techniques facilitate the\nexplainability or interpretability of machine learning models, enabling users\nto discern the basis of the decision and possibly avert undesirable behavior.\nThis comprehensive survey details the advancements of explainable AI methods,\nfrom inherently interpretable models to modern approaches for achieving\ninterpretability of various black box models, including large language models\n(LLMs). Additionally, we review explainable AI techniques that leverage LLM and\nvision-language model (VLM) frameworks to automate or improve the\nexplainability of other machine learning models. The use of LLM and VLM as\ninterpretability methods particularly enables high-level, semantically\nmeaningful explanations of model decisions and behavior. Throughout the paper,\nwe highlight the scientific principles, strengths and weaknesses of\nstate-of-the-art methods and outline different areas of improvement. Where\nappropriate, we also present qualitative and quantitative comparison results of\nvarious methods to show how they compare. Finally, we discuss the key\nchallenges of XAI and directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) has continued to achieve tremendous success in\nrecent times. However, the decision logic of these frameworks is often not\ntransparent, making it difficult for stakeholders to understand, interpret or\nexplain their behavior. This limitation hinders trust in machine learning\nsystems and causes a general reluctance towards their adoption in practical\napplications, particularly in mission-critical domains like healthcare and\nautonomous driving. Explainable AI (XAI) techniques facilitate the\nexplainability or interpretability of machine learning models, enabling users\nto discern the basis of the decision and possibly avert undesirable behavior.\nThis comprehensive survey details the advancements of explainable AI methods,\nfrom inherently interpretable models to modern approaches for achieving\ninterpretability of various black box models, including large language models\n(LLMs). Additionally, we review explainable AI techniques that leverage LLM and\nvision-language model (VLM) frameworks to automate or improve the\nexplainability of other machine learning models. The use of LLM and VLM as\ninterpretability methods particularly enables high-level, semantically\nmeaningful explanations of model decisions and behavior. Throughout the paper,\nwe highlight the scientific principles, strengths and weaknesses of\nstate-of-the-art methods and outline different areas of improvement. Where\nappropriate, we also present qualitative and quantitative comparison results of\nvarious methods to show how they compare. Finally, we discuss the key\nchallenges of XAI and directions for future research."
                },
                "authors": [
                    {
                        "name": "Fuseini Mumuni"
                    },
                    {
                        "name": "Alhassan Mumuni"
                    }
                ],
                "author_detail": {
                    "name": "Alhassan Mumuni"
                },
                "author": "Alhassan Mumuni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.19070v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.19070v3",
                "updated": "2025-01-17T06:13:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    6,
                    13,
                    20,
                    4,
                    17,
                    0
                ],
                "published": "2023-10-29T16:49:45Z",
                "published_parsed": [
                    2023,
                    10,
                    29,
                    16,
                    49,
                    45,
                    6,
                    302,
                    0
                ],
                "title": "Myriad: Large Multimodal Model by Applying Vision Experts for Industrial\n  Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Myriad: Large Multimodal Model by Applying Vision Experts for Industrial\n  Anomaly Detection"
                },
                "summary": "Due to the training configuration, traditional industrial anomaly detection\n(IAD) methods have to train a specific model for each deployment scenario,\nwhich is insufficient to meet the requirements of modern design and\nmanufacturing. On the contrary, large multimodal models~(LMMs) have shown\neminent generalization ability on various vision tasks, and their perception\nand comprehension capabilities imply the potential of applying LMMs on IAD\ntasks. However, we observe that even though the LMMs have abundant knowledge\nabout industrial anomaly detection in the textual domain, the LMMs are unable\nto leverage the knowledge due to the modality gap between textual and visual\ndomains. To stimulate the relevant knowledge in LMMs and adapt the LMMs towards\nanomaly detection tasks, we introduce existing IAD methods as vision experts\nand present a novel large multimodal model applying vision experts for\nindustrial anomaly detection~(abbreviated to {Myriad}). Specifically, we\nutilize the anomaly map generated by the vision experts as guidance for LMMs,\nsuch that the vision model is guided to pay more attention to anomalous\nregions. Then, the visual features are modulated via an adapter to fit the\nanomaly detection tasks, which are fed into the language model together with\nthe vision expert guidance and human instructions to generate the final\noutputs. Extensive experiments are applied on MVTec-AD, VisA, and PCB Bank\nbenchmarks demonstrate that our proposed method not only performs favorably\nagainst state-of-the-art methods, but also inherits the flexibility and\ninstruction-following ability of LMMs in the field of IAD. Source code and\npre-trained models are publicly available at\n\\url{https://github.com/tzjtatata/Myriad}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the training configuration, traditional industrial anomaly detection\n(IAD) methods have to train a specific model for each deployment scenario,\nwhich is insufficient to meet the requirements of modern design and\nmanufacturing. On the contrary, large multimodal models~(LMMs) have shown\neminent generalization ability on various vision tasks, and their perception\nand comprehension capabilities imply the potential of applying LMMs on IAD\ntasks. However, we observe that even though the LMMs have abundant knowledge\nabout industrial anomaly detection in the textual domain, the LMMs are unable\nto leverage the knowledge due to the modality gap between textual and visual\ndomains. To stimulate the relevant knowledge in LMMs and adapt the LMMs towards\nanomaly detection tasks, we introduce existing IAD methods as vision experts\nand present a novel large multimodal model applying vision experts for\nindustrial anomaly detection~(abbreviated to {Myriad}). Specifically, we\nutilize the anomaly map generated by the vision experts as guidance for LMMs,\nsuch that the vision model is guided to pay more attention to anomalous\nregions. Then, the visual features are modulated via an adapter to fit the\nanomaly detection tasks, which are fed into the language model together with\nthe vision expert guidance and human instructions to generate the final\noutputs. Extensive experiments are applied on MVTec-AD, VisA, and PCB Bank\nbenchmarks demonstrate that our proposed method not only performs favorably\nagainst state-of-the-art methods, but also inherits the flexibility and\ninstruction-following ability of LMMs in the field of IAD. Source code and\npre-trained models are publicly available at\n\\url{https://github.com/tzjtatata/Myriad}."
                },
                "authors": [
                    {
                        "name": "Yuanze Li"
                    },
                    {
                        "name": "Haolin Wang"
                    },
                    {
                        "name": "Shihao Yuan"
                    },
                    {
                        "name": "Ming Liu"
                    },
                    {
                        "name": "Debin Zhao"
                    },
                    {
                        "name": "Yiwen Guo"
                    },
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Guangming Shi"
                    },
                    {
                        "name": "Wangmeng Zuo"
                    }
                ],
                "author_detail": {
                    "name": "Wangmeng Zuo"
                },
                "author": "Wangmeng Zuo",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.19070v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.19070v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05844v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05844v2",
                "updated": "2025-01-17T05:33:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    5,
                    33,
                    54,
                    4,
                    17,
                    0
                ],
                "published": "2024-11-06T15:32:28Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    15,
                    32,
                    28,
                    2,
                    311,
                    0
                ],
                "title": "LEGO-GraphRAG: Modularizing Graph-based Retrieval-Augmented Generation\n  for Design Space Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEGO-GraphRAG: Modularizing Graph-based Retrieval-Augmented Generation\n  for Design Space Exploration"
                },
                "summary": "GraphRAG integrates (knowledge) graphs with large language models (LLMs) to\nimprove reasoning accuracy and contextual relevance. Despite its promising\napplications and strong relevance to multiple research communities, such as\ndatabases and natural language processing, GraphRAG currently lacks modular\nworkflow analysis, systematic solution frameworks, and insightful empirical\nstudies. To bridge these gaps, we propose LEGO-GraphRAG, a modular framework\nthat enables: 1) fine-grained decomposition of the GraphRAG workflow, 2)\nsystematic classification of existing techniques and implemented GraphRAG\ninstances, and 3) creation of new GraphRAG instances. Our framework facilitates\ncomprehensive empirical studies of GraphRAG on large-scale real-world graphs\nand diverse query sets, revealing insights into balancing reasoning quality,\nruntime efficiency, and token or GPU cost, that are essential for building\nadvanced GraphRAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphRAG integrates (knowledge) graphs with large language models (LLMs) to\nimprove reasoning accuracy and contextual relevance. Despite its promising\napplications and strong relevance to multiple research communities, such as\ndatabases and natural language processing, GraphRAG currently lacks modular\nworkflow analysis, systematic solution frameworks, and insightful empirical\nstudies. To bridge these gaps, we propose LEGO-GraphRAG, a modular framework\nthat enables: 1) fine-grained decomposition of the GraphRAG workflow, 2)\nsystematic classification of existing techniques and implemented GraphRAG\ninstances, and 3) creation of new GraphRAG instances. Our framework facilitates\ncomprehensive empirical studies of GraphRAG on large-scale real-world graphs\nand diverse query sets, revealing insights into balancing reasoning quality,\nruntime efficiency, and token or GPU cost, that are essential for building\nadvanced GraphRAG systems."
                },
                "authors": [
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Zengyi Gao"
                    },
                    {
                        "name": "Zhiyang Li"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "Kevin Zhou"
                    },
                    {
                        "name": "Jianliang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jianliang Xu"
                },
                "author": "Jianliang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05844v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05844v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09959v1",
                "updated": "2025-01-17T05:21:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    5,
                    21,
                    49,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T05:21:49Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    5,
                    21,
                    49,
                    4,
                    17,
                    0
                ],
                "title": "A Survey on Multi-Turn Interaction Capabilities of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Multi-Turn Interaction Capabilities of Large Language Models"
                },
                "summary": "Multi-turn interaction in the dialogue system research refers to a system's\nability to maintain context across multiple dialogue turns, enabling it to\ngenerate coherent and contextually relevant responses. Recent advancements in\nlarge language models (LLMs) have significantly expanded the scope of\nmulti-turn interaction, moving beyond chatbots to enable more dynamic agentic\ninteractions with users or environments. In this paper, we provide a focused\nreview of the multi-turn capabilities of LLMs, which are critical for a wide\nrange of downstream applications, including conversational search and\nrecommendation, consultation services, and interactive tutoring. This survey\nexplores four key aspects: (1) the core model capabilities that contribute to\neffective multi-turn interaction, (2) how multi-turn interaction is evaluated\nin current practice, (3) the general algorithms used to enhance multi-turn\ninteraction, and (4) potential future directions for research in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn interaction in the dialogue system research refers to a system's\nability to maintain context across multiple dialogue turns, enabling it to\ngenerate coherent and contextually relevant responses. Recent advancements in\nlarge language models (LLMs) have significantly expanded the scope of\nmulti-turn interaction, moving beyond chatbots to enable more dynamic agentic\ninteractions with users or environments. In this paper, we provide a focused\nreview of the multi-turn capabilities of LLMs, which are critical for a wide\nrange of downstream applications, including conversational search and\nrecommendation, consultation services, and interactive tutoring. This survey\nexplores four key aspects: (1) the core model capabilities that contribute to\neffective multi-turn interaction, (2) how multi-turn interaction is evaluated\nin current practice, (3) the general algorithms used to enhance multi-turn\ninteraction, and (4) potential future directions for research in this field."
                },
                "authors": [
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Xinyi Dai"
                    },
                    {
                        "name": "Yaxiong Wu"
                    },
                    {
                        "name": "Qu Yang"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "Draft Version, 14 pages, Ongoing refinement over time",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09957v1",
                "updated": "2025-01-17T05:19:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    5,
                    19,
                    14,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T05:19:14Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    5,
                    19,
                    14,
                    4,
                    17,
                    0
                ],
                "title": "FRAG: A Flexible Modular Framework for Retrieval-Augmented Generation\n  based on Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FRAG: A Flexible Modular Framework for Retrieval-Augmented Generation\n  based on Knowledge Graphs"
                },
                "summary": "To mitigate the hallucination and knowledge deficiency in large language\nmodels (LLMs), Knowledge Graph (KG)-based Retrieval-Augmented Generation (RAG)\nhas shown promising potential by utilizing KGs as external resource to enhance\nLLMs reasoning.However, existing KG-RAG approaches struggle with a trade-off\nbetween flexibility and retrieval quality.Modular methods prioritize\nflexibility by avoiding the use of KG-fine-tuned models during retrieval,\nleading to fixed retrieval strategies and suboptimal retrieval\nquality.Conversely, coupled methods embed KG information within models to\nimprove retrieval quality, but at the expense of flexibility.In this paper, we\npropose a novel flexible modular KG-RAG framework, termed FRAG, which\nsynergizes the advantages of both approaches.FRAG estimates the hop range of\nreasoning paths based solely on the query and classify it as either simple or\ncomplex.To match the complexity of the query, tailored pipelines are applied to\nensure efficient and accurate reasoning path retrieval, thus fostering the\nfinal reasoning process.By using the query text instead of the KG to infer the\nstructural information of reasoning paths and employing adaptable retrieval\nstrategies, FRAG improves retrieval quality while maintaining\nflexibility.Moreover, FRAG does not require extra LLMs fine-tuning or calls,\nsignificantly boosting efficiency and conserving resources.Extensive\nexperiments show that FRAG achieves state-of-the-art performance with high\nefficiency and low resource consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To mitigate the hallucination and knowledge deficiency in large language\nmodels (LLMs), Knowledge Graph (KG)-based Retrieval-Augmented Generation (RAG)\nhas shown promising potential by utilizing KGs as external resource to enhance\nLLMs reasoning.However, existing KG-RAG approaches struggle with a trade-off\nbetween flexibility and retrieval quality.Modular methods prioritize\nflexibility by avoiding the use of KG-fine-tuned models during retrieval,\nleading to fixed retrieval strategies and suboptimal retrieval\nquality.Conversely, coupled methods embed KG information within models to\nimprove retrieval quality, but at the expense of flexibility.In this paper, we\npropose a novel flexible modular KG-RAG framework, termed FRAG, which\nsynergizes the advantages of both approaches.FRAG estimates the hop range of\nreasoning paths based solely on the query and classify it as either simple or\ncomplex.To match the complexity of the query, tailored pipelines are applied to\nensure efficient and accurate reasoning path retrieval, thus fostering the\nfinal reasoning process.By using the query text instead of the KG to infer the\nstructural information of reasoning paths and employing adaptable retrieval\nstrategies, FRAG improves retrieval quality while maintaining\nflexibility.Moreover, FRAG does not require extra LLMs fine-tuning or calls,\nsignificantly boosting efficiency and conserving resources.Extensive\nexperiments show that FRAG achieves state-of-the-art performance with high\nefficiency and low resource consumption."
                },
                "authors": [
                    {
                        "name": "Zengyi Gao"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Hairu Wang"
                    },
                    {
                        "name": "Ao Ke"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S Kevin Zhou"
                },
                "author": "S Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09955v1",
                "updated": "2025-01-17T05:04:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    5,
                    4,
                    23,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T05:04:23Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    5,
                    4,
                    23,
                    4,
                    17,
                    0
                ],
                "title": "Metamorphic Testing for Smart Contract Validation:A Case Study of\n  Ethereum-Based Crowdfunding Contracts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metamorphic Testing for Smart Contract Validation:A Case Study of\n  Ethereum-Based Crowdfunding Contracts"
                },
                "summary": "Blockchain smart contracts play a crucial role in automating and securing\nagreements in diverse domains such as finance, healthcare, and supply chains.\nDespite their critical applications, testing these contracts often receives\nless attention than their development, leaving significant risks due to the\nimmutability of smart contracts post-deployment. A key challenge in the testing\nof smart contracts is the oracle problem, where the exact expected outcomes are\nnot well defined, complicating systematic testing efforts.\n  Metamorphic Testing (MT) addresses the oracle problem by using Metamorphic\nRelations (MRs) to validate smart contracts. MRs define how output should\nchange relative to specific input modifications, determining whether the tests\npass or fail. In this work, we apply MT to test an Ethereum-based crowdfunding\nsmart contract, focusing on core functionalities such as state transitions and\ndonation tracking.\n  We identify a set of MRs tailored for smart contract testing and generate\ntest cases for these MRs. To assess the effectiveness of this approach, we use\nthe Vertigo mutation testing tool to create faulty versions of the smart\ncontract. The experimental results show that our MRs detected 25.65% of the\ntotal mutants generated, with the most effective MRs achieving a mutant-killing\nrate of 89%. These results highlight the utility of MT to ensure the\nreliability and quality of blockchain-based smart contracts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blockchain smart contracts play a crucial role in automating and securing\nagreements in diverse domains such as finance, healthcare, and supply chains.\nDespite their critical applications, testing these contracts often receives\nless attention than their development, leaving significant risks due to the\nimmutability of smart contracts post-deployment. A key challenge in the testing\nof smart contracts is the oracle problem, where the exact expected outcomes are\nnot well defined, complicating systematic testing efforts.\n  Metamorphic Testing (MT) addresses the oracle problem by using Metamorphic\nRelations (MRs) to validate smart contracts. MRs define how output should\nchange relative to specific input modifications, determining whether the tests\npass or fail. In this work, we apply MT to test an Ethereum-based crowdfunding\nsmart contract, focusing on core functionalities such as state transitions and\ndonation tracking.\n  We identify a set of MRs tailored for smart contract testing and generate\ntest cases for these MRs. To assess the effectiveness of this approach, we use\nthe Vertigo mutation testing tool to create faulty versions of the smart\ncontract. The experimental results show that our MRs detected 25.65% of the\ntotal mutants generated, with the most effective MRs achieving a mutant-killing\nrate of 89%. These results highlight the utility of MT to ensure the\nreliability and quality of blockchain-based smart contracts."
                },
                "authors": [
                    {
                        "name": "Irving Jared Villanueva"
                    },
                    {
                        "name": "Madhusudan Srinivasan"
                    },
                    {
                        "name": "Faqeer Ur Rehman"
                    }
                ],
                "author_detail": {
                    "name": "Faqeer Ur Rehman"
                },
                "author": "Faqeer Ur Rehman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09954v1",
                "updated": "2025-01-17T04:57:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    4,
                    57,
                    42,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T04:57:42Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    4,
                    57,
                    42,
                    4,
                    17,
                    0
                ],
                "title": "AIRCHITECT v2: Learning the Hardware Accelerator Design Space through\n  Unified Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIRCHITECT v2: Learning the Hardware Accelerator Design Space through\n  Unified Representations"
                },
                "summary": "Design space exploration (DSE) plays a crucial role in enabling custom\nhardware architectures, particularly for emerging applications like AI, where\noptimized and specialized designs are essential. With the growing complexity of\ndeep neural networks (DNNs) and the introduction of advanced foundational\nmodels (FMs), the design space for DNN accelerators is expanding at an\nexponential rate. Additionally, this space is highly non-uniform and\nnon-convex, making it increasingly difficult to navigate and optimize.\nTraditional DSE techniques rely on search-based methods, which involve\niterative sampling of the design space to find the optimal solution. However,\nthis process is both time-consuming and often fails to converge to the global\noptima for such design spaces. Recently, AIrchitect v1, the first attempt to\naddress the limitations of search-based techniques, transformed DSE into a\nconstant-time classification problem using recommendation networks. In this\nwork, we propose AIrchitect v2, a more accurate and generalizable\nlearning-based DSE technique applicable to large-scale design spaces that\novercomes the shortcomings of earlier approaches. Specifically, we devise an\nencoder-decoder transformer model that (a) encodes the complex design space\ninto a uniform intermediate representation using contrastive learning and (b)\nleverages a novel unified representation blending the advantages of\nclassification and regression to effectively explore the large DSE space\nwithout sacrificing accuracy. Experimental results evaluated on 10^5 real DNN\nworkloads demonstrate that, on average, AIrchitect v2 outperforms existing\ntechniques by 15% in identifying optimal design points. Furthermore, to\ndemonstrate the generalizability of our method, we evaluate performance on\nunseen model workloads (LLMs) and attain a 1.7x improvement in inference\nlatency on the identified hardware architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design space exploration (DSE) plays a crucial role in enabling custom\nhardware architectures, particularly for emerging applications like AI, where\noptimized and specialized designs are essential. With the growing complexity of\ndeep neural networks (DNNs) and the introduction of advanced foundational\nmodels (FMs), the design space for DNN accelerators is expanding at an\nexponential rate. Additionally, this space is highly non-uniform and\nnon-convex, making it increasingly difficult to navigate and optimize.\nTraditional DSE techniques rely on search-based methods, which involve\niterative sampling of the design space to find the optimal solution. However,\nthis process is both time-consuming and often fails to converge to the global\noptima for such design spaces. Recently, AIrchitect v1, the first attempt to\naddress the limitations of search-based techniques, transformed DSE into a\nconstant-time classification problem using recommendation networks. In this\nwork, we propose AIrchitect v2, a more accurate and generalizable\nlearning-based DSE technique applicable to large-scale design spaces that\novercomes the shortcomings of earlier approaches. Specifically, we devise an\nencoder-decoder transformer model that (a) encodes the complex design space\ninto a uniform intermediate representation using contrastive learning and (b)\nleverages a novel unified representation blending the advantages of\nclassification and regression to effectively explore the large DSE space\nwithout sacrificing accuracy. Experimental results evaluated on 10^5 real DNN\nworkloads demonstrate that, on average, AIrchitect v2 outperforms existing\ntechniques by 15% in identifying optimal design points. Furthermore, to\ndemonstrate the generalizability of our method, we evaluate performance on\nunseen model workloads (LLMs) and attain a 1.7x improvement in inference\nlatency on the identified hardware architecture."
                },
                "authors": [
                    {
                        "name": "Jamin Seo"
                    },
                    {
                        "name": "Akshat Ramachandran"
                    },
                    {
                        "name": "Yu-Chuan Chuang"
                    },
                    {
                        "name": "Anirudh Itagi"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "arxiv_comment": "Accepted to DATE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00771v2",
                "updated": "2025-01-17T04:47:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    4,
                    47,
                    11,
                    4,
                    17,
                    0
                ],
                "published": "2024-10-01T15:07:07Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    15,
                    7,
                    7,
                    1,
                    275,
                    0
                ],
                "title": "Empowering Large Language Model for Continual Video Question Answering\n  with Collaborative Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering Large Language Model for Continual Video Question Answering\n  with Collaborative Prompting"
                },
                "summary": "In recent years, the rapid increase in online video content has underscored\nthe limitations of static Video Question Answering (VideoQA) models trained on\nfixed datasets, as they struggle to adapt to new questions or tasks posed by\nnewly available content. In this paper, we explore the novel challenge of\nVideoQA within a continual learning framework, and empirically identify a\ncritical issue: fine-tuning a large language model (LLM) for a sequence of\ntasks often results in catastrophic forgetting. To address this, we propose\nCollaborative Prompting (ColPro), which integrates specific question constraint\nprompting, knowledge acquisition prompting, and visual temporal awareness\nprompting. These prompts aim to capture textual question context, visual\ncontent, and video temporal dynamics in VideoQA, a perspective underexplored in\nprior research. Experimental results on the NExT-QA and DramaQA datasets show\nthat ColPro achieves superior performance compared to existing approaches,\nachieving 55.14\\% accuracy on NExT-QA and 71.24\\% accuracy on DramaQA,\nhighlighting its practical relevance and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the rapid increase in online video content has underscored\nthe limitations of static Video Question Answering (VideoQA) models trained on\nfixed datasets, as they struggle to adapt to new questions or tasks posed by\nnewly available content. In this paper, we explore the novel challenge of\nVideoQA within a continual learning framework, and empirically identify a\ncritical issue: fine-tuning a large language model (LLM) for a sequence of\ntasks often results in catastrophic forgetting. To address this, we propose\nCollaborative Prompting (ColPro), which integrates specific question constraint\nprompting, knowledge acquisition prompting, and visual temporal awareness\nprompting. These prompts aim to capture textual question context, visual\ncontent, and video temporal dynamics in VideoQA, a perspective underexplored in\nprior research. Experimental results on the NExT-QA and DramaQA datasets show\nthat ColPro achieves superior performance compared to existing approaches,\nachieving 55.14\\% accuracy on NExT-QA and 71.24\\% accuracy on DramaQA,\nhighlighting its practical relevance and effectiveness."
                },
                "authors": [
                    {
                        "name": "Chen Cai"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Jianjun Gao"
                    },
                    {
                        "name": "Wenyang Liu"
                    },
                    {
                        "name": "Ye Lu"
                    },
                    {
                        "name": "Runzhong Zhang"
                    },
                    {
                        "name": "Kim-Hui Yap"
                    }
                ],
                "author_detail": {
                    "name": "Kim-Hui Yap"
                },
                "author": "Kim-Hui Yap",
                "arxiv_comment": "Accepted by main EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.10444v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.10444v5",
                "updated": "2025-01-17T04:45:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    4,
                    45,
                    45,
                    4,
                    17,
                    0
                ],
                "published": "2023-09-19T09:04:15Z",
                "published_parsed": [
                    2023,
                    9,
                    19,
                    9,
                    4,
                    15,
                    1,
                    262,
                    0
                ],
                "title": "Exploring Iterative Enhancement for Improving Learnersourced\n  Multiple-Choice Question Explanations with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Iterative Enhancement for Improving Learnersourced\n  Multiple-Choice Question Explanations with Large Language Models"
                },
                "summary": "Large language models exhibit superior capabilities in processing and\nunderstanding language, yet their applications in educational contexts remain\nunderexplored. Learnersourcing enhances learning by engaging students in\ncreating their own educational content. When learnersourcing multiple-choice\nquestions, creating explanations for the solution of a question is a crucial\nstep; it helps other students understand the solution and promotes a deeper\nunderstanding of related concepts. However, it is often difficult for students\nto craft effective solution explanations, due to limited subject understanding.\nTo help scaffold the task of automated explanation generation, we present and\nevaluate a framework called \"ILearner-LLM\", that iteratively enhances the\ngenerated explanations for the given questions with large language models.\nComprising an explanation generation model and an explanation evaluation model,\nthe framework generates high-quality student-aligned explanations by\niteratively feeding the quality rating score from the evaluation model back\ninto the instruction prompt of the explanation generation model. Experimental\nresults demonstrate the effectiveness of our ILearner-LLM on LLaMA2-13B and\nGPT-4 to generate higher quality explanations that are closer to those written\nby students on five PeerWise datasets. Our findings represent a promising path\nto enrich the learnersourcing experience for students and to enhance the\ncapabilities of large language models for educational applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models exhibit superior capabilities in processing and\nunderstanding language, yet their applications in educational contexts remain\nunderexplored. Learnersourcing enhances learning by engaging students in\ncreating their own educational content. When learnersourcing multiple-choice\nquestions, creating explanations for the solution of a question is a crucial\nstep; it helps other students understand the solution and promotes a deeper\nunderstanding of related concepts. However, it is often difficult for students\nto craft effective solution explanations, due to limited subject understanding.\nTo help scaffold the task of automated explanation generation, we present and\nevaluate a framework called \"ILearner-LLM\", that iteratively enhances the\ngenerated explanations for the given questions with large language models.\nComprising an explanation generation model and an explanation evaluation model,\nthe framework generates high-quality student-aligned explanations by\niteratively feeding the quality rating score from the evaluation model back\ninto the instruction prompt of the explanation generation model. Experimental\nresults demonstrate the effectiveness of our ILearner-LLM on LLaMA2-13B and\nGPT-4 to generate higher quality explanations that are closer to those written\nby students on five PeerWise datasets. Our findings represent a promising path\nto enrich the learnersourcing experience for students and to enhance the\ncapabilities of large language models for educational applications."
                },
                "authors": [
                    {
                        "name": "Qiming Bao"
                    },
                    {
                        "name": "Juho Leinonen"
                    },
                    {
                        "name": "Alex Yuxuan Peng"
                    },
                    {
                        "name": "Wanjun Zhong"
                    },
                    {
                        "name": "Gal Gendron"
                    },
                    {
                        "name": "Timothy Pistotti"
                    },
                    {
                        "name": "Alice Huang"
                    },
                    {
                        "name": "Paul Denny"
                    },
                    {
                        "name": "Michael Witbrock"
                    },
                    {
                        "name": "Jiamou Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiamou Liu"
                },
                "author": "Jiamou Liu",
                "arxiv_comment": "The short version (v4) has been accepted as a non-archival workshop\n  paper at AGI@ICLR 2024, and the full version has been accepted by the main\n  track of AAAI/EAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.10444v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.10444v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.09430v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.09430v5",
                "updated": "2025-01-17T04:39:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    4,
                    39,
                    38,
                    4,
                    17,
                    0
                ],
                "published": "2023-10-13T22:29:15Z",
                "published_parsed": [
                    2023,
                    10,
                    13,
                    22,
                    29,
                    15,
                    4,
                    286,
                    0
                ],
                "title": "Assessing and Enhancing the Robustness of Large Language Models with\n  Task Structure Variations for Logical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing and Enhancing the Robustness of Large Language Models with\n  Task Structure Variations for Logical Reasoning"
                },
                "summary": "Large language models (LLMs), such as LLaMA, Alpaca, Vicuna, GPT-3.5 and\nGPT-4, have advanced the performance of AI systems on various natural language\nprocessing tasks to human-like levels. However, their generalisation and\nrobustness when performing logical reasoning has not been sufficiently\nassessed. To comprehensively evaluate this ability, we develop three new\nlogical reasoning datasets named \"ReClor-plus\", \"LogiQA-plus\" and\n\"LogiQAv2-plus\" that extend standard logical reasoning datasets to evaluate the\nrobustness of the LLM's reasoning. For each, we create three subsets: the first\nwith randomly shuffled options, the second with the correct choices replaced by\n\"none of the other options is correct\", and the third with a combination of\nshuffling and substitution. Experiments on these datasets show that these\nsimple augmentations greatly hinder the models' performance. Despite their high\nperformance on the original publicly available datasets, we find that all\nmodels perform poorly on these newly constructed datasets. We also demonstrate\nthat introducing task variations into the training set can markedly improve the\nmodel's performance on both the original and our developed datasets. Finally,\nwe show that applying logic-driven data augmentation for fine-tuning and\nprompting can enhance generalisation in both discriminative and generative\nmodels, offering a path to improving their robustness for tasks involving\nlogical reasoning. Source code and data are made publicly available at\nhttps://github.com/Strong-AI-Lab/Logical-and-abstract-reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), such as LLaMA, Alpaca, Vicuna, GPT-3.5 and\nGPT-4, have advanced the performance of AI systems on various natural language\nprocessing tasks to human-like levels. However, their generalisation and\nrobustness when performing logical reasoning has not been sufficiently\nassessed. To comprehensively evaluate this ability, we develop three new\nlogical reasoning datasets named \"ReClor-plus\", \"LogiQA-plus\" and\n\"LogiQAv2-plus\" that extend standard logical reasoning datasets to evaluate the\nrobustness of the LLM's reasoning. For each, we create three subsets: the first\nwith randomly shuffled options, the second with the correct choices replaced by\n\"none of the other options is correct\", and the third with a combination of\nshuffling and substitution. Experiments on these datasets show that these\nsimple augmentations greatly hinder the models' performance. Despite their high\nperformance on the original publicly available datasets, we find that all\nmodels perform poorly on these newly constructed datasets. We also demonstrate\nthat introducing task variations into the training set can markedly improve the\nmodel's performance on both the original and our developed datasets. Finally,\nwe show that applying logic-driven data augmentation for fine-tuning and\nprompting can enhance generalisation in both discriminative and generative\nmodels, offering a path to improving their robustness for tasks involving\nlogical reasoning. Source code and data are made publicly available at\nhttps://github.com/Strong-AI-Lab/Logical-and-abstract-reasoning."
                },
                "authors": [
                    {
                        "name": "Qiming Bao"
                    },
                    {
                        "name": "Gael Gendron"
                    },
                    {
                        "name": "Alex Yuxuan Peng"
                    },
                    {
                        "name": "Wanjun Zhong"
                    },
                    {
                        "name": "Neset Tan"
                    },
                    {
                        "name": "Yang Chen"
                    },
                    {
                        "name": "Michael Witbrock"
                    },
                    {
                        "name": "Jiamou Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiamou Liu"
                },
                "author": "Jiamou Liu",
                "arxiv_comment": "The short version (v3) was accepted for oral presentation at the\n  first LLM@IJCAI 2023 non-archival symposium, and the full version was\n  accepted by ICONIP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.09430v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.09430v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09434v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09434v2",
                "updated": "2025-01-17T04:36:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    4,
                    36,
                    1,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-16T10:04:19Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    4,
                    19,
                    3,
                    16,
                    0
                ],
                "title": "Agile System Development Lifecycle for AI Systems: Decision Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agile System Development Lifecycle for AI Systems: Decision Architecture"
                },
                "summary": "Agile system development life cycle (SDLC) focuses on typical functional and\nnon-functional system requirements for developing traditional software systems.\nHowever, Artificial Intelligent (AI) systems are different in nature and have\ndistinct attributes such as (1) autonomy, (2) adaptiveness, (3) content\ngeneration, (4) decision-making, (5) predictability and (6) recommendation.\nAgile SDLC needs to be enhanced to support the AI system development and\nongoing post-deployment adaptation. The challenge is: how can agile SDLC be\nenhanced to support AI systems? The scope of this paper is limited to AI system\nenabled decision automation. Thus, this paper proposes the use of decision\nscience to enhance the agile SDLC to support the AI system development.\nDecision science is the study of decision-making, which seems useful to\nidentify, analyse and describe decisions and their architecture subject to\nautomation via AI systems. Specifically, this paper discusses the decision\narchitecture in detail within the overall context of agile SDLC for AI systems.\nThe application of the proposed approach is demonstrated with the help of an\nexample scenario of insurance claim processing. This initial work indicated the\nusability of a decision science to enhancing the agile SDLC for designing and\nimplementing the AI systems for decision-automation. This work provides an\ninitial foundation for further work in this new area of decision architecture\nand agile SDLC for AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agile system development life cycle (SDLC) focuses on typical functional and\nnon-functional system requirements for developing traditional software systems.\nHowever, Artificial Intelligent (AI) systems are different in nature and have\ndistinct attributes such as (1) autonomy, (2) adaptiveness, (3) content\ngeneration, (4) decision-making, (5) predictability and (6) recommendation.\nAgile SDLC needs to be enhanced to support the AI system development and\nongoing post-deployment adaptation. The challenge is: how can agile SDLC be\nenhanced to support AI systems? The scope of this paper is limited to AI system\nenabled decision automation. Thus, this paper proposes the use of decision\nscience to enhance the agile SDLC to support the AI system development.\nDecision science is the study of decision-making, which seems useful to\nidentify, analyse and describe decisions and their architecture subject to\nautomation via AI systems. Specifically, this paper discusses the decision\narchitecture in detail within the overall context of agile SDLC for AI systems.\nThe application of the proposed approach is demonstrated with the help of an\nexample scenario of insurance claim processing. This initial work indicated the\nusability of a decision science to enhancing the agile SDLC for designing and\nimplementing the AI systems for decision-automation. This work provides an\ninitial foundation for further work in this new area of decision architecture\nand agile SDLC for AI systems."
                },
                "authors": [
                    {
                        "name": "Asif Q. Gill"
                    }
                ],
                "author_detail": {
                    "name": "Asif Q. Gill"
                },
                "author": "Asif Q. Gill",
                "arxiv_comment": "11, 4",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09434v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09434v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01892v2",
                "updated": "2025-01-17T04:29:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    4,
                    29,
                    47,
                    4,
                    17,
                    0
                ],
                "published": "2024-07-02T02:27:46Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    2,
                    27,
                    46,
                    1,
                    184,
                    0
                ],
                "title": "GRASP: A Grid-Based Benchmark for Evaluating Commonsense Spatial\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRASP: A Grid-Based Benchmark for Evaluating Commonsense Spatial\n  Reasoning"
                },
                "summary": "Spatial reasoning, an important faculty of human cognition with many\npractical applications, is one of the core commonsense skills that is not\npurely language-based and, for satisfying (as opposed to optimal) solutions,\nrequires some minimum degree of planning. Existing benchmarks of Commonsense\nSpatial Reasoning (CSR) tend to evaluate how Large Language Models (LLMs)\ninterpret text-based spatial $\\textit{descriptions}$ rather than directly\nevaluate a plan produced by the LLM in response to a $\\textit{specific}$\nspatial reasoning problem. In this paper, we construct a large-scale benchmark\ncalled GRASP, which consists of 16,000 grid-based environments where the agent\nis tasked with an energy collection problem. These environments include 100\ngrid instances instantiated using each of the 160 different grid settings,\ninvolving five different energy distributions, two modes of agent starting\nposition, and two distinct obstacle configurations, as well as three kinds of\nagent constraints. Using GRASP, we compare classic baseline approaches, such as\nrandom walk and greedy search methods, with advanced LLMs like GPT-3.5-Turbo,\nGPT-4o, and GPT-o1-mini. The experimental results indicate that even these\nadvanced LLMs struggle to consistently achieve satisfactory solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial reasoning, an important faculty of human cognition with many\npractical applications, is one of the core commonsense skills that is not\npurely language-based and, for satisfying (as opposed to optimal) solutions,\nrequires some minimum degree of planning. Existing benchmarks of Commonsense\nSpatial Reasoning (CSR) tend to evaluate how Large Language Models (LLMs)\ninterpret text-based spatial $\\textit{descriptions}$ rather than directly\nevaluate a plan produced by the LLM in response to a $\\textit{specific}$\nspatial reasoning problem. In this paper, we construct a large-scale benchmark\ncalled GRASP, which consists of 16,000 grid-based environments where the agent\nis tasked with an energy collection problem. These environments include 100\ngrid instances instantiated using each of the 160 different grid settings,\ninvolving five different energy distributions, two modes of agent starting\nposition, and two distinct obstacle configurations, as well as three kinds of\nagent constraints. Using GRASP, we compare classic baseline approaches, such as\nrandom walk and greedy search methods, with advanced LLMs like GPT-3.5-Turbo,\nGPT-4o, and GPT-o1-mini. The experimental results indicate that even these\nadvanced LLMs struggle to consistently achieve satisfactory solutions."
                },
                "authors": [
                    {
                        "name": "Zhisheng Tang"
                    },
                    {
                        "name": "Mayank Kejriwal"
                    }
                ],
                "author_detail": {
                    "name": "Mayank Kejriwal"
                },
                "author": "Mayank Kejriwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17640v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17640v2",
                "updated": "2025-01-17T04:26:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    4,
                    26,
                    44,
                    4,
                    17,
                    0
                ],
                "published": "2024-09-26T08:44:38Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    44,
                    38,
                    3,
                    270,
                    0
                ],
                "title": "T3: A Novel Zero-shot Transfer Learning Framework Iteratively Training\n  on an Assistant Task for a Target Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T3: A Novel Zero-shot Transfer Learning Framework Iteratively Training\n  on an Assistant Task for a Target Task"
                },
                "summary": "Long text summarization, gradually being essential for efficiently processing\nlarge volumes of information, stays challenging for Large Language Models\n(LLMs) such as GPT and LLaMA families because of the insufficient open-sourced\ntraining datasets and the high requirement of contextual details dealing. To\naddress the issue, we design a novel zero-shot transfer learning framework,\nabbreviated as T3, to iteratively training a baseline LLM on an assistant task\nfor the target task, where the former should own richer data resources and\nshare structural or semantic similarity with the latter. In practice, T3 is\napproached to deal with the long text summarization task by utilizing question\nanswering as the assistant task, and further validated its effectiveness on the\nBBC summary, NarraSum, FairytaleQA, and NLQuAD datasets, with up to nearly 14%\nimprovement in ROUGE, 35% improvement in BLEU, and 16% improvement in Factscore\ncompared to three baseline LLMs, demonstrating its potential for more\nassistant-target task combinations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long text summarization, gradually being essential for efficiently processing\nlarge volumes of information, stays challenging for Large Language Models\n(LLMs) such as GPT and LLaMA families because of the insufficient open-sourced\ntraining datasets and the high requirement of contextual details dealing. To\naddress the issue, we design a novel zero-shot transfer learning framework,\nabbreviated as T3, to iteratively training a baseline LLM on an assistant task\nfor the target task, where the former should own richer data resources and\nshare structural or semantic similarity with the latter. In practice, T3 is\napproached to deal with the long text summarization task by utilizing question\nanswering as the assistant task, and further validated its effectiveness on the\nBBC summary, NarraSum, FairytaleQA, and NLQuAD datasets, with up to nearly 14%\nimprovement in ROUGE, 35% improvement in BLEU, and 16% improvement in Factscore\ncompared to three baseline LLMs, demonstrating its potential for more\nassistant-target task combinations."
                },
                "authors": [
                    {
                        "name": "Xindi Tong"
                    },
                    {
                        "name": "Yujin Zhu"
                    },
                    {
                        "name": "Shijian Fan"
                    },
                    {
                        "name": "Liang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Liang Xu"
                },
                "author": "Liang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17640v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17640v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.11156v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.11156v4",
                "updated": "2025-01-17T04:21:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    4,
                    21,
                    47,
                    4,
                    17,
                    0
                ],
                "published": "2023-03-17T17:53:19Z",
                "published_parsed": [
                    2023,
                    3,
                    17,
                    17,
                    53,
                    19,
                    4,
                    76,
                    0
                ],
                "title": "Can AI-Generated Text be Reliably Detected?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can AI-Generated Text be Reliably Detected?"
                },
                "summary": "Large Language Models (LLMs) perform impressively well in various\napplications. However, the potential for misuse of these models in activities\nsuch as plagiarism, generating fake news, and spamming has raised concern about\ntheir responsible use. Consequently, the reliable detection of AI-generated\ntext has become a critical area of research. AI text detectors have shown to be\neffective under their specific settings. In this paper, we stress-test the\nrobustness of these AI text detectors in the presence of an attacker. We\nintroduce recursive paraphrasing attack to stress test a wide range of\ndetection schemes, including the ones using the watermarking as well as neural\nnetwork-based detectors, zero shot classifiers, and retrieval-based detectors.\nOur experiments conducted on passages, each approximately 300 tokens long,\nreveal the varying sensitivities of these detectors to our attacks. Our\nfindings indicate that while our recursive paraphrasing method can\nsignificantly reduce detection rates, it only slightly degrades text quality in\nmany cases, highlighting potential vulnerabilities in current detection systems\nin the presence of an attacker. Additionally, we investigate the susceptibility\nof watermarked LLMs to spoofing attacks aimed at misclassifying human-written\ntext as AI-generated. We demonstrate that an attacker can infer hidden AI text\nsignatures without white-box access to the detection method, potentially\nleading to reputational risks for LLM developers. Finally, we provide a\ntheoretical framework connecting the AUROC of the best possible detector to the\nTotal Variation distance between human and AI text distributions. This analysis\noffers insights into the fundamental challenges of reliable detection as\nlanguage models continue to advance. Our code is publicly available at\nhttps://github.com/vinusankars/Reliability-of-AI-text-detectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) perform impressively well in various\napplications. However, the potential for misuse of these models in activities\nsuch as plagiarism, generating fake news, and spamming has raised concern about\ntheir responsible use. Consequently, the reliable detection of AI-generated\ntext has become a critical area of research. AI text detectors have shown to be\neffective under their specific settings. In this paper, we stress-test the\nrobustness of these AI text detectors in the presence of an attacker. We\nintroduce recursive paraphrasing attack to stress test a wide range of\ndetection schemes, including the ones using the watermarking as well as neural\nnetwork-based detectors, zero shot classifiers, and retrieval-based detectors.\nOur experiments conducted on passages, each approximately 300 tokens long,\nreveal the varying sensitivities of these detectors to our attacks. Our\nfindings indicate that while our recursive paraphrasing method can\nsignificantly reduce detection rates, it only slightly degrades text quality in\nmany cases, highlighting potential vulnerabilities in current detection systems\nin the presence of an attacker. Additionally, we investigate the susceptibility\nof watermarked LLMs to spoofing attacks aimed at misclassifying human-written\ntext as AI-generated. We demonstrate that an attacker can infer hidden AI text\nsignatures without white-box access to the detection method, potentially\nleading to reputational risks for LLM developers. Finally, we provide a\ntheoretical framework connecting the AUROC of the best possible detector to the\nTotal Variation distance between human and AI text distributions. This analysis\noffers insights into the fundamental challenges of reliable detection as\nlanguage models continue to advance. Our code is publicly available at\nhttps://github.com/vinusankars/Reliability-of-AI-text-detectors."
                },
                "authors": [
                    {
                        "name": "Vinu Sankar Sadasivan"
                    },
                    {
                        "name": "Aounon Kumar"
                    },
                    {
                        "name": "Sriram Balasubramanian"
                    },
                    {
                        "name": "Wenxiao Wang"
                    },
                    {
                        "name": "Soheil Feizi"
                    }
                ],
                "author_detail": {
                    "name": "Soheil Feizi"
                },
                "author": "Soheil Feizi",
                "arxiv_comment": "Published in Transactions on Machine Learning Research (TMLR)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.11156v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.11156v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16239v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16239v3",
                "updated": "2025-01-17T04:19:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    4,
                    19,
                    43,
                    4,
                    17,
                    0
                ],
                "published": "2024-11-25T09:54:42Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    54,
                    42,
                    0,
                    330,
                    0
                ],
                "title": "CS-Eval: A Comprehensive Large Language Model Benchmark for\n  CyberSecurity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CS-Eval: A Comprehensive Large Language Model Benchmark for\n  CyberSecurity"
                },
                "summary": "Over the past year, there has been a notable rise in the use of large\nlanguage models (LLMs) for academic research and industrial practices within\nthe cybersecurity field. However, it remains a lack of comprehensive and\npublicly accessible benchmarks to evaluate the performance of LLMs on\ncybersecurity tasks. To address this gap, we introduce CS-Eval, a publicly\naccessible, comprehensive and bilingual LLM benchmark specifically designed for\ncybersecurity. CS-Eval synthesizes the research hotspots from academia and\npractical applications from industry, curating a diverse set of high-quality\nquestions across 42 categories within cybersecurity, systematically organized\ninto three cognitive levels: knowledge, ability, and application. Through an\nextensive evaluation of a wide range of LLMs using CS-Eval, we have uncovered\nvaluable insights. For instance, while GPT-4 generally excels overall, other\nmodels may outperform it in certain specific subcategories. Additionally, by\nconducting evaluations over several months, we observed significant\nimprovements in many LLMs' abilities to solve cybersecurity tasks. The\nbenchmarks are now publicly available at https://github.com/CS-EVAL/CS-Eval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past year, there has been a notable rise in the use of large\nlanguage models (LLMs) for academic research and industrial practices within\nthe cybersecurity field. However, it remains a lack of comprehensive and\npublicly accessible benchmarks to evaluate the performance of LLMs on\ncybersecurity tasks. To address this gap, we introduce CS-Eval, a publicly\naccessible, comprehensive and bilingual LLM benchmark specifically designed for\ncybersecurity. CS-Eval synthesizes the research hotspots from academia and\npractical applications from industry, curating a diverse set of high-quality\nquestions across 42 categories within cybersecurity, systematically organized\ninto three cognitive levels: knowledge, ability, and application. Through an\nextensive evaluation of a wide range of LLMs using CS-Eval, we have uncovered\nvaluable insights. For instance, while GPT-4 generally excels overall, other\nmodels may outperform it in certain specific subcategories. Additionally, by\nconducting evaluations over several months, we observed significant\nimprovements in many LLMs' abilities to solve cybersecurity tasks. The\nbenchmarks are now publicly available at https://github.com/CS-EVAL/CS-Eval."
                },
                "authors": [
                    {
                        "name": "Zhengmin Yu"
                    },
                    {
                        "name": "Jiutian Zeng"
                    },
                    {
                        "name": "Siyi Chen"
                    },
                    {
                        "name": "Wenhan Xu"
                    },
                    {
                        "name": "Dandan Xu"
                    },
                    {
                        "name": "Xiangyu Liu"
                    },
                    {
                        "name": "Zonghao Ying"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Min Yang"
                },
                "author": "Min Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16239v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16239v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16950v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16950v5",
                "updated": "2025-01-17T03:43:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    3,
                    43,
                    53,
                    4,
                    17,
                    0
                ],
                "published": "2024-03-25T17:11:28Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    17,
                    11,
                    28,
                    0,
                    85,
                    0
                ],
                "title": "Aligning with Human Judgement: The Role of Pairwise Preference in Large\n  Language Model Evaluators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning with Human Judgement: The Role of Pairwise Preference in Large\n  Language Model Evaluators"
                },
                "summary": "Large Language Models (LLMs) have demonstrated promising capabilities as\nautomatic evaluators in assessing the quality of generated natural language.\nHowever, LLMs still exhibit biases in evaluation and often struggle to generate\ncoherent evaluations that align with human assessments. In this work, we first\nconduct a systematic study of the misalignment between LLM evaluators and human\nevaluation, revealing that existing calibration methods aimed at mitigating\nbiases of LLMs are insufficient for effectively aligning LLM evaluators.\nInspired by the use of preference data in RLHF, we formulate the evaluation as\na ranking problem and introduce Pairwise-preference Search (PAIRS), an\nuncertainty-guided search-based rank aggregation method that employs LLMs to\nconduct pairwise comparisons locally and efficiently ranks candidate texts\nglobally. PAIRS achieves state-of-the-art performance on representative\nevaluation tasks in long-form generations and demonstrates significant\nimprovements over direct scoring. Furthermore, we provide insights into the\nrole of pairwise preference in quantifying the transitivity of LLMs and\ndemonstrate how PAIRS benefits from calibration using debiased pairwise\nevaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated promising capabilities as\nautomatic evaluators in assessing the quality of generated natural language.\nHowever, LLMs still exhibit biases in evaluation and often struggle to generate\ncoherent evaluations that align with human assessments. In this work, we first\nconduct a systematic study of the misalignment between LLM evaluators and human\nevaluation, revealing that existing calibration methods aimed at mitigating\nbiases of LLMs are insufficient for effectively aligning LLM evaluators.\nInspired by the use of preference data in RLHF, we formulate the evaluation as\na ranking problem and introduce Pairwise-preference Search (PAIRS), an\nuncertainty-guided search-based rank aggregation method that employs LLMs to\nconduct pairwise comparisons locally and efficiently ranks candidate texts\nglobally. PAIRS achieves state-of-the-art performance on representative\nevaluation tasks in long-form generations and demonstrates significant\nimprovements over direct scoring. Furthermore, we provide insights into the\nrole of pairwise preference in quantifying the transitivity of LLMs and\ndemonstrate how PAIRS benefits from calibration using debiased pairwise\nevaluations."
                },
                "authors": [
                    {
                        "name": "Yinhong Liu"
                    },
                    {
                        "name": "Han Zhou"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Ehsan Shareghi"
                    },
                    {
                        "name": "Ivan Vuli"
                    },
                    {
                        "name": "Anna Korhonen"
                    },
                    {
                        "name": "Nigel Collier"
                    }
                ],
                "author_detail": {
                    "name": "Nigel Collier"
                },
                "author": "Nigel Collier",
                "arxiv_comment": "This paper has been accepted by COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16950v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16950v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02933v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02933v4",
                "updated": "2025-01-17T03:19:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    3,
                    19,
                    16,
                    4,
                    17,
                    0
                ],
                "published": "2024-04-03T01:09:41Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    1,
                    9,
                    41,
                    2,
                    94,
                    0
                ],
                "title": "NL2KQL: From Natural Language to Kusto Query",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NL2KQL: From Natural Language to Kusto Query"
                },
                "summary": "Data is growing rapidly in volume and complexity. Proficiency in database\nquery languages is pivotal for crafting effective queries. As coding assistants\nbecome more prevalent, there is significant opportunity to enhance database\nquery languages. The Kusto Query Language (KQL) is a widely used query language\nfor large semi-structured data such as logs, telemetries, and time-series for\nbig data analytics platforms. This paper introduces NL2KQL an innovative\nframework that uses large language models (LLMs) to convert natural language\nqueries (NLQs) to KQL queries. The proposed NL2KQL framework includes several\nkey components: Schema Refiner which narrows down the schema to its most\npertinent elements; the Few-shot Selector which dynamically selects relevant\nexamples from a few-shot dataset; and the Query Refiner which repairs syntactic\nand semantic errors in KQL queries. Additionally, this study outlines a method\nfor generating large datasets of synthetic NLQ-KQL pairs which are valid within\na specific database contexts. To validate NL2KQL's performance, we utilize an\narray of online (based on query execution) and offline (based on query parsing)\nmetrics. Through ablation studies, the significance of each framework component\nis examined, and the datasets used for benchmarking are made publicly\navailable. This work is the first of its kind and is compared with available\nbaselines to demonstrate its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data is growing rapidly in volume and complexity. Proficiency in database\nquery languages is pivotal for crafting effective queries. As coding assistants\nbecome more prevalent, there is significant opportunity to enhance database\nquery languages. The Kusto Query Language (KQL) is a widely used query language\nfor large semi-structured data such as logs, telemetries, and time-series for\nbig data analytics platforms. This paper introduces NL2KQL an innovative\nframework that uses large language models (LLMs) to convert natural language\nqueries (NLQs) to KQL queries. The proposed NL2KQL framework includes several\nkey components: Schema Refiner which narrows down the schema to its most\npertinent elements; the Few-shot Selector which dynamically selects relevant\nexamples from a few-shot dataset; and the Query Refiner which repairs syntactic\nand semantic errors in KQL queries. Additionally, this study outlines a method\nfor generating large datasets of synthetic NLQ-KQL pairs which are valid within\na specific database contexts. To validate NL2KQL's performance, we utilize an\narray of online (based on query execution) and offline (based on query parsing)\nmetrics. Through ablation studies, the significance of each framework component\nis examined, and the datasets used for benchmarking are made publicly\navailable. This work is the first of its kind and is compared with available\nbaselines to demonstrate its effectiveness."
                },
                "authors": [
                    {
                        "name": "Xinye Tang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Jeremias Eichelbaum"
                    },
                    {
                        "name": "Mahan Das"
                    },
                    {
                        "name": "Alex Klein"
                    },
                    {
                        "name": "Nihal Irmak Pakis"
                    },
                    {
                        "name": "William Blum"
                    },
                    {
                        "name": "Daniel L Mace"
                    },
                    {
                        "name": "Tanvi Raja"
                    },
                    {
                        "name": "Namrata Padmanabhan"
                    },
                    {
                        "name": "Ye Xing"
                    }
                ],
                "author_detail": {
                    "name": "Ye Xing"
                },
                "author": "Ye Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02933v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02933v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10960v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10960v4",
                "updated": "2025-01-17T03:09:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    3,
                    9,
                    24,
                    4,
                    17,
                    0
                ],
                "published": "2024-07-15T17:55:42Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    55,
                    42,
                    0,
                    197,
                    0
                ],
                "title": "Fast Matrix Multiplications for Lookup Table-Quantized LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Matrix Multiplications for Lookup Table-Quantized LLMs"
                },
                "summary": "The deployment of large language models (LLMs) is often constrained by memory\nbandwidth, where the primary bottleneck is the cost of transferring model\nparameters from the GPU's global memory to its registers. When coupled with\ncustom kernels that fuse the dequantization and matmul operations, weight-only\nquantization can thus enable faster inference by reducing the amount of memory\nmovement. However, developing high-performance kernels for weight-quantized\nLLMs presents substantial challenges, especially when the weights are\ncompressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform,\nlookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup\ntable engine for LUT-quantized LLMs, which uses offline restructuring of the\nquantized weight matrix to minimize bit manipulations associated with\nunpacking, and vectorization and duplication of the lookup table to mitigate\nshared memory bandwidth constraints. At batch sizes < 32 and quantization group\nsize of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster\nthan existing GEMM kernels. As an application of FLUTE, we explore a simple\nextension to lookup table-based NormalFloat quantization and apply it to\nquantize LLaMA3 to various configurations, obtaining competitive quantization\nperformance against strong baselines while obtaining an end-to-end throughput\nincrease of 1.5 to 2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often constrained by memory\nbandwidth, where the primary bottleneck is the cost of transferring model\nparameters from the GPU's global memory to its registers. When coupled with\ncustom kernels that fuse the dequantization and matmul operations, weight-only\nquantization can thus enable faster inference by reducing the amount of memory\nmovement. However, developing high-performance kernels for weight-quantized\nLLMs presents substantial challenges, especially when the weights are\ncompressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform,\nlookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup\ntable engine for LUT-quantized LLMs, which uses offline restructuring of the\nquantized weight matrix to minimize bit manipulations associated with\nunpacking, and vectorization and duplication of the lookup table to mitigate\nshared memory bandwidth constraints. At batch sizes < 32 and quantization group\nsize of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster\nthan existing GEMM kernels. As an application of FLUTE, we explore a simple\nextension to lookup table-based NormalFloat quantization and apply it to\nquantize LLaMA3 to various configurations, obtaining competitive quantization\nperformance against strong baselines while obtaining an end-to-end throughput\nincrease of 1.5 to 2 times."
                },
                "authors": [
                    {
                        "name": "Han Guo"
                    },
                    {
                        "name": "William Brandon"
                    },
                    {
                        "name": "Radostin Cholakov"
                    },
                    {
                        "name": "Jonathan Ragan-Kelley"
                    },
                    {
                        "name": "Eric P. Xing"
                    },
                    {
                        "name": "Yoon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Yoon Kim"
                },
                "author": "Yoon Kim",
                "arxiv_comment": "EMNLP 2024 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10960v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10960v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09929v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09929v1",
                "updated": "2025-01-17T02:55:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    2,
                    55,
                    23,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T02:55:23Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    2,
                    55,
                    23,
                    4,
                    17,
                    0
                ],
                "title": "Steering Large Language Models with Feature Guided Activation Additions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering Large Language Models with Feature Guided Activation Additions"
                },
                "summary": "Effective and reliable control over large language model (LLM) behavior is a\nsignificant challenge. While activation steering methods, which add steering\nvectors to a model's hidden states, are a promising approach, existing\ntechniques often lack precision and interpretability in how they influence\nmodel outputs. We introduce Feature Guided Activation Additions (FGAA), a novel\nactivation steering method that leverages insights from Contrastive Activation\nAddition (CAA) and Sparse Autoencoder-Targeted Steering (SAE-TS). By operating\nin the latent space of a Sparse Autoencoder (SAE) and employing optimization\ntechniques to select desired SAE features, FGAA constructs precise steering\nvectors that provide better steering effects while maintaining coherence of\nsteered model outputs. In this regard, evaluations on Gemma-2-2B and Gemma-2-9B\nmodels across various steering tasks demonstrate that FGAA outperforms existing\nsteering methods of CAA, SAE decoder steering, and SAE-TS. Our results also\nhighlight important trade-offs between steering scale and general model\ncapabilities that are consistent across all tested steering methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective and reliable control over large language model (LLM) behavior is a\nsignificant challenge. While activation steering methods, which add steering\nvectors to a model's hidden states, are a promising approach, existing\ntechniques often lack precision and interpretability in how they influence\nmodel outputs. We introduce Feature Guided Activation Additions (FGAA), a novel\nactivation steering method that leverages insights from Contrastive Activation\nAddition (CAA) and Sparse Autoencoder-Targeted Steering (SAE-TS). By operating\nin the latent space of a Sparse Autoencoder (SAE) and employing optimization\ntechniques to select desired SAE features, FGAA constructs precise steering\nvectors that provide better steering effects while maintaining coherence of\nsteered model outputs. In this regard, evaluations on Gemma-2-2B and Gemma-2-9B\nmodels across various steering tasks demonstrate that FGAA outperforms existing\nsteering methods of CAA, SAE decoder steering, and SAE-TS. Our results also\nhighlight important trade-offs between steering scale and general model\ncapabilities that are consistent across all tested steering methods."
                },
                "authors": [
                    {
                        "name": "Samuel Soo"
                    },
                    {
                        "name": "Wesley Teng"
                    },
                    {
                        "name": "Chandrasekaran Balaganesh"
                    }
                ],
                "author_detail": {
                    "name": "Chandrasekaran Balaganesh"
                },
                "author": "Chandrasekaran Balaganesh",
                "arxiv_comment": "7 maintext pages, 14 appendix pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09929v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09928v1",
                "updated": "2025-01-17T02:48:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    2,
                    48,
                    29,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T02:48:29Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    2,
                    48,
                    29,
                    4,
                    17,
                    0
                ],
                "title": "Dialogue Benchmark Generation from Knowledge Graphs with Cost-Effective\n  Retrieval-Augmented LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue Benchmark Generation from Knowledge Graphs with Cost-Effective\n  Retrieval-Augmented LLMs"
                },
                "summary": "Dialogue benchmarks are crucial in training and evaluating chatbots engaging\nin domain-specific conversations. Knowledge graphs (KGs) represent semantically\nrich and well-organized data spanning various domains, such as DBLP, DBpedia,\nand YAGO. Traditionally, dialogue benchmarks have been manually created from\ndocuments, neglecting the potential of KGs in automating this process. Some\nquestion-answering benchmarks are automatically generated using extensive\npreprocessing from KGs, but they do not support dialogue generation. This paper\nintroduces Chatty-Gen, a novel multi-stage retrieval-augmented generation\nplatform for automatically generating high-quality dialogue benchmarks tailored\nto a specific domain using a KG. Chatty-Gen decomposes the generation process\ninto manageable stages and uses assertion rules for automatic validation\nbetween stages. Our approach enables control over intermediate results to\nprevent time-consuming restarts due to hallucinations. It also reduces reliance\non costly and more powerful commercial LLMs. Chatty-Gen eliminates upfront\nprocessing of the entire KG using efficient query-based retrieval to find\nrepresentative subgraphs based on the dialogue context. Our experiments with\nseveral real and large KGs demonstrate that Chatty-Gen significantly\noutperforms state-of-the-art systems and ensures consistent model and system\nperformance across multiple LLMs of diverse capabilities, such as GPT-4o,\nGemini 1.5, Llama 3, and Mistral.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue benchmarks are crucial in training and evaluating chatbots engaging\nin domain-specific conversations. Knowledge graphs (KGs) represent semantically\nrich and well-organized data spanning various domains, such as DBLP, DBpedia,\nand YAGO. Traditionally, dialogue benchmarks have been manually created from\ndocuments, neglecting the potential of KGs in automating this process. Some\nquestion-answering benchmarks are automatically generated using extensive\npreprocessing from KGs, but they do not support dialogue generation. This paper\nintroduces Chatty-Gen, a novel multi-stage retrieval-augmented generation\nplatform for automatically generating high-quality dialogue benchmarks tailored\nto a specific domain using a KG. Chatty-Gen decomposes the generation process\ninto manageable stages and uses assertion rules for automatic validation\nbetween stages. Our approach enables control over intermediate results to\nprevent time-consuming restarts due to hallucinations. It also reduces reliance\non costly and more powerful commercial LLMs. Chatty-Gen eliminates upfront\nprocessing of the entire KG using efficient query-based retrieval to find\nrepresentative subgraphs based on the dialogue context. Our experiments with\nseveral real and large KGs demonstrate that Chatty-Gen significantly\noutperforms state-of-the-art systems and ensures consistent model and system\nperformance across multiple LLMs of diverse capabilities, such as GPT-4o,\nGemini 1.5, Llama 3, and Mistral."
                },
                "authors": [
                    {
                        "name": "Reham Omar"
                    },
                    {
                        "name": "Omij Mangukiya"
                    },
                    {
                        "name": "Essam Mansour"
                    }
                ],
                "author_detail": {
                    "name": "Essam Mansour"
                },
                "author": "Essam Mansour",
                "arxiv_comment": "The paper is publsihed in SIGMOD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07832v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07832v8",
                "updated": "2025-01-17T02:18:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    2,
                    18,
                    0,
                    4,
                    17,
                    0
                ],
                "published": "2024-07-31T14:49:35Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    49,
                    35,
                    2,
                    213,
                    0
                ],
                "title": "LADDER: Language Driven Slice Discovery and Error Rectification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LADDER: Language Driven Slice Discovery and Error Rectification"
                },
                "summary": "Error slice discovery is crucial to diagnose and mitigate model errors.\nCurrent clustering or discrete attribute-based slice discovery methods face key\nlimitations: 1) clustering results in incoherent slices, while assigning\ndiscrete attributes to slices leads to incomplete coverage of error patterns\ndue to missing or insufficient attributes; 2) these methods lack complex\nreasoning, preventing them from fully explaining model biases; 3) they fail to\nintegrate \\textit{domain knowledge}, limiting their usage in specialized fields\n\\eg radiology. We propose\\ladder (\\underline{La}nguage-\\underline{D}riven\n\\underline{D}iscovery and \\underline{E}rror \\underline{R}ectification), to\naddress the limitations by: (1) leveraging the flexibility of natural language\nto address incompleteness, (2) employing LLM's latent \\textit{domain knowledge}\nand advanced reasoning to analyze sentences and derive testable hypotheses\ndirectly, identifying biased attributes, and form coherent error slices without\nclustering. Existing mitigation methods typically address only the\nworst-performing group, often amplifying errors in other subgroups. In\ncontrast,\\ladder generates pseudo attributes from the discovered hypotheses to\nmitigate errors across all biases without explicit attribute annotations or\nprior knowledge of bias. Rigorous evaluations on 6 datasets spanning natural\nand medical images -- comparing 200+ classifiers with diverse architectures,\npretraining strategies, and LLMs -- show that\\ladder consistently outperforms\nexisting baselines in discovering and mitigating biases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Error slice discovery is crucial to diagnose and mitigate model errors.\nCurrent clustering or discrete attribute-based slice discovery methods face key\nlimitations: 1) clustering results in incoherent slices, while assigning\ndiscrete attributes to slices leads to incomplete coverage of error patterns\ndue to missing or insufficient attributes; 2) these methods lack complex\nreasoning, preventing them from fully explaining model biases; 3) they fail to\nintegrate \\textit{domain knowledge}, limiting their usage in specialized fields\n\\eg radiology. We propose\\ladder (\\underline{La}nguage-\\underline{D}riven\n\\underline{D}iscovery and \\underline{E}rror \\underline{R}ectification), to\naddress the limitations by: (1) leveraging the flexibility of natural language\nto address incompleteness, (2) employing LLM's latent \\textit{domain knowledge}\nand advanced reasoning to analyze sentences and derive testable hypotheses\ndirectly, identifying biased attributes, and form coherent error slices without\nclustering. Existing mitigation methods typically address only the\nworst-performing group, often amplifying errors in other subgroups. In\ncontrast,\\ladder generates pseudo attributes from the discovered hypotheses to\nmitigate errors across all biases without explicit attribute annotations or\nprior knowledge of bias. Rigorous evaluations on 6 datasets spanning natural\nand medical images -- comparing 200+ classifiers with diverse architectures,\npretraining strategies, and LLMs -- show that\\ladder consistently outperforms\nexisting baselines in discovering and mitigating biases."
                },
                "authors": [
                    {
                        "name": "Shantanu Ghosh"
                    },
                    {
                        "name": "Rayan Syed"
                    },
                    {
                        "name": "Chenyu Wang"
                    },
                    {
                        "name": "Clare B. Poynton"
                    },
                    {
                        "name": "Shyam Visweswaran"
                    },
                    {
                        "name": "Kayhan Batmanghelich"
                    }
                ],
                "author_detail": {
                    "name": "Kayhan Batmanghelich"
                },
                "author": "Kayhan Batmanghelich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07832v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07832v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09913v1",
                "updated": "2025-01-17T02:02:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    2,
                    2,
                    12,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T02:02:12Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    2,
                    2,
                    12,
                    4,
                    17,
                    0
                ],
                "title": "Towards A Litmus Test for Common Sense",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards A Litmus Test for Common Sense"
                },
                "summary": "This paper is the second in a planned series aimed at envisioning a path to\nsafe and beneficial artificial intelligence. Building on the conceptual\ninsights of \"Common Sense Is All You Need,\" we propose a more formal litmus\ntest for common sense, adopting an axiomatic approach that combines minimal\nprior knowledge (MPK) constraints with diagonal or Godel-style arguments to\ncreate tasks beyond the agent's known concept set. We discuss how this approach\napplies to the Abstraction and Reasoning Corpus (ARC), acknowledging\ntraining/test data constraints, physical or virtual embodiment, and large\nlanguage models (LLMs). We also integrate observations regarding emergent\ndeceptive hallucinations, in which more capable AI systems may intentionally\nfabricate plausible yet misleading outputs to disguise knowledge gaps. The\noverarching theme is that scaling AI without ensuring common sense risks\nintensifying such deceptive tendencies, thereby undermining safety and trust.\nAligning with the broader goal of developing beneficial AI without causing\nharm, our axiomatic litmus test not only diagnoses whether an AI can handle\ntruly novel concepts but also provides a stepping stone toward an ethical,\nreliable foundation for future safe, beneficial, and aligned artificial\nintelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper is the second in a planned series aimed at envisioning a path to\nsafe and beneficial artificial intelligence. Building on the conceptual\ninsights of \"Common Sense Is All You Need,\" we propose a more formal litmus\ntest for common sense, adopting an axiomatic approach that combines minimal\nprior knowledge (MPK) constraints with diagonal or Godel-style arguments to\ncreate tasks beyond the agent's known concept set. We discuss how this approach\napplies to the Abstraction and Reasoning Corpus (ARC), acknowledging\ntraining/test data constraints, physical or virtual embodiment, and large\nlanguage models (LLMs). We also integrate observations regarding emergent\ndeceptive hallucinations, in which more capable AI systems may intentionally\nfabricate plausible yet misleading outputs to disguise knowledge gaps. The\noverarching theme is that scaling AI without ensuring common sense risks\nintensifying such deceptive tendencies, thereby undermining safety and trust.\nAligning with the broader goal of developing beneficial AI without causing\nharm, our axiomatic litmus test not only diagnoses whether an AI can handle\ntruly novel concepts but also provides a stepping stone toward an ethical,\nreliable foundation for future safe, beneficial, and aligned artificial\nintelligence."
                },
                "authors": [
                    {
                        "name": "Hugo Latapie"
                    }
                ],
                "author_detail": {
                    "name": "Hugo Latapie"
                },
                "author": "Hugo Latapie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09910v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09910v1",
                "updated": "2025-01-17T01:48:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    48,
                    15,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T01:48:15Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    48,
                    15,
                    4,
                    17,
                    0
                ],
                "title": "Chatbot apologies: Beyond bullshit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chatbot apologies: Beyond bullshit"
                },
                "summary": "Apologies serve essential functions for moral agents such as expressing\nremorse, taking responsibility, and repairing trust. LLM-based chatbots\nroutinely produce output that has the linguistic form of an apology. However,\nthey do this simply because they are echoing the kinds of things that humans\nsay. Moreover, there are reasons to think that chatbots are not the kind of\nlinguistic or moral agents capable of apology. To put the point bluntly:\nChatbot apologies are bullshit. This paper offers several arguments for this\nconclusion, drawing on the nature of morally-serious apologies, the linguistic\nagency required to perform them, and the moral agency required for them to\nmatter. We conclude by considering some consequences for how chatbots should be\ndesigned and how we ought to think about them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apologies serve essential functions for moral agents such as expressing\nremorse, taking responsibility, and repairing trust. LLM-based chatbots\nroutinely produce output that has the linguistic form of an apology. However,\nthey do this simply because they are echoing the kinds of things that humans\nsay. Moreover, there are reasons to think that chatbots are not the kind of\nlinguistic or moral agents capable of apology. To put the point bluntly:\nChatbot apologies are bullshit. This paper offers several arguments for this\nconclusion, drawing on the nature of morally-serious apologies, the linguistic\nagency required to perform them, and the moral agency required for them to\nmatter. We conclude by considering some consequences for how chatbots should be\ndesigned and how we ought to think about them."
                },
                "authors": [
                    {
                        "name": "P. D. Magnus"
                    },
                    {
                        "name": "Alessandra Buccella"
                    },
                    {
                        "name": "Jason D'Cruz"
                    }
                ],
                "author_detail": {
                    "name": "Jason D'Cruz"
                },
                "author": "Jason D'Cruz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09910v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09910v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09909v1",
                "updated": "2025-01-17T01:45:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    45,
                    35,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T01:45:35Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    45,
                    35,
                    4,
                    17,
                    0
                ],
                "title": "Demo: Interactive Visualization of Semantic Relationships in a\n  Biomedical Project's Talent Knowledge Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demo: Interactive Visualization of Semantic Relationships in a\n  Biomedical Project's Talent Knowledge Graph"
                },
                "summary": "We present an interactive visualization of the Cell Map for AI Talent\nKnowledge Graph (CM4AI TKG), a detailed semantic space comprising approximately\n28,000 experts and 1,000 datasets focused on the biomedical field. Our tool\nleverages transformer-based embeddings, WebGL visualization techniques, and\ngenerative AI, specifically Large Language Models (LLMs), to provide a\nresponsive and user-friendly interface. This visualization supports the\nexploration of around 29,000 nodes, assisting users in identifying potential\ncollaborators and dataset users within the health and biomedical research\nfields. Our solution transcends the limitations of conventional graph\nvisualization tools like Gephi, particularly in handling large-scale\ninteractive graphs. We utilize GPT-4o to furnish detailed justifications for\nrecommended collaborators and dataset users, promoting informed\ndecision-making. Key functionalities include responsive search and exploration,\nas well as GenAI-driven recommendations, all contributing to a nuanced\nrepresentation of the convergence between biomedical and AI research\nlandscapes. In addition to benefiting the Bridge2AI and CM4AI communities, this\nadaptable visualization framework can be extended to other biomedical knowledge\ngraphs, fostering advancements in medical AI and healthcare innovation through\nimproved user interaction and data exploration. The demonstration is available\nat: https://jiawei-alpha.vercel.app/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an interactive visualization of the Cell Map for AI Talent\nKnowledge Graph (CM4AI TKG), a detailed semantic space comprising approximately\n28,000 experts and 1,000 datasets focused on the biomedical field. Our tool\nleverages transformer-based embeddings, WebGL visualization techniques, and\ngenerative AI, specifically Large Language Models (LLMs), to provide a\nresponsive and user-friendly interface. This visualization supports the\nexploration of around 29,000 nodes, assisting users in identifying potential\ncollaborators and dataset users within the health and biomedical research\nfields. Our solution transcends the limitations of conventional graph\nvisualization tools like Gephi, particularly in handling large-scale\ninteractive graphs. We utilize GPT-4o to furnish detailed justifications for\nrecommended collaborators and dataset users, promoting informed\ndecision-making. Key functionalities include responsive search and exploration,\nas well as GenAI-driven recommendations, all contributing to a nuanced\nrepresentation of the convergence between biomedical and AI research\nlandscapes. In addition to benefiting the Bridge2AI and CM4AI communities, this\nadaptable visualization framework can be extended to other biomedical knowledge\ngraphs, fostering advancements in medical AI and healthcare innovation through\nimproved user interaction and data exploration. The demonstration is available\nat: https://jiawei-alpha.vercel.app/."
                },
                "authors": [
                    {
                        "name": "Jiawei Xu"
                    },
                    {
                        "name": "Zhandos Sembay"
                    },
                    {
                        "name": "Swathi Thaker"
                    },
                    {
                        "name": "Pamela Payne-Foster"
                    },
                    {
                        "name": "Jake Yue Chen"
                    },
                    {
                        "name": "Ying Ding"
                    }
                ],
                "author_detail": {
                    "name": "Ying Ding"
                },
                "author": "Ying Ding",
                "arxiv_comment": "Accepted by GenAI for Health Workshop @ NeurIPS 2024, Vancouver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20550v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20550v2",
                "updated": "2025-01-17T01:44:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    44,
                    44,
                    4,
                    17,
                    0
                ],
                "published": "2024-09-30T17:51:15Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    51,
                    15,
                    0,
                    274,
                    0
                ],
                "title": "LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism,\n  and Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism,\n  and Mitigation"
                },
                "summary": "Code generation aims to automatically generate code from input requirements,\nsignificantly enhancing development efficiency. Recent large language models\n(LLMs) based approaches have shown promising results and revolutionized code\ngeneration task. Despite the promising performance, LLMs often generate\ncontents with hallucinations, especially for the code generation scenario\nrequiring the handling of complex contextual dependencies in practical\ndevelopment process. Although previous study has analyzed hallucinations in\nLLM-powered code generation, the study is limited to standalone function\ngeneration. In this paper, we conduct an empirical study to study the\nphenomena, mechanism, and mitigation of LLM hallucinations within more\npractical and complex development contexts in repository-level generation\nscenario. First, we manually examine the code generation results from six\nmainstream LLMs to establish a hallucination taxonomy of LLM-generated code.\nNext, we elaborate on the phenomenon of hallucinations, analyze their\ndistribution across different models. We then analyze causes of hallucinations\nand identify four potential factors contributing to hallucinations. Finally, we\npropose an RAG-based mitigation method, which demonstrates consistent\neffectiveness in all studied LLMs. The replication package including code,\ndata, and experimental results is available at\nhttps://github.com/DeepSoftwareAnalytics/LLMCodingHallucination",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation aims to automatically generate code from input requirements,\nsignificantly enhancing development efficiency. Recent large language models\n(LLMs) based approaches have shown promising results and revolutionized code\ngeneration task. Despite the promising performance, LLMs often generate\ncontents with hallucinations, especially for the code generation scenario\nrequiring the handling of complex contextual dependencies in practical\ndevelopment process. Although previous study has analyzed hallucinations in\nLLM-powered code generation, the study is limited to standalone function\ngeneration. In this paper, we conduct an empirical study to study the\nphenomena, mechanism, and mitigation of LLM hallucinations within more\npractical and complex development contexts in repository-level generation\nscenario. First, we manually examine the code generation results from six\nmainstream LLMs to establish a hallucination taxonomy of LLM-generated code.\nNext, we elaborate on the phenomenon of hallucinations, analyze their\ndistribution across different models. We then analyze causes of hallucinations\nand identify four potential factors contributing to hallucinations. Finally, we\npropose an RAG-based mitigation method, which demonstrates consistent\neffectiveness in all studied LLMs. The replication package including code,\ndata, and experimental results is available at\nhttps://github.com/DeepSoftwareAnalytics/LLMCodingHallucination"
                },
                "authors": [
                    {
                        "name": "Ziyao Zhang"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Jiachi Chen"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_comment": "Accepted by ISSTA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20550v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20550v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18540v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18540v2",
                "updated": "2025-01-17T01:43:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    43,
                    21,
                    4,
                    17,
                    0
                ],
                "published": "2024-02-28T18:23:49Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    18,
                    23,
                    49,
                    2,
                    59,
                    0
                ],
                "title": "Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt\n  Templates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt\n  Templates"
                },
                "summary": "Public LLMs such as the Llama 2-Chat underwent alignment training and were\nconsidered safe. Recently Qi et al. [2024] reported that even benign\nfine-tuning on seemingly safe datasets can give rise to unsafe behaviors in the\nmodels. The current paper is about methods and best practices to mitigate such\nloss of alignment. We focus on the setting where a public model is fine-tuned\nbefore serving users for specific usage, where the model should improve on the\ndownstream task while maintaining alignment. Through extensive experiments on\nseveral chat models (Meta's Llama 2-Chat, Mistral AI's Mistral 7B Instruct\nv0.2, and OpenAI's GPT-3.5 Turbo), this paper uncovers that the prompt\ntemplates used during fine-tuning and inference play a crucial role in\npreserving safety alignment, and proposes the ``Pure Tuning, Safe Testing''\n(PTST) strategy -- fine-tune models without a safety prompt, but include it at\ntest time. This seemingly counterintuitive strategy incorporates an intended\ndistribution shift to encourage alignment preservation. Fine-tuning experiments\non GSM8K, ChatDoctor, and OpenOrca show that PTST significantly reduces the\nrise of unsafe behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public LLMs such as the Llama 2-Chat underwent alignment training and were\nconsidered safe. Recently Qi et al. [2024] reported that even benign\nfine-tuning on seemingly safe datasets can give rise to unsafe behaviors in the\nmodels. The current paper is about methods and best practices to mitigate such\nloss of alignment. We focus on the setting where a public model is fine-tuned\nbefore serving users for specific usage, where the model should improve on the\ndownstream task while maintaining alignment. Through extensive experiments on\nseveral chat models (Meta's Llama 2-Chat, Mistral AI's Mistral 7B Instruct\nv0.2, and OpenAI's GPT-3.5 Turbo), this paper uncovers that the prompt\ntemplates used during fine-tuning and inference play a crucial role in\npreserving safety alignment, and proposes the ``Pure Tuning, Safe Testing''\n(PTST) strategy -- fine-tune models without a safety prompt, but include it at\ntest time. This seemingly counterintuitive strategy incorporates an intended\ndistribution shift to encourage alignment preservation. Fine-tuning experiments\non GSM8K, ChatDoctor, and OpenOrca show that PTST significantly reduces the\nrise of unsafe behaviors."
                },
                "authors": [
                    {
                        "name": "Kaifeng Lyu"
                    },
                    {
                        "name": "Haoyu Zhao"
                    },
                    {
                        "name": "Xinran Gu"
                    },
                    {
                        "name": "Dingli Yu"
                    },
                    {
                        "name": "Anirudh Goyal"
                    },
                    {
                        "name": "Sanjeev Arora"
                    }
                ],
                "author_detail": {
                    "name": "Sanjeev Arora"
                },
                "author": "Sanjeev Arora",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18540v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18540v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09906v1",
                "updated": "2025-01-17T01:36:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    36,
                    52,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T01:36:52Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    36,
                    52,
                    4,
                    17,
                    0
                ],
                "title": "Position: Open and Closed Large Language Models in Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: Open and Closed Large Language Models in Healthcare"
                },
                "summary": "This position paper analyzes the evolving roles of open-source and\nclosed-source large language models (LLMs) in healthcare, emphasizing their\ndistinct contributions and the scientific community's response to their\ndevelopment. Due to their advanced reasoning capabilities, closed LLMs, such as\nGPT-4, have dominated high-performance applications, particularly in medical\nimaging and multimodal diagnostics. Conversely, open LLMs, like Meta's LLaMA,\nhave gained popularity for their adaptability and cost-effectiveness, enabling\nresearchers to fine-tune models for specific domains, such as mental health and\npatient communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This position paper analyzes the evolving roles of open-source and\nclosed-source large language models (LLMs) in healthcare, emphasizing their\ndistinct contributions and the scientific community's response to their\ndevelopment. Due to their advanced reasoning capabilities, closed LLMs, such as\nGPT-4, have dominated high-performance applications, particularly in medical\nimaging and multimodal diagnostics. Conversely, open LLMs, like Meta's LLaMA,\nhave gained popularity for their adaptability and cost-effectiveness, enabling\nresearchers to fine-tune models for specific domains, such as mental health and\npatient communication."
                },
                "authors": [
                    {
                        "name": "Jiawei Xu"
                    },
                    {
                        "name": "Ying Ding"
                    },
                    {
                        "name": "Yi Bu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Bu"
                },
                "author": "Yi Bu",
                "arxiv_comment": "Accepted by GenAI for Health Workshop @ NeurIPS 2024, Vancouver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04421v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04421v2",
                "updated": "2025-01-17T01:11:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    11,
                    16,
                    4,
                    17,
                    0
                ],
                "published": "2024-09-06T17:30:45Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    17,
                    30,
                    45,
                    4,
                    250,
                    0
                ],
                "title": "RLPF: Reinforcement Learning from Prediction Feedback for User\n  Summarization with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RLPF: Reinforcement Learning from Prediction Feedback for User\n  Summarization with LLMs"
                },
                "summary": "LLM-powered personalization agent systems employ Large Language Models (LLMs)\nto predict users' behavior from their past activities. However, their\neffectiveness often hinges on the ability to effectively leverage extensive,\nlong user historical data due to its inherent noise and length of such data.\nExisting pretrained LLMs may generate summaries that are concise but lack the\nnecessary context for downstream tasks, hindering their utility in\npersonalization systems. To address these challenges, we introduce\nReinforcement Learning from Prediction Feedback (RLPF). RLPF fine-tunes LLMs to\ngenerate concise, human-readable user summaries that are optimized for\ndownstream task performance. By maximizing the usefulness of the generated\nsummaries, RLPF effectively distills extensive user history data while\npreserving essential information for downstream tasks. Our empirical evaluation\ndemonstrates significant improvements in both extrinsic downstream task utility\nand intrinsic summary quality, surpassing baseline methods by up to 22% on\ndownstream task performance and achieving an up to 84.59% win rate on\nFactuality, Abstractiveness, and Readability. RLPF also achieves a remarkable\n74% reduction in context length while improving performance on 16 out of 19\nunseen tasks and/or datasets, showcasing its generalizability. This approach\noffers a promising solution for enhancing LLM personalization by effectively\ntransforming long, noisy user histories into informative and human-readable\nrepresentations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-powered personalization agent systems employ Large Language Models (LLMs)\nto predict users' behavior from their past activities. However, their\neffectiveness often hinges on the ability to effectively leverage extensive,\nlong user historical data due to its inherent noise and length of such data.\nExisting pretrained LLMs may generate summaries that are concise but lack the\nnecessary context for downstream tasks, hindering their utility in\npersonalization systems. To address these challenges, we introduce\nReinforcement Learning from Prediction Feedback (RLPF). RLPF fine-tunes LLMs to\ngenerate concise, human-readable user summaries that are optimized for\ndownstream task performance. By maximizing the usefulness of the generated\nsummaries, RLPF effectively distills extensive user history data while\npreserving essential information for downstream tasks. Our empirical evaluation\ndemonstrates significant improvements in both extrinsic downstream task utility\nand intrinsic summary quality, surpassing baseline methods by up to 22% on\ndownstream task performance and achieving an up to 84.59% win rate on\nFactuality, Abstractiveness, and Readability. RLPF also achieves a remarkable\n74% reduction in context length while improving performance on 16 out of 19\nunseen tasks and/or datasets, showcasing its generalizability. This approach\noffers a promising solution for enhancing LLM personalization by effectively\ntransforming long, noisy user histories into informative and human-readable\nrepresentations."
                },
                "authors": [
                    {
                        "name": "Jiaxing Wu"
                    },
                    {
                        "name": "Lin Ning"
                    },
                    {
                        "name": "Luyang Liu"
                    },
                    {
                        "name": "Harrison Lee"
                    },
                    {
                        "name": "Neo Wu"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Sushant Prakash"
                    },
                    {
                        "name": "Shawn O'Banion"
                    },
                    {
                        "name": "Bradley Green"
                    },
                    {
                        "name": "Jun Xie"
                    }
                ],
                "author_detail": {
                    "name": "Jun Xie"
                },
                "author": "Jun Xie",
                "arxiv_comment": "AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04421v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04421v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09891v1",
                "updated": "2025-01-17T00:41:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    0,
                    41,
                    44,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T00:41:44Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    0,
                    41,
                    44,
                    4,
                    17,
                    0
                ],
                "title": "Evolving Deeper LLM Thinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolving Deeper LLM Thinking"
                },
                "summary": "We explore an evolutionary search strategy for scaling inference time compute\nin Large Language Models. The proposed approach, Mind Evolution, uses a\nlanguage model to generate, recombine and refine candidate responses. The\nproposed approach avoids the need to formalize the underlying inference problem\nwhenever a solution evaluator is available. Controlling for inference cost, we\nfind that Mind Evolution significantly outperforms other inference strategies\nsuch as Best-of-N and Sequential Revision in natural language planning tasks.\nIn the TravelPlanner and Natural Plan benchmarks, Mind Evolution solves more\nthan 98% of the problem instances using Gemini 1.5 Pro without the use of a\nformal solver.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore an evolutionary search strategy for scaling inference time compute\nin Large Language Models. The proposed approach, Mind Evolution, uses a\nlanguage model to generate, recombine and refine candidate responses. The\nproposed approach avoids the need to formalize the underlying inference problem\nwhenever a solution evaluator is available. Controlling for inference cost, we\nfind that Mind Evolution significantly outperforms other inference strategies\nsuch as Best-of-N and Sequential Revision in natural language planning tasks.\nIn the TravelPlanner and Natural Plan benchmarks, Mind Evolution solves more\nthan 98% of the problem instances using Gemini 1.5 Pro without the use of a\nformal solver."
                },
                "authors": [
                    {
                        "name": "Kuang-Huei Lee"
                    },
                    {
                        "name": "Ian Fischer"
                    },
                    {
                        "name": "Yueh-Hua Wu"
                    },
                    {
                        "name": "Dave Marwood"
                    },
                    {
                        "name": "Shumeet Baluja"
                    },
                    {
                        "name": "Dale Schuurmans"
                    },
                    {
                        "name": "Xinyun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xinyun Chen"
                },
                "author": "Xinyun Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09888v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09888v1",
                "updated": "2025-01-17T00:23:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    0,
                    23,
                    44,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T00:23:44Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    0,
                    23,
                    44,
                    4,
                    17,
                    0
                ],
                "title": "Understanding the Effectiveness of LLMs in Automated Self-Admitted\n  Technical Debt Repayment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the Effectiveness of LLMs in Automated Self-Admitted\n  Technical Debt Repayment"
                },
                "summary": "Self-Admitted Technical Debt (SATD), cases where developers intentionally\nacknowledge suboptimal solutions in code through comments, poses a significant\nchallenge to software maintainability. Left unresolved, SATD can degrade code\nquality and increase maintenance costs. While Large Language Models (LLMs) have\nshown promise in tasks like code generation and program repair, their potential\nin automated SATD repayment remains underexplored.\n  In this paper, we identify three key challenges in training and evaluating\nLLMs for SATD repayment: (1) dataset representativeness and scalability, (2)\nremoval of irrelevant SATD repayments, and (3) limitations of existing\nevaluation metrics. To address the first two dataset-related challenges, we\nadopt a language-independent SATD tracing tool and design a 10-step filtering\npipeline to extract SATD repayments from repositories, resulting two\nlarge-scale datasets: 58,722 items for Python and 97,347 items for Java. To\nimprove evaluation, we introduce two diff-based metrics, BLEU-diff and\nCrystalBLEU-diff, which measure code changes rather than whole code.\nAdditionally, we propose another new metric, LEMOD, which is both interpretable\nand informative. Using our new benchmarks and evaluation metrics, we evaluate\ntwo types of automated SATD repayment methods: fine-tuning smaller models, and\nprompt engineering with five large-scale models. Our results reveal that\nfine-tuned small models achieve comparable Exact Match (EM) scores to\nprompt-based approaches but underperform on BLEU-based metrics and LEMOD.\nNotably, Gemma-2-9B leads in EM, addressing 10.1% of Python and 8.1% of Java\nSATDs, while Llama-3.1-70B-Instruct and GPT-4o-mini excel on BLEU-diff,\nCrystalBLEU-diff, and LEMOD metrics. Our work contributes a robust benchmark,\nimproved evaluation metrics, and a comprehensive evaluation of LLMs, advancing\nresearch on automated SATD repayment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Admitted Technical Debt (SATD), cases where developers intentionally\nacknowledge suboptimal solutions in code through comments, poses a significant\nchallenge to software maintainability. Left unresolved, SATD can degrade code\nquality and increase maintenance costs. While Large Language Models (LLMs) have\nshown promise in tasks like code generation and program repair, their potential\nin automated SATD repayment remains underexplored.\n  In this paper, we identify three key challenges in training and evaluating\nLLMs for SATD repayment: (1) dataset representativeness and scalability, (2)\nremoval of irrelevant SATD repayments, and (3) limitations of existing\nevaluation metrics. To address the first two dataset-related challenges, we\nadopt a language-independent SATD tracing tool and design a 10-step filtering\npipeline to extract SATD repayments from repositories, resulting two\nlarge-scale datasets: 58,722 items for Python and 97,347 items for Java. To\nimprove evaluation, we introduce two diff-based metrics, BLEU-diff and\nCrystalBLEU-diff, which measure code changes rather than whole code.\nAdditionally, we propose another new metric, LEMOD, which is both interpretable\nand informative. Using our new benchmarks and evaluation metrics, we evaluate\ntwo types of automated SATD repayment methods: fine-tuning smaller models, and\nprompt engineering with five large-scale models. Our results reveal that\nfine-tuned small models achieve comparable Exact Match (EM) scores to\nprompt-based approaches but underperform on BLEU-based metrics and LEMOD.\nNotably, Gemma-2-9B leads in EM, addressing 10.1% of Python and 8.1% of Java\nSATDs, while Llama-3.1-70B-Instruct and GPT-4o-mini excel on BLEU-diff,\nCrystalBLEU-diff, and LEMOD metrics. Our work contributes a robust benchmark,\nimproved evaluation metrics, and a comprehensive evaluation of LLMs, advancing\nresearch on automated SATD repayment."
                },
                "authors": [
                    {
                        "name": "Mohammad Sadegh Sheikhaei"
                    },
                    {
                        "name": "Yuan Tian"
                    },
                    {
                        "name": "Shaowei Wang"
                    },
                    {
                        "name": "Bowen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Xu"
                },
                "author": "Bowen Xu",
                "arxiv_comment": "This is a preprint submitted to ACM Transactions on Software\n  Engineering and Methodology (TOSEM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09888v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09888v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09887v1",
                "updated": "2025-01-17T00:18:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    0,
                    18,
                    34,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T00:18:34Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    0,
                    18,
                    34,
                    4,
                    17,
                    0
                ],
                "title": "FLORA: Formal Language Model Enables Robust Training-free Zero-shot\n  Object Referring Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLORA: Formal Language Model Enables Robust Training-free Zero-shot\n  Object Referring Analysis"
                },
                "summary": "Object Referring Analysis (ORA), commonly known as referring expression\ncomprehension, requires the identification and localization of specific objects\nin an image based on natural descriptions. Unlike generic object detection, ORA\nrequires both accurate language understanding and precise visual localization,\nmaking it inherently more complex. Although recent pre-trained large visual\ngrounding detectors have achieved significant progress, they heavily rely on\nextensively labeled data and time-consuming learning. To address these, we\nintroduce a novel, training-free framework for zero-shot ORA, termed FLORA\n(Formal Language for Object Referring and Analysis). FLORA harnesses the\ninherent reasoning capabilities of large language models (LLMs) and integrates\na formal language model - a logical framework that regulates language within\nstructured, rule-based descriptions - to provide effective zero-shot ORA. More\nspecifically, our formal language model (FLM) enables an effective,\nlogic-driven interpretation of object descriptions without necessitating any\ntraining processes. Built upon FLM-regulated LLM outputs, we further devise a\nBayesian inference framework and employ appropriate off-the-shelf interpretive\nmodels to finalize the reasoning, delivering favorable robustness against LLM\nhallucinations and compelling ORA performance in a training-free manner. In\npractice, our FLORA boosts the zero-shot performance of existing pretrained\ngrounding detectors by up to around 45%. Our comprehensive evaluation across\ndifferent challenging datasets also confirms that FLORA consistently surpasses\ncurrent state-of-the-art zero-shot methods in both detection and segmentation\ntasks associated with zero-shot ORA. We believe our probabilistic parsing and\nreasoning of the LLM outputs elevate the reliability and interpretability of\nzero-shot ORA. We shall release codes upon publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object Referring Analysis (ORA), commonly known as referring expression\ncomprehension, requires the identification and localization of specific objects\nin an image based on natural descriptions. Unlike generic object detection, ORA\nrequires both accurate language understanding and precise visual localization,\nmaking it inherently more complex. Although recent pre-trained large visual\ngrounding detectors have achieved significant progress, they heavily rely on\nextensively labeled data and time-consuming learning. To address these, we\nintroduce a novel, training-free framework for zero-shot ORA, termed FLORA\n(Formal Language for Object Referring and Analysis). FLORA harnesses the\ninherent reasoning capabilities of large language models (LLMs) and integrates\na formal language model - a logical framework that regulates language within\nstructured, rule-based descriptions - to provide effective zero-shot ORA. More\nspecifically, our formal language model (FLM) enables an effective,\nlogic-driven interpretation of object descriptions without necessitating any\ntraining processes. Built upon FLM-regulated LLM outputs, we further devise a\nBayesian inference framework and employ appropriate off-the-shelf interpretive\nmodels to finalize the reasoning, delivering favorable robustness against LLM\nhallucinations and compelling ORA performance in a training-free manner. In\npractice, our FLORA boosts the zero-shot performance of existing pretrained\ngrounding detectors by up to around 45%. Our comprehensive evaluation across\ndifferent challenging datasets also confirms that FLORA consistently surpasses\ncurrent state-of-the-art zero-shot methods in both detection and segmentation\ntasks associated with zero-shot ORA. We believe our probabilistic parsing and\nreasoning of the LLM outputs elevate the reliability and interpretability of\nzero-shot ORA. We shall release codes upon publication."
                },
                "authors": [
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Zijing Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zijing Chen"
                },
                "author": "Zijing Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09879v1",
                "updated": "2025-01-16T23:31:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    23,
                    31,
                    49,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T23:31:49Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    23,
                    31,
                    49,
                    3,
                    16,
                    0
                ],
                "title": "Testing Refactoring Engine via Historical Bug Report driven LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing Refactoring Engine via Historical Bug Report driven LLM"
                },
                "summary": "Refactoring is the process of restructuring existing code without changing\nits external behavior while improving its internal structure. Refactoring\nengines are integral components of modern Integrated Development Environments\n(IDEs) and can automate or semi-automate this process to enhance code\nreadability, reduce complexity, and improve the maintainability of software\nproducts. Similar to traditional software systems such as compilers,\nrefactoring engines may also contain bugs that can lead to unexpected\nbehaviors. In this paper, we propose a novel approach called RETESTER, a\nLLM-based framework for automated refactoring engine testing. Specifically, by\nusing input program structure templates extracted from historical bug reports\nand input program characteristics that are error-prone, we design\nchain-of-thought (CoT) prompts to perform refactoring-preserving\ntransformations. The generated variants are then tested on the latest version\nof refactoring engines using differential testing. We evaluate RETESTER on two\nmost popular modern refactoring engines (i.e., ECLIPSE, and INTELLIJ IDEA). It\nsuccessfully revealed 18 new bugs in the latest version of those refactoring\nengines. By the time we submit our paper, seven of them were confirmed by their\ndevelopers, and three were fixed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refactoring is the process of restructuring existing code without changing\nits external behavior while improving its internal structure. Refactoring\nengines are integral components of modern Integrated Development Environments\n(IDEs) and can automate or semi-automate this process to enhance code\nreadability, reduce complexity, and improve the maintainability of software\nproducts. Similar to traditional software systems such as compilers,\nrefactoring engines may also contain bugs that can lead to unexpected\nbehaviors. In this paper, we propose a novel approach called RETESTER, a\nLLM-based framework for automated refactoring engine testing. Specifically, by\nusing input program structure templates extracted from historical bug reports\nand input program characteristics that are error-prone, we design\nchain-of-thought (CoT) prompts to perform refactoring-preserving\ntransformations. The generated variants are then tested on the latest version\nof refactoring engines using differential testing. We evaluate RETESTER on two\nmost popular modern refactoring engines (i.e., ECLIPSE, and INTELLIJ IDEA). It\nsuccessfully revealed 18 new bugs in the latest version of those refactoring\nengines. By the time we submit our paper, seven of them were confirmed by their\ndevelopers, and three were fixed."
                },
                "authors": [
                    {
                        "name": "Haibo Wang"
                    },
                    {
                        "name": "Zhuolin Xu"
                    },
                    {
                        "name": "Shin Hwei Tan"
                    }
                ],
                "author_detail": {
                    "name": "Shin Hwei Tan"
                },
                "author": "Shin Hwei Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09877v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09877v1",
                "updated": "2025-01-16T23:22:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    23,
                    22,
                    17,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T23:22:17Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    23,
                    22,
                    17,
                    3,
                    16,
                    0
                ],
                "title": "CLAP-S: Support Set Based Adaptation for Downstream Fiber-optic Acoustic\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLAP-S: Support Set Based Adaptation for Downstream Fiber-optic Acoustic\n  Recognition"
                },
                "summary": "Contrastive Language-Audio Pretraining (CLAP) models have demonstrated\nunprecedented performance in various acoustic signal recognition tasks.\nFiber-optic-based acoustic recognition is one of the most important downstream\ntasks and plays a significant role in environmental sensing. Adapting CLAP for\nfiber-optic acoustic recognition has become an active research area. As a\nnon-conventional acoustic sensor, fiber-optic acoustic recognition presents a\nchallenging, domain-specific, low-shot deployment environment with significant\ndomain shifts due to unique frequency response and noise characteristics. To\naddress these challenges, we propose a support-based adaptation method, CLAP-S,\nwhich linearly interpolates a CLAP Adapter with the Support Set, leveraging\nboth implicit knowledge through fine-tuning and explicit knowledge retrieved\nfrom memory for cross-domain generalization. Experimental results show that our\nmethod delivers competitive performance on both laboratory-recorded fiber-optic\nESC-50 datasets and a real-world fiber-optic gunshot-firework dataset. Our\nresearch also provides valuable insights for other downstream acoustic\nrecognition tasks. The code and gunshot-firework dataset are available at\nhttps://github.com/Jingchensun/clap-s.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Language-Audio Pretraining (CLAP) models have demonstrated\nunprecedented performance in various acoustic signal recognition tasks.\nFiber-optic-based acoustic recognition is one of the most important downstream\ntasks and plays a significant role in environmental sensing. Adapting CLAP for\nfiber-optic acoustic recognition has become an active research area. As a\nnon-conventional acoustic sensor, fiber-optic acoustic recognition presents a\nchallenging, domain-specific, low-shot deployment environment with significant\ndomain shifts due to unique frequency response and noise characteristics. To\naddress these challenges, we propose a support-based adaptation method, CLAP-S,\nwhich linearly interpolates a CLAP Adapter with the Support Set, leveraging\nboth implicit knowledge through fine-tuning and explicit knowledge retrieved\nfrom memory for cross-domain generalization. Experimental results show that our\nmethod delivers competitive performance on both laboratory-recorded fiber-optic\nESC-50 datasets and a real-world fiber-optic gunshot-firework dataset. Our\nresearch also provides valuable insights for other downstream acoustic\nrecognition tasks. The code and gunshot-firework dataset are available at\nhttps://github.com/Jingchensun/clap-s."
                },
                "authors": [
                    {
                        "name": "Jingchen Sun"
                    },
                    {
                        "name": "Shaobo Han"
                    },
                    {
                        "name": "Wataru Kohno"
                    },
                    {
                        "name": "Changyou Chen"
                    }
                ],
                "author_detail": {
                    "name": "Changyou Chen"
                },
                "author": "Changyou Chen",
                "arxiv_comment": "Accepted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09877v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09877v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09870v1",
                "updated": "2025-01-16T22:54:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    22,
                    54,
                    12,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T22:54:12Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    22,
                    54,
                    12,
                    3,
                    16,
                    0
                ],
                "title": "An LLM-Guided Tutoring System for Social Skills Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM-Guided Tutoring System for Social Skills Training"
                },
                "summary": "Social skills training targets behaviors necessary for success in social\ninteractions. However, traditional classroom training for such skills is often\ninsufficient to teach effective communication -- one-to-one interaction in\nreal-world scenarios is preferred to lecture-style information delivery. This\npaper introduces a framework that allows instructors to collaborate with large\nlanguage models to dynamically design realistic scenarios for students to\ncommunicate. Our framework uses these scenarios to enable student rehearsal,\nprovide immediate feedback, and visualize performance for both students and\ninstructors. Unlike traditional intelligent tutoring systems, instructors can\neasily co-create scenarios with a large language model without technical\nskills. Additionally, the system generates new scenario branches in real time\nwhen existing options do not fit the student's response.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social skills training targets behaviors necessary for success in social\ninteractions. However, traditional classroom training for such skills is often\ninsufficient to teach effective communication -- one-to-one interaction in\nreal-world scenarios is preferred to lecture-style information delivery. This\npaper introduces a framework that allows instructors to collaborate with large\nlanguage models to dynamically design realistic scenarios for students to\ncommunicate. Our framework uses these scenarios to enable student rehearsal,\nprovide immediate feedback, and visualize performance for both students and\ninstructors. Unlike traditional intelligent tutoring systems, instructors can\neasily co-create scenarios with a large language model without technical\nskills. Additionally, the system generates new scenario branches in real time\nwhen existing options do not fit the student's response."
                },
                "authors": [
                    {
                        "name": "Michael Guevarra"
                    },
                    {
                        "name": "Indronil Bhattacharjee"
                    },
                    {
                        "name": "Srijita Das"
                    },
                    {
                        "name": "Christabel Wayllace"
                    },
                    {
                        "name": "Carrie Demmans Epp"
                    },
                    {
                        "name": "Matthew E. Taylor"
                    },
                    {
                        "name": "Alan Tay"
                    }
                ],
                "author_detail": {
                    "name": "Alan Tay"
                },
                "arxiv_affiliation": "Illumia Labs",
                "author": "Alan Tay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09866v1",
                "updated": "2025-01-16T22:36:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    22,
                    36,
                    0,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T22:36:00Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    22,
                    36,
                    0,
                    3,
                    16,
                    0
                ],
                "title": "Fine-grained Testing for Autonomous Driving Software: a Study on\n  Autoware with LLM-driven Unit Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained Testing for Autonomous Driving Software: a Study on\n  Autoware with LLM-driven Unit Testing"
                },
                "summary": "Testing autonomous driving systems (ADS) is critical to ensuring their\nreliability and safety. Existing ADS testing works focuses on designing\nscenarios to evaluate system-level behaviors, while fine-grained testing of ADS\nsource code has received comparatively little attention. To address this gap,\nwe present the first study on testing, specifically unit testing, for ADS\nsource code. Our study focuses on an industrial ADS framework, Autoware. We\nanalyze both human-written test cases and those generated by large language\nmodels (LLMs). Our findings reveal that human-written test cases in Autoware\nexhibit limited test coverage, and significant challenges remain in applying\nLLM-generated tests for Autoware unit testing. To overcome these challenges, we\npropose AwTest-LLM, a novel approach to enhance test coverage and improve test\ncase pass rates across Autoware packages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing autonomous driving systems (ADS) is critical to ensuring their\nreliability and safety. Existing ADS testing works focuses on designing\nscenarios to evaluate system-level behaviors, while fine-grained testing of ADS\nsource code has received comparatively little attention. To address this gap,\nwe present the first study on testing, specifically unit testing, for ADS\nsource code. Our study focuses on an industrial ADS framework, Autoware. We\nanalyze both human-written test cases and those generated by large language\nmodels (LLMs). Our findings reveal that human-written test cases in Autoware\nexhibit limited test coverage, and significant challenges remain in applying\nLLM-generated tests for Autoware unit testing. To overcome these challenges, we\npropose AwTest-LLM, a novel approach to enhance test coverage and improve test\ncase pass rates across Autoware packages."
                },
                "authors": [
                    {
                        "name": "Wenhan Wang"
                    },
                    {
                        "name": "Xuan Xie"
                    },
                    {
                        "name": "Yuheng Huang"
                    },
                    {
                        "name": "Renzhi Wang"
                    },
                    {
                        "name": "An Ran Chen"
                    },
                    {
                        "name": "Lei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lei Ma"
                },
                "author": "Lei Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09861v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09861v1",
                "updated": "2025-01-16T22:20:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    22,
                    20,
                    4,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T22:20:04Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    22,
                    20,
                    4,
                    3,
                    16,
                    0
                ],
                "title": "Optimization is Better than Generation: Optimizing Commit Message\n  Leveraging Human-written Commit Message",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization is Better than Generation: Optimizing Commit Message\n  Leveraging Human-written Commit Message"
                },
                "summary": "Commit messages are crucial in software development, supporting maintenance\ntasks and communication among developers. While Large Language Models (LLMs)\nhave advanced Commit Message Generation (CMG) using various software contexts,\nsome contexts developers consider are often missed by CMG techniques and can't\nbe easily retrieved or even retrieved at all by automated tools. To address\nthis, we propose Commit Message Optimization (CMO), which enhances\nhuman-written messages by leveraging LLMs and search-based optimization. CMO\nstarts with human-written messages and iteratively improves them by integrating\nkey contexts and feedback from external evaluators. Our extensive evaluation\nshows CMO generates commit messages that are significantly more Rational,\nComprehensive, and Expressive while outperforming state-of-the-art CMG methods\nand human messages 88.2%-95.4% of the time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commit messages are crucial in software development, supporting maintenance\ntasks and communication among developers. While Large Language Models (LLMs)\nhave advanced Commit Message Generation (CMG) using various software contexts,\nsome contexts developers consider are often missed by CMG techniques and can't\nbe easily retrieved or even retrieved at all by automated tools. To address\nthis, we propose Commit Message Optimization (CMO), which enhances\nhuman-written messages by leveraging LLMs and search-based optimization. CMO\nstarts with human-written messages and iteratively improves them by integrating\nkey contexts and feedback from external evaluators. Our extensive evaluation\nshows CMO generates commit messages that are significantly more Rational,\nComprehensive, and Expressive while outperforming state-of-the-art CMG methods\nand human messages 88.2%-95.4% of the time."
                },
                "authors": [
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "David Farag"
                    },
                    {
                        "name": "Christian Petrov"
                    },
                    {
                        "name": "Iftekhar Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Iftekhar Ahmed"
                },
                "author": "Iftekhar Ahmed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09861v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09861v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09858v1",
                "updated": "2025-01-16T22:11:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    22,
                    11,
                    3,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T22:11:03Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    22,
                    11,
                    3,
                    3,
                    16,
                    0
                ],
                "title": "From Explainability to Interpretability: Interpretable Policies in\n  Reinforcement Learning Via Model Explanation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Explainability to Interpretability: Interpretable Policies in\n  Reinforcement Learning Via Model Explanation"
                },
                "summary": "Deep reinforcement learning (RL) has shown remarkable success in complex\ndomains, however, the inherent black box nature of deep neural network policies\nraises significant challenges in understanding and trusting the decision-making\nprocesses. While existing explainable RL methods provide local insights, they\nfail to deliver a global understanding of the model, particularly in\nhigh-stakes applications. To overcome this limitation, we propose a novel\nmodel-agnostic approach that bridges the gap between explainability and\ninterpretability by leveraging Shapley values to transform complex deep RL\npolicies into transparent representations. The proposed approach offers two key\ncontributions: a novel approach employing Shapley values to policy\ninterpretation beyond local explanations and a general framework applicable to\noff-policy and on-policy algorithms. We evaluate our approach with three\nexisting deep RL algorithms and validate its performance in two classic control\nenvironments. The results demonstrate that our approach not only preserves the\noriginal models' performance but also generates more stable interpretable\npolicies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep reinforcement learning (RL) has shown remarkable success in complex\ndomains, however, the inherent black box nature of deep neural network policies\nraises significant challenges in understanding and trusting the decision-making\nprocesses. While existing explainable RL methods provide local insights, they\nfail to deliver a global understanding of the model, particularly in\nhigh-stakes applications. To overcome this limitation, we propose a novel\nmodel-agnostic approach that bridges the gap between explainability and\ninterpretability by leveraging Shapley values to transform complex deep RL\npolicies into transparent representations. The proposed approach offers two key\ncontributions: a novel approach employing Shapley values to policy\ninterpretation beyond local explanations and a general framework applicable to\noff-policy and on-policy algorithms. We evaluate our approach with three\nexisting deep RL algorithms and validate its performance in two classic control\nenvironments. The results demonstrate that our approach not only preserves the\noriginal models' performance but also generates more stable interpretable\npolicies."
                },
                "authors": [
                    {
                        "name": "Peilang Li"
                    },
                    {
                        "name": "Umer Siddique"
                    },
                    {
                        "name": "Yongcan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Yongcan Cao"
                },
                "author": "Yongcan Cao",
                "arxiv_comment": "Accepted to Deployable AI (DAI) Workshop at the Thirty-Ninth AAAI\n  Conference on Artificial Intelligence (AAAI-25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07719v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07719v2",
                "updated": "2025-01-16T21:00:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    21,
                    0,
                    53,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-13T22:09:44Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    22,
                    9,
                    44,
                    0,
                    13,
                    0
                ],
                "title": "Entailed Between the Lines: Incorporating Implication into NLI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entailed Between the Lines: Incorporating Implication into NLI"
                },
                "summary": "Much of human communication depends on implication, conveying meaning beyond\nliteral words to express a wider range of thoughts, intentions, and feelings.\nFor models to better understand and facilitate human communication, they must\nbe responsive to the text's implicit meaning. We focus on Natural Language\nInference (NLI), a core tool for many language tasks, and find that\nstate-of-the-art NLI models and datasets struggle to recognize a range of cases\nwhere entailment is implied, rather than explicit from the text. We formalize\nimplied entailment as an extension of the NLI task and introduce the Implied\nNLI dataset (INLI) to help today's LLMs both recognize a broader variety of\nimplied entailments and to distinguish between implicit and explicit\nentailment. We show how LLMs fine-tuned on INLI understand implied entailment\nand can generalize this understanding across datasets and domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Much of human communication depends on implication, conveying meaning beyond\nliteral words to express a wider range of thoughts, intentions, and feelings.\nFor models to better understand and facilitate human communication, they must\nbe responsive to the text's implicit meaning. We focus on Natural Language\nInference (NLI), a core tool for many language tasks, and find that\nstate-of-the-art NLI models and datasets struggle to recognize a range of cases\nwhere entailment is implied, rather than explicit from the text. We formalize\nimplied entailment as an extension of the NLI task and introduce the Implied\nNLI dataset (INLI) to help today's LLMs both recognize a broader variety of\nimplied entailments and to distinguish between implicit and explicit\nentailment. We show how LLMs fine-tuned on INLI understand implied entailment\nand can generalize this understanding across datasets and domains."
                },
                "authors": [
                    {
                        "name": "Shreya Havaldar"
                    },
                    {
                        "name": "Hamidreza Alvari"
                    },
                    {
                        "name": "John Palowitch"
                    },
                    {
                        "name": "Mohammad Javad Hosseini"
                    },
                    {
                        "name": "Senaka Buthpitiya"
                    },
                    {
                        "name": "Alex Fabrikant"
                    }
                ],
                "author_detail": {
                    "name": "Alex Fabrikant"
                },
                "author": "Alex Fabrikant",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07719v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07719v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09833v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09833v1",
                "updated": "2025-01-16T20:42:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    20,
                    42,
                    17,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T20:42:17Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    20,
                    42,
                    17,
                    3,
                    16,
                    0
                ],
                "title": "EraseBench: Understanding The Ripple Effects of Concept Erasure\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EraseBench: Understanding The Ripple Effects of Concept Erasure\n  Techniques"
                },
                "summary": "Concept erasure techniques have recently gained significant attention for\ntheir potential to remove unwanted concepts from text-to-image models. While\nthese methods often demonstrate success in controlled scenarios, their\nrobustness in real-world applications and readiness for deployment remain\nuncertain. In this work, we identify a critical gap in evaluating sanitized\nmodels, particularly in terms of their performance across various concept\ndimensions. We systematically investigate the failure modes of current concept\nerasure techniques, with a focus on visually similar, binomial, and\nsemantically related concepts. We propose that these interconnected\nrelationships give rise to a phenomenon of concept entanglement resulting in\nripple effects and degradation in image quality. To facilitate more\ncomprehensive evaluation, we introduce EraseBENCH, a multi-dimensional\nbenchmark designed to assess concept erasure methods with greater depth. Our\ndataset includes over 100 diverse concepts and more than 1,000 tailored\nprompts, paired with a comprehensive suite of metrics that together offer a\nholistic view of erasure efficacy. Our findings reveal that even\nstate-of-the-art techniques struggle with maintaining quality post-erasure,\nindicating that these approaches are not yet ready for real-world deployment.\nThis highlights the gap in reliability of the concept erasure techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concept erasure techniques have recently gained significant attention for\ntheir potential to remove unwanted concepts from text-to-image models. While\nthese methods often demonstrate success in controlled scenarios, their\nrobustness in real-world applications and readiness for deployment remain\nuncertain. In this work, we identify a critical gap in evaluating sanitized\nmodels, particularly in terms of their performance across various concept\ndimensions. We systematically investigate the failure modes of current concept\nerasure techniques, with a focus on visually similar, binomial, and\nsemantically related concepts. We propose that these interconnected\nrelationships give rise to a phenomenon of concept entanglement resulting in\nripple effects and degradation in image quality. To facilitate more\ncomprehensive evaluation, we introduce EraseBENCH, a multi-dimensional\nbenchmark designed to assess concept erasure methods with greater depth. Our\ndataset includes over 100 diverse concepts and more than 1,000 tailored\nprompts, paired with a comprehensive suite of metrics that together offer a\nholistic view of erasure efficacy. Our findings reveal that even\nstate-of-the-art techniques struggle with maintaining quality post-erasure,\nindicating that these approaches are not yet ready for real-world deployment.\nThis highlights the gap in reliability of the concept erasure techniques."
                },
                "authors": [
                    {
                        "name": "Ibtihel Amara"
                    },
                    {
                        "name": "Ahmed Imtiaz Humayun"
                    },
                    {
                        "name": "Ivana Kajic"
                    },
                    {
                        "name": "Zarana Parekh"
                    },
                    {
                        "name": "Natalie Harris"
                    },
                    {
                        "name": "Sarah Young"
                    },
                    {
                        "name": "Chirag Nagpal"
                    },
                    {
                        "name": "Najoung Kim"
                    },
                    {
                        "name": "Junfeng He"
                    },
                    {
                        "name": "Cristina Nader Vasconcelos"
                    },
                    {
                        "name": "Deepak Ramachandran"
                    },
                    {
                        "name": "Goolnoosh Farnadi"
                    },
                    {
                        "name": "Katherine Heller"
                    },
                    {
                        "name": "Mohammad Havaei"
                    },
                    {
                        "name": "Negar Rostamzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Negar Rostamzadeh"
                },
                "author": "Negar Rostamzadeh",
                "arxiv_comment": "11 pages main; 9 pages supplemental material",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09833v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09833v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09825v1",
                "updated": "2025-01-16T20:24:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    20,
                    24,
                    56,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T20:24:56Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    20,
                    24,
                    56,
                    3,
                    16,
                    0
                ],
                "title": "Bridging Language Barriers in Healthcare: A Study on Arabic LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Language Barriers in Healthcare: A Study on Arabic LLMs"
                },
                "summary": "This paper investigates the challenges of developing large language models\n(LLMs) proficient in both multilingual understanding and medical knowledge. We\ndemonstrate that simply translating medical data does not guarantee strong\nperformance on clinical tasks in the target language. Our experiments reveal\nthat the optimal language mix in training data varies significantly across\ndifferent medical tasks. We find that larger models with carefully calibrated\nlanguage ratios achieve superior performance on native-language clinical tasks.\nFurthermore, our results suggest that relying solely on fine-tuning may not be\nthe most effective approach for incorporating new language knowledge into LLMs.\nInstead, data and computationally intensive pretraining methods may still be\nnecessary to achieve optimal performance in multilingual medical settings.\nThese findings provide valuable guidance for building effective and inclusive\nmedical AI systems for diverse linguistic communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the challenges of developing large language models\n(LLMs) proficient in both multilingual understanding and medical knowledge. We\ndemonstrate that simply translating medical data does not guarantee strong\nperformance on clinical tasks in the target language. Our experiments reveal\nthat the optimal language mix in training data varies significantly across\ndifferent medical tasks. We find that larger models with carefully calibrated\nlanguage ratios achieve superior performance on native-language clinical tasks.\nFurthermore, our results suggest that relying solely on fine-tuning may not be\nthe most effective approach for incorporating new language knowledge into LLMs.\nInstead, data and computationally intensive pretraining methods may still be\nnecessary to achieve optimal performance in multilingual medical settings.\nThese findings provide valuable guidance for building effective and inclusive\nmedical AI systems for diverse linguistic communities."
                },
                "authors": [
                    {
                        "name": "Nada Saadi"
                    },
                    {
                        "name": "Tathagata Raha"
                    },
                    {
                        "name": "Clment Christophe"
                    },
                    {
                        "name": "Marco AF Pimentel"
                    },
                    {
                        "name": "Ronnie Rajan"
                    },
                    {
                        "name": "Praveen K Kanithi"
                    }
                ],
                "author_detail": {
                    "name": "Praveen K Kanithi"
                },
                "author": "Praveen K Kanithi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09804v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09804v1",
                "updated": "2025-01-16T19:23:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    19,
                    23,
                    11,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T19:23:11Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    19,
                    23,
                    11,
                    3,
                    16,
                    0
                ],
                "title": "Enhancing Generalization in Chain of Thought Reasoning for Smaller\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Generalization in Chain of Thought Reasoning for Smaller\n  Models"
                },
                "summary": "Chain-of-Thought (CoT) reasoning in smaller language models is a challenging\nnatural language process problem yet highly desirable in many real-life\napplications. Existing CoT knowledge distillation methods often suffer from\noverly conservative memorization in smaller LLMs, leading to low generalization\nconfidence. As fully preserving the CoT ability of teacher model is impossible,\nwe hypothesize that adversarial CoT fine-tuning is crucial for developing\nsmaller LLM with robust CoT generalization. To this end, we propose\n\\textit{PRompt-Assisted Domain-Adversarial fine-tuning} (PRADA), a principled\nfine-tuning framework that integrates diverse CoT domains. Specifically, PRADA\npioneers two CoT improvements in smaller LLM: (1) Recovering the\ndomain-invariant feature insight which typically lost during distillation with\ndomain adversarial fine-tuning; (2) Enhancing the domain adaptability of CoT\nprompt engineering by employing domain-adversarial approaches. We theoretically\ndemonstrate the effectiveness of our approach and empirically show that it\nsignificantly outperforms the state of the arts in a wide range of tasks.\nMoreover, our empirical findings reveal that the smaller LLM, when leveraging\nPRADA, aligns closely with domain knowledge, thereby improving the\nexplainability of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning in smaller language models is a challenging\nnatural language process problem yet highly desirable in many real-life\napplications. Existing CoT knowledge distillation methods often suffer from\noverly conservative memorization in smaller LLMs, leading to low generalization\nconfidence. As fully preserving the CoT ability of teacher model is impossible,\nwe hypothesize that adversarial CoT fine-tuning is crucial for developing\nsmaller LLM with robust CoT generalization. To this end, we propose\n\\textit{PRompt-Assisted Domain-Adversarial fine-tuning} (PRADA), a principled\nfine-tuning framework that integrates diverse CoT domains. Specifically, PRADA\npioneers two CoT improvements in smaller LLM: (1) Recovering the\ndomain-invariant feature insight which typically lost during distillation with\ndomain adversarial fine-tuning; (2) Enhancing the domain adaptability of CoT\nprompt engineering by employing domain-adversarial approaches. We theoretically\ndemonstrate the effectiveness of our approach and empirically show that it\nsignificantly outperforms the state of the arts in a wide range of tasks.\nMoreover, our empirical findings reveal that the smaller LLM, when leveraging\nPRADA, aligns closely with domain knowledge, thereby improving the\nexplainability of our approach."
                },
                "authors": [
                    {
                        "name": "Maxwell J. Yin"
                    },
                    {
                        "name": "Dingyi Jiang"
                    },
                    {
                        "name": "Yongbing Chen"
                    },
                    {
                        "name": "Boyu Wang"
                    },
                    {
                        "name": "Charles Ling"
                    }
                ],
                "author_detail": {
                    "name": "Charles Ling"
                },
                "author": "Charles Ling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09804v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09801v1",
                "updated": "2025-01-16T19:12:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    19,
                    12,
                    25,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T19:12:25Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    19,
                    12,
                    25,
                    3,
                    16,
                    0
                ],
                "title": "Conversational Text Extraction with Large Language Models Using\n  Retrieval-Augmented Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Text Extraction with Large Language Models Using\n  Retrieval-Augmented Systems"
                },
                "summary": "This study introduces a system leveraging Large Language Models (LLMs) to\nextract text and enhance user interaction with PDF documents via a\nconversational interface. Utilizing Retrieval-Augmented Generation (RAG), the\nsystem provides informative responses to user inquiries while highlighting\nrelevant passages within the PDF. Upon user upload, the system processes the\nPDF, employing sentence embeddings to create a document-specific vector store.\nThis vector store enables efficient retrieval of pertinent sections in response\nto user queries. The LLM then engages in a conversational exchange, using the\nretrieved information to extract text and generate comprehensive, contextually\naware answers. While our approach demonstrates competitive ROUGE values\ncompared to existing state-of-the-art techniques for text extraction and\nsummarization, we acknowledge that further qualitative evaluation is necessary\nto fully assess its effectiveness in real-world applications. The proposed\nsystem gives competitive ROUGE values as compared to existing state-of-the-art\ntechniques for text extraction and summarization, thus offering a valuable tool\nfor researchers, students, and anyone seeking to efficiently extract knowledge\nand gain insights from documents through an intuitive question-answering\ninterface.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces a system leveraging Large Language Models (LLMs) to\nextract text and enhance user interaction with PDF documents via a\nconversational interface. Utilizing Retrieval-Augmented Generation (RAG), the\nsystem provides informative responses to user inquiries while highlighting\nrelevant passages within the PDF. Upon user upload, the system processes the\nPDF, employing sentence embeddings to create a document-specific vector store.\nThis vector store enables efficient retrieval of pertinent sections in response\nto user queries. The LLM then engages in a conversational exchange, using the\nretrieved information to extract text and generate comprehensive, contextually\naware answers. While our approach demonstrates competitive ROUGE values\ncompared to existing state-of-the-art techniques for text extraction and\nsummarization, we acknowledge that further qualitative evaluation is necessary\nto fully assess its effectiveness in real-world applications. The proposed\nsystem gives competitive ROUGE values as compared to existing state-of-the-art\ntechniques for text extraction and summarization, thus offering a valuable tool\nfor researchers, students, and anyone seeking to efficiently extract knowledge\nand gain insights from documents through an intuitive question-answering\ninterface."
                },
                "authors": [
                    {
                        "name": "Soham Roy"
                    },
                    {
                        "name": "Mitul Goswami"
                    },
                    {
                        "name": "Nisharg Nargund"
                    },
                    {
                        "name": "Suneeta Mohanty"
                    },
                    {
                        "name": "Prasant Kumar Pattnaik"
                    }
                ],
                "author_detail": {
                    "name": "Prasant Kumar Pattnaik"
                },
                "author": "Prasant Kumar Pattnaik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09798v1",
                "updated": "2025-01-16T19:01:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    19,
                    1,
                    25,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T19:01:25Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    19,
                    1,
                    25,
                    3,
                    16,
                    0
                ],
                "title": "Computing Optimization-Based Prompt Injections Against Closed-Weights\n  Models By Misusing a Fine-Tuning API",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computing Optimization-Based Prompt Injections Against Closed-Weights\n  Models By Misusing a Fine-Tuning API"
                },
                "summary": "We surface a new threat to closed-weight Large Language Models (LLMs) that\nenables an attacker to compute optimization-based prompt injections.\nSpecifically, we characterize how an attacker can leverage the loss-like\ninformation returned from the remote fine-tuning interface to guide the search\nfor adversarial prompts. The fine-tuning interface is hosted by an LLM vendor\nand allows developers to fine-tune LLMs for their tasks, thus providing\nutility, but also exposes enough information for an attacker to compute\nadversarial prompts. Through an experimental analysis, we characterize the\nloss-like values returned by the Gemini fine-tuning API and demonstrate that\nthey provide a useful signal for discrete optimization of adversarial prompts\nusing a greedy search algorithm. Using the PurpleLlama prompt injection\nbenchmark, we demonstrate attack success rates between 65% and 82% on Google's\nGemini family of LLMs. These attacks exploit the classic utility-security\ntradeoff - the fine-tuning interface provides a useful feature for developers\nbut also exposes the LLMs to powerful attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We surface a new threat to closed-weight Large Language Models (LLMs) that\nenables an attacker to compute optimization-based prompt injections.\nSpecifically, we characterize how an attacker can leverage the loss-like\ninformation returned from the remote fine-tuning interface to guide the search\nfor adversarial prompts. The fine-tuning interface is hosted by an LLM vendor\nand allows developers to fine-tune LLMs for their tasks, thus providing\nutility, but also exposes enough information for an attacker to compute\nadversarial prompts. Through an experimental analysis, we characterize the\nloss-like values returned by the Gemini fine-tuning API and demonstrate that\nthey provide a useful signal for discrete optimization of adversarial prompts\nusing a greedy search algorithm. Using the PurpleLlama prompt injection\nbenchmark, we demonstrate attack success rates between 65% and 82% on Google's\nGemini family of LLMs. These attacks exploit the classic utility-security\ntradeoff - the fine-tuning interface provides a useful feature for developers\nbut also exposes the LLMs to powerful attacks."
                },
                "authors": [
                    {
                        "name": "Andrey Labunets"
                    },
                    {
                        "name": "Nishit V. Pandya"
                    },
                    {
                        "name": "Ashish Hooda"
                    },
                    {
                        "name": "Xiaohan Fu"
                    },
                    {
                        "name": "Earlence Fernandes"
                    }
                ],
                "author_detail": {
                    "name": "Earlence Fernandes"
                },
                "author": "Earlence Fernandes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09757v1",
                "updated": "2025-01-16T18:59:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    59,
                    53,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T18:59:53Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    59,
                    53,
                    3,
                    16,
                    0
                ],
                "title": "Distilling Multi-modal Large Language Models for Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Multi-modal Large Language Models for Autonomous Driving"
                },
                "summary": "Autonomous driving demands safe motion planning, especially in critical\n\"long-tail\" scenarios. Recent end-to-end autonomous driving systems leverage\nlarge language models (LLMs) as planners to improve generalizability to rare\nevents. However, using LLMs at test time introduces high computational costs.\nTo address this, we propose DiMA, an end-to-end autonomous driving system that\nmaintains the efficiency of an LLM-free (or vision-based) planner while\nleveraging the world knowledge of an LLM. DiMA distills the information from a\nmulti-modal LLM to a vision-based end-to-end planner through a set of specially\ndesigned surrogate tasks. Under a joint training strategy, a scene encoder\ncommon to both networks produces structured representations that are\nsemantically grounded as well as aligned to the final planning objective.\nNotably, the LLM is optional at inference, enabling robust planning without\ncompromising on efficiency. Training with DiMA results in a 37% reduction in\nthe L2 trajectory error and an 80% reduction in the collision rate of the\nvision-based planner, as well as a 44% trajectory error reduction in longtail\nscenarios. DiMA also achieves state-of-the-art performance on the nuScenes\nplanning benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous driving demands safe motion planning, especially in critical\n\"long-tail\" scenarios. Recent end-to-end autonomous driving systems leverage\nlarge language models (LLMs) as planners to improve generalizability to rare\nevents. However, using LLMs at test time introduces high computational costs.\nTo address this, we propose DiMA, an end-to-end autonomous driving system that\nmaintains the efficiency of an LLM-free (or vision-based) planner while\nleveraging the world knowledge of an LLM. DiMA distills the information from a\nmulti-modal LLM to a vision-based end-to-end planner through a set of specially\ndesigned surrogate tasks. Under a joint training strategy, a scene encoder\ncommon to both networks produces structured representations that are\nsemantically grounded as well as aligned to the final planning objective.\nNotably, the LLM is optional at inference, enabling robust planning without\ncompromising on efficiency. Training with DiMA results in a 37% reduction in\nthe L2 trajectory error and an 80% reduction in the collision rate of the\nvision-based planner, as well as a 44% trajectory error reduction in longtail\nscenarios. DiMA also achieves state-of-the-art performance on the nuScenes\nplanning benchmark."
                },
                "authors": [
                    {
                        "name": "Deepti Hegde"
                    },
                    {
                        "name": "Rajeev Yasarla"
                    },
                    {
                        "name": "Hong Cai"
                    },
                    {
                        "name": "Shizhong Han"
                    },
                    {
                        "name": "Apratim Bhattacharyya"
                    },
                    {
                        "name": "Shweta Mahajan"
                    },
                    {
                        "name": "Litian Liu"
                    },
                    {
                        "name": "Risheek Garrepalli"
                    },
                    {
                        "name": "Vishal M. Patel"
                    },
                    {
                        "name": "Fatih Porikli"
                    }
                ],
                "author_detail": {
                    "name": "Fatih Porikli"
                },
                "author": "Fatih Porikli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09781v1",
                "updated": "2025-01-16T18:59:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    59,
                    10,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T18:59:10Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    59,
                    10,
                    3,
                    16,
                    0
                ],
                "title": "VideoWorld: Exploring Knowledge Learning from Unlabeled Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoWorld: Exploring Knowledge Learning from Unlabeled Videos"
                },
                "summary": "This work explores whether a deep generative model can learn complex\nknowledge solely from visual input, in contrast to the prevalent focus on\ntext-based models like large language models (LLMs). We develop VideoWorld, an\nauto-regressive video generation model trained on unlabeled video data, and\ntest its knowledge acquisition abilities in video-based Go and robotic control\ntasks. Our experiments reveal two key findings: (1) video-only training\nprovides sufficient information for learning knowledge, including rules,\nreasoning and planning capabilities, and (2) the representation of visual\nchange is crucial for knowledge acquisition. To improve both the efficiency and\nefficacy of this process, we introduce the Latent Dynamics Model (LDM) as a key\ncomponent of VideoWorld. Remarkably, VideoWorld reaches a 5-dan professional\nlevel in the Video-GoBench with just a 300-million-parameter model, without\nrelying on search algorithms or reward mechanisms typical in reinforcement\nlearning. In robotic tasks, VideoWorld effectively learns diverse control\noperations and generalizes across environments, approaching the performance of\noracle models in CALVIN and RLBench. This study opens new avenues for knowledge\nacquisition from visual data, with all code, data, and models open-sourced for\nfurther research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores whether a deep generative model can learn complex\nknowledge solely from visual input, in contrast to the prevalent focus on\ntext-based models like large language models (LLMs). We develop VideoWorld, an\nauto-regressive video generation model trained on unlabeled video data, and\ntest its knowledge acquisition abilities in video-based Go and robotic control\ntasks. Our experiments reveal two key findings: (1) video-only training\nprovides sufficient information for learning knowledge, including rules,\nreasoning and planning capabilities, and (2) the representation of visual\nchange is crucial for knowledge acquisition. To improve both the efficiency and\nefficacy of this process, we introduce the Latent Dynamics Model (LDM) as a key\ncomponent of VideoWorld. Remarkably, VideoWorld reaches a 5-dan professional\nlevel in the Video-GoBench with just a 300-million-parameter model, without\nrelying on search algorithms or reward mechanisms typical in reinforcement\nlearning. In robotic tasks, VideoWorld effectively learns diverse control\noperations and generalizes across environments, approaching the performance of\noracle models in CALVIN and RLBench. This study opens new avenues for knowledge\nacquisition from visual data, with all code, data, and models open-sourced for\nfurther research."
                },
                "authors": [
                    {
                        "name": "Zhongwei Ren"
                    },
                    {
                        "name": "Yunchao Wei"
                    },
                    {
                        "name": "Xun Guo"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Bingyi Kang"
                    },
                    {
                        "name": "Jiashi Feng"
                    },
                    {
                        "name": "Xiaojie Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojie Jin"
                },
                "author": "Xiaojie Jin",
                "arxiv_comment": "Code and models are released at:\n  https://maverickren.github.io/VideoWorld.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09754v1",
                "updated": "2025-01-16T18:59:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    59,
                    3,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T18:59:03Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    59,
                    3,
                    3,
                    16,
                    0
                ],
                "title": "Lost in Translation, Found in Context: Sign Language Translation with\n  Contextual Cues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost in Translation, Found in Context: Sign Language Translation with\n  Contextual Cues"
                },
                "summary": "Our objective is to translate continuous sign language into spoken language\ntext. Inspired by the way human interpreters rely on context for accurate\ntranslation, we incorporate additional contextual cues together with the\nsigning video, into a new translation framework. Specifically, besides visual\nsign recognition features that encode the input video, we integrate\ncomplementary textual information from (i) captions describing the background\nshow, (ii) translation of previous sentences, as well as (iii) pseudo-glosses\ntranscribing the signing. These are automatically extracted and inputted along\nwith the visual features to a pre-trained large language model (LLM), which we\nfine-tune to generate spoken language translations in text form. Through\nextensive ablation studies, we show the positive contribution of each input cue\nto the translation performance. We train and evaluate our approach on BOBSL --\nthe largest British Sign Language dataset currently available. We show that our\ncontextual approach significantly enhances the quality of the translations\ncompared to previously reported results on BOBSL, and also to state-of-the-art\nmethods that we implement as baselines. Furthermore, we demonstrate the\ngenerality of our approach by applying it also to How2Sign, an American Sign\nLanguage dataset, and achieve competitive results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our objective is to translate continuous sign language into spoken language\ntext. Inspired by the way human interpreters rely on context for accurate\ntranslation, we incorporate additional contextual cues together with the\nsigning video, into a new translation framework. Specifically, besides visual\nsign recognition features that encode the input video, we integrate\ncomplementary textual information from (i) captions describing the background\nshow, (ii) translation of previous sentences, as well as (iii) pseudo-glosses\ntranscribing the signing. These are automatically extracted and inputted along\nwith the visual features to a pre-trained large language model (LLM), which we\nfine-tune to generate spoken language translations in text form. Through\nextensive ablation studies, we show the positive contribution of each input cue\nto the translation performance. We train and evaluate our approach on BOBSL --\nthe largest British Sign Language dataset currently available. We show that our\ncontextual approach significantly enhances the quality of the translations\ncompared to previously reported results on BOBSL, and also to state-of-the-art\nmethods that we implement as baselines. Furthermore, we demonstrate the\ngenerality of our approach by applying it also to How2Sign, an American Sign\nLanguage dataset, and achieve competitive results."
                },
                "authors": [
                    {
                        "name": "Youngjoon Jang"
                    },
                    {
                        "name": "Haran Raajesh"
                    },
                    {
                        "name": "Liliane Momeni"
                    },
                    {
                        "name": "Gl Varol"
                    },
                    {
                        "name": "Andrew Zisserman"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Zisserman"
                },
                "author": "Andrew Zisserman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09749v1",
                "updated": "2025-01-16T18:57:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    57,
                    20,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T18:57:20Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    57,
                    20,
                    3,
                    16,
                    0
                ],
                "title": "Enhancing Lexicon-Based Text Embeddings with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Lexicon-Based Text Embeddings with Large Language Models"
                },
                "summary": "Recent large language models (LLMs) have demonstrated exceptional performance\non general-purpose text embedding tasks. While dense embeddings have dominated\nrelated research, we introduce the first Lexicon-based EmbeddiNgS (LENS)\nleveraging LLMs that achieve competitive performance on these tasks. Regarding\nthe inherent tokenization redundancy issue and unidirectional attention\nlimitations in traditional causal LLMs, LENS consolidates the vocabulary space\nthrough token embedding clustering, and investigates bidirectional attention\nand various pooling strategies. Specifically, LENS simplifies lexicon matching\nby assigning each dimension to a specific token cluster, where semantically\nsimilar tokens are grouped together, and unlocking the full potential of LLMs\nthrough bidirectional attention. Extensive experiments demonstrate that LENS\noutperforms dense embeddings on the Massive Text Embedding Benchmark (MTEB),\ndelivering compact feature representations that match the sizes of dense\ncounterparts. Notably, combining LENSE with dense embeddings achieves\nstate-of-the-art performance on the retrieval subset of MTEB (i.e. BEIR).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) have demonstrated exceptional performance\non general-purpose text embedding tasks. While dense embeddings have dominated\nrelated research, we introduce the first Lexicon-based EmbeddiNgS (LENS)\nleveraging LLMs that achieve competitive performance on these tasks. Regarding\nthe inherent tokenization redundancy issue and unidirectional attention\nlimitations in traditional causal LLMs, LENS consolidates the vocabulary space\nthrough token embedding clustering, and investigates bidirectional attention\nand various pooling strategies. Specifically, LENS simplifies lexicon matching\nby assigning each dimension to a specific token cluster, where semantically\nsimilar tokens are grouped together, and unlocking the full potential of LLMs\nthrough bidirectional attention. Extensive experiments demonstrate that LENS\noutperforms dense embeddings on the Massive Text Embedding Benchmark (MTEB),\ndelivering compact feature representations that match the sizes of dense\ncounterparts. Notably, combining LENSE with dense embeddings achieves\nstate-of-the-art performance on the retrieval subset of MTEB (i.e. BEIR)."
                },
                "authors": [
                    {
                        "name": "Yibin Lei"
                    },
                    {
                        "name": "Tao Shen"
                    },
                    {
                        "name": "Yu Cao"
                    },
                    {
                        "name": "Andrew Yates"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Yates"
                },
                "author": "Andrew Yates",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.08965v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.08965v3",
                "updated": "2025-01-16T18:56:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    56,
                    27,
                    3,
                    16,
                    0
                ],
                "published": "2024-05-14T21:12:01Z",
                "published_parsed": [
                    2024,
                    5,
                    14,
                    21,
                    12,
                    1,
                    1,
                    135,
                    0
                ],
                "title": "Meaning-Typed Programming: Language-level Abstractions and Runtime for\n  GenAI Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meaning-Typed Programming: Language-level Abstractions and Runtime for\n  GenAI Applications"
                },
                "summary": "Software is rapidly evolving from being programmed with traditional logical\ncode, to neuro-integrated applications that leverage generative AI and large\nlanguage models (LLMs) for application functionality. This shift increases the\ncomplexity of building applications, as developers now must reasoning about,\nprogram, and prompt LLMs. Despite efforts to create tools to assist with prompt\nengineering, these solutions often introduce additional layers of complexity to\nthe development of neuro-integrated applications. This paper proposes\nmeaning-typed programming (MTP), a novel approach to simplify the creation of\nneuro-integrated applications by introducing new language-level abstractions\nthat hide the complexities of LLM integration. Our key insight is that typical\nconventional code already possesses a high level of semantic richness that can\nbe automatically reasoned about, as it is designed to be readable and\nmaintainable by humans. Leveraging this insight, we conceptualize LLMs as\nmeaning-typed code constructs and introduce a by abstraction at the language\nlevel, MT-IR, a new meaning-based intermediate representation at the compiler\nlevel, and MT Runtime, an automated run-time engine for LLM integration and\noperations. We implement MTP in a production-grade Python super-set language\ncalled Jac and perform an extensive evaluation. Our results demonstrate that\nMTP not only simplifies the development process but also meets or exceeds the\nefficacy of state-of-the-art manual and tool-assisted prompt engineering\ntechniques in terms of accuracy and usability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software is rapidly evolving from being programmed with traditional logical\ncode, to neuro-integrated applications that leverage generative AI and large\nlanguage models (LLMs) for application functionality. This shift increases the\ncomplexity of building applications, as developers now must reasoning about,\nprogram, and prompt LLMs. Despite efforts to create tools to assist with prompt\nengineering, these solutions often introduce additional layers of complexity to\nthe development of neuro-integrated applications. This paper proposes\nmeaning-typed programming (MTP), a novel approach to simplify the creation of\nneuro-integrated applications by introducing new language-level abstractions\nthat hide the complexities of LLM integration. Our key insight is that typical\nconventional code already possesses a high level of semantic richness that can\nbe automatically reasoned about, as it is designed to be readable and\nmaintainable by humans. Leveraging this insight, we conceptualize LLMs as\nmeaning-typed code constructs and introduce a by abstraction at the language\nlevel, MT-IR, a new meaning-based intermediate representation at the compiler\nlevel, and MT Runtime, an automated run-time engine for LLM integration and\noperations. We implement MTP in a production-grade Python super-set language\ncalled Jac and perform an extensive evaluation. Our results demonstrate that\nMTP not only simplifies the development process but also meets or exceeds the\nefficacy of state-of-the-art manual and tool-assisted prompt engineering\ntechniques in terms of accuracy and usability."
                },
                "authors": [
                    {
                        "name": "Jason Mars"
                    },
                    {
                        "name": "Yiping Kang"
                    },
                    {
                        "name": "Jayanaka L. Dantanarayana"
                    },
                    {
                        "name": "Kugesan Sivasothynathan"
                    },
                    {
                        "name": "Christopher Clarke"
                    },
                    {
                        "name": "Baichuan Li"
                    },
                    {
                        "name": "Krisztian Flautner"
                    },
                    {
                        "name": "Lingjia Tang"
                    }
                ],
                "author_detail": {
                    "name": "Lingjia Tang"
                },
                "author": "Lingjia Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.08965v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.08965v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09745v1",
                "updated": "2025-01-16T18:55:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    55,
                    38,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T18:55:38Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    55,
                    38,
                    3,
                    16,
                    0
                ],
                "title": "Suggesting Code Edits in Interactive Machine Learning Notebooks Using\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Suggesting Code Edits in Interactive Machine Learning Notebooks Using\n  Large Language Models"
                },
                "summary": "Machine learning developers frequently use interactive computational\nnotebooks, such as Jupyter notebooks, to host code for data processing and\nmodel training. Jupyter notebooks provide a convenient tool for writing machine\nlearning pipelines and interactively observing outputs, however, maintaining\nJupyter notebooks, e.g., to add new features or fix bugs, can be challenging\ndue to the length and complexity of the notebooks. Moreover, there is no\nexisting benchmark related to developer edits on Jupyter notebooks. To address\nthis, we present the first dataset of 48,398 Jupyter notebook edits derived\nfrom 20,095 revisions of 792 machine learning repositories on GitHub, and\nperform the first study of the using LLMs to predict code edits in Jupyter\nnotebooks. Our dataset captures granular details of cell-level and line-level\nmodifications, offering a foundation for understanding real-world maintenance\npatterns in machine learning workflows. We observed that the edits on Jupyter\nnotebooks are highly localized, with changes averaging only 166 lines of code\nin repositories. While larger models outperform smaller counterparts in code\nediting, all models have low accuracy on our dataset even after finetuning,\ndemonstrating the complexity of real-world machine learning maintenance tasks.\nOur findings emphasize the critical role of contextual information in improving\nmodel performance and point toward promising avenues for advancing large\nlanguage models' capabilities in engineering machine learning code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning developers frequently use interactive computational\nnotebooks, such as Jupyter notebooks, to host code for data processing and\nmodel training. Jupyter notebooks provide a convenient tool for writing machine\nlearning pipelines and interactively observing outputs, however, maintaining\nJupyter notebooks, e.g., to add new features or fix bugs, can be challenging\ndue to the length and complexity of the notebooks. Moreover, there is no\nexisting benchmark related to developer edits on Jupyter notebooks. To address\nthis, we present the first dataset of 48,398 Jupyter notebook edits derived\nfrom 20,095 revisions of 792 machine learning repositories on GitHub, and\nperform the first study of the using LLMs to predict code edits in Jupyter\nnotebooks. Our dataset captures granular details of cell-level and line-level\nmodifications, offering a foundation for understanding real-world maintenance\npatterns in machine learning workflows. We observed that the edits on Jupyter\nnotebooks are highly localized, with changes averaging only 166 lines of code\nin repositories. While larger models outperform smaller counterparts in code\nediting, all models have low accuracy on our dataset even after finetuning,\ndemonstrating the complexity of real-world machine learning maintenance tasks.\nOur findings emphasize the critical role of contextual information in improving\nmodel performance and point toward promising avenues for advancing large\nlanguage models' capabilities in engineering machine learning code."
                },
                "authors": [
                    {
                        "name": "Bihui Jin"
                    },
                    {
                        "name": "Jiayue Wang"
                    },
                    {
                        "name": "Pengyu Nie"
                    }
                ],
                "author_detail": {
                    "name": "Pengyu Nie"
                },
                "author": "Pengyu Nie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01945v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01945v2",
                "updated": "2025-01-16T18:53:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    53,
                    23,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-03T18:51:18Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    18,
                    51,
                    18,
                    4,
                    3,
                    0
                ],
                "title": "Cold-Start Recommendation towards the Era of Large Language Models\n  (LLMs): A Comprehensive Survey and Roadmap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold-Start Recommendation towards the Era of Large Language Models\n  (LLMs): A Comprehensive Survey and Roadmap"
                },
                "summary": "Cold-start problem is one of the long-standing challenges in recommender\nsystems, focusing on accurately modeling new or interaction-limited users or\nitems to provide better recommendations. Due to the diversification of internet\nplatforms and the exponential growth of users and items, the importance of\ncold-start recommendation (CSR) is becoming increasingly evident. At the same\ntime, large language models (LLMs) have achieved tremendous success and possess\nstrong capabilities in modeling user and item information, providing new\npotential for cold-start recommendations. However, the research community on\nCSR still lacks a comprehensive review and reflection in this field. Based on\nthis, in this paper, we stand in the context of the era of large language\nmodels and provide a comprehensive review and discussion on the roadmap,\nrelated literature, and future directions of CSR. Specifically, we have\nconducted an exploration of the development path of how existing CSR utilizes\ninformation, from content features, graph relations, and domain information, to\nthe world knowledge possessed by large language models, aiming to provide new\ninsights for both the research and industrial communities on CSR. Related\nresources of cold-start recommendations are collected and continuously updated\nfor the community in\nhttps://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold-start problem is one of the long-standing challenges in recommender\nsystems, focusing on accurately modeling new or interaction-limited users or\nitems to provide better recommendations. Due to the diversification of internet\nplatforms and the exponential growth of users and items, the importance of\ncold-start recommendation (CSR) is becoming increasingly evident. At the same\ntime, large language models (LLMs) have achieved tremendous success and possess\nstrong capabilities in modeling user and item information, providing new\npotential for cold-start recommendations. However, the research community on\nCSR still lacks a comprehensive review and reflection in this field. Based on\nthis, in this paper, we stand in the context of the era of large language\nmodels and provide a comprehensive review and discussion on the roadmap,\nrelated literature, and future directions of CSR. Specifically, we have\nconducted an exploration of the development path of how existing CSR utilizes\ninformation, from content features, graph relations, and domain information, to\nthe world knowledge possessed by large language models, aiming to provide new\ninsights for both the research and industrial communities on CSR. Related\nresources of cold-start recommendations are collected and continuously updated\nfor the community in\nhttps://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation."
                },
                "authors": [
                    {
                        "name": "Weizhi Zhang"
                    },
                    {
                        "name": "Yuanchen Bei"
                    },
                    {
                        "name": "Liangwei Yang"
                    },
                    {
                        "name": "Henry Peng Zou"
                    },
                    {
                        "name": "Peilin Zhou"
                    },
                    {
                        "name": "Aiwei Liu"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Jianling Wang"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Feiran Huang"
                    },
                    {
                        "name": "Sheng Zhou"
                    },
                    {
                        "name": "Jiajun Bu"
                    },
                    {
                        "name": "Allen Lin"
                    },
                    {
                        "name": "James Caverlee"
                    },
                    {
                        "name": "Fakhri Karray"
                    },
                    {
                        "name": "Irwin King"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01945v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01945v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09732v1",
                "updated": "2025-01-16T18:30:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    30,
                    37,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T18:30:37Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    30,
                    37,
                    3,
                    16,
                    0
                ],
                "title": "Inference-Time Scaling for Diffusion Models beyond Scaling Denoising\n  Steps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Scaling for Diffusion Models beyond Scaling Denoising\n  Steps"
                },
                "summary": "Generative models have made significant impacts across various domains,\nlargely due to their ability to scale during training by increasing data,\ncomputational resources, and model size, a phenomenon characterized by the\nscaling laws. Recent research has begun to explore inference-time scaling\nbehavior in Large Language Models (LLMs), revealing how performance can further\nimprove with additional computation during inference. Unlike LLMs, diffusion\nmodels inherently possess the flexibility to adjust inference-time computation\nvia the number of denoising steps, although the performance gains typically\nflatten after a few dozen. In this work, we explore the inference-time scaling\nbehavior of diffusion models beyond increasing denoising steps and investigate\nhow the generation performance can further improve with increased computation.\nSpecifically, we consider a search problem aimed at identifying better noises\nfor the diffusion sampling process. We structure the design space along two\naxes: the verifiers used to provide feedback, and the algorithms used to find\nbetter noise candidates. Through extensive experiments on class-conditioned and\ntext-conditioned image generation benchmarks, our findings reveal that\nincreasing inference-time compute leads to substantial improvements in the\nquality of samples generated by diffusion models, and with the complicated\nnature of images, combinations of the components in the framework can be\nspecifically chosen to conform with different application scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models have made significant impacts across various domains,\nlargely due to their ability to scale during training by increasing data,\ncomputational resources, and model size, a phenomenon characterized by the\nscaling laws. Recent research has begun to explore inference-time scaling\nbehavior in Large Language Models (LLMs), revealing how performance can further\nimprove with additional computation during inference. Unlike LLMs, diffusion\nmodels inherently possess the flexibility to adjust inference-time computation\nvia the number of denoising steps, although the performance gains typically\nflatten after a few dozen. In this work, we explore the inference-time scaling\nbehavior of diffusion models beyond increasing denoising steps and investigate\nhow the generation performance can further improve with increased computation.\nSpecifically, we consider a search problem aimed at identifying better noises\nfor the diffusion sampling process. We structure the design space along two\naxes: the verifiers used to provide feedback, and the algorithms used to find\nbetter noise candidates. Through extensive experiments on class-conditioned and\ntext-conditioned image generation benchmarks, our findings reveal that\nincreasing inference-time compute leads to substantial improvements in the\nquality of samples generated by diffusion models, and with the complicated\nnature of images, combinations of the components in the framework can be\nspecifically chosen to conform with different application scenario."
                },
                "authors": [
                    {
                        "name": "Nanye Ma"
                    },
                    {
                        "name": "Shangyuan Tong"
                    },
                    {
                        "name": "Haolin Jia"
                    },
                    {
                        "name": "Hexiang Hu"
                    },
                    {
                        "name": "Yu-Chuan Su"
                    },
                    {
                        "name": "Mingda Zhang"
                    },
                    {
                        "name": "Xuan Yang"
                    },
                    {
                        "name": "Yandong Li"
                    },
                    {
                        "name": "Tommi Jaakkola"
                    },
                    {
                        "name": "Xuhui Jia"
                    },
                    {
                        "name": "Saining Xie"
                    }
                ],
                "author_detail": {
                    "name": "Saining Xie"
                },
                "author": "Saining Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09716v1",
                "updated": "2025-01-16T18:05:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    5,
                    28,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T18:05:28Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    5,
                    28,
                    3,
                    16,
                    0
                ],
                "title": "Intelligent OLSR Routing Protocol Optimization for VANETs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent OLSR Routing Protocol Optimization for VANETs"
                },
                "summary": "Recent advances in wireless technologies have given rise to the emergence of\nvehicular ad hoc networks (VANETs). In such networks, the limited coverage of\nWiFi and the high mobility of the nodes generate frequent topology changes and\nnetwork fragmentations. For these reasons, and taking into account that there\nis no central manager entity, routing packets through the network is a\nchallenging task. Therefore, offering an efficient routing strategy is crucial\nto the deployment of VANETs. This paper deals with the optimal parameter\nsetting of the optimized link state routing (OLSR), which is a well-known\nmobile ad hoc network routing protocol, by defining an optimization problem.\nThis way, a series of representative metaheuristic algorithms (particle swarm\noptimization, differential evolution, genetic algorithm, and simulated\nannealing) are studied in this paper to find automatically optimal\nconfigurations of this routing protocol. In addition, a set of realistic VANET\nscenarios (based in the city of M\\'alaga) have been defined to accurately\nevaluate the performance of the network under our automatic OLSR. In the\nexperiments, our tuned OLSR configurations result in better quality of service\n(QoS) than the standard request for comments (RFC 3626), as well as several\nhuman experts, making it amenable for utilization in VANET configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in wireless technologies have given rise to the emergence of\nvehicular ad hoc networks (VANETs). In such networks, the limited coverage of\nWiFi and the high mobility of the nodes generate frequent topology changes and\nnetwork fragmentations. For these reasons, and taking into account that there\nis no central manager entity, routing packets through the network is a\nchallenging task. Therefore, offering an efficient routing strategy is crucial\nto the deployment of VANETs. This paper deals with the optimal parameter\nsetting of the optimized link state routing (OLSR), which is a well-known\nmobile ad hoc network routing protocol, by defining an optimization problem.\nThis way, a series of representative metaheuristic algorithms (particle swarm\noptimization, differential evolution, genetic algorithm, and simulated\nannealing) are studied in this paper to find automatically optimal\nconfigurations of this routing protocol. In addition, a set of realistic VANET\nscenarios (based in the city of M\\'alaga) have been defined to accurately\nevaluate the performance of the network under our automatic OLSR. In the\nexperiments, our tuned OLSR configurations result in better quality of service\n(QoS) than the standard request for comments (RFC 3626), as well as several\nhuman experts, making it amenable for utilization in VANET configurations."
                },
                "authors": [
                    {
                        "name": "Jamal Toutouh"
                    },
                    {
                        "name": "Jos Garca-Nieto"
                    },
                    {
                        "name": "Enrique Alba"
                    }
                ],
                "author_detail": {
                    "name": "Enrique Alba"
                },
                "author": "Enrique Alba",
                "arxiv_doi": "10.1109/TVT.2012.2188552",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVT.2012.2188552",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.09716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Transactions on Vehicular Technology, vol. 61, no. 4, pp.\n  1884-1894, May 2012",
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09709v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09709v1",
                "updated": "2025-01-16T18:00:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    0,
                    6,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T18:00:06Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    0,
                    6,
                    3,
                    16,
                    0
                ],
                "title": "CyberMentor: AI Powered Learning Tool Platform to Address Diverse\n  Student Needs in Cybersecurity Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CyberMentor: AI Powered Learning Tool Platform to Address Diverse\n  Student Needs in Cybersecurity Education"
                },
                "summary": "Many non-traditional students in cybersecurity programs often lack access to\nadvice from peers, family members and professors, which can hinder their\neducational experiences. Additionally, these students may not fully benefit\nfrom various LLM-powered AI assistants due to issues like content relevance,\nlocality of advice, minimum expertise, and timing. This paper addresses these\nchallenges by introducing an application designed to provide comprehensive\nsupport by answering questions related to knowledge, skills, and career\npreparation advice tailored to the needs of these students. We developed a\nlearning tool platform, CyberMentor, to address the diverse needs and pain\npoints of students majoring in cybersecurity. Powered by agentic workflow and\nGenerative Large Language Models (LLMs), the platform leverages\nRetrieval-Augmented Generation (RAG) for accurate and contextually relevant\ninformation retrieval to achieve accessibility and personalization. We\ndemonstrated its value in addressing knowledge requirements for cybersecurity\neducation and for career marketability, in tackling skill requirements for\nanalytical and programming assignments, and in delivering real time on demand\nlearning support. Using three use scenarios, we showcased CyberMentor in\nfacilitating knowledge acquisition and career preparation and providing\nseamless skill-based guidance and support. We also employed the LangChain\nprompt-based evaluation methodology to evaluate the platform's impact,\nconfirming its strong performance in helpfulness, correctness, and\ncompleteness. These results underscore the system's ability to support students\nin developing practical cybersecurity skills while improving equity and\nsustainability within higher education. Furthermore, CyberMentor's open-source\ndesign allows for adaptation across other disciplines, fostering educational\ninnovation and broadening its potential impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many non-traditional students in cybersecurity programs often lack access to\nadvice from peers, family members and professors, which can hinder their\neducational experiences. Additionally, these students may not fully benefit\nfrom various LLM-powered AI assistants due to issues like content relevance,\nlocality of advice, minimum expertise, and timing. This paper addresses these\nchallenges by introducing an application designed to provide comprehensive\nsupport by answering questions related to knowledge, skills, and career\npreparation advice tailored to the needs of these students. We developed a\nlearning tool platform, CyberMentor, to address the diverse needs and pain\npoints of students majoring in cybersecurity. Powered by agentic workflow and\nGenerative Large Language Models (LLMs), the platform leverages\nRetrieval-Augmented Generation (RAG) for accurate and contextually relevant\ninformation retrieval to achieve accessibility and personalization. We\ndemonstrated its value in addressing knowledge requirements for cybersecurity\neducation and for career marketability, in tackling skill requirements for\nanalytical and programming assignments, and in delivering real time on demand\nlearning support. Using three use scenarios, we showcased CyberMentor in\nfacilitating knowledge acquisition and career preparation and providing\nseamless skill-based guidance and support. We also employed the LangChain\nprompt-based evaluation methodology to evaluate the platform's impact,\nconfirming its strong performance in helpfulness, correctness, and\ncompleteness. These results underscore the system's ability to support students\nin developing practical cybersecurity skills while improving equity and\nsustainability within higher education. Furthermore, CyberMentor's open-source\ndesign allows for adaptation across other disciplines, fostering educational\ninnovation and broadening its potential impact."
                },
                "authors": [
                    {
                        "name": "Tianyu Wang"
                    },
                    {
                        "name": "Nianjun Zhou"
                    },
                    {
                        "name": "Zhixiong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhixiong Chen"
                },
                "author": "Zhixiong Chen",
                "arxiv_comment": "11 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09709v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09709v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.3.2; I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09706v1",
                "updated": "2025-01-16T17:58:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    17,
                    58,
                    32,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T17:58:32Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    17,
                    58,
                    32,
                    3,
                    16,
                    0
                ],
                "title": "Domain Adaptation of Foundation LLMs for e-Commerce",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain Adaptation of Foundation LLMs for e-Commerce"
                },
                "summary": "We present the e-Llama models: 8 billion and 70 billion parameter large\nlanguage models that are adapted towards the e-commerce domain. These models\nare meant as foundation models with deep knowledge about e-commerce, that form\na base for instruction- and fine-tuning. The e-Llama models are obtained by\ncontinuously pretraining the Llama 3.1 base models on 1 trillion tokens of\ndomain-specific data.\n  We discuss our approach and motivate our choice of hyperparameters with a\nseries of ablation studies. To quantify how well the models have been adapted\nto the e-commerce domain, we define and implement a set of multilingual,\ne-commerce specific evaluation tasks.\n  We show that, when carefully choosing the training setup, the Llama 3.1\nmodels can be adapted towards the new domain without sacrificing significant\nperformance on general domain tasks. We also explore the possibility of merging\nthe adapted model and the base model for a better control of the performance\ntrade-off between domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the e-Llama models: 8 billion and 70 billion parameter large\nlanguage models that are adapted towards the e-commerce domain. These models\nare meant as foundation models with deep knowledge about e-commerce, that form\na base for instruction- and fine-tuning. The e-Llama models are obtained by\ncontinuously pretraining the Llama 3.1 base models on 1 trillion tokens of\ndomain-specific data.\n  We discuss our approach and motivate our choice of hyperparameters with a\nseries of ablation studies. To quantify how well the models have been adapted\nto the e-commerce domain, we define and implement a set of multilingual,\ne-commerce specific evaluation tasks.\n  We show that, when carefully choosing the training setup, the Llama 3.1\nmodels can be adapted towards the new domain without sacrificing significant\nperformance on general domain tasks. We also explore the possibility of merging\nthe adapted model and the base model for a better control of the performance\ntrade-off between domains."
                },
                "authors": [
                    {
                        "name": "Christian Herold"
                    },
                    {
                        "name": "Michael Kozielski"
                    },
                    {
                        "name": "Tala Bazazo"
                    },
                    {
                        "name": "Pavel Petrushkov"
                    },
                    {
                        "name": "Hadi Hashemi"
                    },
                    {
                        "name": "Patrycja Cieplicka"
                    },
                    {
                        "name": "Dominika Basaj"
                    },
                    {
                        "name": "Shahram Khadivi"
                    }
                ],
                "author_detail": {
                    "name": "Shahram Khadivi"
                },
                "author": "Shahram Khadivi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]